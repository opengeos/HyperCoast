{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Part 1. Import the necessary libraries and define VAE model and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import hypercoast\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from scipy.interpolate import griddata\n",
    "from datetime import datetime\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder_layer(x)\n",
    "        mu = self.fc1(x)\n",
    "        log_var = self.fc2(x)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        return x_reconstructed, mu, log_var\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    L1 = F.l1_loss(recon_x, x, reduction=\"mean\")\n",
    "    BCE = F.mse_loss(recon_x, x, reduction=\"mean\")\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return L1\n",
    "\n",
    "\n",
    "def load_real_data(excel_path, selected_bands, split_ratio=0.7):\n",
    "\n",
    "    rounded_bands = [int(round(b)) for b in selected_bands]\n",
    "    band_cols = [f\"Rrs_{b}\" for b in rounded_bands]\n",
    "\n",
    "    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n",
    "    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n",
    "\n",
    "    df_rrs_selected = df_rrs[[\"GLORIA_ID\"] + band_cols]\n",
    "    df_param_selected = df_param[[\"GLORIA_ID\", \"chl-a\"]]\n",
    "    df_merged = pd.merge(\n",
    "        df_rrs_selected, df_param_selected, on=\"GLORIA_ID\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    mask_rrs_valid = df_merged[band_cols].notna().all(axis=1)\n",
    "    mask_tss_valid = df_merged[\"chl-a\"].notna()\n",
    "    df_filtered = df_merged[mask_rrs_valid & mask_tss_valid].reset_index(drop=True)\n",
    "\n",
    "    lower = df_filtered[\"chl-a\"].quantile(0)\n",
    "    top = df_filtered[\"chl-a\"].quantile(1)\n",
    "    df_filtered = df_filtered[\n",
    "        (df_filtered[\"chl-a\"] >= lower) & (df_filtered[\"chl-a\"] <= top)\n",
    "    ].reset_index(drop=True)\n",
    "    all_sample_ids = df_filtered[\"GLORIA_ID\"].astype(str).tolist()\n",
    "\n",
    "    Rrs_array = df_filtered[band_cols].values\n",
    "    Chl_array = df_filtered[[\"chl-a\"]].values\n",
    "\n",
    "    scalers_Rrs = [\n",
    "        MinMaxScaler(feature_range=(1, 10)) for _ in range(Rrs_array.shape[0])\n",
    "    ]\n",
    "    Rrs_normalized = np.array(\n",
    "        [\n",
    "            scalers_Rrs[i].fit_transform(row.reshape(-1, 1)).flatten()\n",
    "            for i, row in enumerate(Rrs_array)\n",
    "        ]\n",
    "    )\n",
    "    Chl_normalized = np.log10(Chl_array + 1)\n",
    "\n",
    "    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n",
    "    Chl_tensor = torch.tensor(Chl_normalized, dtype=torch.float32)\n",
    "    dataset = TensorDataset(Rrs_tensor, Chl_tensor)\n",
    "\n",
    "    num_samples = len(dataset)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_size = int(split_ratio * num_samples)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    train_ids = [all_sample_ids[i] for i in train_indices]\n",
    "    test_ids = [all_sample_ids[i] for i in test_indices]\n",
    "\n",
    "    train_dl = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)\n",
    "\n",
    "    input_dim = Rrs_tensor.shape[1]\n",
    "    output_dim = Chl_tensor.shape[1]\n",
    "\n",
    "    return train_dl, test_dl, input_dim, output_dim, train_ids, test_ids\n",
    "\n",
    "\n",
    "def load_real_test(excel_path, selected_bands, max_allowed_diff=1.0):\n",
    "\n",
    "    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n",
    "    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n",
    "\n",
    "    sample_ids = df_rrs[\"Site Label\"].astype(str).tolist()\n",
    "    sample_dates = df_rrs[\"Date\"].astype(str).tolist()\n",
    "\n",
    "    rrs_wavelengths = []\n",
    "    rrs_cols = []\n",
    "    for col in df_rrs.columns:\n",
    "        try:\n",
    "            wl = float(col)\n",
    "            rrs_wavelengths.append(wl)\n",
    "            rrs_cols.append(col)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    band_cols = []\n",
    "    matched_bands = []\n",
    "    for target_band in selected_bands:\n",
    "        diffs = [abs(wl - target_band) for wl in rrs_wavelengths]\n",
    "        min_diff = min(diffs)\n",
    "        best_idx = diffs.index(min_diff)\n",
    "        band_cols.append(rrs_cols[best_idx])\n",
    "        matched_bands.append(rrs_wavelengths[best_idx])\n",
    "    Rrs_array = df_rrs[band_cols].values\n",
    "    Chl_array = df_param[[\"Chl-a\"]].values\n",
    "\n",
    "    scalers_Rrs = [\n",
    "        MinMaxScaler(feature_range=(1, 10)) for _ in range(Rrs_array.shape[0])\n",
    "    ]\n",
    "    Rrs_normalized = np.array(\n",
    "        [\n",
    "            scalers_Rrs[i].fit_transform(row.reshape(-1, 1)).flatten()\n",
    "            for i, row in enumerate(Rrs_array)\n",
    "        ]\n",
    "    )\n",
    "    Chl_normalized = np.log10(Chl_array + 1)\n",
    "    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n",
    "    Chl_tensor = torch.tensor(Chl_normalized, dtype=torch.float32)\n",
    "    dataset = TensorDataset(Rrs_tensor, Chl_tensor)\n",
    "\n",
    "    dataset = TensorDataset(Rrs_tensor, Chl_tensor)\n",
    "    test_dl = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0)\n",
    "\n",
    "    input_dim = Rrs_tensor.shape[1]\n",
    "    output_dim = Chl_tensor.shape[1]\n",
    "\n",
    "    return test_dl, input_dim, output_dim, sample_ids, sample_dates\n",
    "\n",
    "\n",
    "def calculate_metrics(predictions, actuals, threshold=100):\n",
    "    \"\"\"\n",
    "    Calculate epsilon, beta and additional metrics (RMSE, RMSLE, MAPE, Bias, MAE).\n",
    "\n",
    "    :param predictions: array-like, predicted values\n",
    "    :param actuals: array-like, actual values\n",
    "    :param threshold: float, relative error threshold\n",
    "    :return: epsilon, beta, rmse, rmsle, mape, bias, mae\n",
    "    \"\"\"\n",
    "    # Apply the threshold to filter out predictions with large relative error\n",
    "    # mask = np.abs(predictions - actuals) / np.abs(actuals+1e-10) < threshold\n",
    "    # filtered_predictions = predictions[mask]\n",
    "    # filtered_actuals = actuals[mask]\n",
    "    # predictions = np.where(predictions <= 1e-10, 1e-10, predictions)\n",
    "    # actuals = np.where(actuals <= 1e-10, 1e-10, actuals)\n",
    "    filtered_predictions = predictions\n",
    "    filtered_actuals = actuals\n",
    "\n",
    "    # Calculate epsilon and beta\n",
    "    log_ratios = np.log10(filtered_predictions / filtered_actuals)\n",
    "    Y = np.median(np.abs(log_ratios))\n",
    "    Z = np.median(log_ratios)\n",
    "    epsilon = 100 * (10**Y - 1)\n",
    "    beta = 50 * np.sign(Z) * (10 ** np.abs(Z) - 1)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    rmse = np.sqrt(np.mean((filtered_predictions - filtered_actuals) ** 2))\n",
    "    rmsle = np.sqrt(\n",
    "        np.mean(\n",
    "            (np.log10(filtered_predictions + 1) - np.log10(filtered_actuals + 1)) ** 2\n",
    "        )\n",
    "    )\n",
    "    mape = 50 * np.median(\n",
    "        np.abs((filtered_predictions - filtered_actuals) / filtered_actuals)\n",
    "    )\n",
    "    bias = 10 ** (np.mean(np.log10(filtered_predictions) - np.log10(filtered_actuals)))\n",
    "    mae = 10 ** np.mean(\n",
    "        np.abs(np.log10(filtered_predictions) - np.log10(filtered_actuals))\n",
    "    )\n",
    "\n",
    "    return epsilon, beta, rmse, rmsle, mape, bias, mae\n",
    "\n",
    "\n",
    "def plot_results(\n",
    "    predictions_rescaled, actuals_rescaled, save_dir, threshold=100, mode=\"test\"\n",
    "):\n",
    "\n",
    "    actuals = actuals_rescaled.flatten()\n",
    "    predictions = predictions_rescaled.flatten()\n",
    "\n",
    "    log_actuals = np.log10(actuals)\n",
    "    log_predictions = np.log10(predictions)\n",
    "\n",
    "    # mask = np.abs(predictions - actuals) / np.abs(actuals+1e-10) < threshold\n",
    "    mask = np.abs(log_predictions - log_actuals) < threshold\n",
    "    filtered_predictions = predictions[mask]\n",
    "    filtered_actuals = actuals[mask]\n",
    "\n",
    "    log_actual = np.log10(np.where(actuals == 0, 1e-10, actuals))\n",
    "    log_prediction = np.log10(np.where(predictions == 0, 1e-10, predictions))\n",
    "\n",
    "    filtered_log_actual = np.log10(\n",
    "        np.where(filtered_actuals == 0, 1e-10, filtered_actuals)\n",
    "    )\n",
    "    filtered_log_prediction = np.log10(\n",
    "        np.where(filtered_predictions == 0, 1e-10, filtered_predictions)\n",
    "    )\n",
    "\n",
    "    epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(\n",
    "        filtered_predictions, filtered_actuals, threshold\n",
    "    )\n",
    "\n",
    "    valid_mask = np.isfinite(filtered_log_actual) & np.isfinite(filtered_log_prediction)\n",
    "    slope, intercept = np.polyfit(\n",
    "        filtered_log_actual[valid_mask], filtered_log_prediction[valid_mask], 1\n",
    "    )\n",
    "    x = np.array([-2, 4])\n",
    "    y = slope * x + intercept\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    plt.plot(x, y, linestyle=\"--\", color=\"blue\", linewidth=0.8)\n",
    "    lims = [-2, 4]\n",
    "    plt.plot(lims, lims, linestyle=\"-\", color=\"black\", linewidth=0.8)\n",
    "\n",
    "    sns.scatterplot(x=log_actual, y=log_prediction, alpha=0.5)\n",
    "\n",
    "    sns.kdeplot(\n",
    "        x=filtered_log_actual,\n",
    "        y=filtered_log_prediction,\n",
    "        levels=3,\n",
    "        color=\"black\",\n",
    "        fill=False,\n",
    "        linewidths=0.8,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Actual Chl-a Values\", fontsize=16, fontname=\"Ubuntu\")\n",
    "    plt.ylabel(\"Predicted Chl-a Values\", fontsize=16, fontname=\"Ubuntu\")\n",
    "    plt.xlim(-2, 4)\n",
    "    plt.ylim(-2, 4)\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "    plt.legend(\n",
    "        title=(\n",
    "            f\"MAE = {mae:.2f}, RMSE = {rmse:.2f}, RMSLE = {rmsle:.2f} \\n\"\n",
    "            f\"Bias = {bias:.2f}, Slope = {slope:.2f} \\n\"\n",
    "            f\"MAPE = {mape:.2f}%, ε = {epsilon:.2f}%, β = {beta:.2f}%\"\n",
    "        ),\n",
    "        fontsize=16,\n",
    "        title_fontsize=12,\n",
    "        prop={\"family\": \"Ubuntu\"},\n",
    "    )\n",
    "\n",
    "    plt.xticks(fontsize=20, fontname=\"Ubuntu\")\n",
    "    plt.yticks(fontsize=20, fontname=\"Ubuntu\")\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir, f\"{mode}_plot.pdf\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_results_to_excel(ids, actuals, predictions, file_path, dates=None):\n",
    "\n",
    "    if dates is not None:\n",
    "        df = pd.DataFrame(\n",
    "            {\"ID\": ids, \"Date\": dates, \"Actual\": actuals, \"Predicted\": predictions}\n",
    "        )\n",
    "    else:\n",
    "        df = pd.DataFrame({\"ID\": ids, \"Actual\": actuals, \"Predicted\": predictions})\n",
    "\n",
    "    df.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Part 2. In this section, we used the VAE deep learning algorithm to train a MSI-Chla model and evaluated its performance on both the training dataset and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, device, epochs=200, optimizer=None, save_dir=None):\n",
    "    model.train()\n",
    "\n",
    "    min_total_loss = float(\"inf\")\n",
    "    best_model_path = os.path.join(save_dir, \"best_model_minloss.pth\")\n",
    "\n",
    "    total_list = []\n",
    "    l1_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_epoch = 0.0\n",
    "        l1_epoch = 0.0\n",
    "\n",
    "        for x, y in train_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            y_pred, mu, log_var = model(x)\n",
    "            loss = loss_function(y_pred, y, mu, log_var)\n",
    "            l1 = loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss_epoch += loss.item()\n",
    "            l1_epoch += l1.item()\n",
    "\n",
    "        avg_total_loss = total_loss_epoch / len(train_dl)\n",
    "        avg_l1 = l1_epoch / len(train_dl)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Total: {avg_total_loss:.4f} | L1: {avg_l1:.4f}\")\n",
    "\n",
    "        total_list.append(avg_total_loss)\n",
    "        l1_list.append(avg_l1)\n",
    "\n",
    "        if avg_total_loss < min_total_loss:\n",
    "            min_total_loss = avg_total_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    return {\"total_loss\": total_list, \"l1_loss\": l1_list, \"best_loss\": min_total_loss}\n",
    "\n",
    "\n",
    "def evaluate(model, test_dl, device):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred, _, _ = model(x)\n",
    "            predictions.append(y_pred.cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "\n",
    "    predictions = np.vstack(predictions).flatten()\n",
    "    actuals = np.vstack(actuals).flatten()\n",
    "    predictions_inverse = np.power(10, predictions) - 1\n",
    "    actuals_inverse = np.power(10, actuals) - 1\n",
    "    return predictions_inverse, actuals_inverse\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    selected_bands = [443, 492, 560, 665, 704, 740]\n",
    "\n",
    "    train_real_dl, test_real_dl, input_dim, output_dim, train_ids, test_ids = (\n",
    "        load_real_data(\n",
    "            \"/home/data/20250727/data/Gloria_updated_07242025.xlsx\", selected_bands\n",
    "        )\n",
    "    )\n",
    "    test_dl1, _, _, test_ids1, test_dates = load_real_test(\n",
    "        \"/home/data/20250727/data/GreatLake_all_data.xlsx\", selected_bands\n",
    "    )\n",
    "    test_dl2, _, _, test_ids2, test_dates2 = load_real_test(\n",
    "        \"/home/data/20250727/data/GOA_insitu_data_07242025updated.xlsx\", selected_bands\n",
    "    )\n",
    "\n",
    "    save_dir = \"/home/data/20250727/vae/MSI01\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    model = VAE(input_dim, output_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "\n",
    "    train_log = train(\n",
    "        model=model,\n",
    "        train_dl=train_real_dl,\n",
    "        device=device,\n",
    "        epochs=400,\n",
    "        optimizer=optimizer,\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    best_train_loss = train_log[\"best_loss\"]\n",
    "\n",
    "    predictions, actuals = evaluate(model, test_real_dl, device)\n",
    "    epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(\n",
    "        predictions, actuals\n",
    "    )\n",
    "    test_loss = rmse\n",
    "\n",
    "    predictions_GL, actuals_GL = evaluate(model, test_dl1, device)\n",
    "    epsilon_GL, beta_GL, rmse_GL, rmsle_GL, mape_GL, bias_GL, mae_GL = (\n",
    "        calculate_metrics(predictions_GL, actuals_GL)\n",
    "    )\n",
    "    predictions_Field_work, actuals_Field_work = evaluate(model, test_dl2, device)\n",
    "\n",
    "    save_results_to_excel(\n",
    "        test_ids, actuals, predictions, os.path.join(save_dir, \"test.xlsx\")\n",
    "    )\n",
    "    save_results_to_excel(\n",
    "        test_ids1,\n",
    "        actuals_GL,\n",
    "        predictions_GL,\n",
    "        os.path.join(save_dir, \"GL.xlsx\"),\n",
    "        dates=test_dates,\n",
    "    )\n",
    "    save_results_to_excel(\n",
    "        test_ids2,\n",
    "        actuals_Field_work,\n",
    "        predictions_Field_work,\n",
    "        os.path.join(save_dir, \"GOA.xlsx\"),\n",
    "        dates=test_dates2,\n",
    "    )\n",
    "\n",
    "    train_Rrs, train_Chl = [], []\n",
    "    for x, y in train_real_dl:\n",
    "        train_Rrs.append(x.cpu().numpy())\n",
    "        train_Chl.append(y.cpu().numpy())\n",
    "    train_data = np.hstack((np.vstack(train_Rrs), np.vstack(train_Chl)))\n",
    "\n",
    "    test_Rrs, test_Chl = [], []\n",
    "    for x, y in test_real_dl:\n",
    "        test_Rrs.append(x.cpu().numpy())\n",
    "        test_Chl.append(y.cpu().numpy())\n",
    "    test_data = np.hstack((np.vstack(test_Rrs), np.vstack(test_Chl)))\n",
    "\n",
    "    np.savetxt(\n",
    "        os.path.join(save_dir, \"train_data.csv\"),\n",
    "        train_data,\n",
    "        delimiter=\",\",\n",
    "        header=\",\".join([f\"Rrs_{i}\" for i in range(input_dim)] + [\"Chla\"]),\n",
    "        comments=\"\",\n",
    "    )\n",
    "    np.savetxt(\n",
    "        os.path.join(save_dir, \"test_data.csv\"),\n",
    "        test_data,\n",
    "        delimiter=\",\",\n",
    "        header=\",\".join([f\"Rrs_{i}\" for i in range(input_dim)] + [\"Chla\"]),\n",
    "        comments=\"\",\n",
    "    )\n",
    "\n",
    "    plot_results(predictions, actuals, save_dir, mode=\"test\")\n",
    "    plot_results(predictions_GL, actuals_GL, save_dir, mode=\"GL\")\n",
    "    plot_results(predictions_Field_work, actuals_Field_work, save_dir, mode=\"GOA\")\n",
    "\n",
    "    print(\n",
    "        f\"Run completed with train loss: {best_train_loss:.4f}, test loss: {test_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Part 3. We need to download the MSI Level-1 data, process it with ACOLITE to generate Level-2 water-leaving reflectance (L2W) products, and then perform model inference to generate chlorophyll-a (Chl-a) concentration maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/data/Chl-MSI/Harvey/Second/processed data/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.nc\"\n",
    "output_path = (\n",
    "    \"/home/data/20250727/vae/MSI/0-100/run_7/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.npy\"\n",
    ")\n",
    "model_path = \"/home/data/20250727/vae/MSI/0-100/run_7/best_model_minloss.pth\"\n",
    "\n",
    "output_dim = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = nc.Dataset(input_path)\n",
    "latitude = dataset.variables[\"lat\"][:]\n",
    "longitude = dataset.variables[\"lon\"][:]\n",
    "\n",
    "all_vars = dataset.variables.keys()\n",
    "bands_to_extract = [\n",
    "    f\"Rrs_{wavelength}\"\n",
    "    for wavelength in range(399, 755)\n",
    "    if f\"Rrs_{wavelength}\" in all_vars\n",
    "]\n",
    "input_dim = len(bands_to_extract)\n",
    "if input_dim == 0:\n",
    "    raise ValueError(\"❌ No valid bands found.\")\n",
    "\n",
    "model = VAE(input_dim, output_dim).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "filtered_Rrs = np.array(\n",
    "    [dataset.variables[band][:] for band in bands_to_extract]\n",
    ")  # shape: (bands, H, W)\n",
    "filtered_Rrs = np.moveaxis(filtered_Rrs, 0, -1)  # shape: (H, W, bands)\n",
    "\n",
    "mask = np.all(~np.isnan(filtered_Rrs), axis=2)\n",
    "\n",
    "\n",
    "def find_closest_band(target, available_bands):\n",
    "    available_waves = [int(b.split(\"_\")[1]) for b in available_bands]\n",
    "    closest_wave = min(available_waves, key=lambda w: abs(w - target))\n",
    "    return f\"Rrs_{closest_wave}\"\n",
    "\n",
    "\n",
    "target_443 = (\n",
    "    \"Rrs_443\"\n",
    "    if \"Rrs_443\" in bands_to_extract\n",
    "    else find_closest_band(443, bands_to_extract)\n",
    ")\n",
    "target_560 = (\n",
    "    \"Rrs_560\"\n",
    "    if \"Rrs_560\" in bands_to_extract\n",
    "    else find_closest_band(560, bands_to_extract)\n",
    ")\n",
    "\n",
    "print(f\"Using {target_443} and {target_560} for mask check.\")\n",
    "\n",
    "idx_443 = bands_to_extract.index(target_443)\n",
    "idx_560 = bands_to_extract.index(target_560)\n",
    "mask &= filtered_Rrs[:, :, idx_443] <= filtered_Rrs[:, :, idx_560]\n",
    "\n",
    "\n",
    "valid_test_data = filtered_Rrs[mask]\n",
    "valid_test_data = np.array(\n",
    "    [\n",
    "        MinMaxScaler(feature_range=(1, 10)).fit_transform(row.reshape(-1, 1)).flatten()\n",
    "        for row in valid_test_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_tensor = TensorDataset(torch.tensor(valid_test_data).float())\n",
    "test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n",
    "\n",
    "predictions_all = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch[0].to(device)\n",
    "        predictions, _, _ = model(batch)\n",
    "        predictions = 10**predictions - 1\n",
    "        predictions_all.append(predictions.cpu().numpy())\n",
    "\n",
    "predictions_all = np.vstack(predictions_all)\n",
    "if predictions_all.shape[-1] == 1:\n",
    "    predictions_all = predictions_all.squeeze(-1)\n",
    "\n",
    "outputs = np.full((filtered_Rrs.shape[0], filtered_Rrs.shape[1]), np.nan)\n",
    "outputs[mask] = predictions_all\n",
    "\n",
    "lat_flat = latitude.flatten()\n",
    "lon_flat = longitude.flatten()\n",
    "output_flat = outputs.flatten()\n",
    "\n",
    "final_output = np.column_stack((lat_flat, lon_flat, output_flat))\n",
    "if isinstance(final_output, np.ma.MaskedArray):\n",
    "    final_output = final_output.filled(np.nan)\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "np.save(output_path, final_output)\n",
    "print(f\"✅ Processed and saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Part 4. We interpolate the Chl-a results saved in the .npy file generated in Part 3 and overlay them onto the RGB image from the ACOLITE-generated Level-2 Reflectance (L2R) file for visualization of the Chl-a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = \"/home/data/Chl-MSI/Harvey/Second/processed data/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2R.nc\"\n",
    "chla_data_file = (\n",
    "    \"/home/data/20250727/vae/MSI/0-100/run_7/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.npy\"\n",
    ")\n",
    "\n",
    "dataset = nc.Dataset(nc_file)\n",
    "lat = dataset.variables[\"lat\"][:]\n",
    "lon = dataset.variables[\"lon\"][:]\n",
    "height, width = lat.shape\n",
    "\n",
    "\n",
    "def find_closest_band(target, var_keys):\n",
    "    available = [v for v in var_keys if v.startswith(\"rhos_\")]\n",
    "    wavelengths = [int(v.split(\"_\")[1]) for v in available]\n",
    "    closest_wave = min(wavelengths, key=lambda w: abs(w - target))\n",
    "    return f\"rhos_{closest_wave}\"\n",
    "\n",
    "\n",
    "all_vars = dataset.variables.keys()\n",
    "\n",
    "R_band = \"rhos_665\" if \"rhos_665\" in all_vars else find_closest_band(665, all_vars)\n",
    "G_band = \"rhos_559\" if \"rhos_559\" in all_vars else find_closest_band(559, all_vars)\n",
    "B_band = \"rhos_492\" if \"rhos_492\" in all_vars else find_closest_band(492, all_vars)\n",
    "print(f\"Using bands: R={R_band}, G={G_band}, B={B_band}\")\n",
    "\n",
    "R = dataset.variables[R_band][:]\n",
    "G = dataset.variables[G_band][:]\n",
    "B = dataset.variables[B_band][:]\n",
    "\n",
    "lat_flat = lat.flatten()\n",
    "lon_flat = lon.flatten()\n",
    "R_flat = R.flatten()\n",
    "G_flat = G.flatten()\n",
    "B_flat = B.flatten()\n",
    "\n",
    "grid_lat = np.linspace(lat.min(), lat.max(), height)\n",
    "grid_lon = np.linspace(lon.min(), lon.max(), width)\n",
    "grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n",
    "\n",
    "R_interp = griddata((lat_flat, lon_flat), R_flat, (grid_lat, grid_lon), method=\"linear\")\n",
    "G_interp = griddata((lat_flat, lon_flat), G_flat, (grid_lat, grid_lon), method=\"linear\")\n",
    "B_interp = griddata((lat_flat, lon_flat), B_flat, (grid_lat, grid_lon), method=\"linear\")\n",
    "\n",
    "rgb_image = np.stack((R_interp, G_interp, B_interp), axis=-1)\n",
    "rgb_image = rgb_image / np.nanmax(rgb_image)\n",
    "rgb_image = np.clip(rgb_image * 5.0, 0, 1)\n",
    "\n",
    "chla_data = np.load(chla_data_file)\n",
    "chla_lat = chla_data[:, 0]\n",
    "chla_lon = chla_data[:, 1]\n",
    "chla_values = chla_data[:, 2]\n",
    "\n",
    "chla_interp = griddata(\n",
    "    (chla_lat, chla_lon), chla_values, (grid_lat, grid_lon), method=\"nearest\"\n",
    ")\n",
    "chla_interp = np.ma.masked_invalid(chla_interp)\n",
    "\n",
    "rgb_valid = np.sum(rgb_image, axis=-1) > 0\n",
    "mask = np.zeros_like(chla_interp, dtype=bool)\n",
    "mask[rgb_valid] = True\n",
    "chla_interp_masked = np.ma.masked_where(~mask, chla_interp)\n",
    "\n",
    "extent = [lon.min(), lon.max(), lat.min(), lat.max()]\n",
    "vmin, vmax = 0, 30\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(rgb_image, extent=extent, origin=\"lower\")\n",
    "im = plt.imshow(\n",
    "    chla_interp_masked,\n",
    "    extent=extent,\n",
    "    cmap=\"jet\",\n",
    "    alpha=1,\n",
    "    origin=\"lower\",\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    ")\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"Chl-a (mg m$^{-3}$)\", fontsize=16)\n",
    "plt.title(\"Chl-a over RGB (Sentinel-2)\", fontsize=18)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
