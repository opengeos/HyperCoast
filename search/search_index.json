{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to HyperCoast","text":"<p>A Python Package for Visualizing and Analyzing Hyperspectral Data in Coastal Environments</p> <ul> <li>Free software: MIT License</li> <li>Documentation: https://hypercoast.org</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>HyperCoast is a Python package designed to provide an accessible and comprehensive set of tools for visualizing and analyzing hyperspectral data in coastal environments. Hyperspectral data refers to the information collected by sensors that capture light across a wide range of wavelengths, beyond what the human eye can see. This data allows scientists to detect and analyze various materials and conditions on the Earth's surface with great detail. Unlike multispectral data, which captures light in a limited number of broad wavelength bands (typically 3 to 10), hyperspectral data captures light in many narrow, contiguous wavelength bands, often numbering in the hundreds. This provides much more detailed spectral information. Leveraging the capabilities of popular packages like Leafmap and PyVista, HyperCoast streamlines the exploration and interpretation of complex hyperspectral remote sensing data from existing spaceborne and airborne missions. It is also poised to support future hyperspectral missions, such as NASA's SBG and GLIMR. It enables researchers and environmental managers to gain deeper insights into the dynamic processes occurring in aquatic environments.</p> <p>HyperCoast supports the reading and visualization of hyperspectral data from various missions, including AVIRIS, NEON, PACE, EMIT, and DESIS, along with other datasets like ECOSTRESS. Users can interactively explore hyperspectral data, extract spectral signatures, change band combinations and colormaps, visualize data in 3D, and perform interactive slicing and thresholding operations (see Figure 1). Additionally, by leveraging the earthaccess package, HyperCoast provides tools for interactively searching NASA's hyperspectral data. This makes HyperCoast a versatile and powerful tool for working with hyperspectral data globally, with a particular focus on coastal regions.</p> <p> Figure 1. An example of visualizing NASA EMIT hyperspectral data using HyperCoast.</p>"},{"location":"#citations","title":"Citations","text":"<p>If you find HyperCoast useful in your research, please consider citing the following papers to support us. Thank you!</p> <ul> <li>Liu, B., &amp; Wu, Q. (2024). HyperCoast: A Python Package for Visualizing and Analyzing Hyperspectral Data in Coastal Environments. Journal of Open Source Software, 9(100), 7025. https://doi.org/10.21105/joss.07025.</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Searching for NASA hyperspectral data interactively</li> <li>Performing atmospheric correction using Acolite</li> <li>Interactive visualization and analysis of hyperspectral data, such as AVIRIS, DESIS, EMIT, PACE, NEON AOP, and Tanager</li> <li>Interactive visualization of NASA ECOSTRESS data</li> <li>Interactive visualization of PACE chlorophyll-a data</li> <li>Interactive extraction and visualization of spectral signatures</li> <li>Changing band combinations and colormaps interactively</li> <li>Visualizing hyperspectral data in 3D</li> <li>Visualizing ERA5 temperature data in 3D</li> <li>Interactive slicing and thresholding of hyperspectral data in 3D</li> <li>Saving spectral signatures as CSV files</li> </ul>"},{"location":"#demos","title":"Demos","text":"<ul> <li>Visualizing hyperspectral data in 3D (notebook)</li> </ul> <ul> <li>Interactive slicing of hyperspectral data in 3D (notebook)</li> </ul> <ul> <li>Interactive thresholding of hyperspectral data in 3D (notebook)</li> </ul> <ul> <li>Visualizing ERA5 temperature data in 3D (notebook)</li> </ul> <ul> <li>Changing band combinations and colormaps interactively (notebook)</li> </ul> <ul> <li>Visualizing NASA AVIRIS hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Visualizing DESIS hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Visualizing NASA EMIT hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Visualizing NASA PACE hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Visualizing NEON AOP hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Interactive visualization of PACE chlorophyll-a data (notebook)</li> </ul>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>The HyperCoast project draws inspiration from the nasa/EMIT-Data-Resources repository. Credits to the original authors. We also acknowledge the NASA EMIT program support through grant no. 80NSSC24K0865.</p>"},{"location":"#license","title":"License","text":"<p>HyperCoast is released under the MIT License. However, some of the modules in HyperCoast adapt code from other open-source projects, which may have different licenses. Please refer to the license notice in each module for more information. Credits to the original authors.</p> <ul> <li>emit.py: Part of the code is adapted from the nasa/EMIT-Data-Resources repository, which is released under the Apache License 2.0.</li> <li>aviris.py: Part of the code is adapted from the jjmcnelis/aviris-ng-notebooks, which is released under the MIT License.</li> </ul>"},{"location":"aviris/","title":"aviris module","text":"<p>This module contains functions to read and process NASA AVIRIS hyperspectral data. More info about the data can be found at https://aviris.jpl.nasa.gov. A portion of the source code is adapted from the jjmcnelis/aviris-ng-notebooks repository available at https://bit.ly/4bRCgqs. It is licensed under the MIT License. Credit goes to the original author Jack McNelis.</p> <p>SPDX-FileCopyrightText = [     \"2024 Jack McNelis jjmcne@gmail.com\", ] SPDX-License-Identifier = \"MIT\"</p>"},{"location":"aviris/#hypercoast.aviris.aviris_to_image","title":"<code>aviris_to_image(dataset, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts an AVIRIS dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The dataset containing the AVIRIS data or the file path to the dataset.</p> required <code>wavelengths</code> <code>np.ndarray</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[rasterio.Dataset]</code> <p>The image converted from the dataset. If     <code>output</code> is provided, the image will be saved to the specified file     and the function will return None.</p> Source code in <code>hypercoast/aviris.py</code> <pre><code>def aviris_to_image(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Optional[np.ndarray] = None,\n    method: str = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Converts an AVIRIS dataset to an image.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The dataset containing the AVIRIS data\n            or the file path to the dataset.\n        wavelengths (np.ndarray, optional): The specific wavelengths to select. If None, all\n            wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n        output (str, optional): The file path where the image will be saved. If\n            None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs (Any): Additional keyword arguments to be passed to\n            `leafmap.array_to_image`.\n\n    Returns:\n        Optional[rasterio.Dataset]: The image converted from the dataset. If\n            `output` is provided, the image will be saved to the specified file\n            and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_aviris(dataset, method=method)\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n\n    return array_to_image(\n        dataset[\"reflectance\"],\n        output=output,\n        transpose=False,\n        dtype=np.float32,\n        **kwargs,\n    )\n</code></pre>"},{"location":"aviris/#hypercoast.aviris.extract_aviris","title":"<code>extract_aviris(dataset, lat, lon, offset=2.0)</code>","text":"<p>Extracts AVIRIS data from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xarray.Dataset</code> <p>The dataset containing the AVIRIS data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <code>offset</code> <code>float</code> <p>The offset from the point to extract. Defaults to 2.0.</p> <code>2.0</code> <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted data.</p> Source code in <code>hypercoast/aviris.py</code> <pre><code>def extract_aviris(\n    dataset: xr.Dataset, lat: float, lon: float, offset: float = 2.0\n) -&gt; xr.DataArray:\n    \"\"\"\n    Extracts AVIRIS data from a given xarray Dataset.\n\n    Args:\n        dataset (xarray.Dataset): The dataset containing the AVIRIS data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n        offset (float, optional): The offset from the point to extract. Defaults to 2.0.\n\n    Returns:\n        xarray.DataArray: The extracted data.\n    \"\"\"\n\n    crs = dataset.attrs[\"crs\"]\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    da = dataset[\"reflectance\"]\n\n    x_con = (da[\"xc\"] &gt; x - offset) &amp; (da[\"xc\"] &lt; x + offset)\n    y_con = (da[\"yc\"] &gt; y - offset) &amp; (da[\"yc\"] &lt; y + offset)\n\n    try:\n        data = da.where(x_con &amp; y_con, drop=True)\n        data = data.mean(dim=[\"x\", \"y\"])\n    except ValueError:\n        data = np.nan * np.ones(da.sizes[\"wavelength\"])\n\n    da = xr.DataArray(\n        data, dims=[\"wavelength\"], coords={\"wavelength\": dataset.coords[\"wavelength\"]}\n    )\n\n    return da\n</code></pre>"},{"location":"aviris/#hypercoast.aviris.read_aviris","title":"<code>read_aviris(filepath, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Reads NASA AVIRIS hyperspectral data and returns an xarray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the AVIRIS data.</p> required <code>wavelengths</code> <code>List[float]</code> <p>The wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the selection method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The dataset containing the reflectance data.</p> Source code in <code>hypercoast/aviris.py</code> <pre><code>def read_aviris(\n    filepath: str,\n    wavelengths: Optional[List[float]] = None,\n    method: str = \"nearest\",\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads NASA AVIRIS hyperspectral data and returns an xarray dataset.\n\n    Args:\n        filepath (str): The path to the AVIRIS data.\n        wavelengths (List[float], optional): The wavelengths to select. If None,\n            all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for selection. Defaults to\n            \"nearest\".\n        **kwargs (Any): Additional arguments to pass to the selection method.\n\n    Returns:\n        xr.Dataset: The dataset containing the reflectance data.\n    \"\"\"\n\n    if filepath.endswith(\".hdr\"):\n        filepath = filepath.replace(\".hdr\", \"\")\n\n    ds = xr.open_dataset(filepath, engine=\"rasterio\")\n\n    wavelength = ds[\"wavelength\"].values.tolist()\n    wavelength = [round(num, 2) for num in wavelength]\n\n    cols = ds.x.size\n    rows = ds.y.size\n\n    rio_transform = ds.rio.transform()\n    geo_transform = list(rio_transform)[:6]\n\n    # get the raster geotransform as its component parts\n    xres, _, xmin, _, yres, ymax = geo_transform\n\n    # generate coordinate arrays\n    xarr = np.array([xmin + i * xres for i in range(0, cols)])\n    yarr = np.array([ymax + i * yres for i in range(0, rows)])\n\n    ds[\"y\"] = xr.DataArray(\n        data=yarr,\n        dims=(\"y\"),\n        name=\"y\",\n        attrs=dict(\n            units=\"m\",\n            standard_name=\"projection_y_coordinate\",\n            long_name=\"y coordinate of projection\",\n        ),\n    )\n\n    ds[\"x\"] = xr.DataArray(\n        data=xarr,\n        dims=(\"x\"),\n        name=\"x\",\n        attrs=dict(\n            units=\"m\",\n            standard_name=\"projection_x_coordinate\",\n            long_name=\"x coordinate of projection\",\n        ),\n    )\n\n    global_atts = ds.attrs\n    global_atts[\"Conventions\"] = \"CF-1.6\"\n    ds.attrs = dict(\n        units=\"unitless\",\n        _FillValue=-9999,\n        grid_mapping=\"crs\",\n        standard_name=\"reflectance\",\n        long_name=\"atmospherically corrected surface reflectance\",\n    )\n    ds.attrs.update(global_atts)\n\n    ds = ds.transpose(\"y\", \"x\", \"band\")\n    ds = ds.drop_vars([\"wavelength\"])\n    ds = ds.rename({\"band\": \"wavelength\", \"band_data\": \"reflectance\"})\n    ds.coords[\"wavelength\"] = wavelength\n    ds.attrs[\"crs\"] = ds.rio.crs.to_string()\n    ds.rio.write_transform(rio_transform)\n\n    if wavelengths is not None:\n        ds = ds.sel(wavelength=wavelengths, method=method, **kwargs)\n    return ds\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v076-aug-07-2024","title":"v0.7.6 - Aug 07, 2024","text":"<p>What's Changed</p> <ul> <li>[pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in #107</li> <li>Clean up file path in notebook by @giswqs in #108</li> <li>Fix read_pace bug for V2 by @giswqs in #109</li> </ul> <p>Full Changelog: v0.7.5...v0.7.6</p>"},{"location":"changelog/#v075-aug-05-2024","title":"v0.7.5 - Aug 05, 2024","text":"<p>What's Changed</p> <ul> <li>ci(Mergify): configuration update by @slowy07 in #99</li> <li>Add support for PCA by @giswqs in #100</li> <li>Update mergify by @giswqs in #101</li> <li>Add support for displaying field data by @giswqs in #106</li> </ul> <p>Full Changelog: v0.7.4...v0.7.5</p>"},{"location":"changelog/#v074-aug-01-2024","title":"v0.7.4 - Aug 01, 2024","text":"<p>What's Changed</p> <ul> <li>Remove AVIRIS image bounds by @giswqs in #96</li> <li>Fix read_pace bug for V2 by @giswqs in #97</li> </ul> <p>Full Changelog: v0.7.3...v0.7.4</p>"},{"location":"changelog/#v073-jul-30-2024","title":"v0.7.3 - Jul 30, 2024","text":"<p>What's Changed</p> <ul> <li>Add reuse license management framework by @giswqs in #83</li> <li>Add reuse dep and file header by @giswqs in #84</li> <li>Pin reuse version by @giswqs in #85</li> <li>Decapitalize Pringle et al title by @giswqs in #89</li> <li>Remove sentence and citations by @giswqs in #90</li> <li>Explain hyperspectral and other jargon by @giswqs in #91</li> <li>Add xlim parameter for the spectral widget by @giswqs in #93</li> </ul> <p>Full Changelog: v0.7.2...v0.7.3</p>"},{"location":"changelog/#v072-jul-23-2024","title":"v0.7.2 - Jul 23, 2024","text":"<p>What's Changed</p> <ul> <li>Update paper by @bingqing-liu in #74</li> <li>Update pyvista dependencies by @giswqs in #75</li> <li>chore: improving testing and typehinting by @slowy07 in #77</li> <li>Add support for generic dataset by @giswqs in #78</li> <li>Clarify license info by @giswqs in #79</li> <li>Add multispectral notebook example by @giswqs in #80</li> <li>Add support for acolite by @giswqs in #81</li> </ul> <p>New Contributors</p> <ul> <li>@slowy07 made their first contribution in #77</li> </ul> <p>Full Changelog: v0.7.0...v0.7.2</p>"},{"location":"changelog/#v071-jul-20-2024","title":"v0.7.1 - Jul 20, 2024","text":"<p>What's Changed</p> <ul> <li>Update paper by @bingqing-liu in #74</li> <li>Update pyvista dependencies by @giswqs in #75</li> <li>chore: improving testing and typehinting by @slowy07 in #77</li> <li>Add support for generic dataset by @giswqs in #78</li> <li>Clarify license info by @giswqs in #79</li> </ul> <p>New Contributors</p> <ul> <li>@slowy07 made their first contribution in #77</li> </ul> <p>Full Changelog: v0.7.0...v0.7.1</p>"},{"location":"changelog/#v070-jul-08-2024","title":"v0.7.0 - Jul 08, 2024","text":"<p>What's Changed</p> <ul> <li>Improve the pace chla function by @giswqs in #70</li> <li>Add JOSS paper draft by @giswqs in #71</li> <li>Add unittests by @giswqs in #72</li> <li>Add EMIT workshop notebook by @giswqs in #73</li> </ul> <p>Full Changelog: v0.6.3...v0.7.0</p>"},{"location":"changelog/#v063-jul-01-2024","title":"v0.6.3 - Jul 01, 2024","text":"<p>What's Changed</p> <ul> <li>Update pace_chla_to_image_function by @giswqs in #69</li> </ul> <p>Full Changelog: v0.6.2...v0.6.3</p>"},{"location":"changelog/#v062-jun-30-2024","title":"v0.6.2 - Jun 30, 2024","text":"<p>What's Changed</p> <ul> <li>Add search_datasets function and improve notebook example by @giswqs in #66</li> <li>Add PACE OCI Level-1 notebook example by @giswqs in #67</li> <li>Add PACE pixel location function by @giswqs in #68</li> </ul> <p>Full Changelog: v0.6.1...v0.6.2</p>"},{"location":"changelog/#v061-jun-30-2024","title":"v0.6.1 - Jun 30, 2024","text":"<p>What's Changed</p> <ul> <li>Add more demos to docs by @giswqs in #63</li> <li>Fix typos by @giswqs in #64</li> <li>Add support for reading PACE OCI L2 data by @giswqs in #65</li> </ul> <p>Full Changelog: v0.6.0...v0.6.1</p>"},{"location":"changelog/#v060-jun-26-2024","title":"v0.6.0 - Jun 26, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for PACE Chlorophyll data by @giswqs in #62</li> </ul> <p>Full Changelog: v0.5.5...v0.6.0</p>"},{"location":"changelog/#v055-jun-25-2024","title":"v0.5.5 - Jun 25, 2024","text":"<p>What's Changed</p> <ul> <li>Add image slicing demos to docs by @giswqs in #56</li> <li>Add dependabot by @giswqs in #58</li> <li>Bump nwtgck/actions-netlify from 2.0 to 3.0 by @dependabot in #59</li> <li>Bump conda-incubator/setup-miniconda from 2 to 3 by @dependabot in #60</li> <li>Add ERA5 temperature data notebook by @giswqs in #61</li> </ul> <p>New Contributors</p> <ul> <li>@dependabot made their first contribution in #59</li> </ul> <p>Full Changelog: v0.5.4...v0.5.5</p>"},{"location":"changelog/#v054-jun-14-2024","title":"v0.5.4 - Jun 14, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for interactive slicing by @giswqs in #54</li> <li>Add image slicing notebook example by @giswqs in #55</li> </ul> <p>Full Changelog: v0.5.3...v0.5.4</p>"},{"location":"changelog/#v053-jun-12-2024","title":"v0.5.3 - Jun 12, 2024","text":"<p>What's Changed</p> <ul> <li>Add EMIT image cube example by @giswqs in #51</li> <li>Refactor read_neon() function to support generalized NEON data reading by @gponce-ars in #52</li> </ul> <p>New Contributors</p> <ul> <li>@gponce-ars made their first contribution in #52</li> </ul> <p>Full Changelog: v0.5.2...v0.5.3</p>"},{"location":"changelog/#v052-jun-11-2024","title":"v0.5.2 - Jun 11, 2024","text":"<p>What's Changed</p> <ul> <li>Fix RGB image reshape bug for image cube by @giswqs in #49</li> </ul> <p>Full Changelog: v0.5.1...v0.5.2</p>"},{"location":"changelog/#v051-jun-11-2024","title":"v0.5.1 - Jun 11, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for 3D visualization by @giswqs in #47</li> <li>Add image cube demo to docs by @giswqs in #48</li> </ul> <p>Full Changelog: v0.5.0...v0.5.1</p>"},{"location":"changelog/#v050-jun-10-2024","title":"v0.5.0 - Jun 10, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for NEON AOP data by @giswqs in #43</li> <li>Add support for AVIRIS data by @giswqs in #44</li> <li>Add support for changing band combinations and colormaps interactively by @giswqs in #46</li> </ul> <p>Full Changelog: v0.4.0...v0.5.0</p>"},{"location":"changelog/#v040-jun-03-2024","title":"v0.4.0 - Jun 03, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for searching and downloading ECOSTRESS data by @giswqs in #37</li> <li>Add support for visualizing DESIS hyperspectral data by @giswqs in #38</li> </ul> <p>Full Changelog: v0.3.3...v0.4.0</p>"},{"location":"changelog/#v033-may-19-2024","title":"v0.3.3 - May 19, 2024","text":"<p>What's Changed</p> <ul> <li>Add functions for searching PACE and EMIT data by @giswqs in #34</li> <li>Add stacking for spectral signatures by @giswqs in #35</li> </ul> <p>Full Changelog: v0.3.2...v0.3.3</p>"},{"location":"changelog/#v032-may-10-2024","title":"v0.3.2 - May 10, 2024","text":"<p>What's Changed</p> <ul> <li>Fix wavelength bug by @giswqs in #27</li> <li>Pin xarray version by @giswqs in #28</li> <li>Fix xr merge error by @giswqs in #29</li> </ul> <p>Full Changelog: v0.3.1...v0.3.2</p>"},{"location":"changelog/#v031-may-09-2024","title":"v0.3.1 - May 09, 2024","text":"<p>Full Changelog: v0.3.0...v0.3.1</p>"},{"location":"changelog/#v030-may-08-2024","title":"v0.3.0 - May 08, 2024","text":"<p>What's Changed</p> <ul> <li>Add filter_pace function by @giswqs in #21</li> <li>Add grid pace function by @giswqs in #22</li> <li>Add fetch depth for docs by @giswqs in #23</li> <li>Add support for multi-band visualization by @giswqs in #24</li> <li>Add extract_pace function by @giswqs in #25</li> <li>Add spectral signature viz for PACE data by @giswqs in #26</li> </ul> <p>Full Changelog: v0.2.0...v0.3.0</p>"},{"location":"changelog/#v020-may-05-2024","title":"v0.2.0 - May 05, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for reading and visualizing PACE data by @giswqs in #20</li> </ul> <p>Full Changelog: v0.1.3...v0.2.0</p>"},{"location":"changelog/#v013-apr-30-2024","title":"v0.1.3 - Apr 30, 2024","text":"<p>What's Changed</p> <ul> <li>Move hvplot import into functions by @giswqs in #18</li> <li>[pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in #17</li> <li>Add pace module by @giswqs in #19</li> </ul> <p>Full Changelog: v0.1.2...v0.1.3</p>"},{"location":"changelog/#v012-apr-25-2024","title":"v0.1.2 - Apr 25, 2024","text":"<p>What's Changed</p> <ul> <li>Add demo gif by @bingqing-liu in #15</li> <li>Fix typos and add method bug by @giswqs in #16</li> </ul> <p>Full Changelog: v0.1.1...v0.1.2</p>"},{"location":"changelog/#v011-apr-22-2024","title":"v0.1.1 - Apr 22, 2024","text":"<p>What's Changed</p> <ul> <li>Add an EMIT notebook by @giswqs in #13</li> <li>Fix dependency issue by @giswqs in #14</li> </ul> <p>Full Changelog: v0.1.0...v0.1.1</p>"},{"location":"changelog/#v010-apr-21-2024","title":"v0.1.0 - Apr 21, 2024","text":"<p>What's Changed</p> <ul> <li>Improve support for visualizing EMIT data by @giswqs in #10</li> <li>Add ui module by @giswqs in #11</li> <li>Add support for displaying spectral signature interactively by @giswqs in #12</li> </ul> <p>Full Changelog: v0.0.3...v0.1.0</p>"},{"location":"changelog/#v003-apr-20-2024","title":"v0.0.3 - Apr 20, 2024","text":"<p>What's Changed</p> <ul> <li>Fix typos by @giswqs in #7</li> <li>Add conda-forge installation instructions by @giswqs in #8</li> <li>Add support for visualizing EMIT data by @giswqs in #9</li> </ul> <p>Full Changelog: v0.0.2...v0.0.3</p>"},{"location":"changelog/#v002-apr-18-2024","title":"v0.0.2 - Apr 18, 2024","text":"<p>What's Changed</p> <ul> <li>[pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in #4</li> <li>Update readme by @Bingqing9027 in #5</li> <li>Add Map class by @giswqs in #6</li> </ul> <p>New Contributors</p> <ul> <li>@pre-commit-ci made their first contribution in #4</li> <li>@Bingqing9027 made their first contribution in #5</li> <li>@giswqs made their first contribution in #6</li> </ul> <p>Full Changelog: v0.0.1...v0.0.2</p>"},{"location":"changelog/#v001-apr-08-2024","title":"v0.0.1 - Apr 08, 2024","text":"<p>What's Changed Full Changelog: v0.0.1</p>"},{"location":"changelog_update/","title":"Changelog update","text":"In\u00a0[\u00a0]: Copied! <pre>import re\n</pre> import re In\u00a0[\u00a0]: Copied! <pre># Copy the release notes from the GitHub release page\nmarkdown_text = \"\"\"\n## What's Changed\n* Update pace_chla_to_image_function by @giswqs in https://github.com/opengeos/HyperCoast/pull/69\n\n\n**Full Changelog**: https://github.com/opengeos/HyperCoast/compare/v0.6.2...v0.6.3\n\"\"\"\n</pre> # Copy the release notes from the GitHub release page markdown_text = \"\"\" ## What's Changed * Update pace_chla_to_image_function by @giswqs in https://github.com/opengeos/HyperCoast/pull/69   **Full Changelog**: https://github.com/opengeos/HyperCoast/compare/v0.6.2...v0.6.3 \"\"\" In\u00a0[\u00a0]: Copied! <pre># Regular expression pattern to match the Markdown hyperlinks\npattern = r\"https://github\\.com/opengeos/HyperCoast/pull/(\\d+)\"\n</pre> # Regular expression pattern to match the Markdown hyperlinks pattern = r\"https://github\\.com/opengeos/HyperCoast/pull/(\\d+)\" In\u00a0[\u00a0]: Copied! <pre># Function to replace matched URLs with the desired format\ndef replace_url(match):\n    pr_number = match.group(1)\n    return f\"[#{pr_number}](https://github.com/opengeos/HyperCoast/pull/{pr_number})\"\n</pre> # Function to replace matched URLs with the desired format def replace_url(match):     pr_number = match.group(1)     return f\"[#{pr_number}](https://github.com/opengeos/HyperCoast/pull/{pr_number})\" In\u00a0[\u00a0]: Copied! <pre># Use re.sub to replace URLs with the desired format\nformatted_text = re.sub(pattern, replace_url, markdown_text)\n</pre> # Use re.sub to replace URLs with the desired format formatted_text = re.sub(pattern, replace_url, markdown_text) In\u00a0[\u00a0]: Copied! <pre>for line in formatted_text.splitlines():\n    if \"Full Changelog\" in line:\n        prefix = line.split(\": \")[0]\n        link = line.split(\": \")[1]\n        version = line.split(\"/\")[-1]\n        formatted_text = (\n            formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")\n            .replace(\"## What's Changed\", \"**What's Changed**\")\n            .replace(\"## New Contributors\", \"**New Contributors**\")\n        )\n</pre> for line in formatted_text.splitlines():     if \"Full Changelog\" in line:         prefix = line.split(\": \")[0]         link = line.split(\": \")[1]         version = line.split(\"/\")[-1]         formatted_text = (             formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")             .replace(\"## What's Changed\", \"**What's Changed**\")             .replace(\"## New Contributors\", \"**New Contributors**\")         ) In\u00a0[\u00a0]: Copied! <pre>with open(\"docs/changelog_update.md\", \"w\") as f:\n    f.write(formatted_text)\n</pre> with open(\"docs/changelog_update.md\", \"w\") as f:     f.write(formatted_text) In\u00a0[\u00a0]: Copied! <pre># Print the formatted text\nprint(formatted_text)\n</pre> # Print the formatted text print(formatted_text) <p>Copy the formatted text and paste it to the CHANGELOG.md file</p>"},{"location":"chla/","title":"chla module","text":"<p>Module for training and evaluating the VAE model for Chl-a concentration estimation.</p>"},{"location":"chla/#hypercoast.chla.VAE","title":"<code> VAE            (Module)         </code>","text":"Source code in <code>hypercoast/chla.py</code> <pre><code>class VAE(nn.Module):\n    def __init__(self, input_dim: int, output_dim: int) -&gt; None:\n        \"\"\"Initializes the VAE model with encoder and decoder layers.\n\n        Args:\n            input_dim (int): Dimension of the input features.\n            output_dim (int): Dimension of the output features.\n        \"\"\"\n        super().__init__()\n\n        # encoder\n        self.encoder_layer = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2),\n            nn.Linear(64, 64),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2),\n        )\n\n        self.fc1 = nn.Linear(64, 32)\n        self.fc2 = nn.Linear(64, 32)\n\n        # decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2),\n            nn.Linear(64, 64),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2),\n            nn.Linear(64, output_dim),\n            nn.Softplus(),\n        )\n\n    def encode(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Encodes the input tensor into mean and log variance.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor]: Mean and log variance tensors.\n        \"\"\"\n        x = self.encoder_layer(x)\n        mu = self.fc1(x)\n        log_var = self.fc2(x)\n        return mu, log_var\n\n    def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Applies the reparameterization trick to sample from the latent space.\n\n        Args:\n            mu (torch.Tensor): Mean tensor.\n            log_var (torch.Tensor): Log variance tensor.\n\n        Returns:\n            torch.Tensor: Sampled latent vector.\n        \"\"\"\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        z = mu + eps * std\n        return z\n\n    def decode(self, z: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decodes the latent vector back to the original space.\n\n        Args:\n            z (torch.Tensor): Latent vector.\n\n        Returns:\n            torch.Tensor: Reconstructed tensor.\n        \"\"\"\n        return self.decoder(z)\n\n    def forward(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Performs a forward pass through the VAE model.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Reconstructed tensor,\n                mean, and log variance.\n        \"\"\"\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        x_reconstructed = self.decode(z)\n        return x_reconstructed, mu, log_var\n</code></pre>"},{"location":"chla/#hypercoast.chla.VAE.__init__","title":"<code>__init__(self, input_dim, output_dim)</code>  <code>special</code>","text":"<p>Initializes the VAE model with encoder and decoder layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of the output features.</p> required Source code in <code>hypercoast/chla.py</code> <pre><code>def __init__(self, input_dim: int, output_dim: int) -&gt; None:\n    \"\"\"Initializes the VAE model with encoder and decoder layers.\n\n    Args:\n        input_dim (int): Dimension of the input features.\n        output_dim (int): Dimension of the output features.\n    \"\"\"\n    super().__init__()\n\n    # encoder\n    self.encoder_layer = nn.Sequential(\n        nn.Linear(input_dim, 64),\n        nn.BatchNorm1d(64),\n        nn.LeakyReLU(0.2),\n        nn.Linear(64, 64),\n        nn.BatchNorm1d(64),\n        nn.LeakyReLU(0.2),\n    )\n\n    self.fc1 = nn.Linear(64, 32)\n    self.fc2 = nn.Linear(64, 32)\n\n    # decoder\n    self.decoder = nn.Sequential(\n        nn.Linear(32, 64),\n        nn.BatchNorm1d(64),\n        nn.LeakyReLU(0.2),\n        nn.Linear(64, 64),\n        nn.BatchNorm1d(64),\n        nn.LeakyReLU(0.2),\n        nn.Linear(64, output_dim),\n        nn.Softplus(),\n    )\n</code></pre>"},{"location":"chla/#hypercoast.chla.VAE.decode","title":"<code>decode(self, z)</code>","text":"<p>Decodes the latent vector back to the original space.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>torch.Tensor</code> <p>Latent vector.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Reconstructed tensor.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def decode(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decodes the latent vector back to the original space.\n\n    Args:\n        z (torch.Tensor): Latent vector.\n\n    Returns:\n        torch.Tensor: Reconstructed tensor.\n    \"\"\"\n    return self.decoder(z)\n</code></pre>"},{"location":"chla/#hypercoast.chla.VAE.encode","title":"<code>encode(self, x)</code>","text":"<p>Encodes the input tensor into mean and log variance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor]</code> <p>Mean and log variance tensors.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Encodes the input tensor into mean and log variance.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: Mean and log variance tensors.\n    \"\"\"\n    x = self.encoder_layer(x)\n    mu = self.fc1(x)\n    log_var = self.fc2(x)\n    return mu, log_var\n</code></pre>"},{"location":"chla/#hypercoast.chla.VAE.forward","title":"<code>forward(self, x)</code>","text":"<p>Performs a forward pass through the VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code> <p>Reconstructed tensor,     mean, and log variance.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def forward(\n    self, x: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Performs a forward pass through the VAE model.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Reconstructed tensor,\n            mean, and log variance.\n    \"\"\"\n    mu, log_var = self.encode(x)\n    z = self.reparameterize(mu, log_var)\n    x_reconstructed = self.decode(z)\n    return x_reconstructed, mu, log_var\n</code></pre>"},{"location":"chla/#hypercoast.chla.VAE.reparameterize","title":"<code>reparameterize(self, mu, log_var)</code>","text":"<p>Applies the reparameterization trick to sample from the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>torch.Tensor</code> <p>Mean tensor.</p> required <code>log_var</code> <code>torch.Tensor</code> <p>Log variance tensor.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Sampled latent vector.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Applies the reparameterization trick to sample from the latent space.\n\n    Args:\n        mu (torch.Tensor): Mean tensor.\n        log_var (torch.Tensor): Log variance tensor.\n\n    Returns:\n        torch.Tensor: Sampled latent vector.\n    \"\"\"\n    std = torch.exp(0.5 * log_var)\n    eps = torch.randn_like(std)\n    z = mu + eps * std\n    return z\n</code></pre>"},{"location":"chla/#hypercoast.chla.calculate_metrics","title":"<code>calculate_metrics(predictions, actuals, threshold=0.8)</code>","text":"<p>Calculates epsilon, beta, and additional metrics (RMSE, RMSLE, MAPE, Bias, MAE).</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray</code> <p>Predicted values.</p> required <code>actuals</code> <code>np.ndarray</code> <p>Actual values.</p> required <code>threshold</code> <code>float</code> <p>Relative error threshold. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>tuple[float, float, float, float, float, float, float]</code> <p>epsilon, beta, rmse, rmsle, mape, bias, mae.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def calculate_metrics(\n    predictions: np.ndarray, actuals: np.ndarray, threshold: float = 0.8\n) -&gt; tuple[float, float, float, float, float, float, float]:\n    \"\"\"Calculates epsilon, beta, and additional metrics (RMSE, RMSLE, MAPE, Bias, MAE).\n\n    Args:\n        predictions (np.ndarray): Predicted values.\n        actuals (np.ndarray): Actual values.\n        threshold (float, optional): Relative error threshold. Defaults to 0.8.\n\n    Returns:\n        tuple[float, float, float, float, float, float, float]: epsilon, beta, rmse, rmsle, mape, bias, mae.\n    \"\"\"\n    # Apply the threshold to filter out predictions with large relative error\n    mask = np.abs(predictions - actuals) / np.abs(actuals + 1e-10) &lt; threshold\n    filtered_predictions = predictions[mask]\n    filtered_actuals = actuals[mask]\n\n    # Calculate epsilon and beta\n    log_ratios = np.log10(filtered_predictions / filtered_actuals)\n    Y = np.median(np.abs(log_ratios))\n    Z = np.median(log_ratios)\n    epsilon = 50 * (10**Y - 1)\n    beta = 50 * np.sign(Z) * (10 ** np.abs(Z) - 1)\n\n    # Calculate additional metrics\n    rmse = np.sqrt(np.mean((filtered_predictions - filtered_actuals) ** 2))\n    rmsle = np.sqrt(\n        np.mean(\n            (np.log10(filtered_predictions + 1) - np.log10(filtered_actuals + 1)) ** 2\n        )\n    )\n    mape = 50 * np.median(\n        np.abs((filtered_predictions - filtered_actuals) / filtered_actuals)\n    )\n    bias = 10 ** (np.mean(np.log10(filtered_predictions) - np.log10(filtered_actuals)))\n    mae = 10 ** np.mean(\n        np.abs(np.log10(filtered_predictions) - np.log10(filtered_actuals))\n    )\n\n    return epsilon, beta, rmse, rmsle, mape, bias, mae\n</code></pre>"},{"location":"chla/#hypercoast.chla.chla_predict","title":"<code>chla_predict(pace_filepath, best_model_path, chla_data_file=None, device=None)</code>","text":"<p>Predicts chlorophyll-a concentration using a pre-trained VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>pace_filepath</code> <code>str</code> <p>Path to the PACE dataset file.</p> required <code>best_model_path</code> <code>str</code> <p>Path to the pre-trained VAE model file.</p> required <code>chla_data_file</code> <code>str</code> <p>Path to save the predicted chlorophyll-a data. Defaults to None.</p> <code>None</code> <code>device</code> <code>torch.device</code> <p>Device to perform inference on. Defaults to None.</p> <code>None</code> Source code in <code>hypercoast/chla.py</code> <pre><code>def chla_predict(\n    pace_filepath: str,\n    best_model_path: str,\n    chla_data_file: str = None,\n    device: torch.device = None,\n) -&gt; None:\n    \"\"\"Predicts chlorophyll-a concentration using a pre-trained VAE model.\n\n    Args:\n        pace_filepath (str): Path to the PACE dataset file.\n        best_model_path (str): Path to the pre-trained VAE model file.\n        chla_data_file (str, optional): Path to save the predicted chlorophyll-a data. Defaults to None.\n        device (torch.device, optional): Device to perform inference on. Defaults to None.\n    \"\"\"\n\n    from .pace import read_pace\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load PACE dataset and prepare data\n    PACE_dataset = read_pace(pace_filepath)\n    da = PACE_dataset[\"Rrs\"]\n    wl = da.wavelength.values\n    Rrs = da.values\n    latitude = da.latitude.values\n    longitude = da.longitude.values\n\n    # Filter wavelengths between 400 and 703 nm\n    indices = np.where((wl &gt;= 400) &amp; (wl &lt;= 703))[0]\n    filtered_Rrs = Rrs[:, :, indices]\n\n    # Save filtered Rrs and wavelength\n    filtered_wl = wl[indices]\n\n    # Create a mask that is 1 where all wavelengths for a given pixel have non-NaN values, and 0 otherwise\n    mask = np.all(~np.isnan(filtered_Rrs), axis=2).astype(int)\n\n    # Define input and output dimensions\n    input_dim = 148\n    output_dim = 1\n\n    # Load test data and mask\n    test_data = filtered_Rrs\n    mask_data = mask\n\n    # Filter valid data using the mask\n    mask = mask_data == 1\n    N = np.sum(mask)\n    valid_test_data = test_data[mask]\n\n    # Normalize data\n    valid_test_data = np.array(\n        [\n            (\n                MinMaxScaler(feature_range=(1, 10))\n                .fit_transform(row.reshape(-1, 1))\n                .flatten()\n                if not np.isnan(row).any()\n                else row\n            )\n            for row in valid_test_data\n        ]\n    )\n    valid_test_data = valid_test_data.reshape(N, input_dim)\n\n    # Create DataLoader for test data\n    test_tensor = TensorDataset(torch.tensor(valid_test_data).float())\n    test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n\n    # Load the pre-trained VAE model\n    model = VAE(input_dim, output_dim).to(device)\n    model.load_state_dict(torch.load(best_model_path, map_location=device))\n    model.eval()\n\n    # Perform inference\n    predictions_all = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch[0].to(device)\n            predictions, _, _ = model(batch)  # VAE model inference\n            predictions_all.append(predictions.cpu().numpy())\n\n    # Concatenate all batch predictions\n    predictions_all = np.vstack(predictions_all)\n\n    # Ensure predictions are in the correct shape\n    if predictions_all.shape[-1] == 1:\n        predictions_all = predictions_all.squeeze(-1)\n    # if predictions_all.dim() == 3:\n    # all_outputs = predictions_all.squeeze(1)\n\n    # Initialize output array with NaNs\n    outputs = np.full((test_data.shape[0], test_data.shape[1]), np.nan)\n\n    # Fill in the valid mask positions with predictions\n    outputs[mask] = predictions_all\n\n    # Flatten latitude, longitude, and predictions for output\n    lat_flat = latitude.flatten()\n    lon_flat = longitude.flatten()\n    output_flat = outputs.flatten()\n\n    # Combine latitude, longitude, and predictions\n    final_output = np.column_stack((lat_flat, lon_flat, output_flat))\n\n    # Save the final output including latitude and longitude\n    if chla_data_file is None:\n        chla_data_file = pace_filepath.replace(\".nc\", \".npy\")\n    np.save(chla_data_file, final_output)\n</code></pre>"},{"location":"chla/#hypercoast.chla.chla_viz","title":"<code>chla_viz(rgb_image_tif_file, chla_data_file, output_file, title='PACE', figsize=(12, 8), cmap='jet')</code>","text":"<p>Visualizes the chlorophyll-a concentration over an RGB image.</p> <p>Parameters:</p> Name Type Description Default <code>rgb_image_tif_file</code> <code>str</code> <p>Path to the RGB image file.</p> required <code>chla_data_file</code> <code>str</code> <p>Path to the chlorophyll-a data file.</p> required <code>output_file</code> <code>str</code> <p>Path to save the output visualization.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"PACE\".</p> <code>'PACE'</code> <code>figsize</code> <code>tuple</code> <p>Figure size for the plot. Defaults to (12, 8).</p> <code>(12, 8)</code> <code>cmap</code> <code>str</code> <p>Colormap for the chlorophyll-a concentration. Defaults to \"jet\".</p> <code>'jet'</code> Source code in <code>hypercoast/chla.py</code> <pre><code>def chla_viz(\n    rgb_image_tif_file: str,\n    chla_data_file: str,\n    output_file: str,\n    title: str = \"PACE\",\n    figsize: tuple = (12, 8),\n    cmap: str = \"jet\",\n) -&gt; None:\n    \"\"\"Visualizes the chlorophyll-a concentration over an RGB image.\n\n    Args:\n        rgb_image_tif_file (str): Path to the RGB image file.\n        chla_data_file (str): Path to the chlorophyll-a data file.\n        output_file (str): Path to save the output visualization.\n        title (str, optional): Title of the plot. Defaults to \"PACE\".\n        figsize (tuple, optional): Figure size for the plot. Defaults to (12, 8).\n        cmap (str, optional): Colormap for the chlorophyll-a concentration. Defaults to \"jet\".\n    \"\"\"\n\n    # Read RGB Image\n    # rgb_image_tif_file = \"data/snapshot-2024-08-10T00_00_00Z.tif\"\n\n    with rasterio.open(rgb_image_tif_file) as dataset:\n        # Read R\u3001G\u3001B bands\n        R = dataset.read(1)\n        G = dataset.read(2)\n        B = dataset.read(3)\n\n        # # Get geographic extent, resolution information.\n        extent = [\n            dataset.bounds.left,\n            dataset.bounds.right,\n            dataset.bounds.bottom,\n            dataset.bounds.top,\n        ]\n        transform = dataset.transform\n        width, height = dataset.width, dataset.height\n\n    # Combine the R, G, B bands into a 3D array.\n    rgb_image = np.stack((R, G, B), axis=-1)\n\n    # Load Chla data\n    chla_data = np.load(chla_data_file)\n    # chla_data = final_output\n\n    # Extract the latitude, longitude, and concentration values of the chlorophyll-a data.\n    latitude = chla_data[:, 0]\n    longitude = chla_data[:, 1]\n    chla_values = chla_data[:, 2]\n\n    # Extract the pixels within the geographic extent of the RGB image.\n    mask = (\n        (latitude &gt;= extent[2])\n        &amp; (latitude &lt;= extent[3])\n        &amp; (longitude &gt;= extent[0])\n        &amp; (longitude &lt;= extent[1])\n    )\n    latitude = latitude[mask]\n    longitude = longitude[mask]\n    chla_values = chla_values[mask]\n\n    # Create a grid with the same resolution as the RGB image.\n    grid_lon = np.linspace(extent[0], extent[1], width)\n    grid_lat = np.linspace(extent[3], extent[2], height)\n    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n\n    # Resample the chlorophyll-a data to the size of the RGB image using interpolation.\n    chla_resampled = griddata(\n        (longitude, latitude), chla_values, (grid_lon, grid_lat), method=\"linear\"\n    )\n\n    # Keep NaN values as transparent regions.\n    chla_resampled = np.ma.masked_invalid(chla_resampled)\n\n    plt.figure(figsize=figsize)\n\n    plt.imshow(rgb_image / 255.0, extent=extent, origin=\"upper\")\n\n    vmin, vmax = 0, 35\n    im = plt.imshow(\n        chla_resampled,\n        extent=extent,\n        cmap=cmap,\n        alpha=0.6,\n        origin=\"upper\",\n        vmin=vmin,\n        vmax=vmax,\n    )\n\n    cbar = plt.colorbar(im, orientation=\"horizontal\")\n    cbar.set_label(\"Chlorophyll-a Concentration (mg/m\u00b3)\")\n\n    plt.title(title)\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n\n    # output_file = \"20241024-2.png\"\n    plt.savefig(output_file, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n    print(f\"Saved overlay image to {output_file}\")\n\n    plt.show()\n</code></pre>"},{"location":"chla/#hypercoast.chla.evaluate","title":"<code>evaluate(model, test_dl, device=None)</code>","text":"<p>Evaluates the VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>VAE</code> <p>VAE model to be evaluated.</p> required <code>test_dl</code> <code>DataLoader</code> <p>DataLoader for test data.</p> required <code>device</code> <code>torch.device</code> <p>Device to evaluate on. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[np.ndarray, np.ndarray]</code> <p>Predictions and actual values.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def evaluate(\n    model: VAE, test_dl: DataLoader, device: torch.device = None\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Evaluates the VAE model.\n\n    Args:\n        model (VAE): VAE model to be evaluated.\n        test_dl (DataLoader): DataLoader for test data.\n        device (torch.device, optional): Device to evaluate on. Defaults to None.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: Predictions and actual values.\n    \"\"\"\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model.eval()\n    predictions, actuals = [], []\n    with torch.no_grad():\n        for x, y in test_dl:\n            x, y = x.to(device), y.to(device)\n            y_pred, _, _ = model(x)\n            predictions.append(y_pred.cpu().numpy())\n            actuals.append(y.cpu().numpy())\n    return np.vstack(predictions), np.vstack(actuals)\n</code></pre>"},{"location":"chla/#hypercoast.chla.load_real_data","title":"<code>load_real_data(aphy_file_path, rrs_file_path)</code>","text":"<p>Loads and preprocesses real data for training and testing.</p> <p>Parameters:</p> Name Type Description Default <code>aphy_file_path</code> <code>str</code> <p>Path to the aphy file.</p> required <code>rrs_file_path</code> <code>str</code> <p>Path to the rrs file.</p> required <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader, int, int]</code> <p>Training DataLoader, testing     DataLoader, input dimension, output dimension.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def load_real_data(\n    aphy_file_path: str, rrs_file_path: str\n) -&gt; tuple[DataLoader, DataLoader, int, int]:\n    \"\"\"Loads and preprocesses real data for training and testing.\n\n    Args:\n        aphy_file_path (str): Path to the aphy file.\n        rrs_file_path (str): Path to the rrs file.\n\n    Returns:\n        tuple[DataLoader, DataLoader, int, int]: Training DataLoader, testing\n            DataLoader, input dimension, output dimension.\n    \"\"\"\n    array1 = np.loadtxt(aphy_file_path, delimiter=\",\", dtype=float)\n    array2 = np.loadtxt(rrs_file_path, delimiter=\",\", dtype=float)\n\n    array1 = array1.reshape(-1, 1)\n\n    Rrs_real = array2\n    Chl_real = array1\n\n    input_dim = Rrs_real.shape[1]\n    output_dim = Chl_real.shape[1]\n\n    scalers_Rrs_real = [\n        MinMaxScaler(feature_range=(1, 10)) for _ in range(Rrs_real.shape[0])\n    ]\n\n    Rrs_real_normalized = np.array(\n        [\n            scalers_Rrs_real[i].fit_transform(row.reshape(-1, 1)).flatten()\n            for i, row in enumerate(Rrs_real)\n        ]\n    )\n\n    Rrs_real_tensor = torch.tensor(Rrs_real_normalized, dtype=torch.float32)\n    Chl_real_tensor = torch.tensor(Chl_real, dtype=torch.float32)\n\n    dataset_real = TensorDataset(Rrs_real_tensor, Chl_real_tensor)\n\n    train_size = int(0.7 * len(dataset_real))\n    test_size = len(dataset_real) - train_size\n    train_dataset_real, test_dataset_real = random_split(\n        dataset_real, [train_size, test_size]\n    )\n\n    train_real_dl = DataLoader(\n        train_dataset_real, batch_size=1024, shuffle=True, num_workers=12\n    )\n    test_real_dl = DataLoader(\n        test_dataset_real, batch_size=1024, shuffle=False, num_workers=12\n    )\n\n    return train_real_dl, test_real_dl, input_dim, output_dim\n</code></pre>"},{"location":"chla/#hypercoast.chla.load_real_test","title":"<code>load_real_test(aphy_file_path, rrs_file_path)</code>","text":"<p>Loads and preprocesses real data for testing.</p> <p>Parameters:</p> Name Type Description Default <code>aphy_file_path</code> <code>str</code> <p>Path to the aphy file.</p> required <code>rrs_file_path</code> <code>str</code> <p>Path to the rrs file.</p> required <p>Returns:</p> Type Description <code>tuple[DataLoader, int, int]</code> <p>Testing DataLoader, input dimension, output dimension.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def load_real_test(\n    aphy_file_path: str, rrs_file_path: str\n) -&gt; tuple[DataLoader, int, int]:\n    \"\"\"Loads and preprocesses real data for testing.\n\n    Args:\n        aphy_file_path (str): Path to the aphy file.\n        rrs_file_path (str): Path to the rrs file.\n\n    Returns:\n        tuple[DataLoader, int, int]: Testing DataLoader, input dimension, output dimension.\n    \"\"\"\n    array1 = np.loadtxt(aphy_file_path, delimiter=\",\", dtype=float)\n    array2 = np.loadtxt(rrs_file_path, delimiter=\",\", dtype=float)\n\n    array1 = array1.reshape(-1, 1)\n\n    Rrs_real = array2\n    Chl_real = array1\n\n    input_dim = Rrs_real.shape[1]\n    output_dim = Chl_real.shape[1]\n\n    scalers_Rrs_real = [\n        MinMaxScaler(feature_range=(1, 10)) for _ in range(Rrs_real.shape[0])\n    ]\n\n    Rrs_real_normalized = np.array(\n        [\n            scalers_Rrs_real[i].fit_transform(row.reshape(-1, 1)).flatten()\n            for i, row in enumerate(Rrs_real)\n        ]\n    )\n\n    Rrs_real_tensor = torch.tensor(Rrs_real_normalized, dtype=torch.float32)\n    Chl_real_tensor = torch.tensor(Chl_real, dtype=torch.float32)\n\n    dataset_real = TensorDataset(Rrs_real_tensor, Chl_real_tensor)\n    dataset_size = int(len(dataset_real))\n    test_real_dl = DataLoader(\n        dataset_real, batch_size=dataset_size, shuffle=False, num_workers=12\n    )\n\n    return test_real_dl, input_dim, output_dim\n</code></pre>"},{"location":"chla/#hypercoast.chla.loss_function","title":"<code>loss_function(recon_x, x, mu, log_var)</code>","text":"<p>Computes the VAE loss function.</p> <p>Parameters:</p> Name Type Description Default <code>recon_x</code> <code>torch.Tensor</code> <p>Reconstructed tensor.</p> required <code>x</code> <code>torch.Tensor</code> <p>Original input tensor.</p> required <code>mu</code> <code>torch.Tensor</code> <p>Mean tensor.</p> required <code>log_var</code> <code>torch.Tensor</code> <p>Log variance tensor.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Computed loss.</p> Source code in <code>hypercoast/chla.py</code> <pre><code>def loss_function(\n    recon_x: torch.Tensor, x: torch.Tensor, mu: torch.Tensor, log_var: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Computes the VAE loss function.\n\n    Args:\n        recon_x (torch.Tensor): Reconstructed tensor.\n        x (torch.Tensor): Original input tensor.\n        mu (torch.Tensor): Mean tensor.\n        log_var (torch.Tensor): Log variance tensor.\n\n    Returns:\n        torch.Tensor: Computed loss.\n    \"\"\"\n    L1 = F.l1_loss(recon_x, x, reduction=\"mean\")\n    BCE = F.mse_loss(recon_x, x, reduction=\"mean\")\n    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    return L1\n</code></pre>"},{"location":"chla/#hypercoast.chla.npy_to_geotiff","title":"<code>npy_to_geotiff(npy_path, out_tif, resolution_m=1000, method='linear', nodata_val=-9999.0, bbox_padding=0.0, lat_col=0, lon_col=1, band_cols=None, band_names=None, wavelengths=None, crs='EPSG:4326', compress='deflate', bigtiff='IF_SAFER')</code>","text":"<p>Convert [lat, lon, band1, band2, ...] scattered points in .npy into a multi-band GeoTIFF (EPSG:4326).</p> <p>Parameters:</p> Name Type Description Default <code>npy_path</code> <code>str</code> <p>Path to the .npy file.</p> required <code>out_tif</code> <code>str</code> <p>Path to the output GeoTIFF file.</p> required <code>resolution_m</code> <code>int</code> <p>Resolution in meters per pixel. Defaults to 1000.</p> <code>1000</code> <code>method</code> <code>str</code> <p>Method for interpolation. Defaults to \"linear\".</p> <code>'linear'</code> <code>nodata_val</code> <code>float</code> <p>Value to fill NaN values. Defaults to -9999.0.</p> <code>-9999.0</code> <code>bbox_padding</code> <code>float</code> <p>Padding around the bounding box. Defaults to 0.0.</p> <code>0.0</code> <code>lat_col</code> <code>int</code> <p>Column index for latitude. Defaults to 0.</p> <code>0</code> <code>lon_col</code> <code>int</code> <p>Column index for longitude. Defaults to 1.</p> <code>1</code> <code>band_cols</code> <code>list</code> <p>Columns to rasterize as bands. Defaults to None.</p> <code>None</code> <code>band_names</code> <code>list</code> <p>Band descriptions. Defaults to None.</p> <code>None</code> <code>wavelengths</code> <code>list</code> <p>Wavelengths. Defaults to None.</p> <code>None</code> <code>crs</code> <code>str</code> <p>Coordinate reference system. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>compress</code> <code>str</code> <p>Compression method. Defaults to \"deflate\".</p> <code>'deflate'</code> <code>bigtiff</code> <code>str</code> <p>BigTIFF mode. Defaults to \"IF_SAFER\".</p> <code>'IF_SAFER'</code> Source code in <code>hypercoast/chla.py</code> <pre><code>def npy_to_geotiff(\n    npy_path,\n    out_tif,\n    resolution_m=1000,  # meters per pixel (lat exact, lon adjusted by cos(lat))\n    method=\"linear\",  # 'linear' | 'nearest' | 'cubic'\n    nodata_val=-9999.0,\n    bbox_padding=0.0,  # degrees padding around bbox\n    lat_col=0,\n    lon_col=1,\n    band_cols=None,  # None=use all columns after lat/lon; or list like [2,3,5,...]\n    band_names=None,  # Optional list of per-band descriptions, same length as output bands\n    wavelengths=None,  # Optional list; if given, will be used in descriptions like \"aphy_440\"\n    crs=\"EPSG:4326\",\n    compress=\"deflate\",\n    bigtiff=\"IF_SAFER\",  # 'YES'|'NO'|'IF_NEEDED'|'IF_SAFER'\n):\n    \"\"\"\n    Convert [lat, lon, band1, band2, ...] scattered points in .npy into a multi-band GeoTIFF (EPSG:4326).\n\n    Args:\n        npy_path (str): Path to the .npy file.\n        out_tif (str): Path to the output GeoTIFF file.\n        resolution_m (int, optional): Resolution in meters per pixel. Defaults to 1000.\n        method (str, optional): Method for interpolation. Defaults to \"linear\".\n        nodata_val (float, optional): Value to fill NaN values. Defaults to -9999.0.\n        bbox_padding (float, optional): Padding around the bounding box. Defaults to 0.0.\n        lat_col (int, optional): Column index for latitude. Defaults to 0.\n        lon_col (int, optional): Column index for longitude. Defaults to 1.\n        band_cols (list, optional): Columns to rasterize as bands. Defaults to None.\n        band_names (list, optional): Band descriptions. Defaults to None.\n        wavelengths (list, optional): Wavelengths. Defaults to None.\n        crs (str, optional): Coordinate reference system. Defaults to \"EPSG:4326\".\n        compress (str, optional): Compression method. Defaults to \"deflate\".\n        bigtiff (str, optional): BigTIFF mode. Defaults to \"IF_SAFER\".\n    \"\"\"\n\n    # 1) Load &amp; basic checks\n    arr = np.load(npy_path)\n    if arr.ndim != 2 or arr.shape[1] &lt; 3:\n        raise ValueError(\"Must be 2D with &gt;=3 columns.\")\n\n    lat = arr[:, lat_col].astype(float)\n    lon = arr[:, lon_col].astype(float)\n\n    # Decide which value columns become bands\n    if band_cols is None:\n        band_cols = [i for i in range(arr.shape[1]) if i not in (lat_col, lon_col)]\n    if isinstance(band_cols, (int, np.integer)):\n        band_cols = [int(band_cols)]\n    if len(band_cols) == 0:\n        raise ValueError(\"No value columns selected for bands.\")\n\n    # 2) Bounds (+ padding)\n    lat_min, lat_max = np.nanmin(lat), np.nanmax(lat)\n    lon_min, lon_max = np.nanmin(lon), np.nanmax(lon)\n    lat_min -= bbox_padding\n    lat_max += bbox_padding\n    lon_min -= bbox_padding\n    lon_max += bbox_padding\n\n    # 3) Meter -&gt; degree resolution (lat exact; lon scaled at center latitude)\n    lat_center = (lat_min + lat_max) / 2.0\n    deg_per_m_lat = 1.0 / 111000.0\n    deg_per_m_lon = 1.0 / (111000.0 * np.cos(np.radians(lat_center)))\n    res_lat_deg = resolution_m * deg_per_m_lat\n    res_lon_deg = resolution_m * deg_per_m_lon\n\n    # 4) Build grid (lon fastest axis -&gt; width; lat -&gt; height)\n    lon_axis = np.arange(lon_min, lon_max + res_lon_deg, res_lon_deg)\n    lat_axis = np.arange(lat_min, lat_max + res_lat_deg, res_lat_deg)\n    Lon, Lat = np.meshgrid(lon_axis, lat_axis)\n\n    # Precompute transform\n    transform = from_origin(lon_axis.min(), lat_axis.max(), res_lon_deg, res_lat_deg)\n\n    # 5) Interpolate each band onto the same grid\n    grids = []\n    for idx in band_cols:\n        vals = arr[:, idx].astype(float)\n\n        # Primary interpolation\n        g = griddata(points=(lon, lat), values=vals, xi=(Lon, Lat), method=method)\n\n        # Fill NaNs with nearest as a safety net\n        if np.isnan(g).any():\n            g_near = griddata(\n                points=(lon, lat), values=vals, xi=(Lon, Lat), method=\"nearest\"\n            )\n            g = np.where(np.isnan(g), g_near, g)\n\n        # Flip vertically because raster origin is top-left\n        grids.append(np.flipud(g).astype(np.float32))\n\n    data_stack = np.stack(grids, axis=0)  # shape: (bands, height, width)\n\n    # 6) Write GeoTIFF\n    profile = {\n        \"driver\": \"GTiff\",\n        \"height\": data_stack.shape[1],\n        \"width\": data_stack.shape[2],\n        \"count\": data_stack.shape[0],\n        \"dtype\": rasterio.float32,\n        \"crs\": crs,\n        \"transform\": transform,\n        \"nodata\": nodata_val,\n        \"compress\": compress,\n        \"tiled\": True,\n        \"blockxsize\": 256,\n        \"blockysize\": 256,\n        \"BIGTIFF\": bigtiff,\n    }\n\n    os.makedirs(os.path.dirname(out_tif) or \".\", exist_ok=True)\n    with rasterio.open(out_tif, \"w\", **profile) as dst:\n        # Write bands\n        for b in range(data_stack.shape[0]):\n            band = data_stack[b]\n            band[~np.isfinite(band)] = nodata_val\n            dst.write(band, b + 1)\n\n        # Add band descriptions\n        descriptions = []\n        n_bands = data_stack.shape[0]\n        if band_names is not None and len(band_names) == n_bands:\n            descriptions = list(map(str, band_names))\n        elif wavelengths is not None and len(wavelengths) == n_bands:\n            # e.g., \"aphy_440\"\n            descriptions = [f\"aphy_{int(wl)}\" for wl in wavelengths]\n        else:\n            # Fallback: use column indices\n            descriptions = [f\"band_{band_cols[b]}\" for b in range(n_bands)]\n\n        for b in range(1, n_bands + 1):\n            dst.set_band_description(b, descriptions[b - 1])\n\n    # 7) Log\n    print(f\"\u2705 GeoTIFF\uff1a{out_tif}\")\n    print(f\"   Size\uff1a{profile['width']} x {profile['height']} pixels\")\n    print(f\"   Bands\uff1a{profile['count']}\")\n    print(\n        f\"   Resolution\uff1a{resolution_m} m/px (\u2248 {res_lon_deg:.6f}\u00b0 \u00d7 {res_lat_deg:.6f}\u00b0 @ {lat_center:.2f}\u00b0)\"\n    )\n    print(\n        f\"   Extent\uff1alon[{lon_min:.6f}, {lon_max:.6f}], lat[{lat_min:.6f}, {lat_max:.6f}]\"\n    )\n    print(f\"   Descriptions\uff1a{descriptions}\")\n</code></pre>"},{"location":"chla/#hypercoast.chla.plot_results","title":"<code>plot_results(predictions_rescaled, actuals_rescaled, save_dir, threshold=0.5, mode='test')</code>","text":"<p>Plots the results of the predictions against the actual values.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_rescaled</code> <code>np.ndarray</code> <p>Rescaled predicted values.</p> required <code>actuals_rescaled</code> <code>np.ndarray</code> <p>Rescaled actual values.</p> required <code>save_dir</code> <code>str</code> <p>Directory to save the plot.</p> required <code>threshold</code> <code>float</code> <p>Relative error threshold. Defaults to 0.5.</p> <code>0.5</code> <code>mode</code> <code>str</code> <p>Mode of the plot (e.g., \"test\"). Defaults to \"test\".</p> <code>'test'</code> Source code in <code>hypercoast/chla.py</code> <pre><code>def plot_results(\n    predictions_rescaled: np.ndarray,\n    actuals_rescaled: np.ndarray,\n    save_dir: str,\n    threshold: float = 0.5,\n    mode: str = \"test\",\n) -&gt; None:\n    \"\"\"Plots the results of the predictions against the actual values.\n\n    Args:\n        predictions_rescaled (np.ndarray): Rescaled predicted values.\n        actuals_rescaled (np.ndarray): Rescaled actual values.\n        save_dir (str): Directory to save the plot.\n        threshold (float, optional): Relative error threshold. Defaults to 0.5.\n        mode (str, optional): Mode of the plot (e.g., \"test\"). Defaults to \"test\".\n    \"\"\"\n\n    actuals = actuals_rescaled.flatten()\n    predictions = predictions_rescaled.flatten()\n\n    mask = np.abs(predictions - actuals) / np.abs(actuals + 1e-10) &lt; 1\n    filtered_predictions = predictions[mask]\n    filtered_actuals = actuals[mask]\n\n    log_actual = np.log10(np.where(actuals == 0, 1e-10, actuals))\n    log_prediction = np.log10(np.where(predictions == 0, 1e-10, predictions))\n\n    filtered_log_actual = np.log10(\n        np.where(filtered_actuals == 0, 1e-10, filtered_actuals)\n    )\n    filtered_log_prediction = np.log10(\n        np.where(filtered_predictions == 0, 1e-10, filtered_predictions)\n    )\n\n    epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(\n        filtered_predictions, filtered_actuals, threshold\n    )\n\n    valid_mask = np.isfinite(filtered_log_actual) &amp; np.isfinite(filtered_log_prediction)\n    slope, intercept = np.polyfit(\n        filtered_log_actual[valid_mask], filtered_log_prediction[valid_mask], 1\n    )\n    x = np.array([-2, 4])\n    y = slope * x + intercept\n\n    _ = plt.figure(figsize=(6, 6))\n\n    plt.plot(x, y, linestyle=\"--\", color=\"blue\", linewidth=0.8)\n    lims = [-2, 4]\n    plt.plot(lims, lims, linestyle=\"-\", color=\"black\", linewidth=0.8)\n\n    sns.scatterplot(x=log_actual, y=log_prediction, alpha=0.5)\n\n    sns.kdeplot(\n        x=filtered_log_actual,\n        y=filtered_log_prediction,\n        levels=3,\n        color=\"black\",\n        fill=False,\n        linewidths=0.8,\n    )\n\n    plt.xlabel(\"Actual $Chla$ Values\", fontsize=16)\n    plt.ylabel(\"Predicted $Chla$ Values\", fontsize=16)\n    plt.xlim(-2, 4)\n    plt.ylim(-2, 4)\n    plt.grid(True, which=\"both\", ls=\"--\")\n\n    plt.legend(\n        title=(\n            f\"MAE = {mae:.2f}, RMSE = {rmse:.2f}, RMSLE = {rmsle:.2f} \\n\"\n            f\"Bias = {bias:.2f}, Slope = {slope:.2f} \\n\"\n            f\"MAPE = {mape:.2f}%, \u03b5 = {epsilon:.2f}%, \u03b2 = {beta:.2f}%\"\n        ),\n        fontsize=16,\n        title_fontsize=12,\n    )\n\n    plt.xticks(fontsize=20)\n    plt.yticks(fontsize=20)\n    plt.show()\n\n    plt.savefig(os.path.join(save_dir, f\"{mode}_plot.pdf\"), bbox_inches=\"tight\")\n    # plt.close()\n</code></pre>"},{"location":"chla/#hypercoast.chla.save_to_csv","title":"<code>save_to_csv(data, file_path)</code>","text":"<p>Saves data to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>Data to be saved.</p> required <code>file_path</code> <code>str</code> <p>Path to the CSV file.</p> required Source code in <code>hypercoast/chla.py</code> <pre><code>def save_to_csv(data: np.ndarray, file_path: str) -&gt; None:\n    \"\"\"Saves data to a CSV file.\n\n    Args:\n        data (np.ndarray): Data to be saved.\n        file_path (str): Path to the CSV file.\n    \"\"\"\n    df = pd.DataFrame(data)\n    df.to_csv(file_path, index=False)\n</code></pre>"},{"location":"chla/#hypercoast.chla.train","title":"<code>train(model, train_dl, epochs=200, device=None, opt=None, model_path='model/vae_model_PACE.pth', best_model_path='model/vae_trans_model_best_Chl_PACE.pth')</code>","text":"<p>Trains the VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>VAE</code> <p>VAE model to be trained.</p> required <code>train_dl</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train. Defaults to 200.</p> <code>200</code> <code>device</code> <code>torch.device</code> <p>Device to train on. Defaults to None.</p> <code>None</code> <code>opt</code> <code>torch.optim.Optimizer</code> <p>Optimizer. Defaults to None.</p> <code>None</code> <code>model_path</code> <code>str</code> <p>Path to save the model. Defaults to \"model/vae_model_PACE.pth\".</p> <code>'model/vae_model_PACE.pth'</code> <code>best_model_path</code> <code>str</code> <p>Path to save the best model. Defaults to \"model/vae_trans_model_best_Chl_PACE.pth\"</p> <code>'model/vae_trans_model_best_Chl_PACE.pth'</code> Source code in <code>hypercoast/chla.py</code> <pre><code>def train(\n    model: VAE,\n    train_dl: DataLoader,\n    epochs: int = 200,\n    device: torch.device = None,\n    opt: torch.optim.Optimizer = None,\n    model_path: str = \"model/vae_model_PACE.pth\",\n    best_model_path: str = \"model/vae_trans_model_best_Chl_PACE.pth\",\n) -&gt; None:\n    \"\"\"Trains the VAE model.\n\n    Args:\n        model (VAE): VAE model to be trained.\n        train_dl (DataLoader): DataLoader for training data.\n        epochs (int, optional): Number of epochs to train. Defaults to 200.\n        device (torch.device, optional): Device to train on. Defaults to None.\n        opt (torch.optim.Optimizer, optional): Optimizer. Defaults to None.\n        model_path (str, optional): Path to save the model. Defaults to\n            \"model/vae_model_PACE.pth\".\n        best_model_path (str, optional): Path to save the best model. Defaults\n            to \"model/vae_trans_model_best_Chl_PACE.pth\"\n    \"\"\"\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    if opt is None:\n        opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n\n    model.train()\n\n    min_total_loss = float(\"inf\")\n    # Save the optimal model\n\n    for epoch in range(epochs):\n        total_loss = 0.0\n        for x, y in train_dl:\n            x, y = x.to(device), y.to(device)\n            y_pred, mu, log_var = model(x)\n            loss = loss_function(y_pred, y, mu, log_var)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n\n        avg_total_loss = total_loss / len(train_dl)\n        print(f\"epoch = {epoch + 1}, total_loss = {avg_total_loss:.4f}\")\n\n        if avg_total_loss &lt; min_total_loss:\n            min_total_loss = avg_total_loss\n            torch.save(model.state_dict(), best_model_path)\n    # Save the model from the last epoch.\n    torch.save(model.state_dict(), model_path)\n</code></pre>"},{"location":"cla/","title":"Fiduciary License Agreement 2.0","text":"<p>based on the</p>"},{"location":"cla/#individual-contributor-exclusive-license-agreement","title":"Individual Contributor Exclusive License Agreement","text":""},{"location":"cla/#including-the-traditional-patent-license-option","title":"(including the Traditional Patent License OPTION)","text":"<p>Thank you for your interest in contributing to Open Geospatial Solutions's HyperCoast (\"We\" or \"Us\").</p> <p>The purpose of this contributor agreement (\"Agreement\") is to clarify and document the rights granted by contributors to Us. To make this document effective, please follow the instructions at https://github.com/opengeos/HyperCoast.</p>"},{"location":"cla/#0-preamble","title":"0. Preamble","text":"<p>Software is deeply embedded in all aspects of our lives and it is important that it empower, rather than restrict us. Free Software gives everybody the rights to use, understand, adapt and share software. These rights help support other fundamental freedoms like freedom of speech, press and privacy.</p> <p>Development of Free Software can follow many patterns. In some cases whole development is handled by a sole programmer or a small group of people. But usually, the creation and maintenance of software is a complex process that requires the contribution of many individuals. This also affects who owns the rights to the software. In the latter case, rights in software are owned jointly by a great number of individuals.</p> <p>To tackle this issue some projects require a full copyright assignment to be signed by all contributors. The problem with such assignments is that they often lack checks and balances that would protect the contributors from potential abuse of power from the new copyright holder.</p> <p>FSFE\u2019s Fiduciary License Agreement (FLA) was created by the Free Software Foundation Europe e.V. with just that in mind \u2013 to concentrate all deciding power within one entity and prevent fragmentation of rights on one hand, while on the other preventing that single entity from abusing its power. The main aim is to ensure that the software covered under the FLA will forever remain Free Software.</p> <p>This process only serves for the transfer of economic rights. So-called moral rights (e.g. authors right to be identified as author) remain with the original author(s) and are inalienable.</p>"},{"location":"cla/#how-to-use-this-fla","title":"How to use this FLA","text":"<p>If You are an employee and have created the Contribution as part of your employment, You need to have Your employer approve this Agreement or sign the Entity version of this document. If You do not own the Copyright in the entire work of authorship, any other author of the Contribution should also sign this \u2013 in any event, please contact Us at opengeos@outlook.com</p>"},{"location":"cla/#1-definitions","title":"1. Definitions","text":"<p>\"You\" means the individual Copyright owner who Submits a Contribution to Us.</p> <p>\"Legal Entity\" means an entity that is not a natural person.</p> <p>\"Affiliate\" means any other Legal Entity that controls, is controlled by, or under common control with that Legal Entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such Legal Entity, whether by contract or otherwise, (ii) ownership of fifty percent (50%) or more of the outstanding shares or securities that vote to elect the management or other persons who direct such Legal Entity or (iii) beneficial ownership of such entity.</p> <p>\"Contribution\" means any original work of authorship, including any original modifications or additions to an existing work of authorship, Submitted by You to Us, in which You own the Copyright.</p> <p>\"Copyright\" means all rights protecting works of authorship, including copyright, moral and neighboring rights, as appropriate, for the full term of their existence.</p> <p>\"Material\" means the software or documentation made available by Us to third parties. When this Agreement covers more than one software project, the Material means the software or documentation to which the Contribution was Submitted. After You Submit the Contribution, it may be included in the Material.</p> <p>\"Submit\" means any act by which a Contribution is transferred to Us by You by means of tangible or intangible media, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Us, but excluding any transfer that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"</p> <p>\"Documentation\" means any non-software portion of a Contribution.</p>"},{"location":"cla/#2-license-grant","title":"2. License grant","text":""},{"location":"cla/#21-copyright-license-to-us","title":"2.1 Copyright license to Us","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Us a worldwide, royalty-free, exclusive, perpetual and irrevocable (except as stated in Section 8.2) license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul>"},{"location":"cla/#22-moral-rights","title":"2.2 Moral rights","text":"<p>Moral Rights remain unaffected to the extent they are recognized and not waivable by applicable law. Notwithstanding, You may add your name to the attribution mechanism customary used in the Materials you Contribute to, such as the header of the source code files of Your Contribution, and We will respect this attribution when using Your Contribution.</p>"},{"location":"cla/#23-copyright-license-back-to-you","title":"2.3 Copyright license back to You","text":"<p>Upon such grant of rights to Us, We immediately grant to You a worldwide, royalty-free, non-exclusive, perpetual and irrevocable license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul> <p>This license back is limited to the Contribution and does not provide any rights to the Material.</p>"},{"location":"cla/#3-patents","title":"3. Patents","text":""},{"location":"cla/#31-patent-license","title":"3.1 Patent license","text":"<p>Subject to the terms and conditions of this Agreement You hereby grant to Us and to recipients of Materials distributed by Us a worldwide, royalty-free, non-exclusive, perpetual and irrevocable (except as stated in Section 3.2) patent license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, to make, have made, use, sell, offer for sale, import and otherwise transfer the Contribution and the Contribution in combination with any Material (and portions of such combination). This license applies to all patents owned or controlled by You, whether already acquired or hereafter acquired, that would be infringed by making, having made, using, selling, offering for sale, importing or otherwise transferring of Your Contribution(s) alone or by combination of Your Contribution(s) with any Material.</p>"},{"location":"cla/#32-revocation-of-patent-license","title":"3.2 Revocation of patent license","text":"<p>You reserve the right to revoke the patent license stated in section 3.1 if We make any infringement claim that is targeted at your Contribution and not asserted for a Defensive Purpose. An assertion of claims of the Patents shall be considered for a \"Defensive Purpose\" if the claims are asserted against an entity that has filed, maintained, threatened, or voluntarily participated in a patent infringement lawsuit against Us or any of Our licensees.</p>"},{"location":"cla/#4-license-obligations-by-us","title":"4. License obligations by Us","text":"<p>We agree to (sub)license the Contribution or any Materials containing, based on or derived from your Contribution under the terms of any licenses the Free Software Foundation classifies as Free Software License and which are approved by the Open Source Initiative as Open Source licenses.</p> <p>We agree to license patents owned or controlled by You only to the extent necessary to (sub)license Your Contribution(s) and the combination of Your Contribution(s) with the Material under the terms of any licenses the Free Software Foundation classifies as Free Software licenses and which are approved by the Open Source Initiative as Open Source licenses..</p>"},{"location":"cla/#5-disclaimer","title":"5. Disclaimer","text":"<p>THE CONTRIBUTION IS PROVIDED \"AS IS\". MORE PARTICULARLY, ALL EXPRESS OR IMPLIED WARRANTIES INCLUDING, WITHOUT LIMITATION, ANY IMPLIED WARRANTY OF SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT ARE EXPRESSLY DISCLAIMED BY YOU TO US AND BY US TO YOU. TO THE EXTENT THAT ANY SUCH WARRANTIES CANNOT BE DISCLAIMED, SUCH WARRANTY IS LIMITED IN DURATION AND EXTENT TO THE MINIMUM PERIOD AND EXTENT PERMITTED BY LAW.</p>"},{"location":"cla/#6-consequential-damage-waiver","title":"6. Consequential damage waiver","text":"<p>TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL YOU OR WE BE LIABLE FOR ANY LOSS OF PROFITS, LOSS OF ANTICIPATED SAVINGS, LOSS OF DATA, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL AND EXEMPLARY DAMAGES ARISING OUT OF THIS AGREEMENT REGARDLESS OF THE LEGAL OR EQUITABLE THEORY (CONTRACT, TORT OR OTHERWISE) UPON WHICH THE CLAIM IS BASED.</p>"},{"location":"cla/#7-approximation-of-disclaimer-and-damage-waiver","title":"7. Approximation of disclaimer and damage waiver","text":"<p>IF THE DISCLAIMER AND DAMAGE WAIVER MENTIONED IN SECTION 5. AND SECTION 6. CANNOT BE GIVEN LEGAL EFFECT UNDER APPLICABLE LOCAL LAW, REVIEWING COURTS SHALL APPLY LOCAL LAW THAT MOST CLOSELY APPROXIMATES AN ABSOLUTE WAIVER OF ALL CIVIL OR CONTRACTUAL LIABILITY IN CONNECTION WITH THE CONTRIBUTION.</p>"},{"location":"cla/#8-term","title":"8. Term","text":"<p>8.1 This Agreement shall come into effect upon Your acceptance of the terms and conditions.</p> <p>8.2 This Agreement shall apply for the term of the copyright and patents licensed here. However, You shall have the right to terminate the Agreement if We do not fulfill the obligations as set forth in Section 4. Such termination must be made in writing.</p> <p>8.3 In the event of a termination of this Agreement Sections 5., 6., 7., 8., and 9. shall survive such termination and shall remain in full force thereafter. For the avoidance of doubt, Free and Open Source Software (sub)licenses that have already been granted for Contributions at the date of the termination shall remain in full force after the termination of this Agreement.</p>"},{"location":"cla/#9-miscellaneous","title":"9. Miscellaneous","text":"<p>9.1 This Agreement and all disputes, claims, actions, suits or other proceedings arising out of this agreement or relating in any way to it shall be governed by the laws of United States excluding its private international law provisions.</p> <p>9.2 This Agreement sets out the entire agreement between You and Us for Your Contributions to Us and overrides all other agreements or understandings.</p> <p>9.3 In case of Your death, this agreement shall continue with Your heirs. In case of more than one heir, all heirs must exercise their rights through a commonly authorized person.</p> <p>9.4 If any provision of this Agreement is found void and unenforceable, such provision will be replaced to the extent possible with a provision that comes closest to the meaning of the original provision and that is enforceable. The terms and conditions set forth in this Agreement shall apply notwithstanding any failure of essential purpose of this Agreement or any limited remedy to the maximum extent possible under law.</p> <p>9.5 You agree to notify Us of any facts or circumstances of which you become aware that would make this Agreement inaccurate in any respect.</p>"},{"location":"cla/#recreate-this-contributor-license-agreement","title":"Recreate this Contributor License Agreement","text":"<p>https://contributoragreements.org/u2s/2kw48zbt5d</p> <p></p> <p></p>"},{"location":"cla/#fiduciary-license-agreement-20_1","title":"Fiduciary License Agreement 2.0","text":"<p>based on the</p>"},{"location":"cla/#entity-contributor-exclusive-license-agreement","title":"Entity Contributor Exclusive License Agreement","text":""},{"location":"cla/#including-the-traditional-patent-license-option_1","title":"(including the Traditional Patent License OPTION)","text":"<p>Thank you for your interest in contributing to Open Geospatial Solutions's HyperCoast (\"We\" or \"Us\").</p> <p>The purpose of this contributor agreement (\"Agreement\") is to clarify and document the rights granted by contributors to Us. To make this document effective, please follow the instructions at https://github.com/opengeos/HyperCoast.</p>"},{"location":"cla/#0-preamble_1","title":"0. Preamble","text":"<p>Software is deeply embedded in all aspects of our lives and it is important that it empower, rather than restrict us. Free Software gives everybody the rights to use, understand, adapt and share software. These rights help support other fundamental freedoms like freedom of speech, press and privacy.</p> <p>Development of Free Software can follow many patterns. In some cases whole development is handled by a sole programmer or a small group of people. But usually, the creation and maintenance of software is a complex process that requires the contribution of many individuals. This also affects who owns the rights to the software. In the latter case, rights in software are owned jointly by a great number of individuals.</p> <p>To tackle this issue some projects require a full copyright assignment to be signed by all contributors. The problem with such assignments is that they often lack checks and balances that would protect the contributors from potential abuse of power from the new copyright holder.</p> <p>FSFE\u2019s Fiduciary License Agreement (FLA) was created by the Free Software Foundation Europe e.V. with just that in mind \u2013 to concentrate all deciding power within one entity and prevent fragmentation of rights on one hand, while on the other preventing that single entity from abusing its power. The main aim is to ensure that the software covered under the FLA will forever remain Free Software.</p> <p>This process only serves for the transfer of economic rights. So-called moral rights (e.g. authors right to be identified as author) remain with the original author(s) and are inalienable.</p>"},{"location":"cla/#how-to-use-this-fla_1","title":"How to use this FLA","text":"<p>If You are an employee and have created the Contribution as part of your employment, You need to have Your employer approve this Agreement or sign the Entity version of this document. If You do not own the Copyright in the entire work of authorship, any other author of the Contribution should also sign this \u2013 in any event, please contact Us at opengeos@outlook.com</p>"},{"location":"cla/#1-definitions_1","title":"1. Definitions","text":"<p>\"You\" means the individual Copyright owner who Submits a Contribution to Us.</p> <p>\"Legal Entity\" means an entity that is not a natural person.</p> <p>\"Affiliate\" means any other Legal Entity that controls, is controlled by, or under common control with that Legal Entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such Legal Entity, whether by contract or otherwise, (ii) ownership of fifty percent (50%) or more of the outstanding shares or securities that vote to elect the management or other persons who direct such Legal Entity or (iii) beneficial ownership of such entity.</p> <p>\"Contribution\" means any original work of authorship, including any original modifications or additions to an existing work of authorship, Submitted by You to Us, in which You own the Copyright.</p> <p>\"Copyright\" means all rights protecting works of authorship, including copyright, moral and neighboring rights, as appropriate, for the full term of their existence.</p> <p>\"Material\" means the software or documentation made available by Us to third parties. When this Agreement covers more than one software project, the Material means the software or documentation to which the Contribution was Submitted. After You Submit the Contribution, it may be included in the Material.</p> <p>\"Submit\" means any act by which a Contribution is transferred to Us by You by means of tangible or intangible media, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Us, but excluding any transfer that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"</p> <p>\"Documentation\" means any non-software portion of a Contribution.</p>"},{"location":"cla/#2-license-grant_1","title":"2. License grant","text":""},{"location":"cla/#21-copyright-license-to-us_1","title":"2.1 Copyright license to Us","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Us a worldwide, royalty-free, exclusive, perpetual and irrevocable (except as stated in Section 8.2) license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul>"},{"location":"cla/#22-moral-rights_1","title":"2.2 Moral rights","text":"<p>Moral Rights remain unaffected to the extent they are recognized and not waivable by applicable law. Notwithstanding, You may add your name to the attribution mechanism customary used in the Materials you Contribute to, such as the header of the source code files of Your Contribution, and We will respect this attribution when using Your Contribution.</p>"},{"location":"cla/#23-copyright-license-back-to-you_1","title":"2.3 Copyright license back to You","text":"<p>Upon such grant of rights to Us, We immediately grant to You a worldwide, royalty-free, non-exclusive, perpetual and irrevocable license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul> <p>This license back is limited to the Contribution and does not provide any rights to the Material.</p>"},{"location":"cla/#3-patents_1","title":"3. Patents","text":""},{"location":"cla/#31-patent-license_1","title":"3.1 Patent license","text":"<p>Subject to the terms and conditions of this Agreement You hereby grant to Us and to recipients of Materials distributed by Us a worldwide, royalty-free, non-exclusive, perpetual and irrevocable (except as stated in Section 3.2) patent license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, to make, have made, use, sell, offer for sale, import and otherwise transfer the Contribution and the Contribution in combination with any Material (and portions of such combination). This license applies to all patents owned or controlled by You, whether already acquired or hereafter acquired, that would be infringed by making, having made, using, selling, offering for sale, importing or otherwise transferring of Your Contribution(s) alone or by combination of Your Contribution(s) with any Material.</p>"},{"location":"cla/#32-revocation-of-patent-license_1","title":"3.2 Revocation of patent license","text":"<p>You reserve the right to revoke the patent license stated in section 3.1 if We make any infringement claim that is targeted at your Contribution and not asserted for a Defensive Purpose. An assertion of claims of the Patents shall be considered for a \"Defensive Purpose\" if the claims are asserted against an entity that has filed, maintained, threatened, or voluntarily participated in a patent infringement lawsuit against Us or any of Our licensees.</p>"},{"location":"cla/#4-license-obligations-by-us_1","title":"4. License obligations by Us","text":"<p>We agree to (sub)license the Contribution or any Materials containing, based on or derived from your Contribution under the terms of any licenses the Free Software Foundation classifies as Free Software License and which are approved by the Open Source Initiative as Open Source licenses.</p> <p>We agree to license patents owned or controlled by You only to the extent necessary to (sub)license Your Contribution(s) and the combination of Your Contribution(s) with the Material under the terms of any licenses the Free Software Foundation classifies as Free Software licenses and which are approved by the Open Source Initiative as Open Source licenses..</p>"},{"location":"cla/#5-disclaimer_1","title":"5. Disclaimer","text":"<p>THE CONTRIBUTION IS PROVIDED \"AS IS\". MORE PARTICULARLY, ALL EXPRESS OR IMPLIED WARRANTIES INCLUDING, WITHOUT LIMITATION, ANY IMPLIED WARRANTY OF SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT ARE EXPRESSLY DISCLAIMED BY YOU TO US AND BY US TO YOU. TO THE EXTENT THAT ANY SUCH WARRANTIES CANNOT BE DISCLAIMED, SUCH WARRANTY IS LIMITED IN DURATION AND EXTENT TO THE MINIMUM PERIOD AND EXTENT PERMITTED BY LAW.</p>"},{"location":"cla/#6-consequential-damage-waiver_1","title":"6. Consequential damage waiver","text":"<p>TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL YOU OR WE BE LIABLE FOR ANY LOSS OF PROFITS, LOSS OF ANTICIPATED SAVINGS, LOSS OF DATA, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL AND EXEMPLARY DAMAGES ARISING OUT OF THIS AGREEMENT REGARDLESS OF THE LEGAL OR EQUITABLE THEORY (CONTRACT, TORT OR OTHERWISE) UPON WHICH THE CLAIM IS BASED.</p>"},{"location":"cla/#7-approximation-of-disclaimer-and-damage-waiver_1","title":"7. Approximation of disclaimer and damage waiver","text":"<p>IF THE DISCLAIMER AND DAMAGE WAIVER MENTIONED IN SECTION 5. AND SECTION 6. CANNOT BE GIVEN LEGAL EFFECT UNDER APPLICABLE LOCAL LAW, REVIEWING COURTS SHALL APPLY LOCAL LAW THAT MOST CLOSELY APPROXIMATES AN ABSOLUTE WAIVER OF ALL CIVIL OR CONTRACTUAL LIABILITY IN CONNECTION WITH THE CONTRIBUTION.</p>"},{"location":"cla/#8-term_1","title":"8. Term","text":"<p>8.1 This Agreement shall come into effect upon Your acceptance of the terms and conditions.</p> <p>8.2 This Agreement shall apply for the term of the copyright and patents licensed here. However, You shall have the right to terminate the Agreement if We do not fulfill the obligations as set forth in Section 4. Such termination must be made in writing.</p> <p>8.3 In the event of a termination of this Agreement Sections 5., 6., 7., 8., and 9. shall survive such termination and shall remain in full force thereafter. For the avoidance of doubt, Free and Open Source Software (sub)licenses that have already been granted for Contributions at the date of the termination shall remain in full force after the termination of this Agreement.</p>"},{"location":"cla/#9-miscellaneous_1","title":"9. Miscellaneous","text":"<p>9.1 This Agreement and all disputes, claims, actions, suits or other proceedings arising out of this agreement or relating in any way to it shall be governed by the laws of United States excluding its private international law provisions.</p> <p>9.2 This Agreement sets out the entire agreement between You and Us for Your Contributions to Us and overrides all other agreements or understandings.</p> <p>9.3 In case of Your death, this agreement shall continue with Your heirs. In case of more than one heir, all heirs must exercise their rights through a commonly authorized person.</p> <p>9.4 If any provision of this Agreement is found void and unenforceable, such provision will be replaced to the extent possible with a provision that comes closest to the meaning of the original provision and that is enforceable. The terms and conditions set forth in this Agreement shall apply notwithstanding any failure of essential purpose of this Agreement or any limited remedy to the maximum extent possible under law.</p> <p>9.5 You agree to notify Us of any facts or circumstances of which you become aware that would make this Agreement inaccurate in any respect.</p>"},{"location":"cla/#recreate-this-contributor-license-agreement_1","title":"Recreate this Contributor License Agreement","text":"<p>https://contributoragreements.org/u2s/2kw48zbt5d</p> <p></p> <p></p>"},{"location":"common/","title":"common module","text":"<p>The common module contains common functions and classes used by the other modules.</p>"},{"location":"common/#hypercoast.common.convert_coords","title":"<code>convert_coords(coords, from_epsg, to_epsg)</code>","text":"<p>Convert a list of coordinates from one EPSG to another.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>List[Tuple[float, float]]</code> <p>List of tuples containing coordinates in the format (latitude, longitude).</p> required <code>from_epsg</code> <code>str</code> <p>Source EPSG code (default is \"epsg:4326\").</p> required <code>to_epsg</code> <code>str</code> <p>Target EPSG code (default is \"epsg:32615\").</p> required <p>Returns:</p> Type Description <code>List[Tuple[float, float]]</code> <p>List of tuples containing converted coordinates in the format (x, y).</p> Source code in <code>hypercoast/common.py</code> <pre><code>def convert_coords(\n    coords: List[Tuple[float, float]], from_epsg: str, to_epsg: str\n) -&gt; List[Tuple[float, float]]:\n    \"\"\"\n    Convert a list of coordinates from one EPSG to another.\n\n    Args:\n        coords: List of tuples containing coordinates in the format (latitude, longitude).\n        from_epsg: Source EPSG code (default is \"epsg:4326\").\n        to_epsg: Target EPSG code (default is \"epsg:32615\").\n\n    Returns:\n        List of tuples containing converted coordinates in the format (x, y).\n    \"\"\"\n    import pyproj\n\n    # Define the coordinate transformation\n    transformer = pyproj.Transformer.from_crs(from_epsg, to_epsg, always_xy=True)\n\n    # Convert each coordinate\n    converted_coords = [transformer.transform(lon, lat) for lat, lon in coords]\n\n    return converted_coords\n</code></pre>"},{"location":"common/#hypercoast.common.download_acolite","title":"<code>download_acolite(outdir='.', platform=None)</code>","text":"<p>Downloads the Acolite release based on the OS platform and extracts it to the specified directory. For more information, see the Acolite manual https://github.com/acolite/acolite/releases.</p> <p>Parameters:</p> Name Type Description Default <code>outdir</code> <code>str</code> <p>The output directory where the file will be Acolite and extracted.</p> <code>'.'</code> <code>platform</code> <code>Optional[str]</code> <p>The platform for which to download acolite. If None, the current system platform is used.                       Valid values are 'linux', 'darwin', and 'windows'.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the extracted Acolite directory.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If the platform is unsupported or the download fails.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def download_acolite(outdir: str = \".\", platform: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Downloads the Acolite release based on the OS platform and extracts it to the specified directory.\n    For more information, see the Acolite manual https://github.com/acolite/acolite/releases.\n\n    Args:\n        outdir (str): The output directory where the file will be Acolite and extracted.\n        platform (Optional[str]): The platform for which to download acolite. If None, the current system platform is used.\n                                  Valid values are 'linux', 'darwin', and 'windows'.\n\n    Returns:\n        str: The path to the extracted Acolite directory.\n\n    Raises:\n        Exception: If the platform is unsupported or the download fails.\n    \"\"\"\n    import platform as pf\n    import requests\n    import tarfile\n    from tqdm import tqdm\n\n    base_url = \"https://github.com/acolite/acolite/releases/download/20231023.0/\"\n\n    if platform is None:\n        platform = pf.system().lower()\n    else:\n        platform = platform.lower()\n\n    if platform == \"linux\":\n        download_url = base_url + \"acolite_py_linux_20231023.0.tar.gz\"\n        root_dir = \"acolite_py_linux\"\n    elif platform == \"darwin\":\n        download_url = base_url + \"acolite_py_mac_20231023.0.tar.gz\"\n        root_dir = \"acolite_py_mac\"\n    elif platform == \"windows\":\n        download_url = base_url + \"acolite_py_win_20231023.0.tar.gz\"\n        root_dir = \"acolite_py_win\"\n    else:\n        print(f\"Unsupported OS platform: {platform}\")\n        return\n\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n\n    extracted_path = os.path.join(outdir, root_dir)\n    file_name = os.path.join(outdir, download_url.split(\"/\")[-1])\n\n    if os.path.exists(file_name):\n        print(f\"{file_name} already exists. Skip downloading.\")\n        return extracted_path\n\n    response = requests.get(download_url, stream=True, timeout=60)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 8192\n\n    if response.status_code == 200:\n        with (\n            open(file_name, \"wb\") as file,\n            tqdm(\n                desc=file_name,\n                total=total_size,\n                unit=\"iB\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as bar,\n        ):\n            for chunk in response.iter_content(chunk_size=block_size):\n                if chunk:\n                    file.write(chunk)\n                    bar.update(len(chunk))\n        print(f\"Downloaded {file_name}\")\n    else:\n        print(f\"Failed to download file from {download_url}\")\n        return\n\n    # Unzip the file\n    with tarfile.open(file_name, \"r:gz\") as tar:\n        tar.extractall(path=outdir)\n\n    print(f\"Extracted to {extracted_path}\")\n    return extracted_path\n</code></pre>"},{"location":"common/#hypercoast.common.download_ecostress","title":"<code>download_ecostress(granules, out_dir=None, threads=8)</code>","text":"<p>Downloads NASA ECOSTRESS granules.</p> <p>Parameters:</p> Name Type Description Default <code>granules</code> <code>List[dict]</code> <p>The granules to download.</p> required <code>out_dir</code> <code>str</code> <p>The output directory where the granules will be downloaded. Defaults to None (current directory).</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to use for downloading. Defaults to 8.</p> <code>8</code> Source code in <code>hypercoast/common.py</code> <pre><code>def download_ecostress(\n    granules: List[dict],\n    out_dir: Optional[str] = None,\n    threads: int = 8,\n) -&gt; None:\n    \"\"\"Downloads NASA ECOSTRESS granules.\n\n    Args:\n        granules (List[dict]): The granules to download.\n        out_dir (str, optional): The output directory where the granules will be\n            downloaded. Defaults to None (current directory).\n        threads (int, optional): The number of threads to use for downloading.\n            Defaults to 8.\n    \"\"\"\n\n    download_nasa_data(granules=granules, out_dir=out_dir, threads=threads)\n</code></pre>"},{"location":"common/#hypercoast.common.download_emit","title":"<code>download_emit(granules, out_dir=None, threads=8)</code>","text":"<p>Downloads NASA EMIT granules.</p> <p>Parameters:</p> Name Type Description Default <code>granules</code> <code>List[dict]</code> <p>The granules to download.</p> required <code>out_dir</code> <code>str</code> <p>The output directory where the granules will be downloaded. Defaults to None (current directory).</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to use for downloading. Defaults to 8.</p> <code>8</code> Source code in <code>hypercoast/common.py</code> <pre><code>def download_emit(\n    granules: List[dict],\n    out_dir: Optional[str] = None,\n    threads: int = 8,\n) -&gt; None:\n    \"\"\"Downloads NASA EMIT granules.\n\n    Args:\n        granules (List[dict]): The granules to download.\n        out_dir (str, optional): The output directory where the granules will be\n            downloaded. Defaults to None (current directory).\n        threads (int, optional): The number of threads to use for downloading.\n            Defaults to 8.\n    \"\"\"\n\n    download_nasa_data(granules=granules, out_dir=out_dir, threads=threads)\n</code></pre>"},{"location":"common/#hypercoast.common.download_file","title":"<code>download_file(url=None, output=None, quiet=True, proxy=None, speed=None, use_cookies=True, verify=True, uid=None, fuzzy=False, resume=False, unzip=True, overwrite=False, subfolder=False)</code>","text":"<p>Download a file from URL, including Google Drive shared URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Google Drive URL is also supported. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>Output filename. Default is basename of URL.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress terminal output. Default is True.</p> <code>True</code> <code>proxy</code> <code>str</code> <p>Proxy. Defaults to None.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.</p> <code>None</code> <code>use_cookies</code> <code>bool</code> <p>Flag to use cookies. Defaults to True.</p> <code>True</code> <code>verify</code> <code>bool | str</code> <p>Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.</p> <code>True</code> <code>uid</code> <code>str</code> <p>Google Drive's file ID. Defaults to None.</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Fuzzy extraction of Google Drive's file Id. Defaults to False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Resume the download from existing tmp file if possible. Defaults to False.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Unzip the file. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <code>subfolder</code> <code>bool</code> <p>Create a subfolder with the same name as the file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The output file path.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def download_file(\n    url: Optional[str] = None,\n    output: Optional[str] = None,\n    quiet: Optional[bool] = True,\n    proxy: Optional[str] = None,\n    speed: Optional[float] = None,\n    use_cookies: Optional[bool] = True,\n    verify: Optional[bool] = True,\n    uid: Optional[str] = None,\n    fuzzy: Optional[bool] = False,\n    resume: Optional[bool] = False,\n    unzip: Optional[bool] = True,\n    overwrite: Optional[bool] = False,\n    subfolder: Optional[bool] = False,\n) -&gt; str:\n    \"\"\"Download a file from URL, including Google Drive shared URL.\n\n    Args:\n        url (str, optional): Google Drive URL is also supported. Defaults to None.\n        output (str, optional): Output filename. Default is basename of URL.\n        quiet (bool, optional): Suppress terminal output. Default is True.\n        proxy (str, optional): Proxy. Defaults to None.\n        speed (float, optional): Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.\n        use_cookies (bool, optional): Flag to use cookies. Defaults to True.\n        verify (bool | str, optional): Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string,\n            in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.\n        uid (str, optional): Google Drive's file ID. Defaults to None.\n        fuzzy (bool, optional): Fuzzy extraction of Google Drive's file Id. Defaults to False.\n        resume (bool, optional): Resume the download from existing tmp file if possible. Defaults to False.\n        unzip (bool, optional): Unzip the file. Defaults to True.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n        subfolder (bool, optional): Create a subfolder with the same name as the file. Defaults to False.\n\n    Returns:\n        str: The output file path.\n    \"\"\"\n    import zipfile\n    import tarfile\n    import gdown\n\n    if output is None:\n        if isinstance(url, str) and url.startswith(\"http\"):\n            output = os.path.basename(url)\n\n    out_dir = os.path.abspath(os.path.dirname(output))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(url, str):\n        if os.path.exists(os.path.abspath(output)) and (not overwrite):\n            print(\n                f\"{output} already exists. Skip downloading. Set overwrite=True to overwrite.\"\n            )\n            return os.path.abspath(output)\n        else:\n            url = github_raw_url(url)\n\n    if \"https://drive.google.com/file/d/\" in url:\n        fuzzy = True\n\n    output = gdown.download(\n        url, output, quiet, proxy, speed, use_cookies, verify, uid, fuzzy, resume\n    )\n\n    if unzip:\n        if output.endswith(\".zip\"):\n            with zipfile.ZipFile(output, \"r\") as zip_ref:\n                if not quiet:\n                    print(\"Extracting files...\")\n                if subfolder:\n                    basename = os.path.splitext(os.path.basename(output))[0]\n\n                    output = os.path.join(out_dir, basename)\n                    if not os.path.exists(output):\n                        os.makedirs(output)\n                    zip_ref.extractall(output)\n                else:\n                    zip_ref.extractall(os.path.dirname(output))\n        elif output.endswith(\".tar.gz\") or output.endswith(\".tar\"):\n            if output.endswith(\".tar.gz\"):\n                mode = \"r:gz\"\n            else:\n                mode = \"r\"\n\n            with tarfile.open(output, mode) as tar_ref:\n                if not quiet:\n                    print(\"Extracting files...\")\n                if subfolder:\n                    basename = os.path.splitext(os.path.basename(output))[0]\n                    output = os.path.join(out_dir, basename)\n                    if not os.path.exists(output):\n                        os.makedirs(output)\n                    tar_ref.extractall(output)\n                else:\n                    tar_ref.extractall(os.path.dirname(output))\n\n    return os.path.abspath(output)\n</code></pre>"},{"location":"common/#hypercoast.common.download_nasa_data","title":"<code>download_nasa_data(granules, out_dir=None, provider=None, threads=8)</code>","text":"<p>Downloads NASA Earthdata granules.</p> <p>Parameters:</p> Name Type Description Default <code>granules</code> <code>List[dict]</code> <p>The granules to download.</p> required <code>out_dir</code> <code>str</code> <p>The output directory where the granules will be downloaded. Defaults to None (current directory).</p> <code>None</code> <code>provider</code> <code>str</code> <p>The provider of the granules.</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to use for downloading. Defaults to 8.</p> <code>8</code> Source code in <code>hypercoast/common.py</code> <pre><code>def download_nasa_data(\n    granules: List[dict],\n    out_dir: Optional[str] = None,\n    provider: Optional[str] = None,\n    threads: int = 8,\n) -&gt; None:\n    \"\"\"Downloads NASA Earthdata granules.\n\n    Args:\n        granules (List[dict]): The granules to download.\n        out_dir (str, optional): The output directory where the granules will be downloaded. Defaults to None (current directory).\n        provider (str, optional): The provider of the granules.\n        threads (int, optional): The number of threads to use for downloading. Defaults to 8.\n    \"\"\"\n\n    leafmap.nasa_data_download(\n        granules=granules, out_dir=out_dir, provider=provider, threads=threads\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.download_pace","title":"<code>download_pace(granules, out_dir=None, threads=8)</code>","text":"<p>Downloads NASA PACE granules.</p> <p>Parameters:</p> Name Type Description Default <code>granules</code> <code>List[dict]</code> <p>The granules to download.</p> required <code>out_dir</code> <code>str</code> <p>The output directory where the granules will be downloaded. Defaults to None (current directory).</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to use for downloading. Defaults to 8.</p> <code>8</code> Source code in <code>hypercoast/common.py</code> <pre><code>def download_pace(\n    granules: List[dict],\n    out_dir: Optional[str] = None,\n    threads: int = 8,\n) -&gt; None:\n    \"\"\"Downloads NASA PACE granules.\n\n    Args:\n        granules (List[dict]): The granules to download.\n        out_dir (str, optional): The output directory where the granules will be\n            downloaded. Defaults to None (current directory).\n        threads (int, optional): The number of threads to use for downloading.\n            Defaults to 8.\n    \"\"\"\n\n    download_nasa_data(granules=granules, out_dir=out_dir, threads=threads)\n</code></pre>"},{"location":"common/#hypercoast.common.extract_date_from_filename","title":"<code>extract_date_from_filename(filename)</code>","text":"<p>Extracts a date from a filename assuming the date is in 'YYYYMMDD' format.</p> <p>This function searches the filename for a sequence of 8 digits that represent a date in 'YYYYMMDD' format. If such a sequence is found, it converts the sequence into a pandas Timestamp object. If no such sequence is found, the function returns None.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename from which to extract the date.</p> required <p>Returns:</p> Type Description <code>Optional[pd.Timestamp]</code> <p>A pandas Timestamp object representing the date found in the filename, or None if no date in 'YYYYMMDD' format is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extract_date_from_filename(\"example_20230101.txt\")\nTimestamp('2023-01-01 00:00:00')\n</code></pre> <pre><code>&gt;&gt;&gt; extract_date_from_filename(\"no_date_in_this_filename.txt\")\nNone\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def extract_date_from_filename(filename: str):\n    \"\"\"\n    Extracts a date from a filename assuming the date is in 'YYYYMMDD' format.\n\n    This function searches the filename for a sequence of 8 digits that represent a date in\n    'YYYYMMDD' format. If such a sequence is found, it converts the sequence into a pandas\n    Timestamp object. If no such sequence is found, the function returns None.\n\n    Args:\n        filename (str): The filename from which to extract the date.\n\n    Returns:\n        Optional[pd.Timestamp]: A pandas Timestamp object representing the date found in the filename,\n        or None if no date in 'YYYYMMDD' format is found.\n\n    Examples:\n        &gt;&gt;&gt; extract_date_from_filename(\"example_20230101.txt\")\n        Timestamp('2023-01-01 00:00:00')\n\n        &gt;&gt;&gt; extract_date_from_filename(\"no_date_in_this_filename.txt\")\n        None\n    \"\"\"\n    import re\n    import pandas as pd\n\n    # Assuming the date format in filename is 'YYYYMMDD'\n    date_match = re.search(r\"\\d{8}\", filename)\n    if date_match:\n        return pd.to_datetime(date_match.group(), format=\"%Y%m%d\")\n    else:\n        return None\n</code></pre>"},{"location":"common/#hypercoast.common.extract_spectral","title":"<code>extract_spectral(ds, lat, lon, name='data')</code>","text":"<p>Extracts spectral signature from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>The dataset containing the spectral data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted data.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def extract_spectral(\n    ds: xr.Dataset, lat: float, lon: float, name: str = \"data\"\n) -&gt; xr.DataArray:\n    \"\"\"\n    Extracts spectral signature from a given xarray Dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the spectral data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n\n    Returns:\n        xarray.DataArray: The extracted data.\n    \"\"\"\n\n    crs = ds.rio.crs\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    values = ds.sel(x=x, y=y, method=\"nearest\")[name].values\n\n    da = xr.DataArray(values, dims=[\"band\"], coords={\"band\": ds.coords[\"band\"]})\n\n    return da\n</code></pre>"},{"location":"common/#hypercoast.common.github_raw_url","title":"<code>github_raw_url(url)</code>","text":"<p>Get the raw URL for a GitHub file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The GitHub URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The raw URL.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def github_raw_url(url: str) -&gt; str:\n    \"\"\"Get the raw URL for a GitHub file.\n\n    Args:\n        url (str): The GitHub URL.\n    Returns:\n        str: The raw URL.\n    \"\"\"\n    if isinstance(url, str) and url.startswith(\"https://github.com/\") and \"blob\" in url:\n        url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n    return url\n</code></pre>"},{"location":"common/#hypercoast.common.image_cube","title":"<code>image_cube(dataset, variable='reflectance', cmap='jet', clim=(0, 0.5), title='Reflectance', rgb_bands=None, rgb_wavelengths=None, rgb_gamma=1.0, rgb_cmap=None, rgb_clim=None, rgb_args=None, widget=None, plotter_args=None, show_axes=True, grid_origin=(0, 0, 0), grid_spacing=(1, 1, 1), **kwargs)</code>","text":"<p>Creates an image cube from a dataset and plots it using PyVista.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[str, xr.Dataset]</code> <p>The dataset to plot. Can be a path to a NetCDF file or an xarray Dataset.</p> required <code>variable</code> <code>str</code> <p>The variable to plot. Defaults to \"reflectance\".</p> <code>'reflectance'</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"jet\".</p> <code>'jet'</code> <code>clim</code> <code>Tuple[float, float]</code> <p>The color limits. Defaults to (0, 0.5).</p> <code>(0, 0.5)</code> <code>title</code> <code>str</code> <p>The title for the scalar bar. Defaults to \"Reflectance\".</p> <code>'Reflectance'</code> <code>rgb_bands</code> <code>Optional[List[int]]</code> <p>The bands to use for the RGB image. Defaults to None.</p> <code>None</code> <code>rgb_wavelengths</code> <code>Optional[List[float]]</code> <p>The wavelengths to use for the RGB image. Defaults to None.</p> <code>None</code> <code>rgb_gamma</code> <code>float</code> <p>The gamma correction for the RGB image. Defaults to 1.</p> <code>1.0</code> <code>rgb_cmap</code> <code>Optional[str]</code> <p>The colormap to use for the RGB image. Defaults to None.</p> <code>None</code> <code>rgb_clim</code> <code>Optional[Tuple[float, float]]</code> <p>The color limits for the RGB image. Defaults to None.</p> <code>None</code> <code>rgb_args</code> <code>Dict[str, Any]</code> <p>Additional arguments for the <code>add_mesh</code> method for the RGB image. Defaults to {}.</p> <code>None</code> <code>widget</code> <code>Optional[str]</code> <p>The widget to use for the image cube. Can be one of the following: \"box\", \"plane\", \"slice\", \"orthogonal\", and \"threshold\". Defaults to None.</p> <code>None</code> <code>plotter_args</code> <code>Dict[str, Any]</code> <p>Additional arguments for the <code>pv.Plotter</code> constructor. Defaults to {}.</p> <code>None</code> <code>show_axes</code> <code>bool</code> <p>Whether to show the axes. Defaults to True.</p> <code>True</code> <code>grid_origin</code> <code>Tuple[float, float, float]</code> <p>The origin of the grid. Defaults to (0, 0, 0).</p> <code>(0, 0, 0)</code> <code>grid_spacing</code> <code>Tuple[float, float, float]</code> <p>The spacing of the grid.</p> <code>(1, 1, 1)</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments for the <code>add_mesh</code> method. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pv.Plotter</code> <p>The PyVista Plotter with the image cube added.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def image_cube(\n    dataset,\n    variable: str = \"reflectance\",\n    cmap: str = \"jet\",\n    clim: Tuple[float, float] = (0, 0.5),\n    title: str = \"Reflectance\",\n    rgb_bands: Optional[List[int]] = None,\n    rgb_wavelengths: Optional[List[float]] = None,\n    rgb_gamma: float = 1.0,\n    rgb_cmap: Optional[str] = None,\n    rgb_clim: Optional[Tuple[float, float]] = None,\n    rgb_args: Dict[str, Any] = None,\n    widget=None,\n    plotter_args: Dict[str, Any] = None,\n    show_axes: bool = True,\n    grid_origin=(0, 0, 0),\n    grid_spacing=(1, 1, 1),\n    **kwargs: Any,\n):\n    \"\"\"\n    Creates an image cube from a dataset and plots it using PyVista.\n\n    Args:\n        dataset (Union[str, xr.Dataset]): The dataset to plot. Can be a path to\n            a NetCDF file or an xarray Dataset.\n        variable (str, optional): The variable to plot. Defaults to \"reflectance\".\n        cmap (str, optional): The colormap to use. Defaults to \"jet\".\n        clim (Tuple[float, float], optional): The color limits. Defaults to (0, 0.5).\n        title (str, optional): The title for the scalar bar. Defaults to \"Reflectance\".\n        rgb_bands (Optional[List[int]], optional): The bands to use for the RGB\n            image. Defaults to None.\n        rgb_wavelengths (Optional[List[float]], optional): The wavelengths to\n            use for the RGB image. Defaults to None.\n        rgb_gamma (float, optional): The gamma correction for the RGB image.\n            Defaults to 1.\n        rgb_cmap (Optional[str], optional): The colormap to use for the RGB image.\n            Defaults to None.\n        rgb_clim (Optional[Tuple[float, float]], optional): The color limits for\n            the RGB image. Defaults to None.\n        rgb_args (Dict[str, Any], optional): Additional arguments for the\n            `add_mesh` method for the RGB image. Defaults to {}.\n        widget (Optional[str], optional): The widget to use for the image cube.\n            Can be one of the following: \"box\", \"plane\", \"slice\", \"orthogonal\",\n            and \"threshold\". Defaults to None.\n        plotter_args (Dict[str, Any], optional): Additional arguments for the\n            `pv.Plotter` constructor. Defaults to {}.\n        show_axes (bool, optional): Whether to show the axes. Defaults to True.\n        grid_origin (Tuple[float, float, float], optional): The origin of the grid.\n            Defaults to (0, 0, 0).\n        grid_spacing (Tuple[float, float, float], optional): The spacing of the grid.\n        **kwargs (Dict[str, Any], optional): Additional arguments for the\n            `add_mesh` method. Defaults to {}.\n\n    Returns:\n        pv.Plotter: The PyVista Plotter with the image cube added.\n    \"\"\"\n\n    import pyvista as pv\n\n    if rgb_args is None:\n        rgb_args = {}\n\n    if plotter_args is None:\n        plotter_args = {}\n\n    allowed_widgets = [\"box\", \"plane\", \"slice\", \"orthogonal\", \"threshold\"]\n\n    if widget is not None:\n        if widget not in allowed_widgets:\n            raise ValueError(f\"widget must be one of the following: {allowed_widgets}\")\n\n    if isinstance(dataset, str):\n        dataset = xr.open_dataset(dataset)\n\n    da = dataset[variable]  # xarray DataArray\n    values = da.to_numpy()\n\n    # Create the spatial reference for the image cube\n    grid = pv.ImageData()\n\n    # Set the grid dimensions: shape because we want to inject our values on the POINT data\n    grid.dimensions = values.shape\n\n    # Edit the spatial reference\n    grid.origin = grid_origin  # The bottom left corner of the data set\n    grid.spacing = grid_spacing  # These are the cell sizes along each axis\n\n    # Add the data values to the cell data\n    grid.point_data[\"values\"] = values.flatten(order=\"F\")  # Flatten the array\n\n    # Plot the image cube with the RGB image overlay\n    p = pv.Plotter(**plotter_args)\n\n    if \"scalar_bar_args\" not in kwargs:\n        kwargs[\"scalar_bar_args\"] = {\"title\": title}\n    else:\n        kwargs[\"scalar_bar_args\"][\"title\"] = title\n\n    if \"show_edges\" not in kwargs:\n        kwargs[\"show_edges\"] = False\n\n    if widget == \"box\":\n        p.add_mesh_clip_box(grid, cmap=cmap, clim=clim, **kwargs)\n    elif widget == \"plane\":\n        if \"normal\" not in kwargs:\n            kwargs[\"normal\"] = (0, 0, 1)\n        if \"invert\" not in kwargs:\n            kwargs[\"invert\"] = True\n        if \"normal_rotation\" not in kwargs:\n            kwargs[\"normal_rotation\"] = False\n        p.add_mesh_clip_plane(grid, cmap=cmap, clim=clim, **kwargs)\n    elif widget == \"slice\":\n        if \"normal\" not in kwargs:\n            kwargs[\"normal\"] = (0, 0, 1)\n        if \"normal_rotation\" not in kwargs:\n            kwargs[\"normal_rotation\"] = False\n        p.add_mesh_slice(grid, cmap=cmap, clim=clim, **kwargs)\n    elif widget == \"orthogonal\":\n        p.add_mesh_slice_orthogonal(grid, cmap=cmap, clim=clim, **kwargs)\n    elif widget == \"threshold\":\n        p.add_mesh_threshold(grid, cmap=cmap, clim=clim, **kwargs)\n    else:\n        p.add_mesh(grid, cmap=cmap, clim=clim, **kwargs)\n\n    if rgb_bands is not None or rgb_wavelengths is not None:\n\n        if rgb_bands is not None:\n            rgb_image = dataset.isel(wavelength=rgb_bands, method=\"nearest\")[\n                variable\n            ].to_numpy()\n        elif rgb_wavelengths is not None:\n            rgb_image = dataset.sel(wavelength=rgb_wavelengths, method=\"nearest\")[\n                variable\n            ].to_numpy()\n\n        x_dim, y_dim = rgb_image.shape[0], rgb_image.shape[1]\n        z_dim = 1\n        im = pv.ImageData(dimensions=(x_dim, y_dim, z_dim))\n\n        # Add scalar data, you may also need to flatten this\n        im.point_data[\"rgb_image\"] = (\n            rgb_image.reshape(-1, rgb_image.shape[2], order=\"F\") * rgb_gamma\n        )\n\n        grid_z_max = grid.bounds[5]\n        im.origin = (0, 0, grid_z_max)\n\n        if rgb_image.shape[2] &lt; 3:\n            if rgb_cmap is None:\n                rgb_cmap = cmap\n            if rgb_clim is None:\n                rgb_clim = clim\n\n            if \"cmap\" not in rgb_args:\n                rgb_args[\"cmap\"] = rgb_cmap\n            if \"clim\" not in rgb_args:\n                rgb_args[\"clim\"] = rgb_clim\n        else:\n            if \"rgb\" not in rgb_args:\n                rgb_args[\"rgb\"] = True\n\n        if \"show_scalar_bar\" not in rgb_args:\n            rgb_args[\"show_scalar_bar\"] = False\n        if \"show_edges\" not in rgb_args:\n            rgb_args[\"show_edges\"] = False\n\n        p.add_mesh(im, **rgb_args)\n\n    if show_axes:\n        p.show_axes()\n\n    return p\n</code></pre>"},{"location":"common/#hypercoast.common.nasa_earth_login","title":"<code>nasa_earth_login(strategy='all', persist=True, **kwargs)</code>","text":"<p>Logs in to NASA Earthdata.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>The login strategy. Defaults to \"all\".</p> <code>'all'</code> <code>persist</code> <code>bool</code> <p>Whether to persist the login. Defaults to True.</p> <code>True</code> Source code in <code>hypercoast/common.py</code> <pre><code>def nasa_earth_login(strategy: str = \"all\", persist: bool = True, **kwargs) -&gt; None:\n    \"\"\"Logs in to NASA Earthdata.\n\n    Args:\n        strategy (str, optional): The login strategy. Defaults to \"all\".\n        persist (bool, optional): Whether to persist the login. Defaults to True.\n    \"\"\"\n    from leafmap import get_api_key\n    import earthaccess\n\n    USERNAME = get_api_key(\"EARTHDATA_USERNAME\")\n    PASSWORD = get_api_key(\"EARTHDATA_PASSWORD\")\n    if (USERNAME is not None) and (PASSWORD is not None):\n        strategy = \"environment\"\n    earthaccess.login(strategy=strategy, persist=persist, **kwargs)\n</code></pre>"},{"location":"common/#hypercoast.common.netcdf_groups","title":"<code>netcdf_groups(filepath)</code>","text":"<p>Get the list of groups in a NetCDF file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the NetCDF file.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of group names in the NetCDF file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; netcdf_groups('path/to/netcdf/file')\n['group1', 'group2', 'group3']\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def netcdf_groups(filepath: str) -&gt; List[str]:\n    \"\"\"\n    Get the list of groups in a NetCDF file.\n\n    Args:\n        filepath (str): The path to the NetCDF file.\n\n    Returns:\n        list: A list of group names in the NetCDF file.\n\n    Example:\n        &gt;&gt;&gt; netcdf_groups('path/to/netcdf/file')\n        ['group1', 'group2', 'group3']\n    \"\"\"\n    import h5netcdf\n\n    with h5netcdf.File(filepath) as file:\n        groups = list(file)\n    return groups\n</code></pre>"},{"location":"common/#hypercoast.common.open_dataset","title":"<code>open_dataset(filename, engine=None, chunks=None, **kwargs)</code>","text":"<p>Opens and returns an xarray Dataset from a file.</p> <p>This function is a wrapper around <code>xarray.open_dataset</code> that allows for additional customization through keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file to open.</p> required <code>engine</code> <code>Optional[str]</code> <p>Name of the engine to use for reading the file. If None, xarray's default engine is used. Examples include 'netcdf4', 'h5netcdf', 'zarr', etc.</p> <code>None</code> <code>chunks</code> <code>Optional[Dict[str, int]]</code> <p>Dictionary specifying how to chunk the dataset along each dimension. For example, <code>{'time': 1}</code> would load the dataset in single-time-step chunks. If None, the dataset is not chunked.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>xarray.open_dataset</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The opened dataset.</p> <p>Examples:</p> <p>Open a NetCDF file without chunking:</p> <pre><code>&gt;&gt;&gt; dataset = open_dataset('path/to/file.nc')\n</code></pre> <p>Open a Zarr dataset, chunking along the 'time' dimension:</p> <pre><code>&gt;&gt;&gt; dataset = open_dataset('path/to/dataset.zarr', engine='zarr', chunks={'time': 10})\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def open_dataset(\n    filename: str,\n    engine: Optional[str] = None,\n    chunks: Optional[Dict[str, int]] = None,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Opens and returns an xarray Dataset from a file.\n\n    This function is a wrapper around `xarray.open_dataset` that allows for additional\n    customization through keyword arguments.\n\n    Args:\n        filename (str): Path to the file to open.\n        engine (Optional[str]): Name of the engine to use for reading the file. If None, xarray's\n            default engine is used. Examples include 'netcdf4', 'h5netcdf', 'zarr', etc.\n        chunks (Optional[Dict[str, int]]): Dictionary specifying how to chunk the dataset along each dimension.\n            For example, `{'time': 1}` would load the dataset in single-time-step chunks. If None,\n            the dataset is not chunked.\n        **kwargs: Additional keyword arguments passed to `xarray.open_dataset`.\n\n    Returns:\n        xr.Dataset: The opened dataset.\n\n    Examples:\n        Open a NetCDF file without chunking:\n        &gt;&gt;&gt; dataset = open_dataset('path/to/file.nc')\n\n        Open a Zarr dataset, chunking along the 'time' dimension:\n        &gt;&gt;&gt; dataset = open_dataset('path/to/dataset.zarr', engine='zarr', chunks={'time': 10})\n    \"\"\"\n\n    try:\n        dataset = xr.open_dataset(filename, engine=engine, chunks=chunks, **kwargs)\n    except OSError:\n        dataset = xr.open_dataset(filename, engine=\"h5netcdf\", chunks=chunks, **kwargs)\n\n    return dataset\n</code></pre>"},{"location":"common/#hypercoast.common.pca","title":"<code>pca(input_file, output_file, n_components=3, **kwargs)</code>","text":"<p>Performs Principal Component Analysis (PCA) on a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The input file containing the data to analyze.</p> required <code>output_file</code> <code>str</code> <p>The output file to save the PCA results.</p> required <code>n_components</code> <code>int</code> <p>The number of principal components to compute. Defaults to 3.</p> <code>3</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the scikit-learn PCA function. For more info, see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>This function does not return a value. It saves the PCA results to the output file.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def pca(input_file, output_file, n_components=3, **kwargs):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on a dataset.\n\n    Args:\n        input_file (str): The input file containing the data to analyze.\n        output_file (str): The output file to save the PCA results.\n        n_components (int, optional): The number of principal components to compute. Defaults to 3.\n        **kwargs: Additional keyword arguments to pass to the scikit-learn PCA function.\n            For more info, see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html.\n\n    Returns:\n        None: This function does not return a value. It saves the PCA results to the output file.\n    \"\"\"\n\n    import rasterio\n    from sklearn.decomposition import PCA\n\n    # Function to load the GeoTIFF image\n    def load_geotiff(file_path):\n        with rasterio.open(file_path) as src:\n            image = src.read()\n            profile = src.profile\n        return image, profile\n\n    # Function to perform PCA\n    def perform_pca(image, n_components=3, **kwargs):\n        # Reshape the image to [n_bands, n_pixels]\n        n_bands, width, height = image.shape\n        image_reshaped = image.reshape(n_bands, width * height).T\n\n        # Perform PCA\n        model = PCA(n_components=n_components, **kwargs)\n        principal_components = model.fit_transform(image_reshaped)\n\n        # Reshape the principal components back to image dimensions\n        pca_image = principal_components.T.reshape(n_components, width, height)\n        return pca_image\n\n    # Function to save the PCA-transformed image\n    def save_geotiff(file_path, image, profile):\n        profile.update(count=image.shape[0])\n        with rasterio.open(file_path, \"w\", **profile) as dst:\n            dst.write(image)\n\n    image, profile = load_geotiff(input_file)\n    pca_image = perform_pca(image, n_components, **kwargs)\n    save_geotiff(output_file, pca_image, profile)\n</code></pre>"},{"location":"common/#hypercoast.common.run_acolite","title":"<code>run_acolite(acolite_dir, settings_file=None, input_file=None, out_dir=None, polygon=None, l2w_parameters=None, rgb_rhot=True, rgb_rhos=True, map_l2w=True, verbose=True, **kwargs)</code>","text":"<p>Runs the Acolite software for atmospheric correction and water quality retrieval. For more information, see the Acolite manual https://github.com/acolite/acolite/releases</p> <p>This function constructs and executes a command to run the Acolite software with the specified parameters. It supports running Acolite with a settings file or with individual parameters specified directly. Additional parameters can be passed as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>acolite_dir</code> <code>str</code> <p>The directory where Acolite is installed.</p> required <code>settings_file</code> <code>Optional[str]</code> <p>The path to the Acolite settings file. If provided, other parameters except <code>verbose</code> are ignored. Defaults to None.</p> <code>None</code> <code>input_file</code> <code>Optional[str]</code> <p>The path to the input file for processing. Defaults to None.</p> <code>None</code> <code>out_dir</code> <code>Optional[str]</code> <p>The directory where output files will be saved. Defaults to None.</p> <code>None</code> <code>polygon</code> <code>Optional[str]</code> <p>The path to a polygon file for spatial subset. Defaults to None.</p> <code>None</code> <code>l2w_parameters</code> <code>Optional[str]</code> <p>Parameters for L2W processing. Defaults to None.</p> <code>None</code> <code>rgb_rhot</code> <code>bool</code> <p>Flag to generate RGB images using rhot. Defaults to True.</p> <code>True</code> <code>rgb_rhos</code> <code>bool</code> <p>Flag to generate RGB images using rhos. Defaults to True.</p> <code>True</code> <code>map_l2w</code> <code>bool</code> <p>Flag to map L2W products. Defaults to True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, prints the command output; otherwise, suppresses it. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional command line arguments to pass to acolite. Such as --l2w_export_geotiff, --merge_tiles, etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>This function does not return a value. It executes the Acolite software.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; run_acolite(\"/path/to/acolite\", input_file=\"/path/to/inputfile\", output=\"/path/to/output\")\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def run_acolite(\n    acolite_dir: str,\n    settings_file: Optional[str] = None,\n    input_file: Optional[str] = None,\n    out_dir: Optional[str] = None,\n    polygon: Optional[str] = None,\n    l2w_parameters: Optional[str] = None,\n    rgb_rhot: bool = True,\n    rgb_rhos: bool = True,\n    map_l2w: bool = True,\n    verbose: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Runs the Acolite software for atmospheric correction and water quality retrieval.\n    For more information, see the Acolite manual https://github.com/acolite/acolite/releases\n\n    This function constructs and executes a command to run the Acolite software with the specified\n    parameters. It supports running Acolite with a settings file or with individual parameters\n    specified directly. Additional parameters can be passed as keyword arguments.\n\n    Args:\n        acolite_dir (str): The directory where Acolite is installed.\n        settings_file (Optional[str], optional): The path to the Acolite settings file. If provided,\n            other parameters except `verbose` are ignored. Defaults to None.\n        input_file (Optional[str], optional): The path to the input file for processing. Defaults to None.\n        out_dir (Optional[str], optional): The directory where output files will be saved. Defaults to None.\n        polygon (Optional[str], optional): The path to a polygon file for spatial subset. Defaults to None.\n        l2w_parameters (Optional[str], optional): Parameters for L2W processing. Defaults to None.\n        rgb_rhot (bool, optional): Flag to generate RGB images using rhot. Defaults to True.\n        rgb_rhos (bool, optional): Flag to generate RGB images using rhos. Defaults to True.\n        map_l2w (bool, optional): Flag to map L2W products. Defaults to True.\n        verbose (bool, optional): If True, prints the command output; otherwise, suppresses it. Defaults to True.\n        **kwargs (Any): Additional command line arguments to pass to acolite. Such as\n            --l2w_export_geotiff, --merge_tiles, etc.\n\n    Returns:\n        None: This function does not return a value. It executes the Acolite software.\n\n    Example:\n        &gt;&gt;&gt; run_acolite(\"/path/to/acolite\", input_file=\"/path/to/inputfile\", output=\"/path/to/output\")\n    \"\"\"\n\n    import subprocess\n    from datetime import datetime\n\n    def get_formatted_current_time(format_str=\"%Y-%m-%d %H:%M:%S\"):\n        current_time = datetime.now()\n        formatted_time = current_time.strftime(format_str)\n        return formatted_time\n\n    acolite_dir_name = os.path.split(acolite_dir)[-1]\n    acolite_exe = \"acolite\"\n    if acolite_dir_name.endswith(\"win\"):\n        acolite_exe += \".exe\"\n\n    if isinstance(input_file, list):\n        input_file = \",\".join(input_file)\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    acolite_exe_path = os.path.join(acolite_dir, \"dist\", \"acolite\", acolite_exe)\n    acolite_exe_path = acolite_exe_path.replace(\"\\\\\", \"/\")\n\n    acolite_cmd = [acolite_exe_path, \"--cli\"]\n\n    if settings_file is not None:\n        acolite_cmd.extend([\"--settings\", settings_file])\n    else:\n        lines = []\n        lines.append(\"## ACOLITE settings\")\n        lines.append(f\"## Written at {get_formatted_current_time()}\")\n        if input_file is not None:\n            input_file = input_file.replace(\"\\\\\", \"/\")\n            lines.append(f\"inputfile={input_file}\")\n        if out_dir is not None:\n            out_dir = out_dir.replace(\"\\\\\", \"/\")\n            lines.append(f\"output={out_dir}\")\n        if polygon is not None:\n            lines.append(f\"polygon={polygon}\")\n        else:\n            lines.append(\"polygon=None\")\n        if l2w_parameters is not None:\n            lines.append(f\"l2w_parameters={l2w_parameters}\")\n        if rgb_rhot:\n            lines.append(\"rgb_rhot=True\")\n        else:\n            lines.append(\"rgb_rhot=False\")\n        if rgb_rhos:\n            lines.append(\"rgb_rhos=True\")\n        else:\n            lines.append(\"rgb_rhos=False\")\n        if map_l2w:\n            lines.append(\"map_l2w=True\")\n        else:\n            lines.append(\"map_l2w=False\")\n\n        for key, value in kwargs.items():\n            lines.append(f\"{key}={value}\")\n\n        lines.append(f\"runid={get_formatted_current_time('%Y%m%d_%H%M%S')}\")\n        settings_filename = f\"acolite_run_{get_formatted_current_time('%Y%m%d_%H%M%S')}_settings_user.txt\"\n        settings_file = os.path.join(out_dir, settings_filename).replace(\"\\\\\", \"/\")\n        with open(settings_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(lines))\n        acolite_cmd.extend([\"--settings\", settings_file])\n\n    if acolite_dir_name.endswith(\"win\"):\n        acolite_cmd = \" \".join(acolite_cmd)\n\n    if verbose:\n        subprocess.run(acolite_cmd, check=True)\n    else:\n        subprocess.run(\n            acolite_cmd,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n            check=True,\n        )\n</code></pre>"},{"location":"common/#hypercoast.common.search_datasets","title":"<code>search_datasets(count=-1, **kwargs)</code>","text":"<p>Searches for datasets using the EarthAccess API with optional filters.</p> <p>This function wraps the <code>earthaccess.search_datasets</code> function, allowing for customized search queries based on a count limit and additional keyword arguments which serve as filters for the search.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>The maximum number of datasets to return. A value of -1 indicates no limit. Defaults to -1.</p> <code>-1</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass as search filters to the EarthAccess API. keyword: case-insensitive and supports wildcards ? and * short_name: e.g. ATL08 doi: DOI for a dataset daac: e.g. NSIDC or PODAAC provider: particular to each DAAC, e.g. POCLOUD, LPDAAC etc. temporal: a tuple representing temporal bounds in the form (date_from, date_to) bounding_box: a tuple representing spatial bounds in the form (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat)</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, where each dictionary contains     information about a dataset found in the search.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; results = search_datasets(count=5, keyword='temperature')\n&gt;&gt;&gt; print(results)\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def search_datasets(count: int = -1, **kwargs: Any) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Searches for datasets using the EarthAccess API with optional filters.\n\n    This function wraps the `earthaccess.search_datasets` function, allowing for\n    customized search queries based on a count limit and additional keyword arguments\n    which serve as filters for the search.\n\n    Args:\n        count (int, optional): The maximum number of datasets to return. A value of -1\n            indicates no limit. Defaults to -1.\n        **kwargs (Any): Additional keyword arguments to pass as search filters to the\n            EarthAccess API.\n            keyword: case-insensitive and supports wildcards ? and *\n            short_name: e.g. ATL08\n            doi: DOI for a dataset\n            daac: e.g. NSIDC or PODAAC\n            provider: particular to each DAAC, e.g. POCLOUD, LPDAAC etc.\n            temporal: a tuple representing temporal bounds in the form (date_from, date_to)\n            bounding_box: a tuple representing spatial bounds in the form\n            (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat)\n\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, where each dictionary contains\n            information about a dataset found in the search.\n\n    Example:\n        &gt;&gt;&gt; results = search_datasets(count=5, keyword='temperature')\n        &gt;&gt;&gt; print(results)\n    \"\"\"\n\n    import earthaccess\n\n    return earthaccess.search_datasets(count=count, **kwargs)\n</code></pre>"},{"location":"common/#hypercoast.common.search_ecostress","title":"<code>search_ecostress(bbox=None, temporal=None, count=-1, short_name='ECO_L2T_LSTE', output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA ECOSTRESS granules.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset. Defaults to \"ECO_L2T_LSTE\".</p> <code>'ECO_L2T_LSTE'</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_ecostress(\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    count: int = -1,\n    short_name: Optional[str] = \"ECO_L2T_LSTE\",\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA ECOSTRESS granules.\n\n    Args:\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset. Defaults to \"ECO_L2T_LSTE\".\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    return search_nasa_data(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.search_emit","title":"<code>search_emit(bbox=None, temporal=None, count=-1, short_name='EMITL2ARFL', output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA EMIT granules.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset. Defaults to \"EMITL2ARFL\".</p> <code>'EMITL2ARFL'</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_emit(\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    count: int = -1,\n    short_name: Optional[str] = \"EMITL2ARFL\",\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA EMIT granules.\n\n    Args:\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset. Defaults to \"EMITL2ARFL\".\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    return search_nasa_data(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.search_nasa_data","title":"<code>search_nasa_data(count=-1, short_name=None, bbox=None, temporal=None, version=None, doi=None, daac=None, provider=None, output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA Earthdata granules.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset.</p> <code>None</code> <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>version</code> <code>str</code> <p>The version of the dataset.</p> <code>None</code> <code>doi</code> <code>str</code> <p>The Digital Object Identifier (DOI) of the dataset.</p> <code>None</code> <code>daac</code> <code>str</code> <p>The Distributed Active Archive Center (DAAC) of the dataset.</p> <code>None</code> <code>provider</code> <code>str</code> <p>The provider of the dataset.</p> <code>None</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_nasa_data(\n    count: int = -1,\n    short_name: Optional[str] = None,\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    version: Optional[str] = None,\n    doi: Optional[str] = None,\n    daac: Optional[str] = None,\n    provider: Optional[str] = None,\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA Earthdata granules.\n\n    Args:\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset.\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        version (str, optional): The version of the dataset.\n        doi (str, optional): The Digital Object Identifier (DOI) of the dataset.\n        daac (str, optional): The Distributed Active Archive Center (DAAC) of the dataset.\n        provider (str, optional): The provider of the dataset.\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    if isinstance(bbox, list):\n        bbox = tuple(bbox)\n\n    return leafmap.nasa_data_search(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        version=version,\n        doi=doi,\n        daac=daac,\n        provider=provider,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.search_pace","title":"<code>search_pace(bbox=None, temporal=None, count=-1, short_name='PACE_OCI_L2_AOP_NRT', output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA PACE granules.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset. Defaults to \"PACE_OCI_L2_AOP_NRT\".</p> <code>'PACE_OCI_L2_AOP_NRT'</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_pace(\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    count: int = -1,\n    short_name: Optional[str] = \"PACE_OCI_L2_AOP_NRT\",\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA PACE granules.\n\n    Args:\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset. Defaults to \"PACE_OCI_L2_AOP_NRT\".\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    return search_nasa_data(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.search_pace_chla","title":"<code>search_pace_chla(bbox=None, temporal=None, count=-1, short_name='PACE_OCI_L3M_CHL_NRT', granule_name='*.DAY.*.0p1deg.*', output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA PACE Chlorophyll granules.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset. Defaults to \"PACE_OCI_L3M_CHL_NRT\".</p> <code>'PACE_OCI_L3M_CHL_NRT'</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_pace_chla(\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    count: int = -1,\n    short_name: Optional[str] = \"PACE_OCI_L3M_CHL_NRT\",\n    granule_name: Optional[str] = \"*.DAY.*.0p1deg.*\",\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA PACE Chlorophyll granules.\n\n    Args:\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset. Defaults to \"PACE_OCI_L3M_CHL_NRT\".\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    return search_nasa_data(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        granule_name=granule_name,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.show_field_data","title":"<code>show_field_data(data, x_col='wavelength', y_col_prefix='(', x_label='Wavelengths (nm)', y_label='Reflectance', use_marker_cluster=True, min_width=400, max_width=600, min_height=200, max_height=250, layer_name='Marker Cluster', m=None, center=(20, 0), zoom=2)</code>","text":"<p>Displays field data on a map with interactive markers and popups showing time series data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, pd.DataFrame]</code> <p>Path to the CSV file or a pandas DataFrame containing the data.</p> required <code>x_col</code> <code>str</code> <p>Column name to use for the x-axis of the charts. Default is \"wavelength\".</p> <code>'wavelength'</code> <code>y_col_prefix</code> <code>str</code> <p>Prefix to identify the columns that contain the location-specific data. Default is \"(\".</p> <code>'('</code> <code>x_label</code> <code>str</code> <p>Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".</p> <code>'Wavelengths (nm)'</code> <code>y_label</code> <code>str</code> <p>Label for the y-axis of the charts. Default is \"Reflectance\".</p> <code>'Reflectance'</code> <code>use_marker_cluster</code> <code>bool</code> <p>Whether to use marker clustering. Default is True.</p> <code>True</code> <code>min_width</code> <code>int</code> <p>Minimum width of the popup. Default is 400.</p> <code>400</code> <code>max_width</code> <code>int</code> <p>Maximum width of the popup. Default is 600.</p> <code>600</code> <code>min_height</code> <code>int</code> <p>Minimum height of the popup. Default is 200.</p> <code>200</code> <code>max_height</code> <code>int</code> <p>Maximum height of the popup. Default is 250.</p> <code>250</code> <code>layer_name</code> <code>str</code> <p>Name of the marker cluster layer. Default is \"Marker Cluster\".</p> <code>'Marker Cluster'</code> <code>m</code> <code>Map</code> <p>An ipyleaflet Map instance to add the markers to. Default is None.</p> <code>None</code> <code>center</code> <code>Tuple[float, float]</code> <p>Center of the map as a tuple of (latitude, longitude). Default is (20, 0).</p> <code>(20, 0)</code> <code>zoom</code> <code>int</code> <p>Zoom level of the map. Default is 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>Map</code> <p>An ipyleaflet Map with the added markers and popups.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def show_field_data(\n    data: Union[str],\n    x_col: str = \"wavelength\",\n    y_col_prefix: str = \"(\",\n    x_label: str = \"Wavelengths (nm)\",\n    y_label: str = \"Reflectance\",\n    use_marker_cluster: bool = True,\n    min_width: int = 400,\n    max_width: int = 600,\n    min_height: int = 200,\n    max_height: int = 250,\n    layer_name: str = \"Marker Cluster\",\n    m: object = None,\n    center: Tuple[float, float] = (20, 0),\n    zoom: int = 2,\n):\n    \"\"\"\n    Displays field data on a map with interactive markers and popups showing time series data.\n\n    Args:\n        data (Union[str, pd.DataFrame]): Path to the CSV file or a pandas DataFrame containing the data.\n        x_col (str): Column name to use for the x-axis of the charts. Default is \"wavelength\".\n        y_col_prefix (str): Prefix to identify the columns that contain the location-specific data. Default is \"(\".\n        x_label (str): Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".\n        y_label (str): Label for the y-axis of the charts. Default is \"Reflectance\".\n        use_marker_cluster (bool): Whether to use marker clustering. Default is True.\n        min_width (int): Minimum width of the popup. Default is 400.\n        max_width (int): Maximum width of the popup. Default is 600.\n        min_height (int): Minimum height of the popup. Default is 200.\n        max_height (int): Maximum height of the popup. Default is 250.\n        layer_name (str): Name of the marker cluster layer. Default is \"Marker Cluster\".\n        m (Map, optional): An ipyleaflet Map instance to add the markers to. Default is None.\n        center (Tuple[float, float]): Center of the map as a tuple of (latitude, longitude). Default is (20, 0).\n        zoom (int): Zoom level of the map. Default is 2.\n\n    Returns:\n        Map: An ipyleaflet Map with the added markers and popups.\n    \"\"\"\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from ipyleaflet import Map, Marker, Popup, MarkerCluster\n    from ipywidgets import Output, VBox\n\n    # Read the CSV file\n    if isinstance(data, str):\n        data = pd.read_csv(data)\n    elif isinstance(data, pd.DataFrame):\n        pass\n    else:\n        raise ValueError(\"data must be a path to a CSV file or a pandas DataFrame\")\n\n    # Extract locations from columns\n    locations = [col for col in data.columns if col.startswith(y_col_prefix)]\n    coordinates = [tuple(map(float, loc.strip(\"()\").split())) for loc in locations]\n\n    # Create the map\n    if m is None:\n        m = Map(center=center, zoom=zoom)\n\n    # Function to create the chart\n    def create_chart(data, title):\n        _, ax = plt.subplots(figsize=(10, 6))  # Adjust the figure size here\n        ax.plot(data[x_col], data[\"values\"])\n        ax.set_title(title)\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n        output = Output()  # Adjust the output widget size here\n        with output:\n            plt.show()\n        return output\n\n    # Define a callback function to create and show the popup\n    def callback_with_popup_creation(location, values):\n        def f(**kwargs):\n            marker_center = kwargs[\"coordinates\"]\n            output = create_chart(values, f\"Location: {location}\")\n            popup = Popup(\n                location=marker_center,\n                child=VBox([output]),\n                min_width=min_width,\n                max_width=max_width,\n                min_height=min_height,\n                max_height=max_height,\n            )\n            m.add_layer(popup)\n\n        return f\n\n    markers = []\n\n    # Add points to the map\n    for i, coord in enumerate(coordinates):\n        location = f\"{coord}\"\n        values = pd.DataFrame({x_col: data[x_col], \"values\": data[locations[i]]})\n        marker = Marker(location=coord, title=location, name=f\"Marker {i + 1}\")\n        marker.on_click(callback_with_popup_creation(location, values))\n        markers.append(marker)\n\n    if use_marker_cluster:\n        marker_cluster = MarkerCluster(markers=markers, name=layer_name)\n        m.add_layer(marker_cluster)\n    else:\n        for marker in markers:\n            m.add_layer(marker)\n\n    return m\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/opengeos/HyperCoast/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>HyperCoast could always use more documentation, whether as part of the official HyperCoast docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/opengeos/HyperCoast/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up HyperCoast for local development.</p> <ol> <li> <p>Fork the HyperCoast repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone https://github.com/&lt;YOUR-GITHUB-USERNAME&gt;/HyperCoast.git\n</code></pre> </li> <li> <p>Create a new conda environment to install HyperCoast and its dependencies. Assuming you have     Anaconda or     Miniconda installed,     this is how you set up your fork for local development:</p> <pre><code>$ conda install -n base mamba -c conda-forge\n$ conda create -n hyper python=3.11\n$ conda activate hyper\n$ mamba install -c conda-forge hypercoast cartopy earthaccess mapclassify pyvista\n$ cd HyperCoast/\n$ pip install -e .\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass     pre-commit and the tests:</p> <pre><code>$ pip install pre-commit\n$ pre-commit install\n$ pre-commit run --all-files\n$ python -m unittest discover tests/\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> <li>Check the status of your pull request on GitHub and make sure     that the tests for the pull request pass for all supported Python versions.</li> <li>Commit more changes to your branch to fix the text errors if necessary.</li> <li>Wait for the pull request to be reviewed by the maintainers.</li> <li>Congratulations! You've made your contribution to HyperCoast!</li> </ol>"},{"location":"contributing/#contributor-agreements","title":"Contributor Agreements","text":"<p>Before your contribution can be accepted, you will need to sign the appropriate contributor agreement. The Contributor License Agreement (CLA) assistant will walk you through the process of signing the CLA. Please follow the instructions provided by the assistant on the pull request.</p> <ul> <li>Individual Contributor Exclusive License Agreement</li> <li>Entity Contributor Exclusive License Agreement</li> </ul>"},{"location":"desis/","title":"desis module","text":"<p>This Module has the functions related to working with a DESIS dataset.</p>"},{"location":"desis/#hypercoast.desis.desis_to_image","title":"<code>desis_to_image(dataset, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts an DESIS dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xarray.Dataset or str</code> <p>The dataset containing the DESIS data or the file path to the dataset.</p> required <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the dataset. If     <code>output</code> is provided, the image will be saved to the specified file     and the function will return None.</p> Source code in <code>hypercoast/desis.py</code> <pre><code>def desis_to_image(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Union[list, tuple] = None,\n    method: Optional[str] = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Converts an DESIS dataset to an image.\n\n    Args:\n        dataset (xarray.Dataset or str): The dataset containing the DESIS data\n            or the file path to the dataset.\n        wavelengths (array-like, optional): The specific wavelengths to select.\n            If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n        output (str, optional): The file path where the image will be saved. If\n            None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to\n            `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the dataset. If\n            `output` is provided, the image will be saved to the specified file\n            and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_desis(dataset, method=method)\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n\n    return array_to_image(\n        dataset[\"reflectance\"], output=output, transpose=False, **kwargs\n    )\n</code></pre>"},{"location":"desis/#hypercoast.desis.extract_desis","title":"<code>extract_desis(ds, lat, lon)</code>","text":"<p>Extracts DESIS data from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>The dataset containing the DESIS data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted data.</p> Source code in <code>hypercoast/desis.py</code> <pre><code>def extract_desis(ds: xr.Dataset, lat: float, lon: float) -&gt; xr.DataArray:\n    \"\"\"\n    Extracts DESIS data from a given xarray Dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the DESIS data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n\n    Returns:\n        xarray.DataArray: The extracted data.\n    \"\"\"\n\n    crs = ds.attrs[\"crs\"]\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    values = ds.sel(x=x, y=y, method=\"nearest\")[\"reflectance\"].values / 10000\n\n    da = xr.DataArray(\n        values, dims=[\"wavelength\"], coords={\"wavelength\": ds.coords[\"wavelength\"]}\n    )\n\n    return da\n</code></pre>"},{"location":"desis/#hypercoast.desis.filter_desis","title":"<code>filter_desis(dataset, lat, lon, return_plot=False, **kwargs)</code>","text":"<p>Filters a DESIS dataset based on latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The DESIS dataset to filter.</p> required <code>lat</code> <code>float or tuple</code> <p>The latitude to filter by. If a tuple or list, it represents a range.</p> required <code>lon</code> <code>float or tuple</code> <p>The longitude to filter by. If a tuple or list, it represents a range.</p> required <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The filtered DESIS data.</p> Source code in <code>hypercoast/desis.py</code> <pre><code>def filter_desis(\n    dataset: xr.Dataset,\n    lat: Union[float, tuple],\n    lon: Union[float, tuple],\n    return_plot: Optional[bool] = False,\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Filters a DESIS dataset based on latitude and longitude.\n\n    Args:\n        dataset (xr.Dataset): The DESIS dataset to filter.\n        lat (float or tuple): The latitude to filter by. If a tuple or list,\n            it represents a range.\n        lon (float or tuple): The longitude to filter by. If a tuple or\n            list, it represents a range.\n\n    Returns:\n        xr.DataArray: The filtered DESIS data.\n    \"\"\"\n\n    if isinstance(lat, list) or isinstance(lat, tuple):\n        min_lat = min(lat)\n        max_lat = max(lat)\n    else:\n        min_lat = lat\n        max_lat = lat\n\n    if isinstance(lon, list) or isinstance(lon, tuple):\n        min_lon = min(lon)\n        max_lon = max(lon)\n    else:\n        min_lon = lon\n        max_lon = lon\n\n    if min_lat == max_lat and min_lon == max_lon:\n        coords = [[min_lat, min_lon]]\n    else:\n        coords = [[min_lat, min_lon], [max_lat, max_lon]]\n    coords = convert_coords(coords, \"epsg:4326\", dataset.rio.crs.to_string())\n\n    if len(coords) == 1:\n        x, y = coords[0]\n        da = dataset.sel(x=x, y=y, method=\"nearest\")[\"reflectance\"]\n    else:\n        x_min, y_min = coords[0]\n        x_max, y_max = coords[1]\n        print(x_min, y_min, x_max, y_max)\n        da = dataset.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))[\"reflectance\"]\n\n    if return_plot:\n        rrs_stack = da.stack(\n            {\"pixel\": [\"latitude\", \"longitude\"]},\n            create_index=False,\n        )\n        rrs_stack.plot.line(hue=\"pixel\", **kwargs)\n    else:\n        return da\n</code></pre>"},{"location":"desis/#hypercoast.desis.read_desis","title":"<code>read_desis(filepath, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Reads DESIS data from a given file and returns an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the file to read.</p> required <code>wavelengths</code> <code>array-like</code> <p>Specific wavelengths to select. If None, all wavelengths are selected.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for selection when wavelengths is not None. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>sel</code> method when bands is not None.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>An xarray Dataset containing the DESIS data.</p> Source code in <code>hypercoast/desis.py</code> <pre><code>def read_desis(\n    filepath: str,\n    wavelengths: Optional[Union[list, tuple]] = None,\n    method: Optional[str] = \"nearest\",\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads DESIS data from a given file and returns an xarray Dataset.\n\n    Args:\n        filepath (str): Path to the file to read.\n        wavelengths (array-like, optional): Specific wavelengths to select. If\n            None, all wavelengths are selected.\n        method (str, optional): Method to use for selection when wavelengths is not\n            None. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to pass to the `sel` method when\n            bands is not None.\n\n    Returns:\n        xr.Dataset: An xarray Dataset containing the DESIS data.\n    \"\"\"\n\n    url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/desis_wavelengths.csv\"\n    df = pd.read_csv(url)\n    dataset = xr.open_dataset(filepath)\n    dataset = dataset.rename(\n        {\"band\": \"wavelength\", \"band_data\": \"reflectance\"}\n    ).transpose(\"y\", \"x\", \"wavelength\")\n    dataset[\"wavelength\"] = df[\"wavelength\"].tolist()\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method, **kwargs)\n\n    dataset.attrs[\"crs\"] = dataset.rio.crs.to_string()\n\n    return dataset\n</code></pre>"},{"location":"emit/","title":"emit module","text":"<p>This Module has the functions related to working with an EMIT dataset. This includes doing things like opening and flattening the data to work in xarray, orthorectification, and visualization.</p> <p>Some source code is adapted from https://github.com/nasa/EMIT-Data-Resources, which is licensed under the Apache License 2.0. Credits to the original authors, including Erik Bolch, Alex Leigh, and others.</p>"},{"location":"emit/#hypercoast.emit.apply_glt","title":"<code>apply_glt(ds_array, glt_array, fill_value=-9999, GLT_NODATA_VALUE=0)</code>","text":"<p>Applies the GLT array to a numpy array of either 2 or 3 dimensions to orthorectify the data.</p> <p>Parameters:</p> Name Type Description Default <code>ds_array</code> <code>numpy.ndarray</code> <p>A numpy array of the desired variable.</p> required <code>glt_array</code> <code>numpy.ndarray</code> <p>A GLT array constructed from EMIT GLT data.</p> required <code>fill_value</code> <code>int</code> <p>The value to fill in the output array where the GLT array has no data. Defaults to -9999.</p> <code>-9999</code> <code>GLT_NODATA_VALUE</code> <code>int</code> <p>The value in the GLT array that indicates no data. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>A numpy array of orthorectified data.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def apply_glt(\n    ds_array: np.ndarray,\n    glt_array: np.ndarray,\n    fill_value: int = -9999,\n    GLT_NODATA_VALUE: Optional[int] = 0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Applies the GLT array to a numpy array of either 2 or 3 dimensions to orthorectify the data.\n\n    Args:\n        ds_array (numpy.ndarray): A numpy array of the desired variable.\n        glt_array (numpy.ndarray): A GLT array constructed from EMIT GLT data.\n        fill_value (int, optional): The value to fill in the output array where the GLT array has no data. Defaults to -9999.\n        GLT_NODATA_VALUE (int, optional): The value in the GLT array that indicates no data. Defaults to 0.\n\n    Returns:\n        numpy.ndarray: A numpy array of orthorectified data.\n    \"\"\"\n\n    # Build Output Dataset\n    if ds_array.ndim == 2:\n        ds_array = ds_array[:, :, np.newaxis]\n    out_ds = np.full(\n        (glt_array.shape[0], glt_array.shape[1], ds_array.shape[-1]),\n        fill_value,\n        dtype=np.float32,\n    )\n    valid_glt = np.all(glt_array != GLT_NODATA_VALUE, axis=-1)\n\n    # Adjust for One based Index - make a copy to prevent decrementing multiple times inside ortho_xr when applying the glt to elev\n    glt_array_copy = glt_array.copy()\n    glt_array_copy[valid_glt] -= 1\n    out_ds[valid_glt, :] = ds_array[\n        glt_array_copy[valid_glt, 1], glt_array_copy[valid_glt, 0], :\n    ]\n    return out_ds\n</code></pre>"},{"location":"emit/#hypercoast.emit.band_mask","title":"<code>band_mask(filepath)</code>","text":"<p>Unpacks the packed band mask to apply to the dataset. Can be used manually or as an input in the emit_xarray() function.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>An EMIT L2A Mask netCDF file.</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>A numpy array that can be used with the emit_xarray function to apply a band mask.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def band_mask(filepath: str) -&gt; np.ndarray:\n    \"\"\"\n    Unpacks the packed band mask to apply to the dataset. Can be used manually or as an input in the emit_xarray() function.\n\n    Args:\n        filepath (str): An EMIT L2A Mask netCDF file.\n\n    Returns:\n        numpy.ndarray: A numpy array that can be used with the emit_xarray function to apply a band mask.\n    \"\"\"\n    # Open Dataset\n    mask_ds = xr.open_dataset(filepath, engine=\"h5netcdf\")\n    # Open band_mask and convert to uint8\n    bmask = mask_ds.band_mask.data.astype(\"uint8\")\n    # Print Flags used\n    unpacked_bmask = np.unpackbits(bmask, axis=-1)\n    # Remove bands &gt; 285\n    unpacked_bmask = unpacked_bmask[:, :, 0:285]\n    # Check for data bands and build mask\n    return unpacked_bmask\n</code></pre>"},{"location":"emit/#hypercoast.emit.coord_vects","title":"<code>coord_vects(ds)</code>","text":"<p>This function calculates the Lat and Lon Coordinate Vectors using the GLT and Metadata from an EMIT dataset read into xarray.</p> <p>lon, lat (numpy.array): longitude and latitude array grid for the dataset</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def coord_vects(ds: xr.Dataset) -&gt; np.ndarray:\n    \"\"\"\n    This function calculates the Lat and Lon Coordinate Vectors using the GLT and Metadata from an EMIT dataset read into xarray.\n\n    Parameters:\n    ds: an xarray.Dataset containing the root variable and metadata of an EMIT dataset\n    loc: an xarray.Dataset containing the 'location' group of an EMIT dataset\n\n    Returns:\n    lon, lat (numpy.array): longitude and latitude array grid for the dataset\n\n    \"\"\"\n    # Retrieve Geotransform from Metadata\n    GT = ds.geotransform\n    # Create Array for Lat and Lon and fill\n    dim_x = ds.glt_x.shape[1]\n    dim_y = ds.glt_x.shape[0]\n    lon = np.zeros(dim_x)\n    lat = np.zeros(dim_y)\n    # Note: no rotation for EMIT Data\n    for x in np.arange(dim_x):\n        x_geo = (GT[0] + 0.5 * GT[1]) + x * GT[1]  # Adjust coordinates to pixel-center\n        lon[x] = x_geo\n    for y in np.arange(dim_y):\n        y_geo = (GT[3] + 0.5 * GT[5]) + y * GT[5]\n        lat[y] = y_geo\n    return lon, lat\n</code></pre>"},{"location":"emit/#hypercoast.emit.emit_to_image","title":"<code>emit_to_image(data, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts an EMIT dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>xarray.Dataset or str</code> <p>The dataset containing the EMIT data or the file path to the dataset.</p> required <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the dataset. If <code>output</code> is provided, the image will be saved to the specified file and the function will return None.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def emit_to_image(\n    data: Union[xr.Dataset, str],\n    wavelengths: Optional[Union[list, tuple]] = None,\n    method: Optional[str] = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Converts an EMIT dataset to an image.\n\n    Args:\n        data (xarray.Dataset or str): The dataset containing the EMIT data or the file path to the dataset.\n        wavelengths (array-like, optional): The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n        output (str, optional): The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the dataset. If `output` is provided, the image will be saved to the specified file and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(data, str):\n        data = read_emit(data, ortho=True)\n\n    ds = data[\"reflectance\"]\n\n    if wavelengths is not None:\n        ds = ds.sel(wavelength=wavelengths, method=method)\n    return array_to_image(ds, transpose=False, output=output, **kwargs)\n</code></pre>"},{"location":"emit/#hypercoast.emit.emit_to_netcdf","title":"<code>emit_to_netcdf(data, output, **kwargs)</code>","text":"<p>Transposes an EMIT dataset and saves it as a NetCDF file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>xarray.Dataset or str</code> <p>The dataset containing the EMIT data or the file path to the dataset.</p> required <code>output</code> <code>str</code> <p>The file path where the NetCDF file will be saved.</p> required <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>xarray.Dataset.to_netcdf</code>.</p> <code>{}</code> Source code in <code>hypercoast/emit.py</code> <pre><code>def emit_to_netcdf(data: Union[xr.Dataset, str], output: str, **kwargs) -&gt; None:\n    \"\"\"\n    Transposes an EMIT dataset and saves it as a NetCDF file.\n\n    Args:\n        data (xarray.Dataset or str): The dataset containing the EMIT data or the file path to the dataset.\n        output (str): The file path where the NetCDF file will be saved.\n        **kwargs: Additional keyword arguments to be passed to `xarray.Dataset.to_netcdf`.\n\n    \"\"\"\n    if isinstance(data, str):\n        data = read_emit(data, ortho=True)\n\n    ds_geo = data.transpose(\"wavelengths\", \"latitude\", \"longitude\")\n    ds_geo.to_netcdf(output, **kwargs)\n</code></pre>"},{"location":"emit/#hypercoast.emit.emit_xarray","title":"<code>emit_xarray(filepath, ortho=False, qmask=None, unpacked_bmask=None, wavelengths=None, method='nearest')</code>","text":"<p>Streamlines opening an EMIT dataset as an xarray.Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>A filepath to an EMIT netCDF file.</p> required <code>ortho</code> <code>bool</code> <p>Whether to orthorectify the dataset or leave in crosstrack/downtrack coordinates. Defaults to False.</p> <code>False</code> <code>qmask</code> <code>numpy.ndarray</code> <p>A numpy array output from the quality_mask function used to mask pixels based on quality flags selected in that function. Any non-orthorectified array with the proper crosstrack and downtrack dimensions can also be used. Defaults to None.</p> <code>None</code> <code>unpacked_bmask</code> <code>numpy.ndarray</code> <p>A numpy array from the band_mask function that can be used to mask band-specific pixels that have been interpolated. Defaults to None.</p> <code>None</code> <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>An xarray.Dataset constructed based on the parameters provided.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def emit_xarray(\n    filepath: str,\n    ortho: Optional[bool] = False,\n    qmask: Optional[np.ndarray] = None,\n    unpacked_bmask: Optional[np.ndarray] = None,\n    wavelengths: Optional[Union[tuple, list]] = None,\n    method: Optional[str] = \"nearest\",\n) -&gt; xr.Dataset:\n    \"\"\"\n    Streamlines opening an EMIT dataset as an xarray.Dataset.\n\n    Args:\n        filepath (str): A filepath to an EMIT netCDF file.\n        ortho (bool, optional): Whether to orthorectify the dataset or leave in crosstrack/downtrack coordinates. Defaults to False.\n        qmask (numpy.ndarray, optional): A numpy array output from the quality_mask function used to mask pixels based on quality flags selected in that function. Any non-orthorectified array with the proper crosstrack and downtrack dimensions can also be used. Defaults to None.\n        unpacked_bmask (numpy.ndarray, optional): A numpy array from the band_mask function that can be used to mask band-specific pixels that have been interpolated. Defaults to None.\n        wavelengths (array-like, optional): The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n\n    Returns:\n        xarray.Dataset: An xarray.Dataset constructed based on the parameters provided.\n    \"\"\"\n    # Grab granule filename to check product\n    from s3fs.core import S3File\n    from fsspec.implementations.http import HTTPFile\n\n    if isinstance(filepath, S3File):\n        granule_id = filepath.info()[\"name\"].split(\"/\", -1)[-1].split(\".\", -1)[0]\n    elif isinstance(filepath, HTTPFile):\n        granule_id = filepath.path.split(\"/\", -1)[-1].split(\".\", -1)[0]\n    else:\n        granule_id = os.path.splitext(os.path.basename(filepath))[0]\n\n    # Read in Data as Xarray Datasets\n    engine, wvl_group = \"h5netcdf\", None\n\n    ds = xr.open_dataset(filepath, engine=engine)\n    loc = xr.open_dataset(filepath, engine=engine, group=\"location\")\n\n    # Check if mineral dataset and read in groups (only ds/loc for minunc)\n\n    if \"L2B_MIN_\" in granule_id:\n        wvl_group = \"mineral_metadata\"\n    elif \"L2B_MINUNC\" not in granule_id:\n        wvl_group = \"sensor_band_parameters\"\n\n    wvl = None\n\n    if wvl_group:\n        wvl = xr.open_dataset(filepath, engine=engine, group=wvl_group)\n\n    # Building Flat Dataset from Components\n    data_vars = {**ds.variables}\n\n    # Format xarray coordinates based upon emit product (no wvl for mineral uncertainty)\n    coords = {\n        \"downtrack\": ([\"downtrack\"], ds.downtrack.data),\n        \"crosstrack\": ([\"crosstrack\"], ds.crosstrack.data),\n        **loc.variables,\n    }\n\n    product_band_map = {\n        \"L2B_MIN_\": \"name\",\n        \"L2A_MASK_\": \"mask_bands\",\n        \"L1B_OBS_\": \"observation_bands\",\n        \"L2A_RFL_\": \"wavelengths\",\n        \"L1B_RAD_\": \"wavelengths\",\n        \"L2A_RFLUNCERT_\": \"wavelengths\",\n    }\n\n    # if band := product_band_map.get(next((k for k in product_band_map.keys() if k in granule_id), 'unknown'), None):\n    # coords['bands'] = wvl[band].data\n\n    if wvl:\n        coords = {**coords, **wvl.variables}\n\n    out_xr = xr.Dataset(data_vars=data_vars, coords=coords, attrs=ds.attrs)\n    out_xr.attrs[\"granule_id\"] = granule_id\n\n    if band := product_band_map.get(\n        next((k for k in product_band_map.keys() if k in granule_id), \"unknown\"), None\n    ):\n        if \"minerals\" in list(out_xr.dims):\n            out_xr = out_xr.swap_dims({\"minerals\": band})\n            out_xr = out_xr.rename({band: \"mineral_name\"})\n        else:\n            out_xr = out_xr.swap_dims({\"bands\": band})\n\n    # Apply Quality and Band Masks, set fill values to NaN\n    for var in list(ds.data_vars):\n        if qmask is not None:\n            out_xr[var].data[qmask == 1] = np.nan\n        if unpacked_bmask is not None:\n            out_xr[var].data[unpacked_bmask == 1] = np.nan\n        out_xr[var].data[out_xr[var].data == -9999] = np.nan\n\n    if ortho is True:\n        out_xr = ortho_xr(out_xr)\n        out_xr.attrs[\"Orthorectified\"] = \"True\"\n\n    if wavelengths is not None:\n        out_xr = out_xr.sel(wavelengths=wavelengths, method=method)\n\n    out_xr = out_xr.rename({\"wavelengths\": \"wavelength\"})\n    return out_xr\n</code></pre>"},{"location":"emit/#hypercoast.emit.envi_header","title":"<code>envi_header(inputpath)</code>","text":"<p>Convert a ENVI binary/header path to a header, handling extensions.</p> <p>Parameters:</p> Name Type Description Default <code>inputpath</code> <code>str</code> <p>Path to ENVI binary file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The header file associated with the input reference. If the header file does not exist, it returns the expected header file path.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def envi_header(inputpath: str) -&gt; str:\n    \"\"\"\n    Convert a ENVI binary/header path to a header, handling extensions.\n\n    Args:\n        inputpath (str): Path to ENVI binary file.\n\n    Returns:\n        str: The header file associated with the input reference. If the header file does not exist, it returns the expected header file path.\n    \"\"\"\n    if (\n        os.path.splitext(inputpath)[-1] == \".img\"\n        or os.path.splitext(inputpath)[-1] == \".dat\"\n        or os.path.splitext(inputpath)[-1] == \".raw\"\n    ):\n        # headers could be at either filename.img.hdr or filename.hdr.  Check both, return the one that exists if it\n        # does, if not return the latter (new file creation presumed).\n        hdrfile = os.path.splitext(inputpath)[0] + \".hdr\"\n        if os.path.isfile(hdrfile):\n            return hdrfile\n        elif os.path.isfile(inputpath + \".hdr\"):\n            return inputpath + \".hdr\"\n        return hdrfile\n    elif os.path.splitext(inputpath)[-1] == \".hdr\":\n        return inputpath\n    else:\n        return inputpath + \".hdr\"\n</code></pre>"},{"location":"emit/#hypercoast.emit.is_adjacent","title":"<code>is_adjacent(scene, same_orbit)</code>","text":"<p>Checks if the scene numbers from the same orbit are adjacent/sequential.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>str</code> <p>The scene number to check.</p> required <code>same_orbit</code> <code>list</code> <p>A list of scene numbers from the same orbit.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the scene numbers are adjacent/sequential, False otherwise.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def is_adjacent(scene: str, same_orbit: list) -&gt; bool:\n    \"\"\"\n    Checks if the scene numbers from the same orbit are adjacent/sequential.\n\n    Args:\n        scene (str): The scene number to check.\n        same_orbit (list): A list of scene numbers from the same orbit.\n\n    Returns:\n        bool: True if the scene numbers are adjacent/sequential, False otherwise.\n    \"\"\"\n    scene_nums = [int(scene.split(\".\")[-2].split(\"_\")[-1]) for scene in same_orbit]\n    return all(b - a == 1 for a, b in zip(scene_nums[:-1], scene_nums[1:]))\n</code></pre>"},{"location":"emit/#hypercoast.emit.merge_emit","title":"<code>merge_emit(datasets, gdf)</code>","text":"<p>Merges xarray datasets formatted using emit_xarray. Note: GDF may only work with a single geometry.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>dict</code> <p>A dictionary of xarray datasets formatted using emit_xarray.</p> required <code>gdf</code> <code>gpd.GeoDataFrame</code> <p>A GeoDataFrame containing the geometry to be used for merging.</p> required <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>A merged xarray dataset.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If there are inconsistencies in the 1D variables across datasets.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def merge_emit(datasets: dict, gdf: gpd.GeoDataFrame) -&gt; xr.Dataset:\n    \"\"\"\n    Merges xarray datasets formatted using emit_xarray. Note: GDF may only work with a single geometry.\n\n    Args:\n        datasets (dict): A dictionary of xarray datasets formatted using emit_xarray.\n        gdf (gpd.GeoDataFrame): A GeoDataFrame containing the geometry to be used for merging.\n\n    Returns:\n        xarray.Dataset: A merged xarray dataset.\n\n    Raises:\n        Exception: If there are inconsistencies in the 1D variables across datasets.\n    \"\"\"\n    from rioxarray.merge import merge_arrays\n\n    nested_data_arrays = {}\n    # loop over datasets\n    for dataset in datasets:\n        # create dictionary of arrays for each dataset\n\n        # create dictionary of 1D variables, which should be consistent across datasets\n        one_d_arrays = {}\n\n        # Dictionary of variables to merge\n        data_arrays = {}\n        # Loop over variables in dataset including elevation\n        for var in list(datasets[dataset].data_vars) + [\"elev\"]:\n            # Get 1D for this variable and add to dictionary\n            if not one_d_arrays:\n                # These should be an array describing the others (wavelengths, mask_bands, etc.)\n                one_dim = [\n                    item\n                    for item in list(datasets[dataset].coords)\n                    if item not in [\"latitude\", \"longitude\", \"spatial_ref\"]\n                    and len(datasets[dataset][item].dims) == 1\n                ]\n                # print(one_dim)\n                for od in one_dim:\n                    one_d_arrays[od] = datasets[dataset].coords[od].data\n\n                # Update format for merging - This could probably be improved\n            da = datasets[dataset][var].reset_coords(\"elev\", drop=False)\n            da = da.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n            if len(da.dims) == 3:\n                if any(item in list(da.coords) for item in one_dim):\n                    da = da.drop_vars(one_dim)\n                da = da.drop_vars(\"elev\")\n                da = da.to_array(name=var).squeeze(\"variable\", drop=True)\n                da = da.transpose(da.dims[-1], da.dims[0], da.dims[1])\n                # print(da.dims)\n            if var == \"elev\":\n                da = da.to_array(name=var).squeeze(\"variable\", drop=True)\n            data_arrays[var] = da\n            nested_data_arrays[dataset] = data_arrays\n\n            # Transpose the nested arrays dict. This is horrible to read, but works to pair up variables (ie mask) from the different granules\n    transposed_dict = {\n        inner_key: {\n            outer_key: inner_dict[inner_key]\n            for outer_key, inner_dict in nested_data_arrays.items()\n        }\n        for inner_key in nested_data_arrays[next(iter(nested_data_arrays))]\n    }\n\n    # remove some unused data\n    del nested_data_arrays, data_arrays, da\n\n    # Merge the arrays using rioxarray.merge_arrays()\n    merged = {}\n    for _var in transposed_dict:\n        merged[_var] = merge_arrays(\n            list(transposed_dict[_var].values()),\n            bounds=gdf.unary_union.bounds,\n            nodata=np.nan,\n        )\n\n    # Create a new xarray dataset from the merged arrays\n    # Create Merged Dataset\n    merged_ds = xr.Dataset(data_vars=merged, coords=one_d_arrays)\n    # Rename x and y to longitude and latitude\n    merged_ds = merged_ds.rename({\"y\": \"latitude\", \"x\": \"longitude\"})\n    del transposed_dict, merged\n    return merged_ds\n</code></pre>"},{"location":"emit/#hypercoast.emit.ortho_browse","title":"<code>ortho_browse(url, glt, spatial_ref, geotransform, white_background=True)</code>","text":"<p>Use an EMIT GLT, geotransform, and spatial ref to orthorectify a browse image. (browse images are in native resolution)</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the browse image.</p> required <code>glt</code> <code>numpy.ndarray</code> <p>A GLT array constructed from EMIT GLT data.</p> required <code>spatial_ref</code> <code>str</code> <p>Spatial reference system.</p> required <code>geotransform</code> <code>list</code> <p>A list of six numbers that define the affine transform between pixel coordinates and map coordinates.</p> required <code>white_background</code> <code>bool</code> <p>If True, the fill value for the orthorectified image is white (255). If False, the fill value is black (0). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>An orthorectified browse image in the form of an xarray DataArray.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def ortho_browse(\n    url: str,\n    glt: np.ndarray,\n    spatial_ref: str,\n    geotransform: list,\n    white_background: Optional[bool] = True,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Use an EMIT GLT, geotransform, and spatial ref to orthorectify a browse image. (browse images are in native resolution)\n\n    Args:\n        url (str): URL of the browse image.\n        glt (numpy.ndarray): A GLT array constructed from EMIT GLT data.\n        spatial_ref (str): Spatial reference system.\n        geotransform (list): A list of six numbers that define the affine transform between pixel coordinates and map coordinates.\n        white_background (bool, optional): If True, the fill value for the orthorectified image is white (255). If False, the fill value is black (0). Defaults to True.\n\n    Returns:\n        xarray.DataArray: An orthorectified browse image in the form of an xarray DataArray.\n    \"\"\"\n    from skimage import io\n\n    # Read Data\n    data = io.imread(url)\n    # Orthorectify using GLT and transpose so band is first dimension\n    if white_background is True:\n        fill = 255\n    else:\n        fill = 0\n    ortho_data = apply_glt(data, glt, fill_value=fill).transpose(2, 0, 1)\n    coords = {\n        \"y\": (\n            [\"y\"],\n            (geotransform[3] + 0.5 * geotransform[5])\n            + np.arange(glt.shape[0]) * geotransform[5],\n        ),\n        \"x\": (\n            [\"x\"],\n            (geotransform[0] + 0.5 * geotransform[1])\n            + np.arange(glt.shape[1]) * geotransform[1],\n        ),\n    }\n    ortho_data = ortho_data.astype(int)\n    ortho_data[ortho_data == -1] = 0\n    # Place in xarray.datarray\n    da = xr.DataArray(ortho_data, dims=[\"band\", \"y\", \"x\"], coords=coords)\n    da.rio.write_crs(spatial_ref, inplace=True)\n    return da\n</code></pre>"},{"location":"emit/#hypercoast.emit.ortho_xr","title":"<code>ortho_xr(ds, GLT_NODATA_VALUE=0, fill_value=-9999)</code>","text":"<p>Uses <code>apply_glt</code> to create an orthorectified xarray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>An xarray dataset produced by emit_xarray.</p> required <code>GLT_NODATA_VALUE</code> <code>int</code> <p>No data value for the GLT tables. Defaults to 0.</p> <code>0</code> <code>fill_value</code> <code>int</code> <p>The fill value for EMIT datasets. Defaults to -9999.</p> <code>-9999</code> <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>An orthocorrected xarray dataset.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def ortho_xr(\n    ds: xr.Dataset,\n    GLT_NODATA_VALUE: Optional[int] = 0,\n    fill_value: Optional[int] = -9999,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Uses `apply_glt` to create an orthorectified xarray dataset.\n\n    Args:\n        ds (xarray.Dataset): An xarray dataset produced by emit_xarray.\n        GLT_NODATA_VALUE (int, optional): No data value for the GLT tables. Defaults to 0.\n        fill_value (int, optional): The fill value for EMIT datasets. Defaults to -9999.\n\n    Returns:\n        xarray.Dataset: An orthocorrected xarray dataset.\n    \"\"\"\n    # Build glt_ds\n\n    glt_ds = np.nan_to_num(\n        np.stack([ds[\"glt_x\"].data, ds[\"glt_y\"].data], axis=-1), nan=GLT_NODATA_VALUE\n    ).astype(int)\n\n    # List Variables\n    var_list = list(ds.data_vars)\n\n    # Remove flat field from data vars - the flat field is only useful with additional information before orthorectification\n    if \"flat_field_update\" in var_list:\n        var_list.remove(\"flat_field_update\")\n\n    # Create empty dictionary for orthocorrected data vars\n    data_vars = {}\n\n    # Extract Rawspace Dataset Variable Values (Typically Reflectance)\n    for var in var_list:\n        raw_ds = ds[var].data\n        var_dims = ds[var].dims\n        # Apply GLT to dataset\n        out_ds = apply_glt(raw_ds, glt_ds, GLT_NODATA_VALUE=GLT_NODATA_VALUE)\n\n        # Mask fill values\n        out_ds[out_ds == fill_value] = np.nan\n\n        # Update variables - Only works for 2 or 3 dimensional arrays\n        if raw_ds.ndim == 2:\n            out_ds = out_ds.squeeze()\n            data_vars[var] = ([\"latitude\", \"longitude\"], out_ds)\n        else:\n            data_vars[var] = ([\"latitude\", \"longitude\", var_dims[-1]], out_ds)\n\n        del raw_ds\n\n    # Calculate Lat and Lon Vectors\n    lon, lat = coord_vects(\n        ds\n    )  # Reorder this function to make sense in case of multiple variables\n\n    # Apply GLT to elevation\n    elev_ds = apply_glt(ds[\"elev\"].data, glt_ds)\n    elev_ds[elev_ds == fill_value] = np.nan\n\n    # Delete glt_ds - no longer needed\n    del glt_ds\n\n    # Create Coordinate Dictionary\n    coords = {\n        \"latitude\": ([\"latitude\"], lat),\n        \"longitude\": ([\"longitude\"], lon),\n        **ds.coords,\n    }  # unpack to add appropriate coordinates\n\n    # Remove Unnecessary Coords\n    for key in [\"downtrack\", \"crosstrack\", \"lat\", \"lon\", \"glt_x\", \"glt_y\", \"elev\"]:\n        del coords[key]\n\n    # Add Orthocorrected Elevation\n    coords[\"elev\"] = ([\"latitude\", \"longitude\"], np.squeeze(elev_ds))\n\n    # Build Output xarray Dataset and assign data_vars array attributes\n    out_xr = xr.Dataset(data_vars=data_vars, coords=coords, attrs=ds.attrs)\n\n    del out_ds\n    # Assign Attributes from Original Datasets\n    for var in var_list:\n        out_xr[var].attrs = ds[var].attrs\n    out_xr.coords[\"latitude\"].attrs = ds[\"lat\"].attrs\n    out_xr.coords[\"longitude\"].attrs = ds[\"lon\"].attrs\n    out_xr.coords[\"elev\"].attrs = ds[\"elev\"].attrs\n\n    # Add Spatial Reference in recognizable format\n    out_xr.rio.write_crs(ds.spatial_ref, inplace=True)\n\n    return out_xr\n</code></pre>"},{"location":"emit/#hypercoast.emit.plot_emit","title":"<code>plot_emit(ds, longitude=None, latitude=None, downtrack=None, crosstrack=None, remove_nans=True, x='wavelengths', y='reflectance', color='black', frame_height=400, frame_width=600, title=None, method='nearest', ortho=True, options={}, **kwargs)</code>","text":"<p>Plots a line graph of the reflectance data from a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset or str</code> <p>The dataset containing the reflectance data or the file path to the dataset.</p> required <code>longitude</code> <code>float</code> <p>The longitude coordinate to select for orthorectified data. Defaults to None.</p> <code>None</code> <code>latitude</code> <code>float</code> <p>The latitude coordinate to select for orthorectified data. Defaults to None.</p> <code>None</code> <code>downtrack</code> <code>int</code> <p>The downtrack coordinate to select for non-orthorectified data. Defaults to None.</p> <code>None</code> <code>crosstrack</code> <code>int</code> <p>The crosstrack coordinate to select for non-orthorectified data. Defaults to None.</p> <code>None</code> <code>remove_nans</code> <code>bool</code> <p>If True, replace non-good wavelengths with NaN. Defaults to True.</p> <code>True</code> <code>x</code> <code>str</code> <p>The x-axis label. Defaults to \"wavelengths\".</p> <code>'wavelengths'</code> <code>y</code> <code>str</code> <p>The y-axis label. Defaults to \"reflectance\".</p> <code>'reflectance'</code> <code>color</code> <code>str</code> <p>The color of the line. Defaults to \"black\".</p> <code>'black'</code> <code>frame_height</code> <code>int</code> <p>The height of the frame. Defaults to 400.</p> <code>400</code> <code>frame_width</code> <code>int</code> <p>The width of the frame. Defaults to 600.</p> <code>600</code> <code>title</code> <code>str</code> <p>The title of the plot. If None, a default title will be generated. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>ortho</code> <code>bool</code> <p>If True, the function will use longitude and latitude for data selection. Defaults to True.</p> <code>True</code> <code>options</code> <code>dict</code> <p>Additional options to be passed to <code>hvplot.line</code>. Defaults to {}.</p> <code>{}</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>hvplot.line</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>hvplot.Plot</code> <p>The line plot of the reflectance data.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def plot_emit(\n    ds: Union[xr.Dataset, str],\n    longitude: Optional[float] = None,\n    latitude: Optional[float] = None,\n    downtrack: Optional[int] = None,\n    crosstrack: Optional[int] = None,\n    remove_nans: Optional[bool] = True,\n    x: str = \"wavelengths\",\n    y: str = \"reflectance\",\n    color: str = \"black\",\n    frame_height: Optional[int] = 400,\n    frame_width: Optional[int] = 600,\n    title: str = None,\n    method: Optional[str] = \"nearest\",\n    ortho: Optional[bool] = True,\n    options: Optional[dict] = {},\n    **kwargs,\n):\n    \"\"\"\n    Plots a line graph of the reflectance data from a given dataset.\n\n    Args:\n        ds (xarray.Dataset or str): The dataset containing the reflectance data or the file path to the dataset.\n        longitude (float, optional): The longitude coordinate to select for orthorectified data. Defaults to None.\n        latitude (float, optional): The latitude coordinate to select for orthorectified data. Defaults to None.\n        downtrack (int, optional): The downtrack coordinate to select for non-orthorectified data. Defaults to None.\n        crosstrack (int, optional): The crosstrack coordinate to select for non-orthorectified data. Defaults to None.\n        remove_nans (bool, optional): If True, replace non-good wavelengths with NaN. Defaults to True.\n        x (str, optional): The x-axis label. Defaults to \"wavelengths\".\n        y (str, optional): The y-axis label. Defaults to \"reflectance\".\n        color (str, optional): The color of the line. Defaults to \"black\".\n        frame_height (int, optional): The height of the frame. Defaults to 400.\n        frame_width (int, optional): The width of the frame. Defaults to 600.\n        title (str, optional): The title of the plot. If None, a default title will be generated. Defaults to None.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n        ortho (bool, optional): If True, the function will use longitude and latitude for data selection. Defaults to True.\n        options (dict, optional): Additional options to be passed to `hvplot.line`. Defaults to {}.\n        **kwargs: Additional keyword arguments to be passed to `hvplot.line`.\n\n    Returns:\n        hvplot.Plot: The line plot of the reflectance data.\n    \"\"\"\n\n    import hvplot.xarray  # noqa F401\n\n    if ortho is True:\n        if longitude is None or latitude is None:\n            raise ValueError(\n                \"Longitude and Latitude must be provided for orthorectified data.\"\n            )\n    else:\n        if downtrack is None or crosstrack is None:\n            raise ValueError(\n                \"Downtrack and Crosstrack must be provided for non-orthorectified data.\"\n            )\n\n    if longitude is not None and latitude is not None:\n        ortho = True\n\n    if downtrack is not None and crosstrack is not None:\n        ortho = False\n\n    if isinstance(ds, str):\n        ds = read_emit(ds, ortho=ortho)\n\n    if remove_nans:\n        ds[\"reflectance\"].data[:, :, ds[\"good_wavelengths\"].data == 0] = np.nan\n\n    if ortho:\n        example = ds[\"reflectance\"].sel(\n            longitude=longitude, latitude=latitude, method=method\n        )\n        if title is None:\n            title = f\"Reflectance at longitude={longitude:.3f}, latitude={latitude:.3f}\"\n\n    else:\n        example = ds[\"reflectance\"].sel(\n            downtrack=downtrack, crosstrack=crosstrack, method=method\n        )\n        if title is None:\n            title = f\"Reflectance at downtrack={downtrack}, crosstrack={crosstrack}\"\n\n    line = example.hvplot.line(\n        y=y,\n        x=x,\n        color=color,\n        frame_height=frame_height,\n        frame_width=frame_width,\n        **kwargs,\n    ).opts(title=title, **options)\n    return line\n</code></pre>"},{"location":"emit/#hypercoast.emit.quality_mask","title":"<code>quality_mask(filepath, quality_bands)</code>","text":"<p>Builds a single layer mask to apply based on the bands selected from an EMIT L2A Mask file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>An EMIT L2A Mask netCDF file.</p> required <code>quality_bands</code> <code>list</code> <p>A list of bands (quality flags only) from the mask file that should be used in creation of mask.</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>A numpy array that can be used with the emit_xarray function to apply a quality mask.</p> <p>Exceptions:</p> Type Description <code>AttributeError</code> <p>If the selected flags include a data band (5 or 6) not just flag bands.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def quality_mask(filepath: str, quality_bands: list) -&gt; np.ndarray:\n    \"\"\"\n    Builds a single layer mask to apply based on the bands selected from an EMIT L2A Mask file.\n\n    Args:\n        filepath (str): An EMIT L2A Mask netCDF file.\n        quality_bands (list): A list of bands (quality flags only) from the mask file that should be used in creation of mask.\n\n    Returns:\n        numpy.ndarray: A numpy array that can be used with the emit_xarray function to apply a quality mask.\n\n    Raises:\n        AttributeError: If the selected flags include a data band (5 or 6) not just flag bands.\n    \"\"\"\n    # Open Dataset\n    mask_ds = xr.open_dataset(filepath, engine=\"h5netcdf\")\n    # Open Sensor band Group\n    mask_parameters_ds = xr.open_dataset(\n        filepath, engine=\"h5netcdf\", group=\"sensor_band_parameters\"\n    )\n    # Print Flags used\n    flags_used = mask_parameters_ds[\"mask_bands\"].data[quality_bands]\n    print(f\"Flags used: {flags_used}\")\n    # Check for data bands and build mask\n    if any(x in quality_bands for x in [5, 6]):\n        err_str = \"Selected flags include a data band (5 or 6) not just flag bands\"\n        raise AttributeError(err_str)\n    else:\n        qmask = np.sum(mask_ds[\"mask\"][:, :, quality_bands].values, axis=-1)\n        qmask[qmask &gt; 1] = 1\n    return qmask\n</code></pre>"},{"location":"emit/#hypercoast.emit.raw_spatial_crop","title":"<code>raw_spatial_crop(ds, shape)</code>","text":"<p>Use a polygon to clip the file GLT, then a bounding box to crop the spatially raw data. Regions clipped in the GLT are set to 0 so a mask will be applied when used to orthorectify the data at a later point in a workflow.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>Raw spatial EMIT data (non-orthorectified) opened with the <code>emit_xarray</code> function.</p> required <code>shape</code> <code>geopandas.GeoDataFrame</code> <p>A polygon opened with geopandas.</p> required <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>A clipped GLT and raw spatial data clipped to a bounding box.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def raw_spatial_crop(ds: xr.Dataset, shape: gpd) -&gt; xr.Dataset:\n    \"\"\"\n    Use a polygon to clip the file GLT, then a bounding box to crop the spatially raw data. Regions clipped in the GLT are set to 0 so a mask will be applied when\n    used to orthorectify the data at a later point in a workflow.\n\n    Args:\n        ds (xarray.Dataset): Raw spatial EMIT data (non-orthorectified) opened with the `emit_xarray` function.\n        shape (geopandas.GeoDataFrame): A polygon opened with geopandas.\n\n    Returns:\n        xarray.Dataset: A clipped GLT and raw spatial data clipped to a bounding box.\n    \"\"\"\n    # Reformat the GLT\n    lon, lat = coord_vects(ds)\n    data_vars = {\n        \"glt_x\": ([\"latitude\", \"longitude\"], ds.glt_x.data),\n        \"glt_y\": ([\"latitude\", \"longitude\"], ds.glt_y.data),\n    }\n    coords = {\n        \"latitude\": ([\"latitude\"], lat),\n        \"longitude\": ([\"longitude\"], lon),\n        \"ortho_y\": ([\"latitude\"], ds.ortho_y.data),\n        \"ortho_x\": ([\"longitude\"], ds.ortho_x.data),\n    }\n    glt_ds = xr.Dataset(data_vars=data_vars, coords=coords, attrs=ds.attrs)\n    glt_ds.rio.write_crs(glt_ds.spatial_ref, inplace=True)\n\n    # Clip the emit glt\n    clipped = glt_ds.rio.clip(shape.geometry.values, shape.crs, all_touched=True)\n\n    # Pull new geotransform from clipped glt\n    clipped_gt = np.array(\n        [float(i) for i in clipped[\"spatial_ref\"].GeoTransform.split(\" \")]\n    )  # THIS GEOTRANSFORM IS OFF BY HALF A PIXEL\n\n    # Create Crosstrack and Downtrack masks for spatially raw dataset -1 is to account for 1 based index. May be a more robust way to do this exists\n    crosstrack_mask = (ds.crosstrack &gt;= np.nanmin(clipped.glt_x.data) - 1) &amp; (\n        ds.crosstrack &lt;= np.nanmax(clipped.glt_x.data) - 1\n    )\n    downtrack_mask = (ds.downtrack &gt;= np.nanmin(clipped.glt_y.data) - 1) &amp; (\n        ds.downtrack &lt;= np.nanmax(clipped.glt_y.data) - 1\n    )\n\n    # Mask Areas outside of crosstrack and downtrack covered by the shape\n    clipped_ds = ds.where((crosstrack_mask &amp; downtrack_mask), drop=True)\n    # Replace Full dataset geotransform with clipped geotransform\n    clipped_ds.attrs[\"geotransform\"] = clipped_gt\n\n    # Drop unnecessary vars from dataset\n    clipped_ds = clipped_ds.drop_vars([\"glt_x\", \"glt_y\", \"downtrack\", \"crosstrack\"])\n\n    # Re-index the GLT to the new array\n    glt_x_data = clipped.glt_x.data - np.nanmin(clipped.glt_x)\n    glt_y_data = clipped.glt_y.data - np.nanmin(clipped.glt_y)\n    clipped_ds = clipped_ds.assign_coords(\n        {\n            \"glt_x\": ([\"ortho_y\", \"ortho_x\"], np.nan_to_num(glt_x_data)),\n            \"glt_y\": ([\"ortho_y\", \"ortho_x\"], np.nan_to_num(glt_y_data)),\n        }\n    )\n    clipped_ds = clipped_ds.assign_coords(\n        {\n            \"downtrack\": (\n                [\"downtrack\"],\n                np.arange(0, clipped_ds[list(ds.data_vars.keys())[0]].shape[0]),\n            ),\n            \"crosstrack\": (\n                [\"crosstrack\"],\n                np.arange(0, clipped_ds[list(ds.data_vars.keys())[0]].shape[1]),\n            ),\n        }\n    )\n\n    return clipped_ds\n</code></pre>"},{"location":"emit/#hypercoast.emit.read_emit","title":"<code>read_emit(filepath, ortho=True, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Opens an EMIT dataset from a file path and assigns new coordinates to it.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The file path to the EMIT dataset.</p> required <code>ortho</code> <code>bool</code> <p>If True, the function will return an orthorectified dataset. Defaults to True.</p> <code>True</code> <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>xr.open_dataset</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>The dataset with new coordinates assigned.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def read_emit(\n    filepath: str,\n    ortho: Optional[bool] = True,\n    wavelengths: Union[tuple, list] = None,\n    method: Optional[str] = \"nearest\",\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Opens an EMIT dataset from a file path and assigns new coordinates to it.\n\n    Args:\n        filepath (str): The file path to the EMIT dataset.\n        ortho (bool, optional): If True, the function will return an orthorectified dataset. Defaults to True.\n        wavelengths (array-like, optional): The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to be passed to `xr.open_dataset`.\n\n    Returns:\n        xarray.Dataset: The dataset with new coordinates assigned.\n\n    \"\"\"\n\n    if ortho is True:\n        return emit_xarray(\n            filepath, ortho=True, wavelengths=wavelengths, method=method, **kwargs\n        )\n    else:\n        ds = xr.open_dataset(filepath, **kwargs)\n        wvl = xr.open_dataset(filepath, group=\"sensor_band_parameters\")\n        loc = xr.open_dataset(filepath, group=\"location\")\n        ds = ds.assign_coords(\n            {\n                \"downtrack\": ([\"downtrack\"], ds.downtrack.data),\n                \"crosstrack\": ([\"crosstrack\"], ds.crosstrack.data),\n                **wvl.variables,\n                **loc.variables,\n            }\n        )\n        ds = ds.swap_dims({\"bands\": \"wavelengths\"})\n        del wvl\n        del loc\n\n        if wavelengths is not None:\n            ds = ds.sel(wavelengths=wavelengths, method=method)\n\n        ds = ds.rename({\"wavelengths\": \"wavelength\"})\n        return ds\n</code></pre>"},{"location":"emit/#hypercoast.emit.viz_emit","title":"<code>viz_emit(ds, wavelengths, cmap='viridis', frame_width=720, method='nearest', ortho=True, aspect='equal', tiles='ESRI', alpha=0.8, title=None, options={}, **kwargs)</code>","text":"<p>Visualizes the reflectance data from a given dataset at specific wavelengths.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset or str</code> <p>The dataset containing the reflectance data or the file path to the dataset.</p> required <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to visualize.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>frame_width</code> <code>int</code> <p>The width of the frame. Defaults to 720.</p> <code>720</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>ortho</code> <code>bool</code> <p>If True, the function will return an orthorectified image. Defaults to True.</p> <code>True</code> <code>aspect</code> <code>str</code> <p>The aspect ratio of the plot. Defaults to \"equal\".</p> <code>'equal'</code> <code>tiles</code> <code>str</code> <p>The tile source to use for the background map. Defaults to \"ESRI\".</p> <code>'ESRI'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the image. Defaults to 0.8.</p> <code>0.8</code> <code>title</code> <code>str</code> <p>The title of the plot. If None, a default title will be generated. Defaults to None.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Additional options to be passed to <code>hvplot.image</code>. Defaults to {}.</p> <code>{}</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>hvplot.image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>hvplot.Plot</code> <p>The image plot of the reflectance data at the specified wavelengths.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def viz_emit(\n    ds: Union[xr.Dataset, str],\n    wavelengths: Union[tuple, list],\n    cmap: Optional[str] = \"viridis\",\n    frame_width: int = 720,\n    method: Optional[str] = \"nearest\",\n    ortho: Optional[bool] = True,\n    aspect: Optional[str] = \"equal\",\n    tiles: str = \"ESRI\",\n    alpha: Optional[float] = 0.8,\n    title: Optional[str] = None,\n    options: Optional[dict] = {},\n    **kwargs,\n):\n    \"\"\"\n    Visualizes the reflectance data from a given dataset at specific wavelengths.\n\n    Args:\n        ds (xarray.Dataset or str): The dataset containing the reflectance data or the file path to the dataset.\n        wavelengths (array-like): The specific wavelengths to visualize.\n        cmap (str, optional): The colormap to use. Defaults to \"viridis\".\n        frame_width (int, optional): The width of the frame. Defaults to 720.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n        ortho (bool, optional): If True, the function will return an orthorectified image. Defaults to True.\n        aspect (str, optional): The aspect ratio of the plot. Defaults to \"equal\".\n        tiles (str, optional): The tile source to use for the background map. Defaults to \"ESRI\".\n        alpha (float, optional): The alpha value for the image. Defaults to 0.8.\n        title (str, optional): The title of the plot. If None, a default title will be generated. Defaults to None.\n        options (dict, optional): Additional options to be passed to `hvplot.image`. Defaults to {}.\n        **kwargs: Additional keyword arguments to be passed to `hvplot.image`.\n\n    Returns:\n        hvplot.Plot: The image plot of the reflectance data at the specified wavelengths.\n    \"\"\"\n    import hvplot.xarray  # noqa F401\n\n    if isinstance(ds, str):\n        ds = read_emit(ds, ortho=ortho)\n\n    if not isinstance(wavelengths, list):\n        wavelengths = [wavelengths]\n    example = ds.sel(wavelength=wavelengths, method=method)\n\n    wavelengths = \", \".join([f\"{w:.3f}\" for w in example[\"wavelength\"]])\n\n    if title is None:\n        title = f\"Reflectance at {wavelengths} {example.wavelength.units}\"\n\n    if ortho:\n        image = example.hvplot.image(\n            cmap=cmap,\n            geo=ortho,\n            tiles=tiles,\n            alpha=alpha,\n            frame_width=frame_width,\n            **kwargs,\n        ).opts(title=title, **options)\n    else:\n        image = example.hvplot.image(\n            cmap=cmap, aspect=aspect, alpha=alpha, frame_width=frame_width, **kwargs\n        ).opts(title=title, **options)\n\n    return image\n</code></pre>"},{"location":"enmap/","title":"enmap module","text":""},{"location":"enmap/#hypercoast.enmap.enmap_to_image","title":"<code>enmap_to_image(dataset, wavelengths=None, method='nearest', output=None, scale_factor=10000.0, rgb_wavelengths=None, **kwargs)</code>","text":"<p>Converts an EnMAP dataset to an RGB image. Automatically masks NoData and scales reflectance if needed.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>Path or xarray Dataset.</p> required <code>wavelengths</code> <code>np.ndarray</code> <p>Wavelengths to select.</p> <code>None</code> <code>method</code> <code>str</code> <p>Selection method (\"nearest\" by default).</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>File path to save image. If None, returns PIL.Image.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>Scale factor to convert integer reflectance to 0\u20131.</p> <code>10000.0</code> <code>rgb_wavelengths</code> <code>list</code> <p>Wavelengths (nm) to use for RGB, e.g., [660, 560, 480].</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Passed to leafmap.array_to_image().</p> <code>{}</code> Source code in <code>hypercoast/enmap.py</code> <pre><code>def enmap_to_image(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Optional[np.ndarray] = None,\n    method: str = \"nearest\",\n    output: Optional[str] = None,\n    scale_factor: float = 10000.0,\n    rgb_wavelengths: Optional[List[float]] = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Converts an EnMAP dataset to an RGB image.\n    Automatically masks NoData and scales reflectance if needed.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): Path or xarray Dataset.\n        wavelengths (np.ndarray, optional): Wavelengths to select.\n        method (str): Selection method (\"nearest\" by default).\n        output (str, optional): File path to save image. If None, returns PIL.Image.\n        scale_factor (float): Scale factor to convert integer reflectance to 0\u20131.\n        rgb_wavelengths (list, optional): Wavelengths (nm) to use for RGB, e.g., [660, 560, 480].\n        **kwargs: Passed to leafmap.array_to_image().\n    \"\"\"\n\n    if isinstance(dataset, str):\n        dataset = read_enmap(dataset, method=method)\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n\n    # Pick default RGB if not provided\n    if rgb_wavelengths is None and wavelengths is None:\n        rgb_wavelengths = [660, 560, 480]  # Red, Green, Blue in nm\n        dataset = dataset.sel(wavelength=rgb_wavelengths, method=\"nearest\")\n\n    arr = dataset[\"reflectance\"]\n\n    if \"_FillValue\" in dataset.attrs:\n        nodata = dataset.attrs[\"_FillValue\"]\n        arr = arr.where(arr != nodata)\n\n    if scale_factor and arr.max() &gt; 1.5:\n        arr = arr / scale_factor\n\n    return array_to_image(\n        arr,\n        output=output,\n        transpose=False,\n        dtype=np.float32,\n        **kwargs,\n    )\n</code></pre>"},{"location":"enmap/#hypercoast.enmap.extract_enmap","title":"<code>extract_enmap(ds, lat, lon)</code>","text":"<p>Extracts EnMAP reflectance spectrum from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>The dataset containing the EnMAP data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted spectrum with wavelength as coordinate.</p> Source code in <code>hypercoast/enmap.py</code> <pre><code>def extract_enmap(ds: xr.Dataset, lat: float, lon: float) -&gt; xr.DataArray:\n    \"\"\"\n    Extracts EnMAP reflectance spectrum from a given xarray Dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the EnMAP data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n\n    Returns:\n        xarray.DataArray: The extracted spectrum with wavelength as coordinate.\n    \"\"\"\n    if \"crs\" not in ds.attrs:\n        raise ValueError(\"Dataset has no CRS in attributes.\")\n\n    crs = ds.attrs[\"crs\"]\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    values = ds.sel(x=x, y=y, method=\"nearest\")[\"reflectance\"].values\n\n    da = xr.DataArray(\n        values,\n        dims=[\"wavelength\"],\n        coords={\"wavelength\": ds.coords[\"wavelength\"]},\n        name=\"reflectance\",\n    )\n\n    return da\n</code></pre>"},{"location":"enmap/#hypercoast.enmap.read_enmap","title":"<code>read_enmap(filepath, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Reads EnMAP hyperspectral GeoTIFF (COG) and returns an xarray dataset. If wavelengths are not in the file, they will be extracted from the associated Metadata XML file in the same directory. Masks NoData values as NaN.</p> Source code in <code>hypercoast/enmap.py</code> <pre><code>def read_enmap(\n    filepath: str,\n    wavelengths: Optional[List[float]] = None,\n    method: str = \"nearest\",\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads EnMAP hyperspectral GeoTIFF (COG) and returns an xarray dataset.\n    If wavelengths are not in the file, they will be extracted from the\n    associated Metadata XML file in the same directory.\n    Masks NoData values as NaN.\n    \"\"\"\n    ds = xr.open_dataset(filepath, engine=\"rasterio\")\n\n    if wavelengths is None:\n        if \"wavelength\" in ds:\n            wavelength = ds[\"wavelength\"].values.tolist()\n        elif \"wavelength\" in ds.attrs:\n            wavelength = [\n                float(w) for w in str(ds.attrs[\"wavelength\"]).replace(\",\", \" \").split()\n            ]\n        else:\n            folder = os.path.dirname(filepath)\n            pattern_lower = os.path.join(folder, \"*METADATA.xml\")\n            pattern_upper = os.path.join(folder, \"*METADATA.XML\")\n\n            matches = glob.glob(pattern_lower) + glob.glob(pattern_upper)\n            if not matches:\n                raise ValueError(\n                    f\"No wavelength information in GeoTIFF and no matching METADATA XML found in {folder}\"\n                )\n            xml_path = matches[0]\n            wavelength = _extract_enmap_wavelengths_from_xml(xml_path)\n\n    wavelength = [round(float(num), 2) for num in wavelength]\n\n    cols, rows = ds.x.size, ds.y.size\n    rio_transform = ds.rio.transform()\n    xres, _, xmin, _, yres, ymax = list(rio_transform)[:6]\n    xarr = np.array([xmin + i * xres for i in range(cols)])\n    yarr = np.array([ymax + i * yres for i in range(rows)])\n\n    ds[\"y\"] = xr.DataArray(\n        yarr,\n        dims=(\"y\",),\n        attrs={\"units\": \"m\", \"standard_name\": \"projection_y_coordinate\"},\n    )\n    ds[\"x\"] = xr.DataArray(\n        xarr,\n        dims=(\"x\",),\n        attrs={\"units\": \"m\", \"standard_name\": \"projection_x_coordinate\"},\n    )\n\n    # Dataset attributes\n    global_atts = ds.attrs\n    global_atts[\"Conventions\"] = \"CF-1.6\"\n    ds.attrs = {\n        \"units\": \"unitless\",\n        \"_FillValue\": 0,\n        \"grid_mapping\": \"crs\",\n        \"standard_name\": \"reflectance\",\n        \"long_name\": \"surface reflectance\",\n    }\n    ds.attrs.update(global_atts)\n\n    ds = ds.transpose(\"y\", \"x\", \"band\")\n    ds = ds.rename({\"band\": \"wavelength\", \"band_data\": \"reflectance\"})\n    ds.coords[\"wavelength\"] = wavelength\n\n    if ds.rio.crs:\n        ds.attrs[\"crs\"] = ds.rio.crs.to_string()\n    ds.rio.write_transform(rio_transform)\n\n    if wavelengths is not None:\n        ds = ds.sel(wavelength=wavelengths, method=method, **kwargs)\n\n    return ds\n</code></pre>"},{"location":"hypercoast/","title":"hypercoast module","text":"<p>Main module.</p>"},{"location":"hypercoast/#hypercoast.hypercoast.Map","title":"<code> Map            (Map)         </code>","text":"<p>A class that extends leafmap.Map to provide additional functionality for     hypercoast.</p> <p>Methods</p> <p>Any methods inherited from leafmap.Map.</p> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>class Map(leafmap.Map):\n    \"\"\"\n    A class that extends leafmap.Map to provide additional functionality for\n        hypercoast.\n\n    Attributes:\n        Any attributes inherited from leafmap.Map.\n\n    Methods:\n        Any methods inherited from leafmap.Map.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes a new instance of the Map class.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments that are passed to the parent\n                class's constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._spectral_data = {}\n        self._plot_options = None\n        self._plot_marker_cluster = ipyleaflet.MarkerCluster(name=\"Marker Cluster\")\n\n    def add(self, obj, position=\"topright\", xlim=None, ylim=None, **kwargs):\n        \"\"\"Add a layer to the map.\n\n        Args:\n            obj (str or object): The name of the layer or a layer object.\n            position (str, optional): The position of the layer widget. Can be\n                'topright', 'topleft', 'bottomright', or 'bottomleft'. Defaults\n                to 'topright'.\n            xlim (tuple, optional): The x-axis limits of the plot. Defaults to None.\n            ylim (tuple, optional): The y-axis limits of the plot. Defaults to None.\n            **kwargs: Arbitrary keyword arguments that are passed to the parent\n                class's add_layer method.\n        \"\"\"\n\n        if isinstance(obj, str):\n            if obj == \"spectral\":\n\n                SpectralWidget(self, position=position, xlim=xlim, ylim=ylim, **kwargs)\n                self.set_plot_options(add_marker_cluster=True)\n            else:\n                super().add(obj, **kwargs)\n\n        else:\n            super().add(obj, **kwargs)\n\n    def search_emit(self, default_dataset=\"EMITL2ARFL\"):\n        \"\"\"\n        Adds a NASA Earth Data search tool to the map with a default dataset for\n            EMIT.\n\n        Args:\n            default_dataset (str, optional): The default dataset to search for.\n                Defaults to \"EMITL2ARFL\".\n        \"\"\"\n        self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n\n    def search_pace(self, default_dataset=\"PACE_OCI_L2_AOP_NRT\"):\n        \"\"\"\n        Adds a NASA Earth Data search tool to the map with a default dataset for\n            PACE.\n\n        Args:\n            default_dataset (str, optional): The default dataset to search for.\n                Defaults to \"PACE_OCI_L2_AOP_NRT\".\n        \"\"\"\n        self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n\n    def search_ecostress(self, default_dataset=\"ECO_L2T_LSTE\"):\n        \"\"\"\n        Adds a NASA Earth Data search tool to the map with a default dataset for\n            ECOSTRESS.\n\n        Args:\n            default_dataset (str, optional): The default dataset to search for.\n                Defaults to \"ECO_L2T_LSTE\".\n        \"\"\"\n        self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n\n    def add_raster(\n        self,\n        source,\n        indexes=None,\n        colormap=None,\n        vmin=None,\n        vmax=None,\n        nodata=None,\n        attribution=None,\n        layer_name=\"Raster\",\n        layer_index=None,\n        zoom_to_layer=True,\n        visible=True,\n        opacity=1.0,\n        array_args=None,\n        client_args={\"cors_all\": False},\n        open_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add a local raster dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'Raster'.\n            layer_index (int, optional): The index of the layer. Defaults to None.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to\n                True.\n            opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            client_args (dict, optional): Additional arguments to pass to\n                localtileserver.TileClient. Defaults to { \"cors_all\": False }.\n            open_args (dict, optional): Additional arguments to pass to\n                rioxarray.open_rasterio.\n\n        \"\"\"\n\n        import rioxarray as rxr\n\n        if array_args is None:\n            array_args = {}\n        if open_args is None:\n            open_args = {}\n\n        if nodata is None:\n            nodata = np.nan\n        super().add_raster(\n            source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            layer_index=layer_index,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            opacity=opacity,\n            array_args=array_args,\n            client_args=client_args,\n            **kwargs,\n        )\n\n        if isinstance(source, str):\n            da = rxr.open_rasterio(source, **open_args)\n            dims = da.dims\n            da = da.transpose(dims[1], dims[2], dims[0])\n\n            xds = da.to_dataset(name=\"data\")\n            self.cog_layer_dict[layer_name][\"xds\"] = xds\n            # if self.cog_layer_dict[layer_name].get(\"hyper\") is None:\n            #     self.cog_layer_dict[layer_name][\"hyper\"] = \"COG\"\n\n    def add_dataset(\n        self,\n        source,\n        indexes=None,\n        colormap=None,\n        vmin=None,\n        vmax=None,\n        nodata=None,\n        attribution=None,\n        layer_name=\"Raster\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        open_args=None,\n        **kwargs,\n    ):\n        import rioxarray as rxr\n        from leafmap import array_to_image\n\n        if array_args is None:\n            array_args = {}\n\n        if open_args is None:\n            open_args = {}\n\n        if isinstance(source, str):\n            da = rxr.open_rasterio(source, **open_args)\n            dims = da.dims\n            da = da.transpose(dims[1], dims[2], dims[0])\n            xds = da.to_dataset(name=\"data\")\n\n        elif not isinstance(source, xr.Dataset):\n            raise ValueError(\n                \"source must be a path to a raster file or an xarray.Dataset object.\"\n            )\n        else:\n            xds = source\n\n        if indexes is None:\n            if xds.sizes[dims[2]] &lt; 3:\n                indexes = [1]\n            elif xds.sizes[dims[2]] &lt; 4:\n                indexes = [1, 2, 3]\n            else:\n                indexes = [3, 2, 1]\n\n        bands = [i - 1 for i in indexes]\n        da = xds.isel(band=bands)[\"data\"]\n        image = array_to_image(da, transpose=False)\n\n        self.add_raster(\n            image,\n            indexes=None,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"type\"] = \"XARRAY\"\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"XARRAY\"\n        self.cog_layer_dict[layer_name][\"band_names\"] = [\n            \"b\" + str(i) for i in xds.coords[\"band\"].values.tolist()\n        ]\n        self.cog_layer_dict[layer_name][\"indexes\"] = indexes\n        self.cog_layer_dict[layer_name][\"vis_bands\"] = [\"b\" + str(i) for i in indexes]\n\n    def add_emit(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=None,\n        vmax=None,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"EMIT\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add an EMIT dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band.\n                    See https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to\n                interpret as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to\n                True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n        \"\"\"\n\n        if array_args is None:\n            array_args = {}\n\n        xds = None\n        if isinstance(source, str):\n\n            xds = read_emit(source)\n            source = emit_to_image(xds, wavelengths=wavelengths)\n        elif isinstance(source, xr.Dataset):\n            xds = source\n            source = emit_to_image(xds, wavelengths=wavelengths)\n\n        self.add_raster(\n            source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"EMIT\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_pace(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=\"jet\",\n        vmin=None,\n        vmax=None,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"PACE\",\n        zoom_to_layer=True,\n        visible=True,\n        method=\"nearest\",\n        gridded=False,\n        array_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add a PACE dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                    https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n        \"\"\"\n\n        if array_args is None:\n            array_args = {}\n\n        if isinstance(source, str):\n\n            source = read_pace(source)\n\n        try:\n            image = pace_to_image(\n                source, wavelengths=wavelengths, method=method, gridded=gridded\n            )\n\n            if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n                colormap = None\n\n            self.add_raster(\n                image,\n                indexes=indexes,\n                colormap=colormap,\n                vmin=vmin,\n                vmax=vmax,\n                nodata=nodata,\n                attribution=attribution,\n                layer_name=layer_name,\n                zoom_to_layer=zoom_to_layer,\n                visible=visible,\n                array_args=array_args,\n                **kwargs,\n            )\n\n            self.cog_layer_dict[layer_name][\"xds\"] = source\n            self.cog_layer_dict[layer_name][\"hyper\"] = \"PACE\"\n            self._update_band_names(layer_name, wavelengths)\n        except Exception as e:\n            print(e)\n\n    def add_desis(\n        self,\n        source,\n        wavelengths=[900, 650, 525],\n        indexes=None,\n        colormap=\"jet\",\n        vmin=None,\n        vmax=None,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"DESIS\",\n        zoom_to_layer=True,\n        visible=True,\n        method=\"nearest\",\n        array_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add a DESIS dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is 'jet'.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n        \"\"\"\n        if array_args is None:\n            array_args = {}\n\n        if isinstance(source, str):\n\n            source = read_desis(source)\n\n        image = desis_to_image(source, wavelengths=wavelengths, method=method)\n\n        if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n            colormap = None\n\n        if isinstance(wavelengths, int):\n            wavelengths = [wavelengths]\n\n        if indexes is None:\n            if isinstance(wavelengths, list) and len(wavelengths) == 1:\n                indexes = [1]\n            else:\n                indexes = [1, 2, 3]\n\n        self.add_raster(\n            image,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = source\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"DESIS\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_wyvern(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=\"jet\",\n        vmin=None,\n        vmax=None,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"WYVERN\",\n        zoom_to_layer=True,\n        visible=True,\n        method=\"nearest\",\n        array_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add a WYVERN dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is 'jet'.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'WYVERN'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n        \"\"\"\n        if array_args is None:\n            array_args = {}\n\n        if isinstance(source, str):\n\n            source = read_wyvern(source)\n\n        image = wyvern_to_image(\n            source,\n            wavelengths=wavelengths,\n            method=method,\n            nodata=nodata,\n            vmin=vmin,\n            vmax=vmax,\n        )\n\n        if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n            colormap = None\n\n        if isinstance(wavelengths, int):\n            wavelengths = [wavelengths]\n\n        if indexes is None:\n            if isinstance(wavelengths, list) and len(wavelengths) == 1:\n                indexes = [1]\n            else:\n                indexes = [1, 2, 3]\n\n        self.add_raster(\n            image,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = source\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"WYVERN\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_neon(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=0,\n        vmax=0.5,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"NEON\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        method=\"nearest\",\n        **kwargs,\n    ):\n        \"\"\"Add an NEON AOP dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the NEON AOP HDF5 file.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                    https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to 0.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to 0.5.\n            nodata (float, optional): The value from the band to use to\n                interpret as not valid data. Defaults to np.nan.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'NEON'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults\n                to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            method (str, optional): The method to use for data interpolation.\n                Defaults to \"nearest\".\n        \"\"\"\n\n        if array_args is None:\n            array_args = {}\n        xds = None\n        if isinstance(source, str):\n\n            xds = read_neon(source)\n            source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n        elif isinstance(source, xr.Dataset):\n            xds = source\n            source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n\n        self.add_raster(\n            source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"NEON\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_aviris(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=0,\n        vmax=0.5,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"AVIRIS\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        method=\"nearest\",\n        **kwargs,\n    ):\n        \"\"\"Add an AVIRIS dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the AVIRIS file.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                    https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to 0.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to 0.5.\n            nodata (float, optional): The value from the band to use to\n                interpret as not valid data. Defaults to np.nan.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'NEON'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults\n                to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            method (str, optional): The method to use for data interpolation.\n                Defaults to \"nearest\".\n        \"\"\"\n        if array_args is None:\n            array_args = {}\n\n        xds = None\n        if isinstance(source, str):\n\n            xds = read_aviris(source)\n            source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n        elif isinstance(source, xr.Dataset):\n            xds = source\n            source = aviris_to_image(xds, wavelengths=wavelengths, method=method)\n\n        self.add_raster(\n            source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"AVIRIS\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_prisma(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=0,\n        vmax=0.5,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"PRISMA\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        method=\"nearest\",\n        **kwargs,\n    ):\n        \"\"\"Add a PRISMA dataset to the map.\n\n        This function reads a PRISMA hyperspectral dataset, optionally selects\n        specific wavelengths, converts the data to an image, and adds it as a\n        raster layer to the map. The dataset can be provided as a file path or\n        as an xarray Dataset.\n\n        Args:\n            source (str or xarray.Dataset): The path to the PRISMA file or an\n                in-memory xarray Dataset containing PRISMA data.\n            wavelengths (list or np.ndarray, optional): Specific wavelengths to\n                select from the dataset. If None, all wavelengths are used.\n                Defaults to None.\n            indexes (int or list, optional): The band(s) to display. Band\n                indexing starts at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See:\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n            vmin (float, optional): The minimum value for color mapping when\n                plotting a single band. Defaults to 0.\n            vmax (float, optional): The maximum value for color mapping when\n                plotting a single band. Defaults to 0.5.\n            nodata (float, optional): Value in the raster to interpret as\n                no-data. Defaults to np.nan.\n            attribution (str, optional): Attribution for the source raster.\n                Defaults to None.\n            layer_name (str, optional): The name to assign to the map layer.\n                Defaults to \"PRISMA\".\n            zoom_to_layer (bool, optional): Whether to zoom the map to the\n                extent of the layer after adding it. Defaults to True.\n            visible (bool, optional): Whether the layer should be visible when\n                first added. Defaults to True.\n            array_args (dict, optional): Additional keyword arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            method (str, optional): Method to use for wavelength interpolation\n                when selecting bands. Options may include \"nearest\", \"linear\",\n                etc. Defaults to \"nearest\".\n            **kwargs: Additional keyword arguments passed to `add_raster`.\n        \"\"\"\n        if array_args is None:\n            array_args = {}\n\n        if isinstance(source, str):\n            xds = read_prisma(source, wavelengths=wavelengths, method=method)\n        else:\n            xds = source\n\n        with tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False) as tmp:\n            temp_path = tmp.name\n\n        prisma_to_image(xds, wavelengths=wavelengths, method=method, output=temp_path)\n\n        self.add_raster(\n            temp_path,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"PRISMA\"\n\n    def add_enmap(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=0,\n        vmax=0.5,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"EnMAP\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        method=\"nearest\",\n        **kwargs,\n    ):\n        \"\"\"Add an EnMAP dataset to the map.\n\n        This function reads an EnMAP hyperspectral dataset, optionally selects\n        specific wavelengths, converts the data to an image, and adds it as a\n        raster layer to the map. The dataset can be provided as a file path or as\n        an xarray Dataset.\n\n        Args:\n            source (str or xarray.Dataset): The path to the EnMAP file or an\n                in-memory xarray Dataset containing EnMAP data.\n            wavelengths (list or np.ndarray, optional): Specific wavelengths to\n                select from the dataset. If None, all wavelengths are used.\n                Defaults to None.\n            indexes (int or list, optional): The band(s) to display. Band\n                indexing starts at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band.\n            vmin (float, optional): The minimum value for color mapping when\n                plotting a single band. Defaults to 0.\n            vmax (float, optional): The maximum value for color mapping when\n                plotting a single band. Defaults to 0.5.\n            nodata (float, optional): Value in the raster to interpret as\n                no-data. Defaults to np.nan.\n            attribution (str, optional): Attribution for the source raster.\n                Defaults to None.\n            layer_name (str, optional): The name to assign to the map layer.\n                Defaults to \"EnMAP\".\n            zoom_to_layer (bool, optional): Whether to zoom the map to the\n                extent of the layer after adding it. Defaults to True.\n            visible (bool, optional): Whether the layer should be visible when\n                first added. Defaults to True.\n            array_args (dict, optional): Additional keyword arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            method (str, optional): Method to use for wavelength interpolation\n                when selecting bands. Defaults to \"nearest\".\n            **kwargs: Additional keyword arguments passed to `add_raster`.\n        \"\"\"\n        if array_args is None:\n            array_args = {}\n\n        if isinstance(source, str):\n            xds = read_enmap(source, wavelengths=wavelengths, method=method)\n        else:\n            xds = source\n\n        with tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False) as tmp:\n            temp_path = tmp.name\n\n        enmap_to_image(xds, wavelengths=wavelengths, method=method, output=temp_path)\n\n        self.add_raster(\n            temp_path,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"EnMAP\"\n\n    def add_tanager(\n        self,\n        source,\n        bands=None,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=0,\n        vmax=120,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"Tanager\",\n        zoom_to_layer=True,\n        visible=True,\n        method=\"nearest\",\n        array_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add a Tanager dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            bands (list, optional): The band indices to select. Defaults to None.\n            wavelengths (list, optional): The wavelength values to select. Takes priority over bands. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                    https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n        \"\"\"\n\n        if array_args is None:\n            array_args = {}\n\n        if isinstance(source, str):\n\n            source = read_tanager(source)\n\n        selected_wavelengths = []\n        if wavelengths is not None:\n            selected_wavelengths = wavelengths\n        elif bands is not None:\n            for band in bands:\n                if isinstance(band, (int, np.integer)) or (\n                    isinstance(band, float) and band &lt; 500\n                ):\n                    # Treat as band index\n                    selected_wavelengths.append(\n                        source.coords[\"wavelength\"].values[int(band)]\n                    )\n                else:\n                    # Treat as wavelength value\n                    selected_wavelengths.append(band)\n\n        else:\n            selected_wavelengths = [876.3, 675.88, 625.83]\n        try:\n            image = tanager_to_image(\n                source, wavelengths=selected_wavelengths, method=method\n            )\n\n            if isinstance(selected_wavelengths, list) and len(selected_wavelengths) &gt; 1:\n                colormap = None\n\n            self.add_raster(\n                image,\n                indexes=indexes,\n                colormap=colormap,\n                vmin=vmin,\n                vmax=vmax,\n                nodata=nodata,\n                attribution=attribution,\n                layer_name=layer_name,\n                zoom_to_layer=zoom_to_layer,\n                visible=visible,\n                array_args=array_args,\n                **kwargs,\n            )\n\n            self.cog_layer_dict[layer_name][\"xds\"] = source\n            self.cog_layer_dict[layer_name][\"vmax\"] = vmax\n            self.cog_layer_dict[layer_name][\"vmin\"] = vmin\n            self.cog_layer_dict[layer_name][\"hyper\"] = \"TANAGER\"\n            self._update_band_names(layer_name, selected_wavelengths)\n        except Exception as e:\n            print(e)\n\n    def add_hyper(self, xds, dtype, wvl_indexes=None, **kwargs):\n        \"\"\"Add a hyperspectral dataset to the map.\n\n        Args:\n            xds (str): The Xarray dataset containing the hyperspectral data.\n            dtype (str): The type of the hyperspectral dataset. Can be one of\n                \"EMIT\", \"PACE\", \"DESIS\", \"NEON\", \"AVIRIS\".\n            **kwargs: Additional keyword arguments to pass to the corresponding\n                add function.\n        \"\"\"\n\n        if wvl_indexes is not None:\n            if dtype == \"XARRAY\":\n                kwargs[\"indexes\"] = [i + 1 for i in wvl_indexes]\n            else:\n                if \"wavelength\" in xds.dims:\n                    kwargs[\"wavelengths\"] = (\n                        xds.isel(wavelength=wvl_indexes)\n                        .coords[\"wavelength\"]\n                        .values.tolist()\n                    )\n                else:\n                    kwargs[\"bands\"] = wvl_indexes\n\n        if dtype == \"EMIT\":\n            self.add_emit(xds, **kwargs)\n        elif dtype == \"PACE\":\n            self.add_pace(xds, **kwargs)\n        elif dtype == \"DESIS\":\n            self.add_desis(xds, **kwargs)\n        elif dtype == \"NEON\":\n            self.add_neon(xds, **kwargs)\n        elif dtype == \"AVIRIS\":\n            self.add_aviris(xds, **kwargs)\n        elif dtype == \"TANAGER\":\n            self.add_tanager(xds, **kwargs)\n        elif dtype == \"XARRAY\":\n            kwargs.pop(\"wavelengths\", None)\n            self.add_dataset(xds, **kwargs)\n\n    def set_plot_options(\n        self,\n        add_marker_cluster=False,\n        plot_type=None,\n        overlay=False,\n        position=\"bottomright\",\n        min_width=None,\n        max_width=None,\n        min_height=None,\n        max_height=None,\n        **kwargs,\n    ):\n        \"\"\"Sets plotting options.\n\n        Args:\n            add_marker_cluster (bool, optional): Whether to add a marker cluster.\n                Defaults to False.\n            sample_scale (float, optional):  A nominal scale in meters of the\n                projection to sample in . Defaults to None.\n            plot_type (str, optional): The plot type can be one of \"None\", \"bar\",\n                \"scatter\" or \"hist\". Defaults to None.\n            overlay (bool, optional): Whether to overlay plotted lines on the\n                figure. Defaults to False.\n            position (str, optional): Position of the control, can be\n                \u2018bottomleft\u2019, \u2018bottomright\u2019, \u2018topleft\u2019, or \u2018topright\u2019. Defaults\n                to 'bottomright'.\n            min_width (int, optional): Min width of the widget (in pixels), if\n                None it will respect the content size. Defaults to None.\n            max_width (int, optional): Max width of the widget (in pixels), if\n                None it will respect the content size. Defaults to None.\n            min_height (int, optional): Min height of the widget (in pixels), if\n                None it will respect the content size. Defaults to None.\n            max_height (int, optional): Max height of the widget (in pixels), if\n                None it will respect the content size. Defaults to None.\n\n        \"\"\"\n        plot_options_dict = {}\n        plot_options_dict[\"add_marker_cluster\"] = add_marker_cluster\n        plot_options_dict[\"plot_type\"] = plot_type\n        plot_options_dict[\"overlay\"] = overlay\n        plot_options_dict[\"position\"] = position\n        plot_options_dict[\"min_width\"] = min_width\n        plot_options_dict[\"max_width\"] = max_width\n        plot_options_dict[\"min_height\"] = min_height\n        plot_options_dict[\"max_height\"] = max_height\n\n        for key in kwargs:\n            plot_options_dict[key] = kwargs[key]\n\n        self._plot_options = plot_options_dict\n\n        if not hasattr(self, \"_plot_marker_cluster\"):\n            self._plot_marker_cluster = ipyleaflet.MarkerCluster(name=\"Marker Cluster\")\n\n        if add_marker_cluster and (self._plot_marker_cluster not in self.layers):\n            self.add(self._plot_marker_cluster)\n\n    def spectral_to_df(self, **kwargs):\n        \"\"\"Converts the spectral data to a pandas DataFrame.\n\n        Returns:\n            pd.DataFrame: The spectral data as a pandas DataFrame.\n        \"\"\"\n        import pandas as pd\n\n        df = pd.DataFrame(self._spectral_data, **kwargs)\n        return df\n\n    def spectral_to_gdf(self, **kwargs):\n        \"\"\"Converts the spectral data to a GeoPandas GeoDataFrame.\n\n        Returns:\n            gpd.DataFrame: The spectral data as a pandas DataFrame.\n        \"\"\"\n        import geopandas as gpd\n        from shapely.geometry import Point\n\n        df = self.spectral_to_df()\n\n        if len(df) == 0:\n            return df\n\n        # Step 1: Extract the coordinates from the columns\n        if \"wavelength\" in df.columns:\n            df = df.rename(columns={\"wavelength\": \"latlon\"})\n        elif \"wavelengths\" in df.columns:\n            df = df.rename(columns={\"wavelengths\": \"latlon\"})\n        coordinates = [col.strip(\"()\").split() for col in df.columns[1:]]\n        coords = [(float(lat), float(lon)) for lat, lon in coordinates]\n\n        # Step 2: Create Point geometries for each coordinate\n        points = [Point(lon, lat) for lat, lon in coords]\n\n        # Step 3: Create a GeoDataFrame\n        df_transposed = df.set_index(\"latlon\").T\n\n        # Convert the column names to strings to ensure compatibility with GeoJSON\n        df_transposed.columns = df_transposed.columns.astype(str)\n\n        # Create the GeoDataFrame\n        gdf = gpd.GeoDataFrame(df_transposed, geometry=points, **kwargs)\n\n        # Set the coordinate reference system (CRS)\n        gdf = gdf.set_geometry(\"geometry\").set_crs(\"EPSG:4326\")\n\n        return gdf\n\n    def spectral_to_csv(self, filename, index=True, **kwargs):\n        \"\"\"Saves the spectral data to a CSV file.\n\n        Args:\n            filename (str): The output CSV file.\n            index (bool, optional): Whether to write the index. Defaults to True.\n        \"\"\"\n        df = self.spectral_to_df()\n        df = df.rename_axis(\"band\")\n        df.to_csv(filename, index=index, **kwargs)\n\n    def _update_band_names(self, layer_name, wavelengths):\n\n        # Function to find the nearest indices\n        def find_nearest_indices(\n            dataarray, selected_wavelengths, dim_name=\"wavelength\"\n        ):\n            indices = []\n            for wavelength in selected_wavelengths:\n                if dim_name == \"band\":\n                    nearest_wavelength = dataarray.sel(\n                        band=wavelength, method=\"nearest\"\n                    )\n                else:\n                    nearest_wavelength = dataarray.sel(\n                        wavelength=wavelength, method=\"nearest\"\n                    )\n                nearest_wavelength_index = nearest_wavelength[dim_name].item()\n                nearest_index = (\n                    dataarray[dim_name].values.tolist().index(nearest_wavelength_index)\n                )\n                indices.append(nearest_index + 1)\n            return indices\n\n        if \"xds\" in self.cog_layer_dict[layer_name]:\n            xds = self.cog_layer_dict[layer_name][\"xds\"]\n            dim_name = \"wavelength\"\n\n            if \"band\" in xds:\n                dim_name = \"band\"\n\n            band_count = xds.sizes[dim_name]\n            band_names = [\"b\" + str(band) for band in range(1, band_count + 1)]\n            self.cog_layer_dict[layer_name][\"band_names\"] = band_names\n\n            try:\n                indexes = find_nearest_indices(xds, wavelengths, dim_name=dim_name)\n                vis_bands = [\"b\" + str(index) for index in indexes]\n                self.cog_layer_dict[layer_name][\"indexes\"] = indexes\n                self.cog_layer_dict[layer_name][\"vis_bands\"] = vis_bands\n            except Exception as e:\n                print(e)\n\n    def add_field_data(\n        self,\n        data: Union[str],\n        x_col: str = \"wavelength\",\n        y_col_prefix: str = \"(\",\n        x_label: str = \"Wavelengths (nm)\",\n        y_label: str = \"Reflectance\",\n        use_marker_cluster: bool = True,\n        min_width: int = 400,\n        max_width: int = 600,\n        min_height: int = 200,\n        max_height: int = 250,\n        layer_name: str = \"Marker Cluster\",\n        **kwargs,\n    ):\n        \"\"\"\n        Displays field data on a map with interactive markers and popups showing time series data.\n\n        Args:\n            data (Union[str, pd.DataFrame]): Path to the CSV file or a pandas DataFrame containing the data.\n            x_col (str): Column name to use for the x-axis of the charts. Default is \"wavelength\".\n            y_col_prefix (str): Prefix to identify the columns that contain the location-specific data. Default is \"(\".\n            x_label (str): Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".\n            y_label (str): Label for the y-axis of the charts. Default is \"Reflectance\".\n            use_marker_cluster (bool): Whether to use marker clustering. Default is True.\n            min_width (int): Minimum width of the popup. Default is 400.\n            max_width (int): Maximum width of the popup. Default is 600.\n            min_height (int): Minimum height of the popup. Default is 200.\n            max_height (int): Maximum height of the popup. Default is 250.\n            layer_name (str): Name of the marker cluster layer. Default is \"Marker Cluster\".\n\n        Returns:\n            Map: An ipyleaflet Map with the added markers and popups.\n        \"\"\"\n        show_field_data(\n            data,\n            x_col,\n            y_col_prefix,\n            x_label=x_label,\n            y_label=y_label,\n            use_marker_cluster=use_marker_cluster,\n            min_width=min_width,\n            max_width=max_width,\n            min_height=min_height,\n            max_height=max_height,\n            layer_name=layer_name,\n            m=self,\n            **kwargs,\n        )\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.__init__","title":"<code>__init__(self, **kwargs)</code>  <code>special</code>","text":"<p>Initializes a new instance of the Map class.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments that are passed to the parent class's constructor.</p> <code>{}</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a new instance of the Map class.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments that are passed to the parent\n            class's constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._spectral_data = {}\n    self._plot_options = None\n    self._plot_marker_cluster = ipyleaflet.MarkerCluster(name=\"Marker Cluster\")\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add","title":"<code>add(self, obj, position='topright', xlim=None, ylim=None, **kwargs)</code>","text":"<p>Add a layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>str or object</code> <p>The name of the layer or a layer object.</p> required <code>position</code> <code>str</code> <p>The position of the layer widget. Can be 'topright', 'topleft', 'bottomright', or 'bottomleft'. Defaults to 'topright'.</p> <code>'topright'</code> <code>xlim</code> <code>tuple</code> <p>The x-axis limits of the plot. Defaults to None.</p> <code>None</code> <code>ylim</code> <code>tuple</code> <p>The y-axis limits of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Arbitrary keyword arguments that are passed to the parent class's add_layer method.</p> <code>{}</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add(self, obj, position=\"topright\", xlim=None, ylim=None, **kwargs):\n    \"\"\"Add a layer to the map.\n\n    Args:\n        obj (str or object): The name of the layer or a layer object.\n        position (str, optional): The position of the layer widget. Can be\n            'topright', 'topleft', 'bottomright', or 'bottomleft'. Defaults\n            to 'topright'.\n        xlim (tuple, optional): The x-axis limits of the plot. Defaults to None.\n        ylim (tuple, optional): The y-axis limits of the plot. Defaults to None.\n        **kwargs: Arbitrary keyword arguments that are passed to the parent\n            class's add_layer method.\n    \"\"\"\n\n    if isinstance(obj, str):\n        if obj == \"spectral\":\n\n            SpectralWidget(self, position=position, xlim=xlim, ylim=ylim, **kwargs)\n            self.set_plot_options(add_marker_cluster=True)\n        else:\n            super().add(obj, **kwargs)\n\n    else:\n        super().add(obj, **kwargs)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_aviris","title":"<code>add_aviris(self, source, wavelengths=None, indexes=None, colormap=None, vmin=0, vmax=0.5, nodata=nan, attribution=None, layer_name='AVIRIS', zoom_to_layer=True, visible=True, array_args=None, method='nearest', **kwargs)</code>","text":"<p>Add an AVIRIS dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the AVIRIS file.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See     https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to 0.5.</p> <code>0.5</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to np.nan.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'NEON'.</p> <code>'AVIRIS'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_aviris(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=0,\n    vmax=0.5,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"AVIRIS\",\n    zoom_to_layer=True,\n    visible=True,\n    array_args=None,\n    method=\"nearest\",\n    **kwargs,\n):\n    \"\"\"Add an AVIRIS dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the AVIRIS file.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to 0.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to 0.5.\n        nodata (float, optional): The value from the band to use to\n            interpret as not valid data. Defaults to np.nan.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'NEON'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults\n            to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n    \"\"\"\n    if array_args is None:\n        array_args = {}\n\n    xds = None\n    if isinstance(source, str):\n\n        xds = read_aviris(source)\n        source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n    elif isinstance(source, xr.Dataset):\n        xds = source\n        source = aviris_to_image(xds, wavelengths=wavelengths, method=method)\n\n    self.add_raster(\n        source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = xds\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"AVIRIS\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_desis","title":"<code>add_desis(self, source, wavelengths=[900, 650, 525], indexes=None, colormap='jet', vmin=None, vmax=None, nodata=nan, attribution=None, layer_name='DESIS', zoom_to_layer=True, visible=True, method='nearest', array_args=None, **kwargs)</code>","text":"<p>Add a DESIS dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See https://matplotlib.org/stable/gallery/color/colormap_reference.html. Default is 'jet'.</p> <code>'jet'</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'EMIT'.</p> <code>'DESIS'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_desis(\n    self,\n    source,\n    wavelengths=[900, 650, 525],\n    indexes=None,\n    colormap=\"jet\",\n    vmin=None,\n    vmax=None,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"DESIS\",\n    zoom_to_layer=True,\n    visible=True,\n    method=\"nearest\",\n    array_args=None,\n    **kwargs,\n):\n    \"\"\"Add a DESIS dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n            https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n            Default is 'jet'.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n    \"\"\"\n    if array_args is None:\n        array_args = {}\n\n    if isinstance(source, str):\n\n        source = read_desis(source)\n\n    image = desis_to_image(source, wavelengths=wavelengths, method=method)\n\n    if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n        colormap = None\n\n    if isinstance(wavelengths, int):\n        wavelengths = [wavelengths]\n\n    if indexes is None:\n        if isinstance(wavelengths, list) and len(wavelengths) == 1:\n            indexes = [1]\n        else:\n            indexes = [1, 2, 3]\n\n    self.add_raster(\n        image,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = source\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"DESIS\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_emit","title":"<code>add_emit(self, source, wavelengths=None, indexes=None, colormap=None, vmin=None, vmax=None, nodata=nan, attribution=None, layer_name='EMIT', zoom_to_layer=True, visible=True, array_args=None, **kwargs)</code>","text":"<p>Add an EMIT dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band.     See https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'EMIT'.</p> <code>'EMIT'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_emit(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=None,\n    vmax=None,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"EMIT\",\n    zoom_to_layer=True,\n    visible=True,\n    array_args=None,\n    **kwargs,\n):\n    \"\"\"Add an EMIT dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band.\n                See https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to\n            interpret as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to\n            True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n    \"\"\"\n\n    if array_args is None:\n        array_args = {}\n\n    xds = None\n    if isinstance(source, str):\n\n        xds = read_emit(source)\n        source = emit_to_image(xds, wavelengths=wavelengths)\n    elif isinstance(source, xr.Dataset):\n        xds = source\n        source = emit_to_image(xds, wavelengths=wavelengths)\n\n    self.add_raster(\n        source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = xds\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"EMIT\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_enmap","title":"<code>add_enmap(self, source, wavelengths=None, indexes=None, colormap=None, vmin=0, vmax=0.5, nodata=nan, attribution=None, layer_name='EnMAP', zoom_to_layer=True, visible=True, array_args=None, method='nearest', **kwargs)</code>","text":"<p>Add an EnMAP dataset to the map.</p> <p>This function reads an EnMAP hyperspectral dataset, optionally selects specific wavelengths, converts the data to an image, and adds it as a raster layer to the map. The dataset can be provided as a file path or as an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str or xarray.Dataset</code> <p>The path to the EnMAP file or an in-memory xarray Dataset containing EnMAP data.</p> required <code>wavelengths</code> <code>list or np.ndarray</code> <p>Specific wavelengths to select from the dataset. If None, all wavelengths are used. Defaults to None.</p> <code>None</code> <code>indexes</code> <code>int or list</code> <p>The band(s) to display. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value for color mapping when plotting a single band. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>The maximum value for color mapping when plotting a single band. Defaults to 0.5.</p> <code>0.5</code> <code>nodata</code> <code>float</code> <p>Value in the raster to interpret as no-data. Defaults to np.nan.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The name to assign to the map layer. Defaults to \"EnMAP\".</p> <code>'EnMAP'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom the map to the extent of the layer after adding it. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer should be visible when first added. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional keyword arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for wavelength interpolation when selecting bands. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>add_raster</code>.</p> <code>{}</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_enmap(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=0,\n    vmax=0.5,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"EnMAP\",\n    zoom_to_layer=True,\n    visible=True,\n    array_args=None,\n    method=\"nearest\",\n    **kwargs,\n):\n    \"\"\"Add an EnMAP dataset to the map.\n\n    This function reads an EnMAP hyperspectral dataset, optionally selects\n    specific wavelengths, converts the data to an image, and adds it as a\n    raster layer to the map. The dataset can be provided as a file path or as\n    an xarray Dataset.\n\n    Args:\n        source (str or xarray.Dataset): The path to the EnMAP file or an\n            in-memory xarray Dataset containing EnMAP data.\n        wavelengths (list or np.ndarray, optional): Specific wavelengths to\n            select from the dataset. If None, all wavelengths are used.\n            Defaults to None.\n        indexes (int or list, optional): The band(s) to display. Band\n            indexing starts at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band.\n        vmin (float, optional): The minimum value for color mapping when\n            plotting a single band. Defaults to 0.\n        vmax (float, optional): The maximum value for color mapping when\n            plotting a single band. Defaults to 0.5.\n        nodata (float, optional): Value in the raster to interpret as\n            no-data. Defaults to np.nan.\n        attribution (str, optional): Attribution for the source raster.\n            Defaults to None.\n        layer_name (str, optional): The name to assign to the map layer.\n            Defaults to \"EnMAP\".\n        zoom_to_layer (bool, optional): Whether to zoom the map to the\n            extent of the layer after adding it. Defaults to True.\n        visible (bool, optional): Whether the layer should be visible when\n            first added. Defaults to True.\n        array_args (dict, optional): Additional keyword arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        method (str, optional): Method to use for wavelength interpolation\n            when selecting bands. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments passed to `add_raster`.\n    \"\"\"\n    if array_args is None:\n        array_args = {}\n\n    if isinstance(source, str):\n        xds = read_enmap(source, wavelengths=wavelengths, method=method)\n    else:\n        xds = source\n\n    with tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False) as tmp:\n        temp_path = tmp.name\n\n    enmap_to_image(xds, wavelengths=wavelengths, method=method, output=temp_path)\n\n    self.add_raster(\n        temp_path,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = xds\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"EnMAP\"\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_field_data","title":"<code>add_field_data(self, data, x_col='wavelength', y_col_prefix='(', x_label='Wavelengths (nm)', y_label='Reflectance', use_marker_cluster=True, min_width=400, max_width=600, min_height=200, max_height=250, layer_name='Marker Cluster', **kwargs)</code>","text":"<p>Displays field data on a map with interactive markers and popups showing time series data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, pd.DataFrame]</code> <p>Path to the CSV file or a pandas DataFrame containing the data.</p> required <code>x_col</code> <code>str</code> <p>Column name to use for the x-axis of the charts. Default is \"wavelength\".</p> <code>'wavelength'</code> <code>y_col_prefix</code> <code>str</code> <p>Prefix to identify the columns that contain the location-specific data. Default is \"(\".</p> <code>'('</code> <code>x_label</code> <code>str</code> <p>Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".</p> <code>'Wavelengths (nm)'</code> <code>y_label</code> <code>str</code> <p>Label for the y-axis of the charts. Default is \"Reflectance\".</p> <code>'Reflectance'</code> <code>use_marker_cluster</code> <code>bool</code> <p>Whether to use marker clustering. Default is True.</p> <code>True</code> <code>min_width</code> <code>int</code> <p>Minimum width of the popup. Default is 400.</p> <code>400</code> <code>max_width</code> <code>int</code> <p>Maximum width of the popup. Default is 600.</p> <code>600</code> <code>min_height</code> <code>int</code> <p>Minimum height of the popup. Default is 200.</p> <code>200</code> <code>max_height</code> <code>int</code> <p>Maximum height of the popup. Default is 250.</p> <code>250</code> <code>layer_name</code> <code>str</code> <p>Name of the marker cluster layer. Default is \"Marker Cluster\".</p> <code>'Marker Cluster'</code> <p>Returns:</p> Type Description <code>Map</code> <p>An ipyleaflet Map with the added markers and popups.</p> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_field_data(\n    self,\n    data: Union[str],\n    x_col: str = \"wavelength\",\n    y_col_prefix: str = \"(\",\n    x_label: str = \"Wavelengths (nm)\",\n    y_label: str = \"Reflectance\",\n    use_marker_cluster: bool = True,\n    min_width: int = 400,\n    max_width: int = 600,\n    min_height: int = 200,\n    max_height: int = 250,\n    layer_name: str = \"Marker Cluster\",\n    **kwargs,\n):\n    \"\"\"\n    Displays field data on a map with interactive markers and popups showing time series data.\n\n    Args:\n        data (Union[str, pd.DataFrame]): Path to the CSV file or a pandas DataFrame containing the data.\n        x_col (str): Column name to use for the x-axis of the charts. Default is \"wavelength\".\n        y_col_prefix (str): Prefix to identify the columns that contain the location-specific data. Default is \"(\".\n        x_label (str): Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".\n        y_label (str): Label for the y-axis of the charts. Default is \"Reflectance\".\n        use_marker_cluster (bool): Whether to use marker clustering. Default is True.\n        min_width (int): Minimum width of the popup. Default is 400.\n        max_width (int): Maximum width of the popup. Default is 600.\n        min_height (int): Minimum height of the popup. Default is 200.\n        max_height (int): Maximum height of the popup. Default is 250.\n        layer_name (str): Name of the marker cluster layer. Default is \"Marker Cluster\".\n\n    Returns:\n        Map: An ipyleaflet Map with the added markers and popups.\n    \"\"\"\n    show_field_data(\n        data,\n        x_col,\n        y_col_prefix,\n        x_label=x_label,\n        y_label=y_label,\n        use_marker_cluster=use_marker_cluster,\n        min_width=min_width,\n        max_width=max_width,\n        min_height=min_height,\n        max_height=max_height,\n        layer_name=layer_name,\n        m=self,\n        **kwargs,\n    )\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_hyper","title":"<code>add_hyper(self, xds, dtype, wvl_indexes=None, **kwargs)</code>","text":"<p>Add a hyperspectral dataset to the map.</p> <p>Parameters:</p> Name Type Description Default <code>xds</code> <code>str</code> <p>The Xarray dataset containing the hyperspectral data.</p> required <code>dtype</code> <code>str</code> <p>The type of the hyperspectral dataset. Can be one of \"EMIT\", \"PACE\", \"DESIS\", \"NEON\", \"AVIRIS\".</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the corresponding add function.</p> <code>{}</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_hyper(self, xds, dtype, wvl_indexes=None, **kwargs):\n    \"\"\"Add a hyperspectral dataset to the map.\n\n    Args:\n        xds (str): The Xarray dataset containing the hyperspectral data.\n        dtype (str): The type of the hyperspectral dataset. Can be one of\n            \"EMIT\", \"PACE\", \"DESIS\", \"NEON\", \"AVIRIS\".\n        **kwargs: Additional keyword arguments to pass to the corresponding\n            add function.\n    \"\"\"\n\n    if wvl_indexes is not None:\n        if dtype == \"XARRAY\":\n            kwargs[\"indexes\"] = [i + 1 for i in wvl_indexes]\n        else:\n            if \"wavelength\" in xds.dims:\n                kwargs[\"wavelengths\"] = (\n                    xds.isel(wavelength=wvl_indexes)\n                    .coords[\"wavelength\"]\n                    .values.tolist()\n                )\n            else:\n                kwargs[\"bands\"] = wvl_indexes\n\n    if dtype == \"EMIT\":\n        self.add_emit(xds, **kwargs)\n    elif dtype == \"PACE\":\n        self.add_pace(xds, **kwargs)\n    elif dtype == \"DESIS\":\n        self.add_desis(xds, **kwargs)\n    elif dtype == \"NEON\":\n        self.add_neon(xds, **kwargs)\n    elif dtype == \"AVIRIS\":\n        self.add_aviris(xds, **kwargs)\n    elif dtype == \"TANAGER\":\n        self.add_tanager(xds, **kwargs)\n    elif dtype == \"XARRAY\":\n        kwargs.pop(\"wavelengths\", None)\n        self.add_dataset(xds, **kwargs)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_neon","title":"<code>add_neon(self, source, wavelengths=None, indexes=None, colormap=None, vmin=0, vmax=0.5, nodata=nan, attribution=None, layer_name='NEON', zoom_to_layer=True, visible=True, array_args=None, method='nearest', **kwargs)</code>","text":"<p>Add an NEON AOP dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the NEON AOP HDF5 file.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See     https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to 0.5.</p> <code>0.5</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to np.nan.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'NEON'.</p> <code>'NEON'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_neon(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=0,\n    vmax=0.5,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"NEON\",\n    zoom_to_layer=True,\n    visible=True,\n    array_args=None,\n    method=\"nearest\",\n    **kwargs,\n):\n    \"\"\"Add an NEON AOP dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the NEON AOP HDF5 file.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to 0.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to 0.5.\n        nodata (float, optional): The value from the band to use to\n            interpret as not valid data. Defaults to np.nan.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'NEON'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults\n            to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n    \"\"\"\n\n    if array_args is None:\n        array_args = {}\n    xds = None\n    if isinstance(source, str):\n\n        xds = read_neon(source)\n        source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n    elif isinstance(source, xr.Dataset):\n        xds = source\n        source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n\n    self.add_raster(\n        source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = xds\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"NEON\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_pace","title":"<code>add_pace(self, source, wavelengths=None, indexes=None, colormap='jet', vmin=None, vmax=None, nodata=nan, attribution=None, layer_name='PACE', zoom_to_layer=True, visible=True, method='nearest', gridded=False, array_args=None, **kwargs)</code>","text":"<p>Add a PACE dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See     https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>'jet'</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'EMIT'.</p> <code>'PACE'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_pace(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=\"jet\",\n    vmin=None,\n    vmax=None,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"PACE\",\n    zoom_to_layer=True,\n    visible=True,\n    method=\"nearest\",\n    gridded=False,\n    array_args=None,\n    **kwargs,\n):\n    \"\"\"Add a PACE dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n    \"\"\"\n\n    if array_args is None:\n        array_args = {}\n\n    if isinstance(source, str):\n\n        source = read_pace(source)\n\n    try:\n        image = pace_to_image(\n            source, wavelengths=wavelengths, method=method, gridded=gridded\n        )\n\n        if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n            colormap = None\n\n        self.add_raster(\n            image,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = source\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"PACE\"\n        self._update_band_names(layer_name, wavelengths)\n    except Exception as e:\n        print(e)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_prisma","title":"<code>add_prisma(self, source, wavelengths=None, indexes=None, colormap=None, vmin=0, vmax=0.5, nodata=nan, attribution=None, layer_name='PRISMA', zoom_to_layer=True, visible=True, array_args=None, method='nearest', **kwargs)</code>","text":"<p>Add a PRISMA dataset to the map.</p> <p>This function reads a PRISMA hyperspectral dataset, optionally selects specific wavelengths, converts the data to an image, and adds it as a raster layer to the map. The dataset can be provided as a file path or as an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str or xarray.Dataset</code> <p>The path to the PRISMA file or an in-memory xarray Dataset containing PRISMA data.</p> required <code>wavelengths</code> <code>list or np.ndarray</code> <p>Specific wavelengths to select from the dataset. If None, all wavelengths are used. Defaults to None.</p> <code>None</code> <code>indexes</code> <code>int or list</code> <p>The band(s) to display. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See: https://matplotlib.org/stable/gallery/color/colormap_reference.html. Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value for color mapping when plotting a single band. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>The maximum value for color mapping when plotting a single band. Defaults to 0.5.</p> <code>0.5</code> <code>nodata</code> <code>float</code> <p>Value in the raster to interpret as no-data. Defaults to np.nan.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The name to assign to the map layer. Defaults to \"PRISMA\".</p> <code>'PRISMA'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom the map to the extent of the layer after adding it. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer should be visible when first added. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional keyword arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for wavelength interpolation when selecting bands. Options may include \"nearest\", \"linear\", etc. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>add_raster</code>.</p> <code>{}</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_prisma(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=0,\n    vmax=0.5,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"PRISMA\",\n    zoom_to_layer=True,\n    visible=True,\n    array_args=None,\n    method=\"nearest\",\n    **kwargs,\n):\n    \"\"\"Add a PRISMA dataset to the map.\n\n    This function reads a PRISMA hyperspectral dataset, optionally selects\n    specific wavelengths, converts the data to an image, and adds it as a\n    raster layer to the map. The dataset can be provided as a file path or\n    as an xarray Dataset.\n\n    Args:\n        source (str or xarray.Dataset): The path to the PRISMA file or an\n            in-memory xarray Dataset containing PRISMA data.\n        wavelengths (list or np.ndarray, optional): Specific wavelengths to\n            select from the dataset. If None, all wavelengths are used.\n            Defaults to None.\n        indexes (int or list, optional): The band(s) to display. Band\n            indexing starts at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See:\n            https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n            Default is greyscale.\n        vmin (float, optional): The minimum value for color mapping when\n            plotting a single band. Defaults to 0.\n        vmax (float, optional): The maximum value for color mapping when\n            plotting a single band. Defaults to 0.5.\n        nodata (float, optional): Value in the raster to interpret as\n            no-data. Defaults to np.nan.\n        attribution (str, optional): Attribution for the source raster.\n            Defaults to None.\n        layer_name (str, optional): The name to assign to the map layer.\n            Defaults to \"PRISMA\".\n        zoom_to_layer (bool, optional): Whether to zoom the map to the\n            extent of the layer after adding it. Defaults to True.\n        visible (bool, optional): Whether the layer should be visible when\n            first added. Defaults to True.\n        array_args (dict, optional): Additional keyword arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        method (str, optional): Method to use for wavelength interpolation\n            when selecting bands. Options may include \"nearest\", \"linear\",\n            etc. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments passed to `add_raster`.\n    \"\"\"\n    if array_args is None:\n        array_args = {}\n\n    if isinstance(source, str):\n        xds = read_prisma(source, wavelengths=wavelengths, method=method)\n    else:\n        xds = source\n\n    with tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False) as tmp:\n        temp_path = tmp.name\n\n    prisma_to_image(xds, wavelengths=wavelengths, method=method, output=temp_path)\n\n    self.add_raster(\n        temp_path,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = xds\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"PRISMA\"\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_raster","title":"<code>add_raster(self, source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args=None, client_args={'cors_all': False}, open_args=None, **kwargs)</code>","text":"<p>Add a local raster dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See https://matplotlib.org/stable/gallery/color/colormap_reference.html. Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'Raster'.</p> <code>'Raster'</code> <code>layer_index</code> <code>int</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>float</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>client_args</code> <code>dict</code> <p>Additional arguments to pass to localtileserver.TileClient. Defaults to { \"cors_all\": False }.</p> <code>{'cors_all': False}</code> <code>open_args</code> <code>dict</code> <p>Additional arguments to pass to rioxarray.open_rasterio.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_raster(\n    self,\n    source,\n    indexes=None,\n    colormap=None,\n    vmin=None,\n    vmax=None,\n    nodata=None,\n    attribution=None,\n    layer_name=\"Raster\",\n    layer_index=None,\n    zoom_to_layer=True,\n    visible=True,\n    opacity=1.0,\n    array_args=None,\n    client_args={\"cors_all\": False},\n    open_args=None,\n    **kwargs,\n):\n    \"\"\"Add a local raster dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n            https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n            Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'Raster'.\n        layer_index (int, optional): The index of the layer. Defaults to None.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to\n            True.\n        opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        client_args (dict, optional): Additional arguments to pass to\n            localtileserver.TileClient. Defaults to { \"cors_all\": False }.\n        open_args (dict, optional): Additional arguments to pass to\n            rioxarray.open_rasterio.\n\n    \"\"\"\n\n    import rioxarray as rxr\n\n    if array_args is None:\n        array_args = {}\n    if open_args is None:\n        open_args = {}\n\n    if nodata is None:\n        nodata = np.nan\n    super().add_raster(\n        source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        layer_index=layer_index,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        opacity=opacity,\n        array_args=array_args,\n        client_args=client_args,\n        **kwargs,\n    )\n\n    if isinstance(source, str):\n        da = rxr.open_rasterio(source, **open_args)\n        dims = da.dims\n        da = da.transpose(dims[1], dims[2], dims[0])\n\n        xds = da.to_dataset(name=\"data\")\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        # if self.cog_layer_dict[layer_name].get(\"hyper\") is None:\n        #     self.cog_layer_dict[layer_name][\"hyper\"] = \"COG\"\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_tanager","title":"<code>add_tanager(self, source, bands=None, wavelengths=None, indexes=None, colormap=None, vmin=0, vmax=120, nodata=nan, attribution=None, layer_name='Tanager', zoom_to_layer=True, visible=True, method='nearest', array_args=None, **kwargs)</code>","text":"<p>Add a Tanager dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>bands</code> <code>list</code> <p>The band indices to select. Defaults to None.</p> <code>None</code> <code>wavelengths</code> <code>list</code> <p>The wavelength values to select. Takes priority over bands. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See     https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>120</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'EMIT'.</p> <code>'Tanager'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_tanager(\n    self,\n    source,\n    bands=None,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=0,\n    vmax=120,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"Tanager\",\n    zoom_to_layer=True,\n    visible=True,\n    method=\"nearest\",\n    array_args=None,\n    **kwargs,\n):\n    \"\"\"Add a Tanager dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        bands (list, optional): The band indices to select. Defaults to None.\n        wavelengths (list, optional): The wavelength values to select. Takes priority over bands. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n    \"\"\"\n\n    if array_args is None:\n        array_args = {}\n\n    if isinstance(source, str):\n\n        source = read_tanager(source)\n\n    selected_wavelengths = []\n    if wavelengths is not None:\n        selected_wavelengths = wavelengths\n    elif bands is not None:\n        for band in bands:\n            if isinstance(band, (int, np.integer)) or (\n                isinstance(band, float) and band &lt; 500\n            ):\n                # Treat as band index\n                selected_wavelengths.append(\n                    source.coords[\"wavelength\"].values[int(band)]\n                )\n            else:\n                # Treat as wavelength value\n                selected_wavelengths.append(band)\n\n    else:\n        selected_wavelengths = [876.3, 675.88, 625.83]\n    try:\n        image = tanager_to_image(\n            source, wavelengths=selected_wavelengths, method=method\n        )\n\n        if isinstance(selected_wavelengths, list) and len(selected_wavelengths) &gt; 1:\n            colormap = None\n\n        self.add_raster(\n            image,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = source\n        self.cog_layer_dict[layer_name][\"vmax\"] = vmax\n        self.cog_layer_dict[layer_name][\"vmin\"] = vmin\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"TANAGER\"\n        self._update_band_names(layer_name, selected_wavelengths)\n    except Exception as e:\n        print(e)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_wyvern","title":"<code>add_wyvern(self, source, wavelengths=None, indexes=None, colormap='jet', vmin=None, vmax=None, nodata=nan, attribution=None, layer_name='WYVERN', zoom_to_layer=True, visible=True, method='nearest', array_args=None, **kwargs)</code>","text":"<p>Add a WYVERN dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See https://matplotlib.org/stable/gallery/color/colormap_reference.html. Default is 'jet'.</p> <code>'jet'</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'WYVERN'.</p> <code>'WYVERN'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_wyvern(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=\"jet\",\n    vmin=None,\n    vmax=None,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"WYVERN\",\n    zoom_to_layer=True,\n    visible=True,\n    method=\"nearest\",\n    array_args=None,\n    **kwargs,\n):\n    \"\"\"Add a WYVERN dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n            https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n            Default is 'jet'.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'WYVERN'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n    \"\"\"\n    if array_args is None:\n        array_args = {}\n\n    if isinstance(source, str):\n\n        source = read_wyvern(source)\n\n    image = wyvern_to_image(\n        source,\n        wavelengths=wavelengths,\n        method=method,\n        nodata=nodata,\n        vmin=vmin,\n        vmax=vmax,\n    )\n\n    if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n        colormap = None\n\n    if isinstance(wavelengths, int):\n        wavelengths = [wavelengths]\n\n    if indexes is None:\n        if isinstance(wavelengths, list) and len(wavelengths) == 1:\n            indexes = [1]\n        else:\n            indexes = [1, 2, 3]\n\n    self.add_raster(\n        image,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = source\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"WYVERN\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.search_ecostress","title":"<code>search_ecostress(self, default_dataset='ECO_L2T_LSTE')</code>","text":"<p>Adds a NASA Earth Data search tool to the map with a default dataset for     ECOSTRESS.</p> <p>Parameters:</p> Name Type Description Default <code>default_dataset</code> <code>str</code> <p>The default dataset to search for. Defaults to \"ECO_L2T_LSTE\".</p> <code>'ECO_L2T_LSTE'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def search_ecostress(self, default_dataset=\"ECO_L2T_LSTE\"):\n    \"\"\"\n    Adds a NASA Earth Data search tool to the map with a default dataset for\n        ECOSTRESS.\n\n    Args:\n        default_dataset (str, optional): The default dataset to search for.\n            Defaults to \"ECO_L2T_LSTE\".\n    \"\"\"\n    self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.search_emit","title":"<code>search_emit(self, default_dataset='EMITL2ARFL')</code>","text":"<p>Adds a NASA Earth Data search tool to the map with a default dataset for     EMIT.</p> <p>Parameters:</p> Name Type Description Default <code>default_dataset</code> <code>str</code> <p>The default dataset to search for. Defaults to \"EMITL2ARFL\".</p> <code>'EMITL2ARFL'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def search_emit(self, default_dataset=\"EMITL2ARFL\"):\n    \"\"\"\n    Adds a NASA Earth Data search tool to the map with a default dataset for\n        EMIT.\n\n    Args:\n        default_dataset (str, optional): The default dataset to search for.\n            Defaults to \"EMITL2ARFL\".\n    \"\"\"\n    self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.search_pace","title":"<code>search_pace(self, default_dataset='PACE_OCI_L2_AOP_NRT')</code>","text":"<p>Adds a NASA Earth Data search tool to the map with a default dataset for     PACE.</p> <p>Parameters:</p> Name Type Description Default <code>default_dataset</code> <code>str</code> <p>The default dataset to search for. Defaults to \"PACE_OCI_L2_AOP_NRT\".</p> <code>'PACE_OCI_L2_AOP_NRT'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def search_pace(self, default_dataset=\"PACE_OCI_L2_AOP_NRT\"):\n    \"\"\"\n    Adds a NASA Earth Data search tool to the map with a default dataset for\n        PACE.\n\n    Args:\n        default_dataset (str, optional): The default dataset to search for.\n            Defaults to \"PACE_OCI_L2_AOP_NRT\".\n    \"\"\"\n    self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.set_plot_options","title":"<code>set_plot_options(self, add_marker_cluster=False, plot_type=None, overlay=False, position='bottomright', min_width=None, max_width=None, min_height=None, max_height=None, **kwargs)</code>","text":"<p>Sets plotting options.</p> <p>Parameters:</p> Name Type Description Default <code>add_marker_cluster</code> <code>bool</code> <p>Whether to add a marker cluster. Defaults to False.</p> <code>False</code> <code>sample_scale</code> <code>float</code> <p>A nominal scale in meters of the projection to sample in . Defaults to None.</p> required <code>plot_type</code> <code>str</code> <p>The plot type can be one of \"None\", \"bar\", \"scatter\" or \"hist\". Defaults to None.</p> <code>None</code> <code>overlay</code> <code>bool</code> <p>Whether to overlay plotted lines on the figure. Defaults to False.</p> <code>False</code> <code>position</code> <code>str</code> <p>Position of the control, can be \u2018bottomleft\u2019, \u2018bottomright\u2019, \u2018topleft\u2019, or \u2018topright\u2019. Defaults to 'bottomright'.</p> <code>'bottomright'</code> <code>min_width</code> <code>int</code> <p>Min width of the widget (in pixels), if None it will respect the content size. Defaults to None.</p> <code>None</code> <code>max_width</code> <code>int</code> <p>Max width of the widget (in pixels), if None it will respect the content size. Defaults to None.</p> <code>None</code> <code>min_height</code> <code>int</code> <p>Min height of the widget (in pixels), if None it will respect the content size. Defaults to None.</p> <code>None</code> <code>max_height</code> <code>int</code> <p>Max height of the widget (in pixels), if None it will respect the content size. Defaults to None.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def set_plot_options(\n    self,\n    add_marker_cluster=False,\n    plot_type=None,\n    overlay=False,\n    position=\"bottomright\",\n    min_width=None,\n    max_width=None,\n    min_height=None,\n    max_height=None,\n    **kwargs,\n):\n    \"\"\"Sets plotting options.\n\n    Args:\n        add_marker_cluster (bool, optional): Whether to add a marker cluster.\n            Defaults to False.\n        sample_scale (float, optional):  A nominal scale in meters of the\n            projection to sample in . Defaults to None.\n        plot_type (str, optional): The plot type can be one of \"None\", \"bar\",\n            \"scatter\" or \"hist\". Defaults to None.\n        overlay (bool, optional): Whether to overlay plotted lines on the\n            figure. Defaults to False.\n        position (str, optional): Position of the control, can be\n            \u2018bottomleft\u2019, \u2018bottomright\u2019, \u2018topleft\u2019, or \u2018topright\u2019. Defaults\n            to 'bottomright'.\n        min_width (int, optional): Min width of the widget (in pixels), if\n            None it will respect the content size. Defaults to None.\n        max_width (int, optional): Max width of the widget (in pixels), if\n            None it will respect the content size. Defaults to None.\n        min_height (int, optional): Min height of the widget (in pixels), if\n            None it will respect the content size. Defaults to None.\n        max_height (int, optional): Max height of the widget (in pixels), if\n            None it will respect the content size. Defaults to None.\n\n    \"\"\"\n    plot_options_dict = {}\n    plot_options_dict[\"add_marker_cluster\"] = add_marker_cluster\n    plot_options_dict[\"plot_type\"] = plot_type\n    plot_options_dict[\"overlay\"] = overlay\n    plot_options_dict[\"position\"] = position\n    plot_options_dict[\"min_width\"] = min_width\n    plot_options_dict[\"max_width\"] = max_width\n    plot_options_dict[\"min_height\"] = min_height\n    plot_options_dict[\"max_height\"] = max_height\n\n    for key in kwargs:\n        plot_options_dict[key] = kwargs[key]\n\n    self._plot_options = plot_options_dict\n\n    if not hasattr(self, \"_plot_marker_cluster\"):\n        self._plot_marker_cluster = ipyleaflet.MarkerCluster(name=\"Marker Cluster\")\n\n    if add_marker_cluster and (self._plot_marker_cluster not in self.layers):\n        self.add(self._plot_marker_cluster)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.spectral_to_csv","title":"<code>spectral_to_csv(self, filename, index=True, **kwargs)</code>","text":"<p>Saves the spectral data to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The output CSV file.</p> required <code>index</code> <code>bool</code> <p>Whether to write the index. Defaults to True.</p> <code>True</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def spectral_to_csv(self, filename, index=True, **kwargs):\n    \"\"\"Saves the spectral data to a CSV file.\n\n    Args:\n        filename (str): The output CSV file.\n        index (bool, optional): Whether to write the index. Defaults to True.\n    \"\"\"\n    df = self.spectral_to_df()\n    df = df.rename_axis(\"band\")\n    df.to_csv(filename, index=index, **kwargs)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.spectral_to_df","title":"<code>spectral_to_df(self, **kwargs)</code>","text":"<p>Converts the spectral data to a pandas DataFrame.</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>The spectral data as a pandas DataFrame.</p> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def spectral_to_df(self, **kwargs):\n    \"\"\"Converts the spectral data to a pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: The spectral data as a pandas DataFrame.\n    \"\"\"\n    import pandas as pd\n\n    df = pd.DataFrame(self._spectral_data, **kwargs)\n    return df\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.spectral_to_gdf","title":"<code>spectral_to_gdf(self, **kwargs)</code>","text":"<p>Converts the spectral data to a GeoPandas GeoDataFrame.</p> <p>Returns:</p> Type Description <code>gpd.DataFrame</code> <p>The spectral data as a pandas DataFrame.</p> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def spectral_to_gdf(self, **kwargs):\n    \"\"\"Converts the spectral data to a GeoPandas GeoDataFrame.\n\n    Returns:\n        gpd.DataFrame: The spectral data as a pandas DataFrame.\n    \"\"\"\n    import geopandas as gpd\n    from shapely.geometry import Point\n\n    df = self.spectral_to_df()\n\n    if len(df) == 0:\n        return df\n\n    # Step 1: Extract the coordinates from the columns\n    if \"wavelength\" in df.columns:\n        df = df.rename(columns={\"wavelength\": \"latlon\"})\n    elif \"wavelengths\" in df.columns:\n        df = df.rename(columns={\"wavelengths\": \"latlon\"})\n    coordinates = [col.strip(\"()\").split() for col in df.columns[1:]]\n    coords = [(float(lat), float(lon)) for lat, lon in coordinates]\n\n    # Step 2: Create Point geometries for each coordinate\n    points = [Point(lon, lat) for lat, lon in coords]\n\n    # Step 3: Create a GeoDataFrame\n    df_transposed = df.set_index(\"latlon\").T\n\n    # Convert the column names to strings to ensure compatibility with GeoJSON\n    df_transposed.columns = df_transposed.columns.astype(str)\n\n    # Create the GeoDataFrame\n    gdf = gpd.GeoDataFrame(df_transposed, geometry=points, **kwargs)\n\n    # Set the coordinate reference system (CRS)\n    gdf = gdf.set_geometry(\"geometry\").set_crs(\"EPSG:4326\")\n\n    return gdf\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#install-using-uv","title":"Install using uv","text":"<p>uv is an extremely fast Python package and project manager, written in Rust. Follow the instructions on the uv website to install uv on your computer. Once uv is installed, you can install hypercoast using the following command:</p> <pre><code>uv venv\nuv pip install hypercoast jupyterlab\n</code></pre> <p>To run JupyterLab, use the following command:</p> <pre><code>uv run jupyter lab\n</code></pre>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>hypercoast is available on PyPI. To install hypercoast, run this command in your terminal:</p> <pre><code>pip install hypercoast\n</code></pre> <p>HyperCoast has some optional dependencies that are not installed by default, such as cartopy, earthaccess, mapclassify, and pyvista. To install all optional dependencies all at once, run the following command:</p> <pre><code>pip install \"hypercoast[extra]\"\n</code></pre>"},{"location":"installation/#install-from-conda-forge","title":"Install from conda-forge","text":"<p>hypercoast is also available on conda-forge. If you have Anaconda or Miniconda installed on your computer, you can install hypercoast using the following command:</p> <pre><code>conda install -c conda-forge hypercoast\n</code></pre> <p>Alternatively, you can create a new conda environment and install hypercoast in the new environment. This is a good practice because it avoids potential conflicts with other packages installed in your base environment.</p> <pre><code>conda install -n base mamba -c conda-forge\nconda create -n hyper python=3.11\nconda activate hyper\nmamba install -c conda-forge hypercoast\n</code></pre> <p>To install the optional dependencies, run the following command:</p> <pre><code>mamba install -c conda-forge cartopy earthaccess mapclassify pyvista trame-vtk trame-vuetify\n</code></pre>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>To install the development version from GitHub using Git, run the following command in your terminal:</p> <pre><code>pip install git+https://github.com/opengeos/hypercoast\n</code></pre>"},{"location":"neon/","title":"neon module","text":"<p>This module contains functions to read and process NEON AOP hyperspectral data. More info about the data can be found at https://bit.ly/3Rfszdc. The source code is adapted from https://bit.ly/3KwyZkn. Credit goes to the original authors.</p>"},{"location":"neon/#hypercoast.neon.extract_neon","title":"<code>extract_neon(ds, lat, lon)</code>","text":"<p>Extracts NEON AOP data from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>The dataset containing the NEON AOP data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted data.</p> Source code in <code>hypercoast/neon.py</code> <pre><code>def extract_neon(ds, lat, lon):\n    \"\"\"\n    Extracts NEON AOP data from a given xarray Dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the NEON AOP data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n\n    Returns:\n        xarray.DataArray: The extracted data.\n    \"\"\"\n\n    crs = ds.attrs[\"crs\"]\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    values = ds.sel(x=x, y=y, method=\"nearest\")[\"reflectance\"].values\n\n    da = xr.DataArray(\n        values, dims=[\"wavelength\"], coords={\"wavelength\": ds.coords[\"wavelength\"]}\n    )\n\n    return da\n</code></pre>"},{"location":"neon/#hypercoast.neon.list_neon_datasets","title":"<code>list_neon_datasets(filepath, print_node=False)</code>","text":"<p>Lists all the datasets in an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the HDF5 file.</p> required <code>print_node</code> <code>bool</code> <p>If True, prints the node object of each dataset. If False, prints the name of each dataset. Defaults to False.</p> <code>False</code> Source code in <code>hypercoast/neon.py</code> <pre><code>def list_neon_datasets(filepath: str, print_node: bool = False) -&gt; None:\n    \"\"\"\n    Lists all the datasets in an HDF5 file.\n\n    Args:\n        filepath (str): The path to the HDF5 file.\n        print_node (bool, optional): If True, prints the node object of each dataset.\n            If False, prints the name of each dataset. Defaults to False.\n    \"\"\"\n\n    f = h5py.File(filepath, \"r\")\n\n    if print_node:\n\n        def list_dataset(_, node):\n            if isinstance(node, h5py.Dataset):\n                print(node)\n\n    else:\n\n        def list_dataset(name, node):\n            if isinstance(node, h5py.Dataset):\n                print(name)\n\n    f.visititems(list_dataset)\n</code></pre>"},{"location":"neon/#hypercoast.neon.neon_to_image","title":"<code>neon_to_image(dataset, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts an NEON dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The dataset containing the NEON data or the file path to the dataset.</p> required <code>wavelengths</code> <code>np.ndarray</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[rasterio.Dataset]</code> <p>The image converted from the dataset. If     <code>output</code> is provided, the image will be saved to the specified file     and the function will return None.</p> Source code in <code>hypercoast/neon.py</code> <pre><code>def neon_to_image(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Optional[np.ndarray] = None,\n    method: str = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Converts an NEON dataset to an image.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The dataset containing the NEON data\n            or the file path to the dataset.\n        wavelengths (np.ndarray, optional): The specific wavelengths to select. If None, all\n            wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n        output (str, optional): The file path where the image will be saved. If\n            None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs (Any): Additional keyword arguments to be passed to\n            `leafmap.array_to_image`.\n\n    Returns:\n        Optional[rasterio.Dataset]: The image converted from the dataset. If\n            `output` is provided, the image will be saved to the specified file\n            and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_neon(dataset, method=method)\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n\n    return array_to_image(\n        dataset[\"reflectance\"],\n        output=output,\n        transpose=False,\n        dtype=np.float32,\n        **kwargs,\n    )\n</code></pre>"},{"location":"neon/#hypercoast.neon.read_neon","title":"<code>read_neon(filepath, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Reads NEON AOP hyperspectral hdf5 files and returns an xarray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the hdf5 file.</p> required <code>wavelengths</code> <code>List[float]</code> <p>The wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the selection method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The dataset containing the reflectance data.</p> Source code in <code>hypercoast/neon.py</code> <pre><code>def read_neon(\n    filepath: str,\n    wavelengths: Optional[List[float]] = None,\n    method: str = \"nearest\",\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads NEON AOP hyperspectral hdf5 files and returns an xarray dataset.\n\n    Args:\n        filepath (str): The path to the hdf5 file.\n        wavelengths (List[float], optional): The wavelengths to select. If None,\n            all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for selection. Defaults to\n            \"nearest\".\n        **kwargs (Any): Additional arguments to pass to the selection method.\n\n    Returns:\n        xr.Dataset: The dataset containing the reflectance data.\n    \"\"\"\n    with h5py.File(filepath, \"r\") as f:\n        # Extract site code dynamically from NEON HDF file metadata\n        # At the root of `keys` NEON stores the site code, which is the `root` folder at the [0] index of object `keys`\n        site_code = list(f.keys())[0]\n\n        # Access the reflectance data using the site code\n        site_refl = f[site_code][\"Reflectance\"]\n\n        # Extract wavelengths\n        wavelengths_list = site_refl[\"Metadata\"][\"Spectral_Data\"][\"Wavelength\"][\n            ()\n        ].tolist()\n        wavelengths_list = [round(num, 2) for num in wavelengths_list]\n\n        # Extract EPSG code\n        epsg_code = site_refl[\"Metadata\"][\"Coordinate_System\"][\"EPSG Code\"][()]\n        epsg_code_number = int(epsg_code.decode(\"utf-8\"))\n\n        # Extract map info\n        mapInfo_string = site_refl[\"Metadata\"][\"Coordinate_System\"][\"Map_Info\"][\n            ()\n        ].decode(\"utf-8\")\n        mapInfo_split = mapInfo_string.split(\",\")\n\n        res = float(mapInfo_split[5]), float(mapInfo_split[6])\n\n        # Extract reflectance array and shape\n        site_reflArray = site_refl[\"Reflectance_Data\"]\n        refl_shape = site_reflArray.shape\n\n        # Calculate coordinates\n        xMin = float(mapInfo_split[3])\n        yMax = float(mapInfo_split[4])\n\n        xMax = xMin + (refl_shape[1] * res[0])\n        yMin = yMax - (refl_shape[0] * res[1])\n\n        # Handle scale factor and no-data value\n        scaleFactor = site_reflArray.attrs[\"Scale_Factor\"]\n        noDataValue = site_reflArray.attrs[\"Data_Ignore_Value\"]\n\n        da = site_reflArray[:, :, :].astype(float)\n        da[da == int(noDataValue)] = np.nan\n        da[da &lt; 0] = np.nan\n        da[da &gt; 10000] = np.nan\n        da = da / scaleFactor\n\n        coords = {\n            \"y\": np.linspace(yMax, yMin, da.shape[0]),\n            \"x\": np.linspace(xMin, xMax, da.shape[1]),\n            \"wavelength\": wavelengths_list,\n        }\n\n        xda = xr.DataArray(\n            da,\n            coords=coords,\n            dims=[\"y\", \"x\", \"wavelength\"],\n            attrs={\n                \"scale_factor\": scaleFactor,\n                \"no_data_value\": noDataValue,\n                \"crs\": f\"EPSG:{epsg_code_number}\",\n                \"transform\": (res[0], 0.0, xMin, 0.0, -res[1], yMax),\n            },\n        )\n\n        if wavelengths is not None:\n            xda = xda.sel(wavelength=wavelengths, method=method, **kwargs)\n\n        dataset = xda.to_dataset(name=\"reflectance\")\n        dataset.attrs = dataset[\"reflectance\"].attrs\n\n    return dataset\n</code></pre>"},{"location":"pace/","title":"pace module","text":"<p>This module contains functions to read and process PACE data.</p>"},{"location":"pace/#hypercoast.pace.apply_kmeans","title":"<code>apply_kmeans(dataset, n_clusters=6, filter_condition=None, plot=True, figsize=(8, 6), colors=None, extent=None, title=None, **kwargs)</code>","text":"<p>Applies K-means clustering to the dataset and optionally plots the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset | str</code> <p>The dataset containing the PACE data or the file path to the dataset.</p> required <code>n_clusters</code> <code>int</code> <p>Number of clusters for K-means. Defaults to 6.</p> <code>6</code> <code>plot</code> <code>bool</code> <p>Whether to plot the data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size for the plot. Defaults to (8, 6).</p> <code>(8, 6)</code> <code>colors</code> <code>list[str]</code> <p>List of colors to use for the clusters. Defaults to None.</p> <code>None</code> <code>extent</code> <code>list[float] | None</code> <p>The extent to zoom in to the specified region. Defaults to None.</p> <code>None</code> <code>title</code> <code>str | None</code> <p>Title for the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>plt.subplots</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[np.ndarray, np.ndarray, np.ndarray]</code> <p>The cluster labels, latitudes, and longitudes.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def apply_kmeans(\n    dataset: Union[xr.Dataset, str],\n    n_clusters: int = 6,\n    filter_condition: Optional[Callable[[xr.DataArray], xr.DataArray]] = None,\n    plot: bool = True,\n    figsize: tuple[int, int] = (8, 6),\n    colors: list[str] = None,\n    extent: list[float] = None,\n    title: str = None,\n    **kwargs,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Applies K-means clustering to the dataset and optionally plots the results.\n\n    Args:\n        dataset (xr.Dataset | str): The dataset containing the PACE data or the file path to the dataset.\n        n_clusters (int, optional): Number of clusters for K-means. Defaults to 6.\n        plot (bool, optional): Whether to plot the data. Defaults to True.\n        figsize (tuple[int, int], optional): Figure size for the plot. Defaults to (8, 6).\n        colors (list[str], optional): List of colors to use for the clusters. Defaults to None.\n        extent (list[float] | None, optional): The extent to zoom in to the specified region. Defaults to None.\n        title (str | None, optional): Title for the plot. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the `plt.subplots` function.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray, np.ndarray]: The cluster labels, latitudes, and longitudes.\n    \"\"\"\n\n    import numpy as np\n    from sklearn.cluster import KMeans\n\n    import matplotlib.pyplot as plt\n    import matplotlib.colors as mcolors\n    import cartopy.crs as ccrs\n    import cartopy.feature as cfeature\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset)\n    elif isinstance(dataset, xr.DataArray):\n        dataset = dataset.to_dataset()\n    elif not isinstance(dataset, xr.Dataset):\n        raise ValueError(\"dataset must be an xarray Dataset\")\n\n    if title is None:\n        title = f\"K-means Clustering with {n_clusters} Clusters\"\n\n    da = dataset[\"Rrs\"]\n\n    reshaped_data = da.values.reshape(-1, da.shape[-1])\n    reshaped_data_no_nan = reshaped_data[~np.isnan(reshaped_data).any(axis=1)]\n\n    # Apply K-means clustering to classify into 5-6 water types.\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n    kmeans.fit(reshaped_data_no_nan)\n\n    # Initialize an array for cluster labels with NaN\n    labels = np.full(reshaped_data.shape[0], np.nan)\n\n    # Assign the computed cluster labels to the non-NaN positions\n    labels[~np.isnan(reshaped_data).any(axis=1)] = kmeans.labels_\n\n    # Reshape the labels back to the original spatial dimensions\n    cluster_labels = labels.reshape(da.shape[:-1])\n\n    if filter_condition is not None:\n        cluster_labels = np.where(filter_condition, cluster_labels, np.nan)\n\n    latitudes = da.coords[\"latitude\"].values\n    longitudes = da.coords[\"longitude\"].values\n\n    if plot:\n\n        # Create a custom discrete color map for K-means clusters\n        if colors is None:\n            colors = [\"#377eb8\", \"#ff7f00\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"]\n        cmap = mcolors.ListedColormap(colors)\n        bounds = np.arange(-0.5, n_clusters, 1)\n        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n\n        # Create a figure and axis with the correct map projection\n\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 100\n\n        if \"subplot_kw\" not in kwargs:\n            kwargs[\"subplot_kw\"] = {\"projection\": ccrs.PlateCarree()}\n\n        fig, ax = plt.subplots(\n            figsize=figsize,\n            **kwargs,\n        )\n\n        # Plot the K-means classification results on the map\n        im = ax.pcolormesh(\n            longitudes,\n            latitudes,\n            cluster_labels,\n            cmap=cmap,\n            norm=norm,\n            transform=ccrs.PlateCarree(),\n        )\n\n        # Add geographic features for context\n        ax.add_feature(cfeature.COASTLINE)\n        ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n        ax.add_feature(cfeature.STATES, linestyle=\"--\")\n\n        # Add gridlines\n        ax.gridlines(draw_labels=True)\n\n        # Set the extent to zoom in to the specified region\n        if extent is not None:\n            ax.set_extent(extent, crs=ccrs.PlateCarree())\n\n        # Add color bar with labels\n        cbar = plt.colorbar(\n            im,\n            ax=ax,\n            orientation=\"vertical\",\n            # pad=0.02,\n            fraction=0.05,\n            ticks=np.arange(n_clusters),\n        )\n        cbar.ax.set_yticklabels([f\"Class {i+1}\" for i in range(n_clusters)])\n        cbar.set_label(\"Water Types\", rotation=270, labelpad=10)\n\n        # Add title\n        ax.set_title(title, fontsize=14)\n        ax.set_xlabel(\"Longitude\")\n        ax.set_ylabel(\"Latitude\")\n\n        # Show the plot\n        plt.show()\n\n    return cluster_labels, latitudes, longitudes\n</code></pre>"},{"location":"pace/#hypercoast.pace.apply_pca","title":"<code>apply_pca(dataset, n_components=3, plot=True, figsize=(8, 6), x_component=0, y_component=1, color='blue', title='PCA of Spectral Data', **kwargs)</code>","text":"<p>Applies Principal Component Analysis (PCA) to the dataset and optionally plots the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset | str</code> <p>The dataset containing the PACE data or the file path to the dataset.</p> required <code>n_components</code> <code>int</code> <p>Number of principal components to compute. Defaults to 3.</p> <code>3</code> <code>plot</code> <code>bool</code> <p>Whether to plot the data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size for the plot. Defaults to (8, 6).</p> <code>(8, 6)</code> <code>x_component</code> <code>int</code> <p>The principal component to plot on the x-axis. Defaults to 0.</p> <code>0</code> <code>y_component</code> <code>int</code> <p>The principal component to plot on the y-axis. Defaults to 1.</p> <code>1</code> <code>color</code> <code>str</code> <p>Color of the scatter plot points. Defaults to \"blue\".</p> <code>'blue'</code> <code>title</code> <code>str</code> <p>Title for the plot. Defaults to \"PCA of Spectral Data\".</p> <code>'PCA of Spectral Data'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>plt.scatter</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The PCA-transformed data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def apply_pca(\n    dataset: Union[xr.Dataset, str],\n    n_components: int = 3,\n    plot: bool = True,\n    figsize: tuple[int, int] = (8, 6),\n    x_component: int = 0,\n    y_component: int = 1,\n    color: str = \"blue\",\n    title: str = \"PCA of Spectral Data\",\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Applies Principal Component Analysis (PCA) to the dataset and optionally plots the results.\n\n    Args:\n        dataset (xr.Dataset | str): The dataset containing the PACE data or the file path to the dataset.\n        n_components (int, optional): Number of principal components to compute. Defaults to 3.\n        plot (bool, optional): Whether to plot the data. Defaults to True.\n        figsize (tuple[int, int], optional): Figure size for the plot. Defaults to (8, 6).\n        x_component (int, optional): The principal component to plot on the x-axis. Defaults to 0.\n        y_component (int, optional): The principal component to plot on the y-axis. Defaults to 1.\n        color (str, optional): Color of the scatter plot points. Defaults to \"blue\".\n        title (str, optional): Title for the plot. Defaults to \"PCA of Spectral Data\".\n        **kwargs: Additional keyword arguments to pass to the `plt.scatter` function.\n\n    Returns:\n        np.ndarray: The PCA-transformed data.\n    \"\"\"\n    from sklearn.decomposition import PCA\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset)\n    elif not isinstance(dataset, xr.Dataset):\n        raise ValueError(\"dataset must be an xarray Dataset\")\n\n    da = dataset[\"Rrs\"]\n\n    # Reshape data to (n_pixels, n_bands)\n    reshaped_data = da.values.reshape(-1, da.shape[-1])\n\n    # Handle NaNs by removing them\n    reshaped_data_no_nan = reshaped_data[~np.isnan(reshaped_data).any(axis=1)]\n\n    # Apply PCA to reduce dimensionality\n    pca = PCA(n_components=n_components)\n    pca_data = pca.fit_transform(reshaped_data_no_nan)\n\n    if plot:\n        plt.figure(figsize=figsize)\n        if \"s\" not in kwargs:\n            kwargs[\"s\"] = 1\n        plt.scatter(\n            pca_data[:, x_component], pca_data[:, y_component], c=color, **kwargs\n        )\n        plt.title(title)\n        plt.xlabel(f\"Principal Component {x_component + 1}\")\n        plt.ylabel(f\"Principal Component {y_component + 1}\")\n        plt.show()\n\n    return pca_data\n</code></pre>"},{"location":"pace/#hypercoast.pace.apply_sam","title":"<code>apply_sam(dataset, n_components=3, n_clusters=6, random_state=0, spectral_library=None, filter_condition=None, plot=True, figsize=(8, 6), extent=None, colors=None, title=None, **kwargs)</code>","text":"<p>Applies Spectral Angle Mapper (SAM) to the dataset and optionally plots the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The dataset containing the PACE data or the file path to the dataset.</p> required <code>n_components</code> <code>int</code> <p>Number of principal components to compute. Defaults to 3.</p> <code>3</code> <code>n_clusters</code> <code>int</code> <p>Number of clusters for K-means. Defaults to 6.</p> <code>6</code> <code>random_state</code> <code>int</code> <p>Random state for K-means. Defaults to 0.</p> <code>0</code> <code>spectral_library</code> <code>Union[str, list[str]]</code> <p>Path to the spectral library or a list of paths. Defaults to None.</p> <code>None</code> <code>filter_condition</code> <code>Callable[[xr.DataArray], xr.DataArray]</code> <p>A function to filter the data. Defaults to None.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>Whether to plot the data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size for the plot. Defaults to (8, 6).</p> <code>(8, 6)</code> <code>extent</code> <code>List[float]</code> <p>The extent to zoom in to the specified region. Defaults to None.</p> <code>None</code> <code>colors</code> <code>List[str]</code> <p>Colors for the clusters. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. Defaults to \"Spectral Angle Mapper\".</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>plt.subplots</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray, np.ndarray]</code> <p>The best match classification, latitudes, and longitudes.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def apply_sam(\n    dataset: Union[xr.Dataset, str],\n    n_components: int = 3,\n    n_clusters: int = 6,\n    random_state: int = 0,\n    spectral_library: Union[str, list[str]] = None,\n    filter_condition: Optional[Callable[[xr.DataArray], xr.DataArray]] = None,\n    plot: bool = True,\n    figsize: tuple[int, int] = (8, 6),\n    extent: list[float] = None,\n    colors: list[str] = None,\n    title: str = None,\n    **kwargs,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Applies Spectral Angle Mapper (SAM) to the dataset and optionally plots the results.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The dataset containing the PACE data or the file path to the dataset.\n        n_components (int, optional): Number of principal components to compute. Defaults to 3.\n        n_clusters (int, optional): Number of clusters for K-means. Defaults to 6.\n        random_state (int, optional): Random state for K-means. Defaults to 0.\n        spectral_library (Union[str, list[str]], optional): Path to the spectral library or a list of paths. Defaults to None.\n        filter_condition (Callable[[xr.DataArray], xr.DataArray], optional): A function to filter the data. Defaults to None.\n        plot (bool, optional): Whether to plot the data. Defaults to True.\n        figsize (Tuple[int, int], optional): Figure size for the plot. Defaults to (8, 6).\n        extent (List[float], optional): The extent to zoom in to the specified region. Defaults to None.\n        colors (List[str], optional): Colors for the clusters. Defaults to None.\n        title (str, optional): Title for the plot. Defaults to \"Spectral Angle Mapper\".\n        **kwargs: Additional keyword arguments to pass to the `plt.subplots` function.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: The best match classification, latitudes, and longitudes.\n    \"\"\"\n    import glob\n    import pandas as pd\n    from sklearn.cluster import KMeans\n    import matplotlib.colors as mcolors\n    import cartopy.crs as ccrs\n    import cartopy.feature as cfeature\n    from sklearn.decomposition import PCA\n    from scipy.interpolate import interp1d\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset)\n    elif isinstance(dataset, xr.DataArray):\n        dataset = dataset.to_dataset()\n    elif not isinstance(dataset, xr.Dataset):\n        raise ValueError(\"dataset must be an xarray Dataset\")\n\n    da = dataset[\"Rrs\"]\n    pace_wavelengths = da[\"wavelength\"].values\n\n    # Reshape data to (n_pixels, n_bands)\n    reshaped_data = da.values.reshape(-1, da.shape[-1])\n\n    # Handle NaNs by removing them\n    reshaped_data_no_nan = reshaped_data[~np.isnan(reshaped_data).any(axis=1)]\n\n    if isinstance(spectral_library, str):\n        endmember_paths = sorted(glob.glob(spectral_library))\n    elif isinstance(spectral_library, list):\n        endmember_paths = spectral_library\n    else:\n        endmember_paths = None\n\n    # Function to load and resample a single CSV spectral library file\n    def load_and_resample_spectral_library(csv_path, target_wavelengths):\n        df = pd.read_csv(csv_path)\n        original_wavelengths = df.iloc[:, 0].values  # First column is wavelength\n        spectra_values = df.iloc[:, 1].values  # Second column is spectral values\n\n        # Interpolation function\n        interp_func = interp1d(\n            original_wavelengths,\n            spectra_values,\n            kind=\"linear\",\n            fill_value=\"extrapolate\",\n        )\n\n        # Resample to the target (PACE) wavelengths\n        resampled_spectra = interp_func(target_wavelengths)\n\n        return resampled_spectra\n\n    if endmember_paths is not None:\n        endmembers = np.array(\n            [\n                load_and_resample_spectral_library(path, pace_wavelengths)\n                for path in endmember_paths\n            ]\n        )\n    else:\n        endmembers = None\n\n    if endmembers is None:\n        # Apply PCA to reduce dimensionality\n        pca = PCA(n_components=n_components)\n        pca_data = pca.fit_transform(reshaped_data_no_nan)\n\n        # Apply K-means to find clusters representing endmembers\n        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n        kmeans.fit(pca_data)\n\n        # The cluster centers in the original spectral space are your endmembers\n        endmembers = pca.inverse_transform(kmeans.cluster_centers_)\n\n    def spectral_angle_mapper(pixel, reference):\n        norm_pixel = np.linalg.norm(pixel)\n        norm_reference = np.linalg.norm(reference)\n        cos_theta = np.dot(pixel, reference) / (norm_pixel * norm_reference)\n        angle = np.arccos(np.clip(cos_theta, -1, 1))\n        return angle\n\n    # Apply SAM for each pixel and each endmember\n    angles = np.zeros((reshaped_data_no_nan.shape[0], endmembers.shape[0]))\n\n    for i in range(reshaped_data_no_nan.shape[0]):\n        for j in range(endmembers.shape[0]):\n            angles[i, j] = spectral_angle_mapper(\n                reshaped_data_no_nan[i, :], endmembers[j, :]\n            )\n\n    # Find the minimum angle (best match) for each pixel\n    best_match = np.argmin(angles, axis=1)\n\n    # Reshape best_match back to the original spatial dimensions\n    original_shape = da.shape[:-1]  # Get the spatial dimensions\n    best_match_full = np.full(reshaped_data.shape[0], np.nan)\n    best_match_full[~np.isnan(reshaped_data).any(axis=1)] = best_match\n    best_match_full = best_match_full.reshape(original_shape)\n\n    if filter_condition is not None:\n        best_match_full = np.where(filter_condition, best_match_full, np.nan)\n\n    latitudes = da.coords[\"latitude\"].values\n    longitudes = da.coords[\"longitude\"].values\n\n    # Plot sample spectra from the CSV files and their resampled versions\n    def plot_sample_spectra(csv_paths, pace_wavelengths):\n        plt.figure(figsize=figsize)\n\n        for i, csv_path in enumerate(csv_paths):\n            df = pd.read_csv(csv_path)\n            original_wavelengths = df.iloc[:, 0].values\n            spectra_values = df.iloc[:, 1].values\n            resampled_spectra = load_and_resample_spectral_library(\n                csv_path, pace_wavelengths\n            )\n\n            plt.plot(\n                original_wavelengths,\n                spectra_values,\n                label=f\"Original Spectra {i+1}\",\n                linestyle=\"--\",\n            )\n            plt.plot(\n                pace_wavelengths, resampled_spectra, label=f\"Resampled Spectra {i+1}\"\n            )\n\n        plt.xlabel(\"Wavelength (nm)\")\n        plt.ylabel(\"Spectral Reflectance\")\n        plt.title(\"Comparison of Original and Resampled Spectra\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n    if plot:\n\n        if endmember_paths is not None:\n\n            plot_sample_spectra(endmember_paths, pace_wavelengths)\n\n        if colors is None:\n            colors = [\"#377eb8\", \"#ff7f00\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"]\n\n        if title is None:\n            title = \"Spectral Angle Mapper Water Type Classification\"\n        # Create a custom discrete color map\n        cmap = mcolors.ListedColormap(colors)\n        if spectral_library is not None:\n            n_clusters = len(endmember_paths)\n        bounds = np.arange(-0.5, n_clusters, 1)\n        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n\n        # Create a figure and axis with the correct map projection\n        fig, ax = plt.subplots(\n            figsize=figsize, subplot_kw={\"projection\": ccrs.PlateCarree()}, **kwargs\n        )\n\n        # Plot the SAM classification results\n        im = ax.pcolormesh(\n            longitudes,\n            latitudes,\n            best_match_full,\n            cmap=cmap,\n            norm=norm,\n            transform=ccrs.PlateCarree(),\n        )\n\n        # Add geographic features for context\n        ax.add_feature(cfeature.COASTLINE)\n        ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n        ax.add_feature(cfeature.STATES, linestyle=\"--\")\n\n        # Add gridlines\n        ax.gridlines(draw_labels=True)\n\n        # Set the extent to zoom in to the specified region\n        if extent is not None:\n            ax.set_extent(extent, crs=ccrs.PlateCarree())\n\n        # Add color bar with labels\n        cbar = plt.colorbar(\n            im,\n            ax=ax,\n            orientation=\"vertical\",\n            # pad=0.02,\n            fraction=0.05,\n            ticks=np.arange(n_clusters),\n        )\n        cbar.ax.set_yticklabels([f\"Class {i+1}\" for i in range(n_clusters)])\n        cbar.set_label(\"Water Types\", rotation=270, labelpad=20)\n\n        # Add title\n        ax.set_title(title, fontsize=14)\n\n        # Show the plot\n        plt.show()\n\n    return best_match_full, latitudes, longitudes\n</code></pre>"},{"location":"pace/#hypercoast.pace.apply_sam_spectral","title":"<code>apply_sam_spectral(dataset, spectral_library=None, filter_condition=None, plot=True, figsize=(8, 6), extent=None, colors=None, title=None, **kwargs)</code>","text":"<p>Applies Spectral Angle Mapper (SAM) to the dataset and optionally plots the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The dataset containing the PACE data or the file path to the dataset.</p> required <code>spectral_library</code> <code>Union[str, list[str]]</code> <p>The spectral library file path or list of file paths.</p> <code>None</code> <code>filter_condition</code> <code>Callable[[xr.DataArray], xr.DataArray]</code> <p>A function to filter the data. Defaults to None.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>Whether to plot the data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size for the plot. Defaults to (8, 6).</p> <code>(8, 6)</code> <code>extent</code> <code>List[float]</code> <p>The extent to zoom in to the specified region. Defaults to None.</p> <code>None</code> <code>colors</code> <code>List[str]</code> <p>Colors for the clusters. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. Defaults to \"Spectral Angle Mapper Water Type Classification\".</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>plt.subplots</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[np.ndarray, np.ndarray, np.ndarray]</code> <p>The best match classification, latitudes, and longitudes.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def apply_sam_spectral(\n    dataset: Union[xr.Dataset, str],\n    spectral_library: Union[str, list[str]] = None,\n    filter_condition: Optional[Callable[[xr.DataArray], xr.DataArray]] = None,\n    plot: bool = True,\n    figsize: tuple[int, int] = (8, 6),\n    extent: list[float] = None,\n    colors: list[str] = None,\n    title: str = None,\n    **kwargs,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Applies Spectral Angle Mapper (SAM) to the dataset and optionally plots the results.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The dataset containing the PACE data or the file path to the dataset.\n        spectral_library (Union[str, list[str]]): The spectral library file path or list of file paths.\n        filter_condition (Callable[[xr.DataArray], xr.DataArray], optional): A function to filter the data. Defaults to None.\n        plot (bool, optional): Whether to plot the data. Defaults to True.\n        figsize (Tuple[int, int], optional): Figure size for the plot. Defaults to (8, 6).\n        extent (List[float], optional): The extent to zoom in to the specified region. Defaults to None.\n        colors (List[str], optional): Colors for the clusters. Defaults to None.\n        title (str, optional): Title for the plot. Defaults to \"Spectral Angle Mapper Water Type Classification\".\n        **kwargs: Additional keyword arguments to pass to the `plt.subplots` function.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: The best match classification, latitudes, and longitudes.\n    \"\"\"\n    import glob\n    import pandas as pd\n    import matplotlib.colors as mcolors\n    import cartopy.crs as ccrs\n    import cartopy.feature as cfeature\n    from scipy.interpolate import interp1d\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset)\n    elif isinstance(dataset, xr.DataArray):\n        dataset = dataset.to_dataset()\n    elif not isinstance(dataset, xr.Dataset):\n        raise ValueError(\"dataset must be an xarray Dataset\")\n\n    da = dataset[\"Rrs\"]\n    pace_wavelengths = da[\"wavelength\"].values\n\n    if isinstance(spectral_library, str):\n        endmember_paths = sorted(glob.glob(spectral_library))\n    elif isinstance(spectral_library, list):\n        endmember_paths = spectral_library\n    else:\n        endmember_paths = None\n\n    # Function to load and resample a single CSV spectral library file\n    def load_and_resample_spectral_library(csv_path, target_wavelengths):\n        df = pd.read_csv(csv_path)\n        original_wavelengths = df.iloc[:, 0].values  # First column is wavelength\n        spectra_values = df.iloc[:, 1].values  # Second column is spectral values\n\n        # Interpolation function\n        interp_func = interp1d(\n            original_wavelengths,\n            spectra_values,\n            kind=\"linear\",\n            fill_value=\"extrapolate\",\n        )\n\n        # Resample to the target (PACE) wavelengths\n        resampled_spectra = interp_func(target_wavelengths)\n\n        return resampled_spectra\n\n    if endmember_paths is not None:\n        endmembers = np.array(\n            [\n                load_and_resample_spectral_library(path, pace_wavelengths)\n                for path in endmember_paths\n            ]\n        )\n    else:\n        endmembers = None\n\n    # Function to calculate spectral angle\n    def spectral_angle_mapper(pixel, reference):\n        norm_pixel = np.linalg.norm(pixel)\n        norm_reference = np.linalg.norm(reference)\n        cos_theta = np.dot(pixel, reference) / (norm_pixel * norm_reference)\n        angle = np.arccos(np.clip(cos_theta, -1, 1))\n        return angle\n\n    # Reshape data to (n_pixels, n_bands)\n    reshaped_data = da.values.reshape(-1, da.shape[-1])\n\n    # Apply SAM for each pixel and each endmember\n    angles = np.zeros((reshaped_data.shape[0], endmembers.shape[0]))\n\n    for i in range(reshaped_data.shape[0]):\n        for j in range(endmembers.shape[0]):\n            angles[i, j] = spectral_angle_mapper(reshaped_data[i, :], endmembers[j, :])\n\n    # Find the minimum angle (best match) for each pixel\n    best_match = np.argmin(angles, axis=1)\n\n    # Reshape best_match back to the original spatial dimensions\n    best_match = best_match.reshape(da.shape[:-1])\n\n    if filter_condition is not None:\n        best_match = np.where(filter_condition, best_match, np.nan)\n\n    latitudes = da.coords[\"latitude\"].values\n    longitudes = da.coords[\"longitude\"].values\n\n    # Plot sample spectra from the CSV files and their resampled versions\n    def plot_sample_spectra(csv_paths, pace_wavelengths):\n        plt.figure(figsize=figsize)\n\n        for i, csv_path in enumerate(csv_paths):\n            df = pd.read_csv(csv_path)\n            original_wavelengths = df.iloc[:, 0].values\n            spectra_values = df.iloc[:, 1].values\n            resampled_spectra = load_and_resample_spectral_library(\n                csv_path, pace_wavelengths\n            )\n\n            plt.plot(\n                original_wavelengths,\n                spectra_values,\n                label=f\"Original Spectra {i+1}\",\n                linestyle=\"--\",\n            )\n            plt.plot(\n                pace_wavelengths, resampled_spectra, label=f\"Resampled Spectra {i+1}\"\n            )\n\n        plt.xlabel(\"Wavelength (nm)\")\n        plt.ylabel(\"Spectral Reflectance\")\n        plt.title(\"Comparison of Original and Resampled Spectra\")\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n    if plot:\n\n        if endmember_paths is not None:\n\n            plot_sample_spectra(endmember_paths, pace_wavelengths)\n\n        if colors is None:\n            colors = [\"#377eb8\", \"#e41a1c\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"]\n\n        if title is None:\n            title = \"Spectral Angle Mapper Water Type Classification\"\n        # Create a custom discrete color map\n        cmap = mcolors.ListedColormap(colors)\n        bounds = np.arange(-0.5, len(endmembers), 1)\n        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n\n        # Create a figure and axis with the correct map projection\n        _, ax = plt.subplots(\n            figsize=figsize, subplot_kw={\"projection\": ccrs.PlateCarree()}, **kwargs\n        )\n\n        # Plot the SAM classification results\n        im = ax.pcolormesh(\n            longitudes,\n            latitudes,\n            best_match,\n            cmap=cmap,\n            norm=norm,\n            transform=ccrs.PlateCarree(),\n        )\n\n        # Add geographic features for context\n        ax.add_feature(cfeature.COASTLINE)\n        ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n        ax.add_feature(cfeature.STATES, linestyle=\"--\")\n\n        # Adding axis labels\n        ax.set_xlabel(\"Longitude\")\n        ax.set_ylabel(\"Latitude\")\n\n        # Adding a title\n        ax.set_title(title, fontsize=14)\n\n        # Adding a color bar with discrete values\n        cbar = plt.colorbar(\n            im,\n            ax=ax,\n            orientation=\"vertical\",\n            # pad=0.02,\n            fraction=0.05,\n            ticks=np.arange(len(endmembers)),\n        )\n        cbar.ax.set_yticklabels([f\"Class {i+1}\" for i in range(len(endmembers))])\n        cbar.set_label(\"Water Types\", rotation=270, labelpad=20)\n\n        # Adding gridlines\n        ax.gridlines(draw_labels=True, linestyle=\"--\", linewidth=0.5)\n\n        # Set the extent to zoom in to the specified region (adjust as needed)\n        if extent is not None:\n            ax.set_extent(extent, crs=ccrs.PlateCarree())\n\n        # Show the plot\n        plt.show()\n\n    return best_match, latitudes, longitudes\n</code></pre>"},{"location":"pace/#hypercoast.pace.cyano_band_ratios","title":"<code>cyano_band_ratios(dataset, plot=True, extent=None, figsize=(12, 6), **kwargs)</code>","text":"<p>Calculates cyanobacteria band ratios from PACE data.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset or str</code> <p>The dataset containing the PACE data or the file path to the dataset.</p> required <code>plot</code> <code>bool</code> <p>Whether to plot the data. Defaults to True.</p> <code>True</code> <code>extent</code> <code>list</code> <p>The extent of the plot. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size. Defaults to (12, 6).</p> <code>(12, 6)</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>plt.subplots</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The cyanobacteria band ratios.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def cyano_band_ratios(\n    dataset: Union[xr.Dataset, str],\n    plot: bool = True,\n    extent: List[float] = None,\n    figsize: tuple[int, int] = (12, 6),\n    **kwargs,\n) -&gt; xr.DataArray:\n    \"\"\"\n    Calculates cyanobacteria band ratios from PACE data.\n\n    Args:\n        dataset (xr.Dataset or str): The dataset containing the PACE data or the file path to the dataset.\n        plot (bool, optional): Whether to plot the data. Defaults to True.\n        extent (list, optional): The extent of the plot. Defaults to None.\n        figsize (tuple, optional): Figure size. Defaults to (12, 6).\n        **kwargs: Additional keyword arguments to pass to the `plt.subplots` function.\n\n    Returns:\n        xr.DataArray: The cyanobacteria band ratios.\n    \"\"\"\n    import cartopy.crs as ccrs\n    import cartopy.feature as cfeature\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset)\n    elif not isinstance(dataset, xr.Dataset):\n        raise ValueError(\"dataset must be an xarray Dataset\")\n\n    da = dataset[\"Rrs\"]\n    data = (\n        (da.sel(wavelength=650) &gt; da.sel(wavelength=620))\n        &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=681))\n        &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=450))\n    )\n\n    if plot:\n        # Create a plot\n        _, ax = plt.subplots(\n            figsize=figsize, subplot_kw={\"projection\": ccrs.PlateCarree()}, **kwargs\n        )\n\n        if extent is not None:\n            ax.set_extent(extent, crs=ccrs.PlateCarree())\n\n        # Plot the data\n        data.plot(\n            ax=ax,\n            transform=ccrs.PlateCarree(),\n            cmap=\"coolwarm\",\n            cbar_kwargs={\"label\": \"Cyano\"},\n        )\n\n        # Add coastlines\n        ax.coastlines()\n\n        # Add state boundaries\n        states_provinces = cfeature.NaturalEarthFeature(\n            category=\"cultural\",\n            name=\"admin_1_states_provinces_lines\",\n            scale=\"50m\",\n            facecolor=\"none\",\n        )\n\n        ax.add_feature(states_provinces, edgecolor=\"gray\")\n\n        # Optionally, add gridlines, labels, etc.\n        ax.gridlines(draw_labels=True)\n        plt.show()\n\n    return data\n</code></pre>"},{"location":"pace/#hypercoast.pace.extract_pace","title":"<code>extract_pace(dataset, latitude, longitude, delta=0.01, return_plot=False, **kwargs)</code>","text":"<p>Extracts data from a PACE dataset for a given latitude and longitude range     and calculates the mean over these dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The PACE dataset or path to the dataset file.</p> required <code>latitude</code> <code>Union[float, Tuple[float, float]]</code> <p>The latitude or range of latitudes to extract data for.</p> required <code>longitude</code> <code>Union[float, Tuple[float, float]]</code> <p>The longitude or range of longitudes to extract data for.</p> required <code>delta</code> <code>float</code> <p>The range to add/subtract to the latitude and longitude if they are not ranges. Defaults to 0.01.</p> <code>0.01</code> <code>return_plot</code> <code>bool</code> <p>Whether to return a plot of the data. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the plot function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[xr.DataArray, plt.figure.Figure]</code> <p>The mean data over the latitude     and longitude dimensions, or a plot of this data if return_plot is True.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def extract_pace(\n    dataset: Union[xr.Dataset, str],\n    latitude: Union[float, Tuple[float, float]],\n    longitude: Union[float, Tuple[float, float]],\n    delta: float = 0.01,\n    return_plot: bool = False,\n    **kwargs,\n) -&gt; Union[xr.DataArray, plt.Figure]:\n    \"\"\"\n    Extracts data from a PACE dataset for a given latitude and longitude range\n        and calculates the mean over these dimensions.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The PACE dataset or path to the dataset file.\n        latitude (Union[float, Tuple[float, float]]): The latitude or range of\n            latitudes to extract data for.\n        longitude (Union[float, Tuple[float, float]]): The longitude or range of\n            longitudes to extract data for.\n        delta (float, optional): The range to add/subtract to the latitude and\n            longitude if they are not ranges. Defaults to 0.01.\n        return_plot (bool, optional): Whether to return a plot of the data. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the plot function.\n\n    Returns:\n        Union[xr.DataArray, plt.figure.Figure]: The mean data over the latitude\n            and longitude dimensions, or a plot of this data if return_plot is True.\n    \"\"\"\n    if isinstance(latitude, list) or isinstance(latitude, tuple):\n        pass\n    else:\n        latitude = (latitude - delta, latitude + delta)\n\n    if isinstance(longitude, list) or isinstance(longitude, tuple):\n        pass\n    else:\n        longitude = (longitude - delta, longitude + delta)\n\n    ds = filter_pace(dataset, latitude, longitude, return_plot=False)\n    data = ds.mean(dim=[\"latitude\", \"longitude\"])\n    if return_plot:\n        return data.plot.line(**kwargs)\n    else:\n        return data\n</code></pre>"},{"location":"pace/#hypercoast.pace.filter_pace","title":"<code>filter_pace(dataset, latitude, longitude, drop=True, return_plot=False, **kwargs)</code>","text":"<p>Filters a PACE dataset based on latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The PACE dataset to filter.</p> required <code>latitude</code> <code>float or tuple</code> <p>The latitude to filter by. If a tuple or list, it represents a range.</p> required <code>longitude</code> <code>float or tuple</code> <p>The longitude to filter by. If a tuple or list, it represents a range.</p> required <code>drop</code> <code>bool</code> <p>Whether to drop the filtered out data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The filtered PACE data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def filter_pace(dataset, latitude, longitude, drop=True, return_plot=False, **kwargs):\n    \"\"\"\n    Filters a PACE dataset based on latitude and longitude.\n\n    Args:\n        dataset (xr.Dataset): The PACE dataset to filter.\n        latitude (float or tuple): The latitude to filter by. If a tuple or list, it represents a range.\n        longitude (float or tuple): The longitude to filter by. If a tuple or list, it represents a range.\n        drop (bool, optional): Whether to drop the filtered out data. Defaults to True.\n\n    Returns:\n        xr.DataArray: The filtered PACE data.\n    \"\"\"\n    if isinstance(latitude, list) or isinstance(latitude, tuple):\n        lat_con = (dataset[\"latitude\"] &gt; latitude[0]) &amp; (\n            dataset[\"latitude\"] &lt; latitude[1]\n        )\n    else:\n        lat_con = dataset[\"latitude\"] == latitude\n\n    if isinstance(longitude, list) or isinstance(longitude, tuple):\n        lon_con = (dataset[\"longitude\"] &gt; longitude[0]) &amp; (\n            dataset[\"longitude\"] &lt; longitude[1]\n        )\n    else:\n        lon_con = dataset[\"longitude\"] == longitude\n\n    da = dataset[\"Rrs\"].where(lat_con &amp; lon_con, drop=drop, **kwargs)\n    da_filtered = da.dropna(dim=\"latitude\", how=\"all\")\n    da_filtered = da_filtered.dropna(dim=\"longitude\", how=\"all\")\n\n    if return_plot:\n        rrs_stack = da_filtered.stack(\n            {\"pixel\": [\"latitude\", \"longitude\"]},\n            create_index=False,\n        )\n        rrs_stack.plot.line(hue=\"pixel\")\n    else:\n        return da_filtered\n</code></pre>"},{"location":"pace/#hypercoast.pace.grid_pace","title":"<code>grid_pace(dataset, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Grids a PACE dataset based on latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The PACE dataset to grid.</p> required <code>wavelengths</code> <code>float or int</code> <p>The wavelength to select.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for griddata interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the xr.Dataset constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The gridded PACE data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def grid_pace(dataset, wavelengths=None, method=\"nearest\", **kwargs):\n    \"\"\"\n    Grids a PACE dataset based on latitude and longitude.\n\n    Args:\n        dataset (xr.Dataset): The PACE dataset to grid.\n        wavelengths (float or int): The wavelength to select.\n        method (str, optional): The method to use for griddata interpolation.\n            Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to pass to the xr.Dataset constructor.\n\n    Returns:\n        xr.DataArray: The gridded PACE data.\n    \"\"\"\n    from scipy.interpolate import griddata\n\n    if wavelengths is None:\n        wavelengths = dataset.coords[\"wavelength\"].values[0]\n\n    # Ensure wavelengths is a list\n    if not isinstance(wavelengths, list):\n        wavelengths = [wavelengths]\n\n    lat = dataset.latitude\n    lon = dataset.longitude\n\n    grid_lat = np.linspace(lat.min().values, lat.max().values, lat.shape[0])\n    grid_lon = np.linspace(lon.min().values, lon.max().values, lon.shape[1])\n    grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n\n    gridded_data_dict = {}\n    for wavelength in wavelengths:\n        data = dataset.sel(wavelength=wavelength, method=\"nearest\")[\"Rrs\"]\n        gridded_data = griddata(\n            (lat.data.flatten(), lon.data.flatten()),\n            data.data.flatten(),\n            (grid_lat_2d, grid_lon_2d),\n            method=method,\n        )\n        gridded_data_dict[wavelength] = gridded_data\n\n    # Create a 3D array with dimensions latitude, longitude, and wavelength\n    gridded_data_3d = np.dstack(list(gridded_data_dict.values()))\n\n    dataset2 = xr.Dataset(\n        {\"Rrs\": ((\"latitude\", \"longitude\", \"wavelength\"), gridded_data_3d)},\n        coords={\n            \"latitude\": (\"latitude\", grid_lat),\n            \"longitude\": (\"longitude\", grid_lon),\n            \"wavelength\": (\"wavelength\", list(gridded_data_dict.keys())),\n        },\n        **kwargs,\n    )\n\n    dataset2[\"Rrs\"].rio.write_crs(\"EPSG:4326\", inplace=True)\n\n    return dataset2\n</code></pre>"},{"location":"pace/#hypercoast.pace.grid_pace_bgc","title":"<code>grid_pace_bgc(dataset, variable='chlor_a', method='nearest', **kwargs)</code>","text":"<p>Grids PACE BGC data using specified interpolation method.</p> <p>This function takes an xarray Dataset containing PACE BGC data, interpolates it onto a regular grid using the specified method, and returns the gridded data as an xarray DataArray with the specified variable.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The input dataset containing PACE BGC data with latitude and longitude coordinates.</p> required <code>variable</code> <code>str</code> <p>The variable within the dataset to grid. Can be one of chlor_a, carbon_phyto, poc, chlor_a_unc, carbon_phyto_unc, and l2_flags. Defaults to \"chlor_a\".</p> <code>'chlor_a'</code> <code>method</code> <code>str</code> <p>The interpolation method to use. Options include \"nearest\", \"linear\", and \"cubic\". Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the xr.Dataset creation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The gridded data as an xarray DataArray, with the specified variable and EPSG:4326 CRS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = hypercoast.read_pace_bgc(\"path_to_your_dataset.nc\")\n&gt;&gt;&gt; gridded_data = grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"nearest\")\n&gt;&gt;&gt; print(gridded_data)\n</code></pre> Source code in <code>hypercoast/pace.py</code> <pre><code>def grid_pace_bgc(\n    dataset: xr.Dataset,\n    variable: str = \"chlor_a\",\n    method: str = \"nearest\",\n    **kwargs: Any,\n) -&gt; xr.DataArray:\n    \"\"\"\n    Grids PACE BGC data using specified interpolation method.\n\n    This function takes an xarray Dataset containing PACE BGC data, interpolates it onto a regular grid\n    using the specified method, and returns the gridded data as an xarray DataArray with the specified\n    variable.\n\n    Args:\n        dataset (xr.Dataset): The input dataset containing PACE BGC data with latitude and longitude coordinates.\n        variable (str, optional): The variable within the dataset to grid. Can be\n            one of chlor_a, carbon_phyto, poc, chlor_a_unc, carbon_phyto_unc, and l2_flags.\n            Defaults to \"chlor_a\".\n        method (str, optional): The interpolation method to use. Options include \"nearest\", \"linear\", and \"cubic\".\n            Defaults to \"nearest\".\n        **kwargs (Any): Additional keyword arguments to pass to the xr.Dataset creation.\n\n    Returns:\n        xr.DataArray: The gridded data as an xarray DataArray, with the specified variable and EPSG:4326 CRS.\n\n    Example:\n        &gt;&gt;&gt; dataset = hypercoast.read_pace_bgc(\"path_to_your_dataset.nc\")\n        &gt;&gt;&gt; gridded_data = grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"nearest\")\n        &gt;&gt;&gt; print(gridded_data)\n    \"\"\"\n    import rioxarray\n    from scipy.interpolate import griddata\n\n    lat = dataset.latitude\n    lon = dataset.longitude\n\n    grid_lat = np.linspace(lat.min().values, lat.max().values, lat.shape[0])\n    grid_lon = np.linspace(lon.min().values, lon.max().values, lon.shape[1])\n    grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n\n    data = dataset[variable]\n    gridded_data = griddata(\n        (lat.data.flatten(), lon.data.flatten()),\n        data.data.flatten(),\n        (grid_lat_2d, grid_lon_2d),\n        method=method,\n    )\n\n    dataset2 = xr.Dataset(\n        {variable: ((\"latitude\", \"longitude\"), gridded_data)},\n        coords={\n            \"latitude\": (\"latitude\", grid_lat),\n            \"longitude\": (\"longitude\", grid_lon),\n        },\n        **kwargs,\n    )\n\n    dataset2 = dataset2[variable].rio.write_crs(\"EPSG:4326\")\n\n    return dataset2\n</code></pre>"},{"location":"pace/#hypercoast.pace.pace_chla_to_image","title":"<code>pace_chla_to_image(data, output=None, **kwargs)</code>","text":"<p>Converts PACE chlorophyll-a data to an image.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>xr.DataArray or str</code> <p>The chlorophyll-a data or the file path to the data.</p> required <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the data. If <code>output</code> is provided, the image will be saved to the specified file and the function will return None.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def pace_chla_to_image(data, output=None, **kwargs):\n    \"\"\"\n    Converts PACE chlorophyll-a data to an image.\n\n    Args:\n        data (xr.DataArray or str): The chlorophyll-a data or the file path to the data.\n        output (str, optional): The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the data. If `output` is provided, the image will be saved to the specified file and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image, image_to_geotiff\n\n    if isinstance(data, str):\n        data = read_pace_chla(data)\n    elif not isinstance(data, xr.DataArray):\n        raise ValueError(\"data must be an xarray DataArray\")\n\n    image = array_to_image(data, transpose=False, output=None, **kwargs)\n\n    if output is not None:\n        image_to_geotiff(image, output, dtype=\"float32\")\n\n    return image\n</code></pre>"},{"location":"pace/#hypercoast.pace.pace_to_image","title":"<code>pace_to_image(dataset, wavelengths=None, method='nearest', gridded=False, output=None, **kwargs)</code>","text":"<p>Converts an PACE dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xarray.Dataset or str</code> <p>The dataset containing the EMIT data or the file path to the dataset.</p> required <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>gridded</code> <code>bool</code> <p>Whether the dataset is a gridded dataset. Defaults to False,</p> <code>False</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the dataset. If <code>output</code> is provided, the image will be saved to the specified file and the function will return None.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def pace_to_image(\n    dataset, wavelengths=None, method=\"nearest\", gridded=False, output=None, **kwargs\n):\n    \"\"\"\n    Converts an PACE dataset to an image.\n\n    Args:\n        dataset (xarray.Dataset or str): The dataset containing the EMIT data or the file path to the dataset.\n        wavelengths (array-like, optional): The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data interpolation. Defaults to \"nearest\".\n        gridded (bool, optional): Whether the dataset is a gridded dataset. Defaults to False,\n        output (str, optional): The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the dataset. If `output` is provided, the image will be saved to the specified file and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset, wavelengths=wavelengths, method=\"nearest\")\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=\"nearest\")\n\n    if not gridded:\n        grid = grid_pace(dataset, wavelengths=wavelengths, method=method)\n    else:\n        grid = dataset\n    data = grid[\"Rrs\"]\n    data.rio.write_crs(\"EPSG:4326\", inplace=True)\n\n    return array_to_image(data, transpose=False, output=output, **kwargs)\n</code></pre>"},{"location":"pace/#hypercoast.pace.read_pace","title":"<code>read_pace(filepath, wavelengths=None, products=None, method='nearest', engine='h5netcdf', **kwargs)</code>","text":"<p>Import data from a L2 PACE file and re-orient to Earth coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>os.PathLike</code> <p>Pathlike string to the file to read.</p> required <code>wavelengths</code> <code>array-like</code> <p>Specific wavelengths to select. If None, all wavelengths are selected.</p> <code>None</code> <code>products</code> <code>array-like</code> <p>Specific data products or variables to select. If None, all products are selected.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for selection when wavelengths is not None. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the xarray <code>sel</code> method when wavelengths is not None.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>An xarray Dataset containing the PACE data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def read_pace(\n    filepath: Union[str, os.PathLike],\n    wavelengths: Union[ArrayLike, None] = None,\n    products: Union[ArrayLike, None] = None,\n    method: str = \"nearest\",\n    engine: str = \"h5netcdf\",\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Import data from a L2 PACE file and re-orient to Earth coordinates.\n\n    Args:\n        filepath (os.PathLike): Pathlike string to the file to read.\n        wavelengths (array-like, optional): Specific wavelengths to select. If None, all wavelengths are selected.\n        products (array-like, optional): Specific data products or variables to select. If None, all products are selected.\n        method (str, optional): Method to use for selection when wavelengths is not None. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to pass to the xarray `sel` method when wavelengths is not None.\n\n    Returns:\n        xr.Dataset: An xarray Dataset containing the PACE data.\n    \"\"\"\n\n    # Import nav group as base dataset.\n    dataset = xr.open_dataset(filepath, group=\"navigation_data\", engine=engine)\n    dataset = dataset.set_coords([\"latitude\", \"longitude\"])\n\n    # Import geophysical data products.\n    product = xr.open_dataset(filepath, group=\"geophysical_data\", engine=engine)\n\n    # Import sensor band parameters.\n    band_params = xr.open_dataset(\n        filepath, group=\"sensor_band_parameters\", engine=engine\n    )\n\n    # Merge datasets and rename dimensions/coordinates.\n    dataset = xr.merge([dataset, product], join=\"outer\", combine_attrs=\"drop_conflicts\")\n    if \"pixel_control_points\" in dataset.dims:\n        dataset = dataset.rename({\"pixel_control_points\": \"pixels_per_line\"})\n\n    # Build rename dict for dimensions\n    rename_dict = {\"number_of_lines\": \"latitude\", \"pixels_per_line\": \"longitude\"}\n    if \"wavelength_3d\" in dataset.dims:\n        rename_dict[\"wavelength_3d\"] = \"wavelength\"\n    dataset = dataset.rename(rename_dict)\n\n    # Set wavelength values from band parameters if available\n    if \"wavelength_3d\" in band_params.coords and \"wavelength\" in dataset.dims:\n        dataset = dataset.assign_coords(\n            wavelength=band_params.coords[\"wavelength_3d\"].values\n        )\n\n    # If specified, only keep products of interest.\n    if products is not None:\n        dataset = dataset[products]\n\n    # If specified, only keep wavelengths of interest for datasets with a wavelength dimension.\n    if wavelengths is not None and \"wavelength\" in dataset.coords:\n        dataset = dataset.sel(wavelength=wavelengths, method=method, **kwargs)\n\n    return dataset\n</code></pre>"},{"location":"pace/#hypercoast.pace.read_pace_aop","title":"<code>read_pace_aop(filepath, engine='h5netcdf', **kwargs)</code>","text":"<p>Reads PACE data from a given file and returns an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the file to read.</p> required <code>wavelengths</code> <code>array-like</code> <p>Specific wavelengths to select. If None, all wavelengths are selected.</p> required <code>method</code> <code>str</code> <p>Method to use for selection when wavelengths is not None. Defaults to \"nearest\".</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>sel</code> method when wavelengths is not None.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>An xarray Dataset containing the PACE data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def read_pace_aop(filepath, engine=\"h5netcdf\", **kwargs):\n    \"\"\"\n    Reads PACE data from a given file and returns an xarray Dataset.\n\n    Args:\n        filepath (str): Path to the file to read.\n        wavelengths (array-like, optional): Specific wavelengths to select. If None, all wavelengths are selected.\n        method (str, optional): Method to use for selection when wavelengths is not None. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to pass to the `sel` method when wavelengths is not None.\n\n    Returns:\n        xr.Dataset: An xarray Dataset containing the PACE data.\n    \"\"\"\n\n    rrs = xr.open_dataset(filepath, engine=engine, group=\"geophysical_data\", **kwargs)[\n        \"Rrs\"\n    ]\n    wvl = xr.open_dataset(\n        filepath, engine=engine, group=\"sensor_band_parameters\", **kwargs\n    )\n    dataset = xr.open_dataset(\n        filepath, engine=engine, group=\"navigation_data\", **kwargs\n    )\n    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n    if \"pixel_control_points\" in dataset.dims:\n        dataset = dataset.rename({\"pixel_control_points\": \"pixels_per_line\"})\n    dataset = xr.merge([rrs, dataset.coords.to_dataset()])\n    dataset.coords[\"wavelength_3d\"] = wvl.coords[\"wavelength_3d\"]\n\n    return dataset\n</code></pre>"},{"location":"pace/#hypercoast.pace.read_pace_bgc","title":"<code>read_pace_bgc(filepath, variable=None, engine='h5netcdf', **kwargs)</code>","text":"<p>Reads PACE BGC data from a specified file and returns an xarray Dataset.</p> <p>This function opens a dataset from a file using the specified engine, optionally selects a single variable, merges geophysical and navigation data, sets appropriate coordinates, and renames dimensions for easier use.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file containing the PACE BGC data.</p> required <code>variable</code> <code>Optional[str]</code> <p>The specific variable to extract from the geophysical_data group. If None, all variables are read. Defaults to None.</p> <code>None</code> <code>engine</code> <code>str</code> <p>The engine to use for reading the file. Defaults to \"h5netcdf\".</p> <code>'h5netcdf'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to <code>xr.open_dataset</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>An xarray Dataset containing the requested PACE BGC data, with merged geophysical and navigation data, set coordinates, and renamed dimensions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = read_pace_bgc(\"path/to/your/datafile.h5\", variable=\"chlor_a\")\n&gt;&gt;&gt; print(dataset)\n</code></pre> Source code in <code>hypercoast/pace.py</code> <pre><code>def read_pace_bgc(\n    filepath: str,\n    variable: Optional[str] = None,\n    engine: str = \"h5netcdf\",\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads PACE BGC data from a specified file and returns an xarray Dataset.\n\n    This function opens a dataset from a file using the specified engine,\n    optionally selects a single variable, merges geophysical and navigation data,\n    sets appropriate coordinates, and renames dimensions for easier use.\n\n    Args:\n        filepath (str): The path to the file containing the PACE BGC data.\n        variable (Optional[str], optional): The specific variable to extract\n            from the geophysical_data group. If None, all variables are read. Defaults to None.\n        engine (str, optional): The engine to use for reading the file. Defaults to \"h5netcdf\".\n        **kwargs (Any): Additional keyword arguments to pass to `xr.open_dataset`.\n\n    Returns:\n        xr.Dataset: An xarray Dataset containing the requested PACE BGC data,\n        with merged geophysical and navigation data, set coordinates, and renamed dimensions.\n\n    Example:\n        &gt;&gt;&gt; dataset = read_pace_bgc(\"path/to/your/datafile.h5\", variable=\"chlor_a\")\n        &gt;&gt;&gt; print(dataset)\n    \"\"\"\n\n    ds = xr.open_dataset(filepath, engine=engine, group=\"geophysical_data\", **kwargs)\n    if variable is not None:\n        ds = ds[variable]\n    dataset = xr.open_dataset(\n        filepath, engine=engine, group=\"navigation_data\", **kwargs\n    )\n    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n    if \"pixel_control_points\" in dataset.dims:\n        dataset = dataset.rename({\"pixel_control_points\": \"pixels_per_line\"})\n    dataset = xr.merge([ds, dataset.coords.to_dataset()])\n    dataset = dataset.rename(\n        {\n            \"number_of_lines\": \"latitude\",\n            \"pixels_per_line\": \"longitude\",\n        }\n    )\n    attrs = xr.open_dataset(filepath, engine=engine, **kwargs).attrs\n    dataset.attrs.update(attrs)\n\n    return dataset\n</code></pre>"},{"location":"pace/#hypercoast.pace.read_pace_chla","title":"<code>read_pace_chla(filepaths, engine='h5netcdf', **kwargs)</code>","text":"<p>Reads chlorophyll-a data from PACE files and applies a logarithmic transformation.</p> <p>This function supports reading from a single file or multiple files. For multiple files, it combines them into a single dataset. It then extracts the chlorophyll-a variable, applies a logarithmic transformation, and sets the coordinate reference system to EPSG:4326.</p> <p>Parameters:</p> Name Type Description Default <code>filepaths</code> <code>Union[str, List[str]]</code> <p>A string or a list of strings containing the file path(s) to the PACE chlorophyll-a data files.</p> required <code>engine</code> <code>str</code> <p>The backend engine to use for reading files. Defaults to \"h5netcdf\".</p> <code>'h5netcdf'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>xr.open_dataset</code> or <code>xr.open_mfdataset</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>An xarray DataArray containing the logarithmically transformed chlorophyll-a data with updated attributes.</p> <p>Examples:</p> <p>Read chlorophyll-a data from a single file:</p> <pre><code>&gt;&gt;&gt; chla_data = read_pace_chla('path/to/single/file.nc')\n</code></pre> <p>Read and combine chlorophyll-a data from multiple files:</p> <pre><code>&gt;&gt;&gt; chla_data = read_pace_chla(['path/to/file1.nc', 'path/to/file2.nc'], combine='by_coords')\n</code></pre> Source code in <code>hypercoast/pace.py</code> <pre><code>def read_pace_chla(\n    filepaths: Union[str, List[str]], engine: str = \"h5netcdf\", **kwargs\n) -&gt; xr.DataArray:\n    \"\"\"\n    Reads chlorophyll-a data from PACE files and applies a logarithmic transformation.\n\n    This function supports reading from a single file or multiple files. For multiple files,\n    it combines them into a single dataset. It then extracts the chlorophyll-a variable,\n    applies a logarithmic transformation, and sets the coordinate reference system to EPSG:4326.\n\n    Args:\n        filepaths: A string or a list of strings containing the file path(s) to the PACE chlorophyll-a data files.\n        engine: The backend engine to use for reading files. Defaults to \"h5netcdf\".\n        **kwargs: Additional keyword arguments to pass to `xr.open_dataset` or `xr.open_mfdataset`.\n\n    Returns:\n        An xarray DataArray containing the logarithmically transformed chlorophyll-a data with updated attributes.\n\n    Examples:\n        Read chlorophyll-a data from a single file:\n        &gt;&gt;&gt; chla_data = read_pace_chla('path/to/single/file.nc')\n\n        Read and combine chlorophyll-a data from multiple files:\n        &gt;&gt;&gt; chla_data = read_pace_chla(['path/to/file1.nc', 'path/to/file2.nc'], combine='by_coords')\n    \"\"\"\n\n    import os\n    import glob\n    import rioxarray\n\n    date = None\n    if isinstance(filepaths, str) and os.path.isfile(filepaths):\n        filepaths = [filepaths]\n    if \"combine\" not in kwargs:\n        kwargs[\"combine\"] = \"nested\"\n    if \"concat_dim\" not in kwargs:\n        kwargs[\"concat_dim\"] = \"date\"\n    dataset = xr.open_mfdataset(filepaths, engine=engine, **kwargs)\n    if not isinstance(filepaths, list):\n        filepaths = glob.glob(filepaths)\n        filepaths.sort()\n\n    dates = [extract_date_from_filename(f) for f in filepaths]\n    date = [timestamp.strftime(\"%Y-%m-%d\") for timestamp in dates]\n    dataset = dataset.assign_coords(date=(\"date\", date))\n\n    chla = np.log10(dataset[\"chlor_a\"])\n    chla.attrs.update(\n        {\n            \"units\": f'lg({dataset[\"chlor_a\"].attrs[\"units\"]})',\n        }\n    )\n\n    if date is not None:\n        chla.attrs[\"date\"] = date\n\n    chla = chla.transpose(\"lat\", \"lon\", \"date\")\n\n    chla.rio.write_crs(\"EPSG:4326\", inplace=True)\n\n    return chla\n</code></pre>"},{"location":"pace/#hypercoast.pace.view_pace_pixel_locations","title":"<code>view_pace_pixel_locations(filepath, step=20, figsize=(8, 6), **kwargs)</code>","text":"<p>Visualizes a subset of PACE pixel locations on a scatter plot.</p> <p>This function reads PACE AOP data from a specified file, subsamples the data according to a step size, and plots the longitude and latitude of the selected pixels using a scatter plot.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file containing the PACE AOP data.</p> required <code>step</code> <code>int</code> <p>The step size for subsampling the data. A smaller step size results in more data points being plotted. Defaults to 20.</p> <code>20</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>plot.scatter</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>A matplotlib figure object containing the scatter plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot = view_pace_pixel_locations(\"path/to/your/datafile.h5\", step=10)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>hypercoast/pace.py</code> <pre><code>def view_pace_pixel_locations(\n    filepath: str, step: int = 20, figsize: Tuple[float, float] = (8, 6), **kwargs: Any\n) -&gt; plt.Figure:\n    \"\"\"\n    Visualizes a subset of PACE pixel locations on a scatter plot.\n\n    This function reads PACE AOP data from a specified file, subsamples the data according to a step size,\n    and plots the longitude and latitude of the selected pixels using a scatter plot.\n\n    Args:\n        filepath (str): The path to the file containing the PACE AOP data.\n        step (int, optional): The step size for subsampling the data. A smaller step size results in more\n            data points being plotted. Defaults to 20.\n        **kwargs (Any): Additional keyword arguments to pass to the `plot.scatter` method.\n\n    Returns:\n        plt.Figure: A matplotlib figure object containing the scatter plot.\n\n    Example:\n        &gt;&gt;&gt; plot = view_pace_pixel_locations(\"path/to/your/datafile.h5\", step=10)\n        &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    # Create a new figure\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Create the plot\n    dataset = read_pace_aop(filepath)\n    number_of_lines = dataset.sizes[\"number_of_lines\"]\n    pixels_per_line = dataset.sizes[\"pixels_per_line\"]\n\n    ax.scatter(\n        dataset.sel(\n            {\n                \"number_of_lines\": slice(None, None, number_of_lines // step),\n                \"pixels_per_line\": slice(None, None, pixels_per_line // step),\n            }\n        ).longitude,\n        dataset.sel(\n            {\n                \"number_of_lines\": slice(None, None, number_of_lines // step),\n                \"pixels_per_line\": slice(None, None, pixels_per_line // step),\n            }\n        ).latitude,\n        **kwargs,\n    )\n\n    # Set labels and title\n    ax.set_xlabel(\"Longitude\")\n    ax.set_ylabel(\"Latitude\")\n    ax.set_title(\"PACE Pixel Locations\")\n\n    return fig\n</code></pre>"},{"location":"pace/#hypercoast.pace.viz_pace","title":"<code>viz_pace(dataset, wavelengths=None, method='nearest', figsize=(6.4, 4.8), cmap='jet', vmin=0, vmax=0.02, ncols=1, crs=None, xlim=None, ylim=None, **kwargs)</code>","text":"<p>Plots PACE data from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>An xarray Dataset containing the PACE data.</p> required <code>wavelengths</code> <code>array-like</code> <p>Specific wavelengths to select. If None, all wavelengths are selected.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for selection when wavelengths is not None. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>figsize</code> <code>tuple</code> <p>Figure size. Defaults to (6.4, 4.8).</p> <code>(6.4, 4.8)</code> <code>cmap</code> <code>str</code> <p>Colormap to use. Defaults to \"jet\".</p> <code>'jet'</code> <code>vmin</code> <code>float</code> <p>Minimum value for the colormap. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>Maximum value for the colormap. Defaults to 0.02.</p> <code>0.02</code> <code>ncols</code> <code>int</code> <p>Number of columns in the plot. Defaults to 1.</p> <code>1</code> <code>crs</code> <code>str or cartopy.crs.CRS</code> <p>Coordinate reference system to use. If None, a simple plot is created. Defaults to None. See https://scitools.org.uk/cartopy/docs/latest/reference/projections.html</p> <code>None</code> <code>xlim</code> <code>array-like</code> <p>Limits for the x-axis. Defaults to None.</p> <code>None</code> <code>ylim</code> <code>array-like</code> <p>Limits for the y-axis. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>plt.subplots</code> function.</p> <code>{}</code> Source code in <code>hypercoast/pace.py</code> <pre><code>def viz_pace(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Optional[Union[List[float], float]] = None,\n    method: str = \"nearest\",\n    figsize: Tuple[float, float] = (6.4, 4.8),\n    cmap: str = \"jet\",\n    vmin: float = 0,\n    vmax: float = 0.02,\n    ncols: int = 1,\n    crs: Optional[str] = None,\n    xlim: Optional[List[float]] = None,\n    ylim: Optional[List[float]] = None,\n    **kwargs,\n):\n    \"\"\"\n    Plots PACE data from a given xarray Dataset.\n\n    Args:\n        dataset (xr.Dataset): An xarray Dataset containing the PACE data.\n        wavelengths (array-like, optional): Specific wavelengths to select. If None, all wavelengths are selected.\n        method (str, optional): Method to use for selection when wavelengths is not None. Defaults to \"nearest\".\n        figsize (tuple, optional): Figure size. Defaults to (6.4, 4.8).\n        cmap (str, optional): Colormap to use. Defaults to \"jet\".\n        vmin (float, optional): Minimum value for the colormap. Defaults to 0.\n        vmax (float, optional): Maximum value for the colormap. Defaults to 0.02.\n        ncols (int, optional): Number of columns in the plot. Defaults to 1.\n        crs (str or cartopy.crs.CRS, optional): Coordinate reference system to use. If None, a simple plot is created. Defaults to None.\n            See https://scitools.org.uk/cartopy/docs/latest/reference/projections.html\n        xlim (array-like, optional): Limits for the x-axis. Defaults to None.\n        ylim (array-like, optional): Limits for the y-axis. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the `plt.subplots` function.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import math\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset, wavelengths, method)\n\n    if wavelengths is not None:\n        if not isinstance(wavelengths, list):\n            wavelengths = [wavelengths]\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n    else:\n        wavelengths = dataset.coords[\"wavelength\"][0].values.tolist()\n\n    lat = dataset.coords[\"latitude\"]\n    lon = dataset.coords[\"longitude\"]\n\n    nrows = math.ceil(len(wavelengths) / ncols)\n\n    if crs is None:\n\n        fig, axes = plt.subplots(\n            nrows=nrows,\n            ncols=ncols,\n            figsize=(figsize[0] * ncols, figsize[1] * nrows),\n            **kwargs,\n        )\n\n        for i in range(nrows):\n            for j in range(ncols):\n                index = i * ncols + j\n                if index &lt; len(wavelengths):\n                    wavelength = wavelengths[index]\n                    data = dataset.sel(wavelength=wavelength, method=method)[\"Rrs\"]\n\n                    if min(nrows, ncols) == 1:\n                        ax = axes[index]\n                    else:\n                        ax = axes[i, j]\n                    im = ax.pcolormesh(\n                        lon, lat, np.squeeze(data), cmap=cmap, vmin=vmin, vmax=vmax\n                    )\n                    ax.set_xlabel(\"Longitude\")\n                    ax.set_ylabel(\"Latitude\")\n                    ax.set_title(\n                        f\"wavelength = {dataset.coords['wavelength'].values[index]} [nm]\"\n                    )\n                    fig.colorbar(im, ax=ax, label=\"Reflectance\")\n\n        plt.tight_layout()\n        plt.show()\n\n    else:\n\n        import cartopy\n        from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n\n        if crs == \"default\":\n            crs = cartopy.crs.PlateCarree()\n\n        if xlim is None:\n            xlim = [math.floor(lon.min()), math.ceil(lon.max())]\n\n        if ylim is None:\n            ylim = [math.floor(lat.min()), math.ceil(lat.max())]\n\n        fig, axes = plt.subplots(\n            nrows=nrows,\n            ncols=ncols,\n            figsize=(figsize[0] * ncols, figsize[1] * nrows),\n            subplot_kw={\"projection\": cartopy.crs.PlateCarree()},\n            **kwargs,\n        )\n\n        for i in range(nrows):\n            for j in range(ncols):\n                index = i * ncols + j\n                if index &lt; len(wavelengths):\n                    wavelength = wavelengths[index]\n                    data = dataset.sel(wavelength=wavelength, method=method)[\"Rrs\"]\n\n                    if min(nrows, ncols) == 1:\n                        ax = axes[index]\n                    else:\n                        ax = axes[i, j]\n                    im = ax.pcolormesh(lon, lat, data, cmap=\"jet\", vmin=0, vmax=0.02)\n                    ax.coastlines()\n                    ax.add_feature(cartopy.feature.STATES, linewidth=0.5)\n                    ax.set_xticks(np.linspace(xlim[0], xlim[1], 5), crs=crs)\n                    ax.set_yticks(np.linspace(ylim[0], ylim[1], 5), crs=crs)\n                    lon_formatter = LongitudeFormatter(zero_direction_label=True)\n                    lat_formatter = LatitudeFormatter()\n                    ax.xaxis.set_major_formatter(lon_formatter)\n                    ax.yaxis.set_major_formatter(lat_formatter)\n                    ax.set_xlabel(\"Longitude\")\n                    ax.set_ylabel(\"Latitude\")\n                    ax.set_title(\n                        f\"wavelength = {dataset.coords['wavelength'].values[index]} [nm]\"\n                    )\n                    plt.colorbar(im, label=\"Reflectance\")\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"pace/#hypercoast.pace.viz_pace_chla","title":"<code>viz_pace_chla(data, date=None, aspect=2, cmap='jet', size=6, **kwargs)</code>","text":"<p>Visualizes PACE chlorophyll-a data using an xarray DataArray.</p> <p>This function supports loading data from a file path (str) or directly using an xarray DataArray. It allows for selection of a specific date for visualization or averages over all dates if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, xr.DataArray]</code> <p>The chlorophyll-a data to visualize. Can be a file path or an xarray DataArray.</p> required <code>date</code> <code>Optional[str]</code> <p>Specific date to visualize. If None, averages over all dates. Defaults to None.</p> <code>None</code> <code>aspect</code> <code>float</code> <p>Aspect ratio of the plot. Defaults to 2.</p> <code>2</code> <code>cmap</code> <code>str</code> <p>Colormap for the plot. Defaults to \"jet\".</p> <code>'jet'</code> <code>size</code> <code>int</code> <p>Size of the plot. Defaults to 6.</p> <code>6</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to <code>xarray.plot</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.plot.facetgrid.FacetGrid</code> <p>The plot generated from the chlorophyll-a data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>data</code> is not a file path (str) or an xarray DataArray.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def viz_pace_chla(\n    data: Union[str, xr.DataArray],\n    date: Optional[str] = None,\n    aspect: float = 2,\n    cmap: str = \"jet\",\n    size: int = 6,\n    **kwargs: Any,\n) -&gt; xr.plot.facetgrid.FacetGrid:\n    \"\"\"\n    Visualizes PACE chlorophyll-a data using an xarray DataArray.\n\n    This function supports loading data from a file path (str) or directly using an xarray DataArray.\n    It allows for selection of a specific date for visualization or averages over all dates if none is specified.\n\n    Args:\n        data (Union[str, xr.DataArray]): The chlorophyll-a data to visualize. Can be a file path or an xarray DataArray.\n        date (Optional[str], optional): Specific date to visualize. If None, averages over all dates. Defaults to None.\n        aspect (float, optional): Aspect ratio of the plot. Defaults to 2.\n        cmap (str, optional): Colormap for the plot. Defaults to \"jet\".\n        size (int, optional): Size of the plot. Defaults to 6.\n        **kwargs (Any): Additional keyword arguments to pass to `xarray.plot`.\n\n    Returns:\n        xr.plot.facetgrid.FacetGrid: The plot generated from the chlorophyll-a data.\n\n    Raises:\n        ValueError: If `data` is not a file path (str) or an xarray DataArray.\n    \"\"\"\n    if isinstance(data, str):\n        data = read_pace_chla(data)\n    elif not isinstance(data, xr.DataArray):\n        raise ValueError(\"data must be an xarray DataArray\")\n\n    if date is not None:\n        data = data.sel(date=date)\n    else:\n        if \"date\" in data.coords:\n            data = data.mean(dim=\"date\")\n\n    return data.plot(aspect=aspect, cmap=cmap, size=size, **kwargs)\n</code></pre>"},{"location":"prisma/","title":"prisma module","text":""},{"location":"prisma/#hypercoast.prisma.extract_prisma","title":"<code>extract_prisma(dataset, lat, lon, offset=15.0)</code>","text":"<p>Extracts an averaged reflectance spectrum from a PRISMA hyperspectral dataset.</p> <p>A square spatial window is centered at the specified latitude and longitude, and the reflectance values within that window are averaged across the spatial dimensions to produce a single spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xarray.Dataset</code> <p>The PRISMA dataset containing reflectance data, with valid CRS information.</p> required <code>lat</code> <code>float</code> <p>Latitude of the center point.</p> required <code>lon</code> <code>float</code> <p>Longitude of the center points.</p> required <code>offset</code> <code>float</code> <p>Half-size of the square window for extraction, expressed in the dataset's projected coordinate units (e.g., meters). Defaults to 15.0.</p> <code>15.0</code> <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>A 1D array containing the averaged reflectance values across wavelengths. If no matching pixels are found, returns NaN values.</p> Source code in <code>hypercoast/prisma.py</code> <pre><code>def extract_prisma(\n    dataset: xr.Dataset,\n    lat: float,\n    lon: float,\n    offset: float = 15.0,\n) -&gt; xr.DataArray:\n    \"\"\"\n    Extracts an averaged reflectance spectrum from a PRISMA hyperspectral dataset.\n\n    A square spatial window is centered at the specified latitude and longitude,\n    and the reflectance values within that window are averaged across the spatial\n    dimensions to produce a single spectrum.\n\n    Args:\n        dataset (xarray.Dataset): The PRISMA dataset containing reflectance data,\n            with valid CRS information.\n        lat (float): Latitude of the center point.\n        lon (float): Longitude of the center points.\n        offset (float, optional): Half-size of the square window for extraction,\n            expressed in the dataset's projected coordinate units (e.g., meters).\n            Defaults to 15.0.\n\n    Returns:\n        xarray.DataArray: A 1D array containing the averaged reflectance values\n        across wavelengths. If no matching pixels are found, returns NaN values.\n    \"\"\"\n    if dataset.rio.crs is None:\n        raise ValueError(\"Dataset CRS not set. Please provide dataset with CRS info.\")\n\n    crs = dataset.rio.crs.to_string()\n\n    # Convert lat/lon to projected coords\n    x_proj, y_proj = convert_coords([(lat, lon)], \"epsg:4326\", crs)[0]\n\n    da = dataset[\"reflectance\"]\n    x_con = (da[\"x\"] &gt; x_proj - offset) &amp; (da[\"x\"] &lt; x_proj + offset)\n    y_con = (da[\"y\"] &gt; y_proj - offset) &amp; (da[\"y\"] &lt; y_proj + offset)\n\n    try:\n        data = da.where(x_con &amp; y_con, drop=True)\n        data = data.mean(dim=[\"x\", \"y\"], skipna=True)\n    except ValueError:\n        # No matching pixels\n        data = np.full(da.sizes[\"wavelength\"], np.nan)\n\n    return xr.DataArray(\n        data,\n        dims=[\"wavelength\"],\n        coords={\"wavelength\": dataset.coords[\"wavelength\"]},\n    )\n</code></pre>"},{"location":"prisma/#hypercoast.prisma.prisma_to_image","title":"<code>prisma_to_image(dataset, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts a PRISMA hyperspectral dataset to a georeferenced image.</p> <p>If given a file path, the dataset is read using <code>read_prisma</code> and converted into a spatially referenced raster image. Optionally, a subset of wavelengths can be selected, and values are scaled or clipped before writing to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The PRISMA dataset or the path to the dataset file (.he5).</p> required <code>wavelengths</code> <code>np.ndarray</code> <p>Wavelengths to select from the dataset. If None, all wavelengths are included. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for wavelength selection (e.g., \"nearest\"). Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>File path to save the output raster. If None, a raster object will be returned instead of being saved. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>leafmap.array_to_image</code>, including data type (<code>dtype</code>), compression, and colormap.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[rasterio.Dataset]</code> <p>If <code>output</code> is None, returns the in-memory raster object created from the dataset. If <code>output</code> is provided, saves the raster to disk and returns the output file path.</p> Source code in <code>hypercoast/prisma.py</code> <pre><code>def prisma_to_image(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Optional[np.ndarray] = None,\n    method: str = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Converts a PRISMA hyperspectral dataset to a georeferenced image.\n\n    If given a file path, the dataset is read using `read_prisma` and converted\n    into a spatially referenced raster image. Optionally, a subset of wavelengths\n    can be selected, and values are scaled or clipped before writing to an image.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The PRISMA dataset or the path to the\n            dataset file (.he5).\n        wavelengths (np.ndarray, optional): Wavelengths to select from the dataset.\n            If None, all wavelengths are included. Defaults to None.\n        method (str, optional): Method to use for wavelength selection (e.g.,\n            \"nearest\"). Defaults to \"nearest\".\n        output (str, optional): File path to save the output raster. If None, a\n            raster object will be returned instead of being saved. Defaults to None.\n        **kwargs (Any): Additional arguments passed to `leafmap.array_to_image`,\n            including data type (`dtype`), compression, and colormap.\n\n    Returns:\n        Optional[rasterio.Dataset]: If `output` is None, returns the in-memory\n        raster object created from the dataset. If `output` is provided, saves\n        the raster to disk and returns the output file path.\n    \"\"\"\n    crs = dataset.rio.crs\n    transform = dataset.rio.transform()\n\n    if isinstance(dataset, str):\n        dataset = read_prisma(dataset)\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n\n    resolution = (transform.a, transform.e)\n\n    output_array = dataset[\"reflectance\"].values\n    if not np.any(np.isfinite(output_array)):\n        print(\"Warning: All reflectance values are NaN. Output image will be blank.\")\n        return None\n\n    output_dtype = kwargs.get(\"dtype\", np.float32)\n\n    vmin, vmax = np.nanpercentile(output_array, (2, 98))\n    output_array = np.clip(output_array, vmin, vmax)\n\n    if output_dtype == np.uint8:\n        output_array = ((output_array - vmin) / (vmax - vmin) * 255).astype(np.uint8)\n    else:\n        kwargs[\"dtype\"] = output_dtype\n\n    result = array_to_image(\n        output_array,\n        output=output,\n        transpose=False,\n        crs=crs,\n        transform=transform,\n        cellsize=resolution,\n        **kwargs,\n    )\n\n    return output if output is not None else result\n</code></pre>"},{"location":"prisma/#hypercoast.prisma.read_prisma","title":"<code>read_prisma(filepath, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Reads PRISMA hyperspectral Level-2 .he5 data and returns an xarray dataset with reflectance values, associated wavelengths, and geospatial metadata.</p> <p>This function loads both VNIR and SWIR spectral cubes, scales the raw integer values to physical reflectance units, merges them into a single spectral dataset, sorts wavelengths in ascending order, and assigns spatial coordinates derived from product corner coordinates. It also writes the CRS and affine transform to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the PRISMA .he5 file.</p> required <code>wavelengths</code> <code>List[float]</code> <p>Wavelengths to select. If None, all available wavelengths are included.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method used when selecting wavelengths (e.g., \"nearest\"). Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to wavelength selection.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[xr.Dataset, str, Affine, Tuple[float, float]]</code> <ul> <li>An xarray.Dataset containing reflectance data with coordinates.<ul> <li>The CRS as an EPSG string.</li> <li>The affine transform describing spatial referencing.</li> <li>The cell size as a tuple (x_res, y_res).</li> </ul> </li> </ul> Source code in <code>hypercoast/prisma.py</code> <pre><code>def read_prisma(\n    filepath: str,\n    wavelengths: Optional[List[float]] = None,\n    method: str = \"nearest\",\n    **kwargs: Any,\n) -&gt; Tuple[xr.Dataset, str, Affine, Tuple[float, float]]:\n    \"\"\"\n    Reads PRISMA hyperspectral Level-2 .he5 data and returns an xarray dataset with\n    reflectance values, associated wavelengths, and geospatial metadata.\n\n    This function loads both VNIR and SWIR spectral cubes, scales the raw integer\n    values to physical reflectance units, merges them into a single spectral\n    dataset, sorts wavelengths in ascending order, and assigns spatial coordinates\n    derived from product corner coordinates. It also writes the CRS and affine\n    transform to the dataset.\n\n    Args:\n        filepath (str): Path to the PRISMA .he5 file.\n        wavelengths (List[float], optional): Wavelengths to select. If None,\n            all available wavelengths are included.\n        method (str, optional): Method used when selecting wavelengths (e.g.,\n            \"nearest\"). Defaults to \"nearest\".\n        **kwargs (Any): Additional keyword arguments passed to wavelength\n            selection.\n\n    Returns:\n        Tuple[xr.Dataset, str, Affine, Tuple[float, float]]:\n            - An xarray.Dataset containing reflectance data with coordinates.\n            - The CRS as an EPSG string.\n            - The affine transform describing spatial referencing.\n            - The cell size as a tuple (x_res, y_res).\n    \"\"\"\n    with h5py.File(filepath, \"r\") as f:\n        vnir_cube_path = \"HDFEOS/SWATHS/PRS_L2D_HCO/Data Fields/VNIR_Cube\"\n        swir_cube_path = \"HDFEOS/SWATHS/PRS_L2D_HCO/Data Fields/SWIR_Cube\"\n        vnir_cube_data = f[vnir_cube_path][()]\n        swir_cube_data = f[swir_cube_path][()]\n        vnir_wavelengths = f.attrs[\"List_Cw_Vnir\"][()]\n        swir_wavelengths = f.attrs[\"List_Cw_Swir\"][()]\n        l2_scale_vnir_min = f.attrs[\"L2ScaleVnirMin\"][()]\n        l2_scale_vnir_max = f.attrs[\"L2ScaleVnirMax\"][()]\n        l2_scale_swir_min = f.attrs[\"L2ScaleSwirMin\"][()]\n        l2_scale_swir_max = f.attrs[\"L2ScaleSwirMax\"][()]\n        epsg_code = f.attrs[\"Epsg_Code\"][()]\n        ul_easting = f.attrs[\"Product_ULcorner_easting\"][()]\n        ul_northing = f.attrs[\"Product_ULcorner_northing\"][()]\n        lr_easting = f.attrs[\"Product_LRcorner_easting\"][()]\n        lr_northing = f.attrs[\"Product_LRcorner_northing\"][()]\n\n    fill_value = -9999\n    max_data_value = 65535\n\n    vnir_cube_data = l2_scale_vnir_min + (\n        vnir_cube_data.astype(np.float32) / max_data_value\n    ) * (l2_scale_vnir_max - l2_scale_vnir_min)\n    swir_cube_data = l2_scale_swir_min + (\n        swir_cube_data.astype(np.float32) / max_data_value\n    ) * (l2_scale_swir_max - l2_scale_swir_min)\n\n    vnir_cube_data[vnir_cube_data == fill_value] = np.nan\n    swir_cube_data[swir_cube_data == fill_value] = np.nan\n\n    combined_reflectance = np.concatenate((vnir_cube_data, swir_cube_data), axis=1)\n    combined_wavelengths = np.concatenate((vnir_wavelengths, swir_wavelengths))\n\n    valid_indices = combined_wavelengths &gt; 0\n    combined_wavelengths = combined_wavelengths[valid_indices]\n    combined_reflectance = combined_reflectance[:, valid_indices, :]\n    sort_indices = np.argsort(combined_wavelengths)\n    combined_wavelengths = combined_wavelengths[sort_indices]\n    combined_reflectance = combined_reflectance[:, sort_indices, :]\n\n    rows = combined_reflectance.shape[0]\n    cols = combined_reflectance.shape[2]\n    x_res = (lr_easting - ul_easting) / cols\n    y_res = (lr_northing - ul_northing) / rows\n\n    transform = Affine.translation(ul_easting, ul_northing) * Affine.scale(x_res, y_res)\n    x_coords = np.array([transform * (i, 0) for i in range(cols)])[:, 0]\n    y_coords = np.array([transform * (0, j) for j in range(rows)])[:, 1]\n\n    ds = xr.Dataset(\n        data_vars=dict(\n            reflectance=(\n                [\"y\", \"wavelength\", \"x\"],\n                combined_reflectance,\n                dict(\n                    units=\"unitless\",\n                    _FillValue=np.nan,\n                    standard_name=\"reflectance\",\n                    long_name=\"Combined atmospherically corrected surface reflectance\",\n                ),\n            ),\n        ),\n        coords=dict(\n            wavelength=(\n                [\"wavelength\"],\n                combined_wavelengths,\n                dict(long_name=\"center wavelength\", units=\"nm\"),\n            ),\n            y=([\"y\"], y_coords, dict(units=\"m\")),\n            x=([\"x\"], x_coords, dict(units=\"m\")),\n        ),\n    )\n\n    ds[\"reflectance\"] = ds.reflectance.transpose(\"y\", \"x\", \"wavelength\")\n\n    transform = Affine.translation(ul_easting, ul_northing) * Affine.scale(x_res, y_res)\n    crs = f\"EPSG:{epsg_code}\"\n    if crs is None:\n        raise ValueError(\n            \"Dataset has no CRS. Please ensure read_prisma writes CRS before returning.\"\n        )\n    ds.rio.write_crs(crs, inplace=True)\n    ds.rio.write_transform(transform, inplace=True)\n\n    global_atts = ds.attrs\n    global_atts[\"Conventions\"] = \"CF-1.6\"\n    ds.attrs = dict(\n        units=\"unitless\",\n        _FillValue=-9999,\n        grid_mapping=\"crs\",\n        standard_name=\"reflectance\",\n        long_name=\"atmospherically corrected surface reflectance\",\n        crs=ds.rio.crs.to_string(),\n    )\n    ds.attrs.update(global_atts)\n\n    return ds\n</code></pre>"},{"location":"tanager/","title":"tanager module","text":""},{"location":"tanager/#hypercoast.tanager.extract_tanager","title":"<code>extract_tanager(dataset, latitude, longitude, delta=0.01, return_plot=False, **kwargs)</code>","text":"<p>Extracts data from a PACE dataset for a given latitude and longitude range     and calculates the mean over these dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The PACE dataset or path to the dataset file.</p> required <code>latitude</code> <code>Union[float, Tuple[float, float]]</code> <p>The latitude or range of latitudes to extract data for.</p> required <code>longitude</code> <code>Union[float, Tuple[float, float]]</code> <p>The longitude or range of longitudes to extract data for.</p> required <code>delta</code> <code>float</code> <p>The range to add/subtract to the latitude and longitude if they are not ranges. Defaults to 0.01.</p> <code>0.01</code> <code>return_plot</code> <code>bool</code> <p>Whether to return a plot of the data. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the plot function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[xr.DataArray, plt.figure.Figure]</code> <p>The mean data over the latitude     and longitude dimensions, or a plot of this data if return_plot is True.</p> Source code in <code>hypercoast/tanager.py</code> <pre><code>def extract_tanager(\n    dataset: Union[xr.Dataset, str],\n    latitude: Union[float, Tuple[float, float]],\n    longitude: Union[float, Tuple[float, float]],\n    delta: float = 0.01,\n    return_plot: bool = False,\n    **kwargs,\n) -&gt; Union[xr.DataArray, plt.Figure]:\n    \"\"\"\n    Extracts data from a PACE dataset for a given latitude and longitude range\n        and calculates the mean over these dimensions.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The PACE dataset or path to the dataset file.\n        latitude (Union[float, Tuple[float, float]]): The latitude or range of\n            latitudes to extract data for.\n        longitude (Union[float, Tuple[float, float]]): The longitude or range of\n            longitudes to extract data for.\n        delta (float, optional): The range to add/subtract to the latitude and\n            longitude if they are not ranges. Defaults to 0.01.\n        return_plot (bool, optional): Whether to return a plot of the data. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the plot function.\n\n    Returns:\n        Union[xr.DataArray, plt.figure.Figure]: The mean data over the latitude\n            and longitude dimensions, or a plot of this data if return_plot is True.\n    \"\"\"\n    if isinstance(latitude, list) or isinstance(latitude, tuple):\n        pass\n    else:\n        latitude = (latitude - delta, latitude + delta)\n\n    if isinstance(longitude, list) or isinstance(longitude, tuple):\n        pass\n    else:\n        longitude = (longitude - delta, longitude + delta)\n\n    ds = filter_tanager(dataset, latitude, longitude, return_plot=False)\n    data = ds.mean(dim=[\"y\", \"x\"])\n    if return_plot:\n        return data.plot.line(**kwargs)\n    else:\n        return data\n</code></pre>"},{"location":"tanager/#hypercoast.tanager.filter_tanager","title":"<code>filter_tanager(dataset, latitude, longitude, drop=True, return_plot=False, **kwargs)</code>","text":"<p>Filters a Tanager dataset based on latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The Tanager dataset to filter.</p> required <code>latitude</code> <code>float or tuple</code> <p>The latitude to filter by. If a tuple or list, it represents a range.</p> required <code>longitude</code> <code>float or tuple</code> <p>The longitude to filter by. If a tuple or list, it represents a range.</p> required <code>drop</code> <code>bool</code> <p>Whether to drop the filtered out data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The filtered Tanager data.</p> Source code in <code>hypercoast/tanager.py</code> <pre><code>def filter_tanager(\n    dataset, latitude, longitude, drop=True, return_plot=False, **kwargs\n):\n    \"\"\"\n    Filters a Tanager dataset based on latitude and longitude.\n\n    Args:\n        dataset (xr.Dataset): The Tanager dataset to filter.\n        latitude (float or tuple): The latitude to filter by. If a tuple or list, it represents a range.\n        longitude (float or tuple): The longitude to filter by. If a tuple or list, it represents a range.\n        drop (bool, optional): Whether to drop the filtered out data. Defaults to True.\n\n    Returns:\n        xr.DataArray: The filtered Tanager data.\n    \"\"\"\n    if isinstance(latitude, list) or isinstance(latitude, tuple):\n        lat_con = (dataset[\"latitude\"] &gt; latitude[0]) &amp; (\n            dataset[\"latitude\"] &lt; latitude[1]\n        )\n    else:\n        lat_con = dataset[\"latitude\"] == latitude\n\n    if isinstance(longitude, list) or isinstance(longitude, tuple):\n        lon_con = (dataset[\"longitude\"] &gt; longitude[0]) &amp; (\n            dataset[\"longitude\"] &lt; longitude[1]\n        )\n    else:\n        lon_con = dataset[\"longitude\"] == longitude\n\n    da = dataset[\"toa_radiance\"].where(lat_con &amp; lon_con, drop=drop, **kwargs)\n    da_filtered = da.dropna(dim=\"y\", how=\"all\")\n    da_filtered = da_filtered.dropna(dim=\"x\", how=\"all\")\n\n    if return_plot:\n        rrs_stack = da_filtered.stack(\n            {\"pixel\": [\"y\", \"x\"]},\n            create_index=False,\n        )\n        rrs_stack.plot.line(hue=\"pixel\")\n    else:\n        return da_filtered\n</code></pre>"},{"location":"tanager/#hypercoast.tanager.grid_tanager","title":"<code>grid_tanager(dataset, bands=None, wavelengths=None, method='nearest', row_range=None, col_range=None, **kwargs)</code>","text":"<p>Grids a Tanager dataset based on latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The Tanager dataset to grid.</p> required <code>bands</code> <code>list</code> <p>The band indices to select. Defaults to None.</p> <code>None</code> <code>wavelengths</code> <code>list</code> <p>The wavelength values to select. Takes priority over bands. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for griddata interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>row_range</code> <code>tuple</code> <p>Row range (start_row, end_row) to subset the data. Defaults to None.</p> <code>None</code> <code>col_range</code> <code>tuple</code> <p>Column range (start_col, end_col) to subset the data. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the xr.Dataset constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The gridded Tanager data.</p> Source code in <code>hypercoast/tanager.py</code> <pre><code>def grid_tanager(\n    dataset,\n    bands=None,\n    wavelengths=None,\n    method=\"nearest\",\n    row_range=None,\n    col_range=None,\n    **kwargs,\n):\n    \"\"\"\n    Grids a Tanager dataset based on latitude and longitude.\n\n    Args:\n        dataset (xr.Dataset): The Tanager dataset to grid.\n        bands (list, optional): The band indices to select. Defaults to None.\n        wavelengths (list, optional): The wavelength values to select. Takes priority over bands. Defaults to None.\n        method (str, optional): The method to use for griddata interpolation.\n            Defaults to \"nearest\".\n        row_range (tuple, optional): Row range (start_row, end_row) to subset the data. Defaults to None.\n        col_range (tuple, optional): Column range (start_col, end_col) to subset the data. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the xr.Dataset constructor.\n\n    Returns:\n        xr.DataArray: The gridded Tanager data.\n    \"\"\"\n    from scipy.interpolate import griddata\n    from scipy.spatial import ConvexHull\n\n    # Priority: wavelengths &gt; bands &gt; default\n    if wavelengths is not None:\n        # Use wavelengths directly\n        if not isinstance(wavelengths, list):\n            wavelengths = [wavelengths]\n        selected_wavelengths = wavelengths\n    elif bands is not None:\n        # Convert bands to wavelengths\n        if not isinstance(bands, list):\n            bands = [bands]\n\n        selected_wavelengths = []\n        for band in bands:\n            if isinstance(band, (int, np.integer)) or (\n                isinstance(band, float) and band &lt; 500\n            ):\n                # Treat as band index\n                selected_wavelengths.append(\n                    dataset.coords[\"wavelength\"].values[int(band)]\n                )\n            else:\n                # Treat as wavelength value\n                selected_wavelengths.append(band)\n    else:\n        # Default to first wavelength\n        selected_wavelengths = dataset.coords[\"wavelength\"].values\n\n    # Apply spatial subset filtering if ranges are provided\n    if row_range is not None or col_range is not None:\n        # Get original array dimensions\n        y_size, x_size = dataset.latitude.shape\n\n        # Determine row and column indices\n        start_row = row_range[0] if row_range is not None else 0\n        end_row = row_range[1] if row_range is not None else y_size\n        start_col = col_range[0] if col_range is not None else 0\n        end_col = col_range[1] if col_range is not None else x_size\n\n        # Ensure indices are within bounds\n        start_row = max(0, min(start_row, y_size))\n        end_row = max(start_row, min(end_row, y_size))\n        start_col = max(0, min(start_col, x_size))\n        end_col = max(start_col, min(end_col, x_size))\n\n        # Subset the dataset using isel for y and x dimensions\n        dataset_subset = dataset.isel(\n            y=slice(start_row, end_row), x=slice(start_col, end_col)\n        )\n\n        # For subsets, return the data directly without interpolation to avoid artifacts\n        selected_data_list = []\n        for wl in selected_wavelengths:\n            data = dataset_subset.sel(wavelength=wl, method=\"nearest\")[\"toa_radiance\"]\n            selected_data_list.append(data.values)\n\n        # Stack wavelengths as the last dimension\n        gridded_data_3d = np.stack(selected_data_list, axis=-1)\n\n        # Create output dataset with proper coordinates\n        lat_subset = dataset_subset.latitude\n        lon_subset = dataset_subset.longitude\n\n        # Create coordinate arrays for the subset\n        y_coords = np.arange(gridded_data_3d.shape[0])\n        x_coords = np.arange(gridded_data_3d.shape[1])\n\n        dataset2 = xr.Dataset(\n            {\"toa_radiance\": ((\"y\", \"x\", \"wavelength\"), gridded_data_3d)},\n            coords={\n                \"y\": (\"y\", y_coords),\n                \"x\": (\"x\", x_coords),\n                \"wavelength\": (\"wavelength\", selected_wavelengths),\n                \"latitude\": ((\"y\", \"x\"), lat_subset.values),\n                \"longitude\": ((\"y\", \"x\"), lon_subset.values),\n            },\n            **kwargs,\n        )\n\n        dataset2[\"toa_radiance\"].rio.write_crs(\"EPSG:4326\", inplace=True)\n        return dataset2\n\n    lat = dataset.latitude\n    lon = dataset.longitude\n\n    # Find valid data points for any wavelength to define spatial mask\n    first_wavelength_data = dataset.sel(\n        wavelength=selected_wavelengths[0], method=\"nearest\"\n    )[\"toa_radiance\"]\n    overall_valid_mask = ~np.isnan(first_wavelength_data.data) &amp; (\n        first_wavelength_data.data &gt; 0\n    )\n\n    if not np.any(overall_valid_mask):\n        # No valid data, return empty grid using valid lat/lon bounds\n        valid_lat_data = lat.data[~np.isnan(lat.data)]\n        valid_lon_data = lon.data[~np.isnan(lon.data)]\n\n        if len(valid_lat_data) == 0 or len(valid_lon_data) == 0:\n            # Fallback to original bounds if no valid subset data\n            grid_lat = np.linspace(lat.min().values, lat.max().values, lat.shape[0])\n            grid_lon = np.linspace(lon.min().values, lon.max().values, lon.shape[1])\n        else:\n            grid_lat = np.linspace(\n                valid_lat_data.min(), valid_lat_data.max(), lat.shape[0]\n            )\n            grid_lon = np.linspace(\n                valid_lon_data.min(), valid_lon_data.max(), lon.shape[1]\n            )\n\n        grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n        gridded_data_dict = {\n            wl: np.full_like(grid_lat_2d, np.nan) for wl in selected_wavelengths\n        }\n    else:\n        # Get valid coordinates for spatial masking\n        valid_lat = lat.data[overall_valid_mask]\n        valid_lon = lon.data[overall_valid_mask]\n\n        # Create grid based on valid data bounds (considering subset if applied)\n        grid_lat = np.linspace(valid_lat.min(), valid_lat.max(), lat.shape[0])\n        grid_lon = np.linspace(valid_lon.min(), valid_lon.max(), lon.shape[1])\n        grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n\n        # For subsets, use simple bounding box instead of convex hull to avoid over-masking\n        if row_range is not None or col_range is not None:\n            # For subsets, just use bounding box\n            inside_hull = (\n                (grid_lat_2d &gt;= valid_lat.min())\n                &amp; (grid_lat_2d &lt;= valid_lat.max())\n                &amp; (grid_lon_2d &gt;= valid_lon.min())\n                &amp; (grid_lon_2d &lt;= valid_lon.max())\n            )\n        else:\n            # For full dataset, use convex hull for better edge handling\n            try:\n                hull = ConvexHull(np.column_stack([valid_lon, valid_lat]))\n                from matplotlib.path import Path\n\n                hull_path = Path(\n                    np.column_stack(\n                        [valid_lon[hull.vertices], valid_lat[hull.vertices]]\n                    )\n                )\n                grid_points = np.column_stack(\n                    [grid_lon_2d.flatten(), grid_lat_2d.flatten()]\n                )\n                inside_hull = hull_path.contains_points(grid_points).reshape(\n                    grid_lat_2d.shape\n                )\n            except:\n                # Fallback: use simple bounding box\n                inside_hull = (\n                    (grid_lat_2d &gt;= valid_lat.min())\n                    &amp; (grid_lat_2d &lt;= valid_lat.max())\n                    &amp; (grid_lon_2d &gt;= valid_lon.min())\n                    &amp; (grid_lon_2d &lt;= valid_lon.max())\n                )\n\n        gridded_data_dict = {}\n        for wl in selected_wavelengths:\n            data = dataset.sel(wavelength=wl, method=\"nearest\")[\"toa_radiance\"]\n\n            # Mask nodata values (both NaN and zero values)\n            data_flat = data.data.flatten()\n            valid_mask = ~np.isnan(data_flat) &amp; (data_flat &gt; 0)\n\n            if not np.any(valid_mask):\n                gridded_data = np.full_like(grid_lat_2d, np.nan)\n            else:\n                gridded_data = griddata(\n                    (lat.data.flatten()[valid_mask], lon.data.flatten()[valid_mask]),\n                    data_flat[valid_mask],\n                    (grid_lat_2d, grid_lon_2d),\n                    method=method,\n                    fill_value=np.nan,\n                )\n                # Apply spatial mask to prevent edge interpolation (only for full dataset)\n                if row_range is None and col_range is None:\n                    gridded_data[~inside_hull] = np.nan\n            gridded_data_dict[wl] = gridded_data\n\n    selected_wavelengths = list(gridded_data_dict.keys())\n    # Create a 3D array with dimensions latitude, longitude, and wavelength\n    gridded_data_3d = np.dstack(list(gridded_data_dict.values()))\n\n    dataset2 = xr.Dataset(\n        {\"toa_radiance\": ((\"latitude\", \"longitude\", \"wavelength\"), gridded_data_3d)},\n        coords={\n            \"latitude\": (\"latitude\", grid_lat),\n            \"longitude\": (\"longitude\", grid_lon),\n            \"wavelength\": (\"wavelength\", selected_wavelengths),\n        },\n        **kwargs,\n    )\n\n    dataset2[\"toa_radiance\"].rio.write_crs(\"EPSG:4326\", inplace=True)\n\n    return dataset2\n</code></pre>"},{"location":"tanager/#hypercoast.tanager.read_tanager","title":"<code>read_tanager(filepath, bands=None, stac_url=None, **kwargs)</code>","text":"<p>Read Planet Tanager HDF5 hyperspectral data and return an xarray.Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str or Path</code> <p>Local file path or HTTPS URL to the .h5 file.</p> required <code>bands</code> <code>list or slice</code> <p>Indices of spectral bands to read.</p> <code>None</code> <code>stac_url</code> <code>str</code> <p>STAC item URL containing wavelength metadata.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments (reserved for future use).</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>Dataset with georeferenced radiance and band metadata.</p> Source code in <code>hypercoast/tanager.py</code> <pre><code>def read_tanager(filepath, bands=None, stac_url=None, **kwargs):\n    \"\"\"\n    Read Planet Tanager HDF5 hyperspectral data and return an xarray.Dataset.\n\n    Parameters:\n        filepath (str or Path): Local file path or HTTPS URL to the .h5 file.\n        bands (list or slice, optional): Indices of spectral bands to read.\n        stac_url (str, optional): STAC item URL containing wavelength metadata.\n        **kwargs: Additional arguments (reserved for future use).\n\n    Returns:\n        xr.Dataset: Dataset with georeferenced radiance and band metadata.\n    \"\"\"\n    if isinstance(filepath, str) and filepath.startswith(\"https://\"):\n        filepath = download_file(filepath)  # You must define this if needed\n\n    if stac_url is None:\n        # Example static fallback STAC URL; update as needed\n        stac_url = (\n            \"https://www.planet.com/data/stac/tanager-core-imagery/coastal-water-bodies/\"\n            \"20250514_193937_64_4001/20250514_193937_64_4001.json\"\n        )\n\n    # Parse STAC metadata\n    stac_item = requests.get(stac_url, timeout=10).json()\n    bands_meta = stac_item[\"assets\"][\"basic_radiance_hdf5\"][\"eo:bands\"]\n\n    wavelengths = np.array([b[\"center_wavelength\"] * 1000 for b in bands_meta])\n    fwhm = np.array([b.get(\"full_width_half_max\", np.nan) * 1000 for b in bands_meta])\n\n    if bands is not None:\n        wavelengths = wavelengths[bands]\n        fwhm = fwhm[bands]\n\n    with h5py.File(filepath, \"r\") as f:\n        data = f[\"HDFEOS/SWATHS/HYP/Data Fields/toa_radiance\"][()]\n        lat = f[\"HDFEOS/SWATHS/HYP/Geolocation Fields/Latitude\"][()]\n        lon = f[\"HDFEOS/SWATHS/HYP/Geolocation Fields/Longitude\"][()]\n\n    if bands is not None:\n        data = data[bands]\n\n    coords = {\n        \"wavelength\": wavelengths,\n        \"fwhm\": (\"wavelength\", fwhm),\n        \"latitude\": ((\"y\", \"x\"), lat),\n        \"longitude\": ((\"y\", \"x\"), lon),\n    }\n\n    da = xr.DataArray(\n        data, dims=(\"wavelength\", \"y\", \"x\"), coords=coords, name=\"toa_radiance\"\n    )\n\n    ds = xr.Dataset(\n        data_vars={\"toa_radiance\": da},\n        coords={\n            \"wavelength\": da.wavelength,\n            \"fwhm\": (\"wavelength\", fwhm),\n            \"latitude\": ((\"y\", \"x\"), lat),\n            \"longitude\": ((\"y\", \"x\"), lon),\n        },\n        attrs={\n            \"source\": \"Planet Tanager HDF5\",\n            \"stac_item\": stac_url,\n        },\n        **kwargs,\n    )\n\n    return ds\n</code></pre>"},{"location":"tanager/#hypercoast.tanager.tanager_to_image","title":"<code>tanager_to_image(dataset, bands=None, wavelengths=None, method='nearest', row_range=None, col_range=None, output=None, **kwargs)</code>","text":"<p>Converts an Tanager dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xarray.Dataset or str</code> <p>The dataset containing the EMIT data or the file path to the dataset.</p> required <code>bands</code> <code>array-like</code> <p>The specific band indices to select. Defaults to None.</p> <code>None</code> <code>wavelengths</code> <code>array-like</code> <p>The specific wavelength values to select. Takes priority over bands. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>row_range</code> <code>tuple</code> <p>Row range (start_row, end_row) to subset the data. Defaults to None.</p> <code>None</code> <code>col_range</code> <code>tuple</code> <p>Column range (start_col, end_col) to subset the data. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the dataset. If <code>output</code> is provided, the image will be saved to the specified file and the function will return None.</p> Source code in <code>hypercoast/tanager.py</code> <pre><code>def tanager_to_image(\n    dataset,\n    bands=None,\n    wavelengths=None,\n    method=\"nearest\",\n    row_range=None,\n    col_range=None,\n    output=None,\n    **kwargs,\n):\n    \"\"\"\n    Converts an Tanager dataset to an image.\n\n    Args:\n        dataset (xarray.Dataset or str): The dataset containing the EMIT data or the file path to the dataset.\n        bands (array-like, optional): The specific band indices to select. Defaults to None.\n        wavelengths (array-like, optional): The specific wavelength values to select. Takes priority over bands. Defaults to None.\n        method (str, optional): The method to use for data interpolation. Defaults to \"nearest\".\n        row_range (tuple, optional): Row range (start_row, end_row) to subset the data. Defaults to None.\n        col_range (tuple, optional): Column range (start_col, end_col) to subset the data. Defaults to None.\n        output (str, optional): The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the dataset. If `output` is provided, the image will be saved to the specified file and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_tanager(dataset, bands=bands)\n\n    grid = grid_tanager(\n        dataset,\n        bands=bands,\n        wavelengths=wavelengths,\n        method=method,\n        row_range=row_range,\n        col_range=col_range,\n    )\n\n    data = grid[\"toa_radiance\"]\n    data.rio.write_crs(\"EPSG:4326\", inplace=True)\n\n    return array_to_image(data, transpose=False, output=output, **kwargs)\n</code></pre>"},{"location":"ui/","title":"ui module","text":"<p>This module contains the user interface for the hypercoast package.</p>"},{"location":"ui/#hypercoast.ui.SpectralWidget","title":"<code> SpectralWidget            (HBox)         </code>","text":"<p>A widget for spectral data visualization on a map.</p> <p>Attributes:</p> Name Type Description <code>_host_map</code> <code>Map</code> <p>The map to host the widget.</p> <code>on_close</code> <code>function</code> <p>Function to be called when the widget is closed.</p> <code>_output_widget</code> <code>widgets.Output</code> <p>The output widget to display results.</p> <code>_output_control</code> <code>ipyleaflet.WidgetControl</code> <p>The control for the output widget.</p> <code>_on_map_interaction</code> <code>function</code> <p>Function to handle map interactions.</p> <code>_spectral_widget</code> <code>SpectralWidget</code> <p>The spectral widget itself.</p> <code>_spectral_control</code> <code>ipyleaflet.WidgetControl</code> <p>The control for the spectral widget.</p> Source code in <code>hypercoast/ui.py</code> <pre><code>class SpectralWidget(widgets.HBox):\n    \"\"\"\n    A widget for spectral data visualization on a map.\n\n    Attributes:\n        _host_map (Map): The map to host the widget.\n        on_close (function): Function to be called when the widget is closed.\n        _output_widget (widgets.Output): The output widget to display results.\n        _output_control (ipyleaflet.WidgetControl): The control for the output widget.\n        _on_map_interaction (function): Function to handle map interactions.\n        _spectral_widget (SpectralWidget): The spectral widget itself.\n        _spectral_control (ipyleaflet.WidgetControl): The control for the spectral widget.\n    \"\"\"\n\n    def __init__(\n        self, host_map, stack=True, position=\"topright\", xlim=None, ylim=None, **kwargs\n    ):\n        \"\"\"\n        Initializes a new instance of the SpectralWidget class.\n\n        Args:\n            host_map (Map): The map to host the widget.\n            stack (bool, optional): Whether to stack the plots. Defaults to True.\n            position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n            xlim (tuple, optional): The x-axis limits. Defaults to None.\n            ylim (tuple, optional): The y-axis limits. Defaults to None.\n        \"\"\"\n        self._host_map = host_map\n        self.on_close = None\n        self._stack = stack\n        self._show_plot = False\n\n        fig_margin = {\"top\": 20, \"bottom\": 35, \"left\": 50, \"right\": 20}\n        fig = plt.figure(\n            # title=None,\n            fig_margin=fig_margin,\n            layout={\"width\": \"500px\", \"height\": \"300px\"},\n        )\n\n        self._fig = fig\n        self._host_map._fig = fig\n\n        layer_names = list(host_map.cog_layer_dict.keys())\n        layers_widget = widgets.Dropdown(options=layer_names)\n        layers_widget.layout.width = \"18ex\"\n\n        close_btn = widgets.Button(\n            icon=\"times\",\n            tooltip=\"Close the widget\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        reset_btn = widgets.Button(\n            icon=\"trash\",\n            tooltip=\"Remove all markers\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        settings_btn = widgets.Button(\n            icon=\"gear\",\n            tooltip=\"Change layer settings\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        stack_btn = widgets.ToggleButton(\n            value=stack,\n            icon=\"area-chart\",\n            tooltip=\"Stack spectral signatures\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        def settings_btn_click(_):\n\n            self._host_map._add_layer_editor(\n                position=\"topright\",\n                layer_dict=self._host_map.cog_layer_dict[layers_widget.value],\n            )\n\n        settings_btn.on_click(settings_btn_click)\n\n        def reset_btn_click(_):\n            if hasattr(self._host_map, \"_plot_marker_cluster\"):\n                self._host_map._plot_marker_cluster.markers = []\n                self._host_map._plot_markers = []\n\n            if hasattr(self._host_map, \"_spectral_data\"):\n                self._host_map._spectral_data = {}\n\n            self._output_widget.clear_output()\n            self._show_plot = False\n            plt.clear()\n\n        reset_btn.on_click(reset_btn_click)\n\n        save_btn = widgets.Button(\n            icon=\"floppy-o\",\n            tooltip=\"Save the data to a CSV\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        def chooser_callback(chooser):\n            if chooser.selected:\n                file_path = chooser.selected\n                self._host_map.spectral_to_csv(file_path)\n                if (\n                    hasattr(self._host_map, \"_file_chooser_control\")\n                    and self._host_map._file_chooser_control in self._host_map.controls\n                ):\n                    self._host_map.remove_control(self._host_map._file_chooser_control)\n                    self._host_map._file_chooser.close()\n\n        def save_btn_click(_):\n            if not hasattr(self._host_map, \"_spectral_data\"):\n                return\n\n            self._output_widget.clear_output()\n            file_chooser = FileChooser(\n                os.getcwd(), layout=widgets.Layout(width=\"454px\")\n            )\n            file_chooser.filter_pattern = \"*.csv\"\n            file_chooser.use_dir_icons = True\n            file_chooser.title = \"Save spectral data to a CSV file\"\n            file_chooser.default_filename = \"spectral_data.csv\"\n            file_chooser.show_hidden = False\n            file_chooser.register_callback(chooser_callback)\n            file_chooser_control = ipyleaflet.WidgetControl(\n                widget=file_chooser, position=\"topright\"\n            )\n            self._host_map.add(file_chooser_control)\n            setattr(self._host_map, \"_file_chooser\", file_chooser)\n            setattr(self._host_map, \"_file_chooser_control\", file_chooser_control)\n\n        save_btn.on_click(save_btn_click)\n\n        def close_widget(_):\n            self.cleanup()\n\n        close_btn.on_click(close_widget)\n\n        super().__init__(\n            [layers_widget, settings_btn, stack_btn, reset_btn, save_btn, close_btn]\n        )\n\n        output = widgets.Output()\n        output_control = ipyleaflet.WidgetControl(widget=output, position=\"bottomright\")\n        self._output_widget = output\n        self._output_control = output_control\n        self._host_map.add(output_control)\n\n        if not hasattr(self._host_map, \"_spectral_data\"):\n            self._host_map._spectral_data = {}\n\n        def handle_interaction(**kwargs):\n\n            latlon = kwargs.get(\"coordinates\")\n            lat = latlon[0]\n            lon = latlon[1]\n            if kwargs.get(\"type\") == \"click\" and self._host_map._layer_editor is None:\n                layer_name = layers_widget.value\n\n                if not hasattr(self._host_map, \"_plot_markers\"):\n                    self._host_map._plot_markers = []\n                markers = self._host_map._plot_markers\n                marker_cluster = self._host_map._plot_marker_cluster\n                markers.append(ipyleaflet.Marker(location=latlon, draggable=False))\n                marker_cluster.markers = markers\n                self._host_map._plot_marker_cluster = marker_cluster\n\n                xlabel = \"Wavelength (nm)\"\n                ylabel = \"Reflectance\"\n\n                ds = self._host_map.cog_layer_dict[layer_name][\"xds\"]\n\n                if self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"XARRAY\":\n                    da = extract_spectral(ds, lat, lon)\n                    xlabel = \"Band\"\n                    ylabel = \"Value\"\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"EMIT\":\n                    da = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")[\n                        \"reflectance\"\n                    ]\n\n                    if \"wavelength\" not in self._host_map._spectral_data:\n                        self._host_map._spectral_data[\"wavelength\"] = ds[\n                            \"wavelength\"\n                        ].values\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"TANAGER\":\n                    da = extract_tanager(ds, lat, lon)\n                    ylabel = \"TOA Radiance\"\n\n                    if \"wavelength\" not in self._host_map._spectral_data:\n                        self._host_map._spectral_data[\"wavelength\"] = ds[\n                            \"wavelength\"\n                        ].values\n\n                    # da = da.swap_dims({\"band\": \"wavelength\"})\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"PACE\":\n                    try:\n                        da = extract_pace(ds, lat, lon)\n                    except:\n                        da = xr.DataArray(\n                            np.full(len(ds[\"wavelength\"]), np.nan),\n                            dims=[\"wavelength\"],\n                            coords={\"wavelength\": ds[\"wavelength\"]},\n                        )\n                    if \"wavelengths\" not in self._host_map._spectral_data:\n                        self._host_map._spectral_data[\"wavelengths\"] = ds[\n                            \"wavelength\"\n                        ].values\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"DESIS\":\n                    da = extract_desis(ds, lat, lon)\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"NEON\":\n                    da = extract_neon(ds, lat, lon)\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"AVIRIS\":\n                    da = extract_aviris(ds, lat, lon)\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"PRISMA\":\n                    da = extract_prisma(ds, lat, lon)\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"EnMAP\":\n                    da = extract_enmap(ds, lat, lon)\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"WYVERN\":\n                    da = extract_wyvern(ds, lat, lon)\n\n                self._host_map._spectral_data[f\"({lat:.4f} {lon:.4f})\"] = da.values\n\n                if self._host_map.cog_layer_dict[layer_name][\"hyper\"] != \"XARRAY\":\n                    da[da &lt; 0] = np.nan\n                    x_axis_options = {\"label_offset\": \"30px\"}\n                else:\n                    x_axis_options = {\n                        \"label_offset\": \"30px\",\n                        \"tick_format\": \"0d\",\n                        \"num_ticks\": da.sizes[\"band\"],\n                    }\n                axes_options = {\n                    \"x\": x_axis_options,\n                    \"y\": {\"label_offset\": \"35px\"},\n                }\n\n                if not stack_btn.value:\n                    plt.clear()\n                    plt.plot(\n                        da.coords[da.dims[0]].values,\n                        da.values,\n                        axes_options=axes_options,\n                    )\n                else:\n                    color = np.random.rand(\n                        3,\n                    )\n                    if \"wavelength\" in da.coords:\n                        xlabel = \"Wavelength (nm)\"\n                        x_values = da[\"wavelength\"].values\n                    else:\n                        xlabel = \"Band\"\n                        x_values = da.coords[da.dims[0]].values\n\n                    plt.plot(\n                        x_values,\n                        da.values,\n                        color=color,\n                        axes_options=axes_options,\n                    )\n                    try:\n                        if isinstance(self._fig.axes[0], bqplot.ColorAxis):\n                            self._fig.axes = self._fig.axes[1:]\n                        elif isinstance(self._fig.axes[-1], bqplot.ColorAxis):\n                            self._fig.axes = self._fig.axes[:-1]\n                    except Exception:\n                        pass\n\n                plt.xlabel(xlabel)\n                plt.ylabel(ylabel)\n                if xlim:\n                    plt.xlim(xlim[0], xlim[1])\n                if ylim:\n                    plt.ylim(ylim[0], ylim[1])\n\n                if not self._show_plot:\n                    with self._output_widget:\n                        plt.show()\n                        self._show_plot = True\n\n                self._host_map.default_style = {\"cursor\": \"crosshair\"}\n\n        self._host_map.on_interaction(handle_interaction)\n        self._on_map_interaction = handle_interaction\n\n        self._spectral_widget = self\n        self._spectral_control = ipyleaflet.WidgetControl(\n            widget=self, position=position\n        )\n        self._host_map.add(self._spectral_control)\n\n    def cleanup(self):\n        \"\"\"Removes the widget from the map and performs cleanup.\"\"\"\n        if self._host_map:\n            self._host_map.default_style = {\"cursor\": \"default\"}\n            self._host_map.on_interaction(self._on_map_interaction, remove=True)\n\n            if self._output_control:\n                self._host_map.remove_control(self._output_control)\n\n                if self._output_widget:\n                    self._output_widget.close()\n                    self._output_widget = None\n\n            if self._spectral_control:\n                self._host_map.remove_control(self._spectral_control)\n                self._spectral_control = None\n\n                if self._spectral_widget:\n                    self._spectral_widget.close()\n                    self._spectral_widget = None\n\n            if hasattr(self._host_map, \"_plot_marker_cluster\"):\n                self._host_map._plot_marker_cluster.markers = []\n                self._host_map._plot_markers = []\n\n            if hasattr(self._host_map, \"_spectral_data\"):\n                self._host_map._spectral_data = {}\n\n            if hasattr(self, \"_output_widget\") and self._output_widget is not None:\n                self._output_widget.clear_output()\n\n        if self.on_close is not None:\n            self.on_close()\n</code></pre>"},{"location":"ui/#hypercoast.ui.SpectralWidget.__init__","title":"<code>__init__(self, host_map, stack=True, position='topright', xlim=None, ylim=None, **kwargs)</code>  <code>special</code>","text":"<p>Initializes a new instance of the SpectralWidget class.</p> <p>Parameters:</p> Name Type Description Default <code>host_map</code> <code>Map</code> <p>The map to host the widget.</p> required <code>stack</code> <code>bool</code> <p>Whether to stack the plots. Defaults to True.</p> <code>True</code> <code>position</code> <code>str</code> <p>The position of the widget on the map. Defaults to \"topright\".</p> <code>'topright'</code> <code>xlim</code> <code>tuple</code> <p>The x-axis limits. Defaults to None.</p> <code>None</code> <code>ylim</code> <code>tuple</code> <p>The y-axis limits. Defaults to None.</p> <code>None</code> Source code in <code>hypercoast/ui.py</code> <pre><code>def __init__(\n    self, host_map, stack=True, position=\"topright\", xlim=None, ylim=None, **kwargs\n):\n    \"\"\"\n    Initializes a new instance of the SpectralWidget class.\n\n    Args:\n        host_map (Map): The map to host the widget.\n        stack (bool, optional): Whether to stack the plots. Defaults to True.\n        position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n        xlim (tuple, optional): The x-axis limits. Defaults to None.\n        ylim (tuple, optional): The y-axis limits. Defaults to None.\n    \"\"\"\n    self._host_map = host_map\n    self.on_close = None\n    self._stack = stack\n    self._show_plot = False\n\n    fig_margin = {\"top\": 20, \"bottom\": 35, \"left\": 50, \"right\": 20}\n    fig = plt.figure(\n        # title=None,\n        fig_margin=fig_margin,\n        layout={\"width\": \"500px\", \"height\": \"300px\"},\n    )\n\n    self._fig = fig\n    self._host_map._fig = fig\n\n    layer_names = list(host_map.cog_layer_dict.keys())\n    layers_widget = widgets.Dropdown(options=layer_names)\n    layers_widget.layout.width = \"18ex\"\n\n    close_btn = widgets.Button(\n        icon=\"times\",\n        tooltip=\"Close the widget\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    reset_btn = widgets.Button(\n        icon=\"trash\",\n        tooltip=\"Remove all markers\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    settings_btn = widgets.Button(\n        icon=\"gear\",\n        tooltip=\"Change layer settings\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    stack_btn = widgets.ToggleButton(\n        value=stack,\n        icon=\"area-chart\",\n        tooltip=\"Stack spectral signatures\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    def settings_btn_click(_):\n\n        self._host_map._add_layer_editor(\n            position=\"topright\",\n            layer_dict=self._host_map.cog_layer_dict[layers_widget.value],\n        )\n\n    settings_btn.on_click(settings_btn_click)\n\n    def reset_btn_click(_):\n        if hasattr(self._host_map, \"_plot_marker_cluster\"):\n            self._host_map._plot_marker_cluster.markers = []\n            self._host_map._plot_markers = []\n\n        if hasattr(self._host_map, \"_spectral_data\"):\n            self._host_map._spectral_data = {}\n\n        self._output_widget.clear_output()\n        self._show_plot = False\n        plt.clear()\n\n    reset_btn.on_click(reset_btn_click)\n\n    save_btn = widgets.Button(\n        icon=\"floppy-o\",\n        tooltip=\"Save the data to a CSV\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    def chooser_callback(chooser):\n        if chooser.selected:\n            file_path = chooser.selected\n            self._host_map.spectral_to_csv(file_path)\n            if (\n                hasattr(self._host_map, \"_file_chooser_control\")\n                and self._host_map._file_chooser_control in self._host_map.controls\n            ):\n                self._host_map.remove_control(self._host_map._file_chooser_control)\n                self._host_map._file_chooser.close()\n\n    def save_btn_click(_):\n        if not hasattr(self._host_map, \"_spectral_data\"):\n            return\n\n        self._output_widget.clear_output()\n        file_chooser = FileChooser(\n            os.getcwd(), layout=widgets.Layout(width=\"454px\")\n        )\n        file_chooser.filter_pattern = \"*.csv\"\n        file_chooser.use_dir_icons = True\n        file_chooser.title = \"Save spectral data to a CSV file\"\n        file_chooser.default_filename = \"spectral_data.csv\"\n        file_chooser.show_hidden = False\n        file_chooser.register_callback(chooser_callback)\n        file_chooser_control = ipyleaflet.WidgetControl(\n            widget=file_chooser, position=\"topright\"\n        )\n        self._host_map.add(file_chooser_control)\n        setattr(self._host_map, \"_file_chooser\", file_chooser)\n        setattr(self._host_map, \"_file_chooser_control\", file_chooser_control)\n\n    save_btn.on_click(save_btn_click)\n\n    def close_widget(_):\n        self.cleanup()\n\n    close_btn.on_click(close_widget)\n\n    super().__init__(\n        [layers_widget, settings_btn, stack_btn, reset_btn, save_btn, close_btn]\n    )\n\n    output = widgets.Output()\n    output_control = ipyleaflet.WidgetControl(widget=output, position=\"bottomright\")\n    self._output_widget = output\n    self._output_control = output_control\n    self._host_map.add(output_control)\n\n    if not hasattr(self._host_map, \"_spectral_data\"):\n        self._host_map._spectral_data = {}\n\n    def handle_interaction(**kwargs):\n\n        latlon = kwargs.get(\"coordinates\")\n        lat = latlon[0]\n        lon = latlon[1]\n        if kwargs.get(\"type\") == \"click\" and self._host_map._layer_editor is None:\n            layer_name = layers_widget.value\n\n            if not hasattr(self._host_map, \"_plot_markers\"):\n                self._host_map._plot_markers = []\n            markers = self._host_map._plot_markers\n            marker_cluster = self._host_map._plot_marker_cluster\n            markers.append(ipyleaflet.Marker(location=latlon, draggable=False))\n            marker_cluster.markers = markers\n            self._host_map._plot_marker_cluster = marker_cluster\n\n            xlabel = \"Wavelength (nm)\"\n            ylabel = \"Reflectance\"\n\n            ds = self._host_map.cog_layer_dict[layer_name][\"xds\"]\n\n            if self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"XARRAY\":\n                da = extract_spectral(ds, lat, lon)\n                xlabel = \"Band\"\n                ylabel = \"Value\"\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"EMIT\":\n                da = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")[\n                    \"reflectance\"\n                ]\n\n                if \"wavelength\" not in self._host_map._spectral_data:\n                    self._host_map._spectral_data[\"wavelength\"] = ds[\n                        \"wavelength\"\n                    ].values\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"TANAGER\":\n                da = extract_tanager(ds, lat, lon)\n                ylabel = \"TOA Radiance\"\n\n                if \"wavelength\" not in self._host_map._spectral_data:\n                    self._host_map._spectral_data[\"wavelength\"] = ds[\n                        \"wavelength\"\n                    ].values\n\n                # da = da.swap_dims({\"band\": \"wavelength\"})\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"PACE\":\n                try:\n                    da = extract_pace(ds, lat, lon)\n                except:\n                    da = xr.DataArray(\n                        np.full(len(ds[\"wavelength\"]), np.nan),\n                        dims=[\"wavelength\"],\n                        coords={\"wavelength\": ds[\"wavelength\"]},\n                    )\n                if \"wavelengths\" not in self._host_map._spectral_data:\n                    self._host_map._spectral_data[\"wavelengths\"] = ds[\n                        \"wavelength\"\n                    ].values\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"DESIS\":\n                da = extract_desis(ds, lat, lon)\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"NEON\":\n                da = extract_neon(ds, lat, lon)\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"AVIRIS\":\n                da = extract_aviris(ds, lat, lon)\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"PRISMA\":\n                da = extract_prisma(ds, lat, lon)\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"EnMAP\":\n                da = extract_enmap(ds, lat, lon)\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"WYVERN\":\n                da = extract_wyvern(ds, lat, lon)\n\n            self._host_map._spectral_data[f\"({lat:.4f} {lon:.4f})\"] = da.values\n\n            if self._host_map.cog_layer_dict[layer_name][\"hyper\"] != \"XARRAY\":\n                da[da &lt; 0] = np.nan\n                x_axis_options = {\"label_offset\": \"30px\"}\n            else:\n                x_axis_options = {\n                    \"label_offset\": \"30px\",\n                    \"tick_format\": \"0d\",\n                    \"num_ticks\": da.sizes[\"band\"],\n                }\n            axes_options = {\n                \"x\": x_axis_options,\n                \"y\": {\"label_offset\": \"35px\"},\n            }\n\n            if not stack_btn.value:\n                plt.clear()\n                plt.plot(\n                    da.coords[da.dims[0]].values,\n                    da.values,\n                    axes_options=axes_options,\n                )\n            else:\n                color = np.random.rand(\n                    3,\n                )\n                if \"wavelength\" in da.coords:\n                    xlabel = \"Wavelength (nm)\"\n                    x_values = da[\"wavelength\"].values\n                else:\n                    xlabel = \"Band\"\n                    x_values = da.coords[da.dims[0]].values\n\n                plt.plot(\n                    x_values,\n                    da.values,\n                    color=color,\n                    axes_options=axes_options,\n                )\n                try:\n                    if isinstance(self._fig.axes[0], bqplot.ColorAxis):\n                        self._fig.axes = self._fig.axes[1:]\n                    elif isinstance(self._fig.axes[-1], bqplot.ColorAxis):\n                        self._fig.axes = self._fig.axes[:-1]\n                except Exception:\n                    pass\n\n            plt.xlabel(xlabel)\n            plt.ylabel(ylabel)\n            if xlim:\n                plt.xlim(xlim[0], xlim[1])\n            if ylim:\n                plt.ylim(ylim[0], ylim[1])\n\n            if not self._show_plot:\n                with self._output_widget:\n                    plt.show()\n                    self._show_plot = True\n\n            self._host_map.default_style = {\"cursor\": \"crosshair\"}\n\n    self._host_map.on_interaction(handle_interaction)\n    self._on_map_interaction = handle_interaction\n\n    self._spectral_widget = self\n    self._spectral_control = ipyleaflet.WidgetControl(\n        widget=self, position=position\n    )\n    self._host_map.add(self._spectral_control)\n</code></pre>"},{"location":"ui/#hypercoast.ui.SpectralWidget.cleanup","title":"<code>cleanup(self)</code>","text":"<p>Removes the widget from the map and performs cleanup.</p> Source code in <code>hypercoast/ui.py</code> <pre><code>def cleanup(self):\n    \"\"\"Removes the widget from the map and performs cleanup.\"\"\"\n    if self._host_map:\n        self._host_map.default_style = {\"cursor\": \"default\"}\n        self._host_map.on_interaction(self._on_map_interaction, remove=True)\n\n        if self._output_control:\n            self._host_map.remove_control(self._output_control)\n\n            if self._output_widget:\n                self._output_widget.close()\n                self._output_widget = None\n\n        if self._spectral_control:\n            self._host_map.remove_control(self._spectral_control)\n            self._spectral_control = None\n\n            if self._spectral_widget:\n                self._spectral_widget.close()\n                self._spectral_widget = None\n\n        if hasattr(self._host_map, \"_plot_marker_cluster\"):\n            self._host_map._plot_marker_cluster.markers = []\n            self._host_map._plot_markers = []\n\n        if hasattr(self._host_map, \"_spectral_data\"):\n            self._host_map._spectral_data = {}\n\n        if hasattr(self, \"_output_widget\") and self._output_widget is not None:\n            self._output_widget.clear_output()\n\n    if self.on_close is not None:\n        self.on_close()\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#import-library","title":"Import library","text":"<p>To use HyperCoast in a project:</p> <pre><code>import hypercoast\n</code></pre>"},{"location":"usage/#search-for-datasets","title":"Search for datasets","text":"<p>To download and access NASA hyperspectral data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, run the following code to log in:</p> <pre><code>hypercoast.nasa_earth_login()\n</code></pre> <p>Collections on NASA Earthdata are discovered with the search_datasets function, which accepts an instrument filter as an easy way to get started. Each of the items in the list of collections returned has a \"short-name\". For example, to search for all datasets with the instrument \"oci\":</p> <pre><code>results = hypercoast.search_datasets(instrument=\"oci\")\ndatasets = set()\nfor item in results:\n    summary = item.summary()\n    short_name = summary[\"short-name\"]\n    if short_name not in datasets:\n        print(short_name)\n    datasets.add(short_name)\nprint(f\"\\nFound {len(datasets)} unique datasets\")\n</code></pre>"},{"location":"usage/#search-for-data-by-short-name","title":"Search for data by short name","text":"<p>Next, we use the <code>search_nasa_data</code> function to find granules within a collection. Let's use the short_name for the PACE/OCI Level-2 data product for bio-optical and biogeochemical properties.</p> <pre><code>results = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    count=1,\n)\n</code></pre> <p>We can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the temporal parameter to request a date range and the bounding_box parameter to request granules that intersect with a bounding box. We can even provide a cloud_cover threshold to limit files that have a lower percentage of cloud cover. We do not provide a count, so we'll get all granules that satisfy the constraints.</p> <pre><code>tspan = (\"2024-04-01\", \"2024-04-16\")\nbbox = (-76.75, 36.97, -75.74, 39.01)\nclouds = (0, 50)\n\nresults, gdf = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    temporal=tspan,\n    bounding_box=bbox,\n    cloud_cover=clouds,\n    return_gdf=True,\n)\n</code></pre> <p>Display the footprints of the granules that match the search criteria.</p> <pre><code>gdf.explore()\n</code></pre> <p>We can also download all the results with one command.</p> <pre><code>hypercoast.download_nasa_data(results, out_dir=\"data\")\n</code></pre>"},{"location":"usage/#search-for-pace-data","title":"Search for PACE data","text":"<p>To search for PACE data, we can use the <code>search_pace</code> function:</p> <pre><code>results, gdf = hypercoast.search_pace(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-05-10\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</code></pre> <p>To download the PACE data, we can use the <code>download_pace</code> function:</p> <pre><code>hypercoast.download_pace(results, out_dir=\"data\")\n</code></pre>"},{"location":"usage/#search-for-emit-data","title":"Search for EMIT data","text":"<p>To search for EMIT data, we can use the <code>search_emit</code> function:</p> <pre><code>results, gdf = hypercoast.search_emit(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-04-01\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</code></pre> <p>To download the EMIT data, we can use the <code>download_emit</code> function:</p> <pre><code>hypercoast.download_emit(results, out_dir=\"data\")\n</code></pre>"},{"location":"usage/#visualize-pace-data","title":"Visualize PACE data","text":"<p>Load the dataset as a <code>xarray.Dataset</code> object:</p> <pre><code>dataset = hypercoast.read_pace(filepath)\n</code></pre> <p>Visualize selected bands of the dataset:</p> <pre><code>hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2, crs=\"default\")\n</code></pre> <p>Visualize the dataset on an interactive map:</p> <pre><code>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450]\nm.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\")\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\")\nm.add(\"spectral\")\nm\n</code></pre>"},{"location":"usage/#visualize-emit-data","title":"Visualize EMIT data","text":"<p>To visualize EMIT data, we can use the <code>read_emit</code> function:</p> <pre><code>dataset = hypercoast.read_emit(filepath)\n</code></pre> <p>Visualize the dataset on an interactive map:</p> <pre><code>m = hypercoast.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\")\nm.add(\"spectral\")\nm\n</code></pre>"},{"location":"usage/#create-an-image-cube","title":"Create an image cube","text":"<p>First , load the dataset as a <code>xarray.Dataset</code> object. Select a region of interest (ROI) using the <code>sel</code> method:</p> <pre><code>dataset = hypercoast.read_emit(filepath)\nds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451))\n</code></pre> <p>Create an image cube using the <code>image_cube</code> function:</p> <pre><code>cube = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.4),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n)\ncube.show()\n</code></pre>"},{"location":"usage/#interactive-slicing-and-thresholding","title":"Interactive slicing and thresholding","text":"<p>First , load the dataset as a <code>xarray.Dataset</code> object. Select a region of interest (ROI) using the <code>sel</code> method:</p> <pre><code>dataset = hypercoast.read_emit(filepath)\nds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93))\n</code></pre> <p>Interactive slicing along the z-axis (band):</p> <pre><code>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\np.show()\n</code></pre> <p>Interactive thresholding:</p> <pre><code>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\np.show()\n</code></pre>"},{"location":"usage/#visualizing-pace-chlorophyll-a-concentration-data","title":"Visualizing PACE chlorophyll-a concentration data","text":"<p>Load all the data files in a directory as an <code>xarray.DataArray</code>:</p> <pre><code>files = \"data/*nc\"\narray = hypercoast.read_pace_chla(files)\n</code></pre> <p>Select a date and visualize the chlorophyll-a concentration data with Matplotlib.</p> <pre><code>hypercoast.viz_pace_chla(array, date=\"2024-06-01\", cmap=\"jet\", size=6)\n</code></pre> <p>If the date is not specified, the data are averaged over the entire time range.</p> <pre><code>hypercoast.viz_pace_chla(array, cmap=\"jet\", size=6)\n</code></pre> <p>Convert the data array to an image that can be displayed on an interactive map.</p> <pre><code>single_image = hypercoast.pace_chla_to_image(single_array)\n</code></pre> <p>Create an interactive map and display the image on the map.</p> <pre><code>m = hypercoast.Map(center=[40, -100], zoom=4)\nm.add_basemap(\"Hybrid\")\nm.add_raster(\n    single_image,\n    cmap=\"jet\",\n    vmin=-1,\n    vmax=2,\n    layer_name=\"Chlorophyll a\",\n    zoom_to_layer=False,\n)\nlabel = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\"\nm.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label)\nm\n</code></pre>"},{"location":"emit_utils/data_loading/","title":"data_loading module","text":"<p>Data loading utilities for EMIT model training and inference.</p> <p>This module provides functions for loading and preprocessing hyperspectral remote sensing data from Excel files for training and testing machine learning models.</p>"},{"location":"emit_utils/data_loading/#hypercoast.emit_utils.data_loading.load_real_data","title":"<code>load_real_data(excel_path, selected_bands, split_ratio=0.7, seed=42, diff_before_norm=False, diff_after_norm=False, target_parameter='TSS', lower_quantile=0.0, upper_quantile=1.0, log_offset=0.01)</code>","text":"<p>Load and preprocess real data using MinMax scaling for training and testing.</p> <p>This function reads hyperspectral Rrs data and water quality parameters from an Excel file, applies sample-wise MinMax scaling and log transformation, and returns DataLoaders ready for model training.</p> <p>Parameters:</p> Name Type Description Default <code>excel_path</code> <code>str</code> <p>Path to Excel file containing 'Rrs' and 'parameter' sheets.</p> required <code>selected_bands</code> <code>List[float]</code> <p>List of wavelengths (nm) to extract from the data.</p> required <code>split_ratio</code> <code>float</code> <p>Proportion of data to use for training (0-1).</p> <code>0.7</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible train/test split.</p> <code>42</code> <code>diff_before_norm</code> <code>bool</code> <p>Whether to apply differencing before normalization.</p> <code>False</code> <code>diff_after_norm</code> <code>bool</code> <p>Whether to apply differencing after normalization.</p> <code>False</code> <code>target_parameter</code> <code>str</code> <p>Name of target parameter column (e.g., 'TSS', 'Chla').</p> <code>'TSS'</code> <code>lower_quantile</code> <code>float</code> <p>Lower quantile for filtering target parameter outliers.</p> <code>0.0</code> <code>upper_quantile</code> <code>float</code> <p>Upper quantile for filtering target parameter outliers.</p> <code>1.0</code> <code>log_offset</code> <code>float</code> <p>Offset added before log transformation to avoid log(0).</p> <code>0.01</code> <p>Returns:</p> Type Description <code>train_dl</code> <p>DataLoader for training data. test_dl: DataLoader for testing data. input_dim: Number of input features. output_dim: Number of output features. train_ids: List of sample IDs in training set. test_ids: List of sample IDs in testing set.</p> Source code in <code>hypercoast/emit_utils/data_loading.py</code> <pre><code>def load_real_data(\n    excel_path: str,\n    selected_bands: List[float],\n    split_ratio: float = 0.7,\n    seed: int = 42,\n    diff_before_norm: bool = False,\n    diff_after_norm: bool = False,\n    target_parameter: str = \"TSS\",\n    lower_quantile: float = 0.0,\n    upper_quantile: float = 1.0,\n    log_offset: float = 0.01,\n) -&gt; Tuple[DataLoader, DataLoader, int, int, List[str], List[str]]:\n    \"\"\"Load and preprocess real data using MinMax scaling for training and testing.\n\n    This function reads hyperspectral Rrs data and water quality parameters from\n    an Excel file, applies sample-wise MinMax scaling and log transformation,\n    and returns DataLoaders ready for model training.\n\n    Args:\n        excel_path: Path to Excel file containing 'Rrs' and 'parameter' sheets.\n        selected_bands: List of wavelengths (nm) to extract from the data.\n        split_ratio: Proportion of data to use for training (0-1).\n        seed: Random seed for reproducible train/test split.\n        diff_before_norm: Whether to apply differencing before normalization.\n        diff_after_norm: Whether to apply differencing after normalization.\n        target_parameter: Name of target parameter column (e.g., 'TSS', 'Chla').\n        lower_quantile: Lower quantile for filtering target parameter outliers.\n        upper_quantile: Upper quantile for filtering target parameter outliers.\n        log_offset: Offset added before log transformation to avoid log(0).\n\n    Returns:\n        train_dl: DataLoader for training data.\n        test_dl: DataLoader for testing data.\n        input_dim: Number of input features.\n        output_dim: Number of output features.\n        train_ids: List of sample IDs in training set.\n        test_ids: List of sample IDs in testing set.\n    \"\"\"\n\n    rounded_bands = [int(round(b)) for b in selected_bands]\n    band_cols = [f\"Rrs_{b}\" for b in rounded_bands]\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n    df_rrs_selected = df_rrs[[\"GLORIA_ID\"] + band_cols]\n    df_param_selected = df_param[[\"GLORIA_ID\", target_parameter]]\n    df_merged = pd.merge(\n        df_rrs_selected, df_param_selected, on=\"GLORIA_ID\", how=\"inner\"\n    )\n\n    # === Filter valid samples ===\n    mask_rrs_valid = df_merged[band_cols].notna().all(axis=1)\n    mask_target_valid = df_merged[target_parameter].notna()\n    df_filtered = df_merged[mask_rrs_valid &amp; mask_target_valid].reset_index(drop=True)\n    print(\n        f\"\u2705 Number of samples after filtering Rrs and {target_parameter}: {len(df_filtered)}\"\n    )\n\n    # === Quantile clipping for target parameter ===\n    lower = df_filtered[target_parameter].quantile(lower_quantile)\n    upper = df_filtered[target_parameter].quantile(upper_quantile)\n    df_filtered = df_filtered[\n        (df_filtered[target_parameter] &gt;= lower)\n        &amp; (df_filtered[target_parameter] &lt;= upper)\n    ].reset_index(drop=True)\n    print(\n        f\"\u2705 Number of samples after removing {target_parameter} quantiles [{lower_quantile}, {upper_quantile}]: {len(df_filtered)}\"\n    )\n\n    # === Extract sample IDs, Rrs, and target parameter ===\n    all_sample_ids = df_filtered[\"GLORIA_ID\"].astype(str).tolist()\n    Rrs_array = df_filtered[band_cols].values\n    param_array = df_filtered[[target_parameter]].values\n\n    if diff_before_norm:\n        Rrs_array = np.diff(Rrs_array, axis=1)\n\n    # === Apply MinMax scaling to [1, 10] for each sample independently ===\n    scalers_Rrs_real = [MinMaxScaler((1, 10)) for _ in range(Rrs_array.shape[0])]\n    Rrs_normalized = np.array(\n        [\n            scalers_Rrs_real[i].fit_transform(row.reshape(-1, 1)).flatten()\n            for i, row in enumerate(Rrs_array)\n        ]\n    )\n\n    if diff_after_norm:\n        Rrs_normalized = np.diff(Rrs_normalized, axis=1)\n\n    # === Transform target parameter to log10(param + log_offset) ===\n    param_transformed = np.log10(param_array + log_offset)\n\n    # === Build Dataset ===\n    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n    param_tensor = torch.tensor(param_transformed, dtype=torch.float32)\n    dataset = TensorDataset(Rrs_tensor, param_tensor)\n\n    # === Split into training and testing sets ===\n    num_samples = len(dataset)\n    indices = np.arange(num_samples)\n    np.random.seed(seed)\n    np.random.shuffle(indices)\n    train_size = int(split_ratio * num_samples)\n    train_indices = indices[:train_size]\n    test_indices = indices[train_size:]\n\n    train_dataset = Subset(dataset, train_indices)\n    test_dataset = Subset(dataset, test_indices)\n\n    train_ids = [all_sample_ids[i] for i in train_indices]\n    test_ids = [all_sample_ids[i] for i in test_indices]\n\n    train_dl = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)\n    test_dl = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = param_tensor.shape[1]\n\n    return (train_dl, test_dl, input_dim, output_dim, train_ids, test_ids)\n</code></pre>"},{"location":"emit_utils/data_loading/#hypercoast.emit_utils.data_loading.load_real_data_Robust","title":"<code>load_real_data_Robust(excel_path, selected_bands, target_parameter='TSS', split_ratio=0.7, seed=42, use_diff=False, lower_quantile=0.0, upper_quantile=1.0, Rrs_range=(0, 0.25), target_range=(-0.5, 0.5))</code>","text":"<p>Load and preprocess real data using robust scaling for training and testing.</p> <p>This function reads hyperspectral remote sensing reflectance (Rrs) data and water quality parameters from an Excel file, applies robust scaling and preprocessing, and returns DataLoaders ready for model training.</p> <p>Parameters:</p> Name Type Description Default <code>excel_path</code> <code>str</code> <p>Path to Excel file containing 'Rrs' and 'parameter' sheets.</p> required <code>selected_bands</code> <code>List[float]</code> <p>List of wavelengths (nm) to extract from the data.</p> required <code>target_parameter</code> <code>str</code> <p>Name of target parameter column (e.g., 'TSS', 'Chla').</p> <code>'TSS'</code> <code>split_ratio</code> <code>float</code> <p>Proportion of data to use for training (0-1).</p> <code>0.7</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible train/test split.</p> <code>42</code> <code>use_diff</code> <code>bool</code> <p>Whether to apply first-order differencing to Rrs spectra.</p> <code>False</code> <code>lower_quantile</code> <code>float</code> <p>Lower quantile for filtering target parameter outliers.</p> <code>0.0</code> <code>upper_quantile</code> <code>float</code> <p>Upper quantile for filtering target parameter outliers.</p> <code>1.0</code> <code>Rrs_range</code> <code>Tuple[float, float]</code> <p>Target range for Rrs normalization.</p> <code>(0, 0.25)</code> <code>target_range</code> <code>Tuple[float, float]</code> <p>Target range for parameter normalization.</p> <code>(-0.5, 0.5)</code> <p>Returns:</p> Type Description <code>train_dl</code> <p>DataLoader for training data. test_dl: DataLoader for testing data. input_dim: Number of input features. output_dim: Number of output features. train_ids: List of sample IDs in training set. test_ids: List of sample IDs in testing set. scaler_Rrs: Fitted scaler for Rrs data. TSS_scalers_dict: Dictionary containing 'log' and 'robust' scalers for target.</p> Source code in <code>hypercoast/emit_utils/data_loading.py</code> <pre><code>def load_real_data_Robust(\n    excel_path: str,\n    selected_bands: List[float],\n    target_parameter: str = \"TSS\",\n    split_ratio: float = 0.7,\n    seed: int = 42,\n    use_diff: bool = False,\n    lower_quantile: float = 0.0,\n    upper_quantile: float = 1.0,\n    Rrs_range: Tuple[float, float] = (0, 0.25),\n    target_range: Tuple[float, float] = (-0.5, 0.5),\n) -&gt; Tuple[DataLoader, DataLoader, int, int, List[str], List[str], Any, Dict[str, Any]]:\n    \"\"\"Load and preprocess real data using robust scaling for training and testing.\n\n    This function reads hyperspectral remote sensing reflectance (Rrs) data and\n    water quality parameters from an Excel file, applies robust scaling and\n    preprocessing, and returns DataLoaders ready for model training.\n\n    Args:\n        excel_path: Path to Excel file containing 'Rrs' and 'parameter' sheets.\n        selected_bands: List of wavelengths (nm) to extract from the data.\n        target_parameter: Name of target parameter column (e.g., 'TSS', 'Chla').\n        split_ratio: Proportion of data to use for training (0-1).\n        seed: Random seed for reproducible train/test split.\n        use_diff: Whether to apply first-order differencing to Rrs spectra.\n        lower_quantile: Lower quantile for filtering target parameter outliers.\n        upper_quantile: Upper quantile for filtering target parameter outliers.\n        Rrs_range: Target range for Rrs normalization.\n        target_range: Target range for parameter normalization.\n\n    Returns:\n        train_dl: DataLoader for training data.\n        test_dl: DataLoader for testing data.\n        input_dim: Number of input features.\n        output_dim: Number of output features.\n        train_ids: List of sample IDs in training set.\n        test_ids: List of sample IDs in testing set.\n        scaler_Rrs: Fitted scaler for Rrs data.\n        TSS_scalers_dict: Dictionary containing 'log' and 'robust' scalers for target.\n    \"\"\"\n\n    rounded_bands = [int(round(b)) for b in selected_bands]\n    band_cols = [f\"Rrs_{b}\" for b in rounded_bands]\n\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n\n    df_rrs_selected = df_rrs[[\"GLORIA_ID\"] + band_cols]\n    df_param_selected = df_param[[\"GLORIA_ID\", target_parameter]]\n    df_merged = pd.merge(\n        df_rrs_selected, df_param_selected, on=\"GLORIA_ID\", how=\"inner\"\n    )\n\n    mask_rrs_valid = df_merged[band_cols].notna().all(axis=1)\n    mask_param_valid = df_merged[target_parameter].notna()\n    df_filtered = df_merged[mask_rrs_valid &amp; mask_param_valid].reset_index(drop=True)\n\n    print(\n        f\"Number of samples after filtering Rrs and {target_parameter}: {len(df_filtered)}\"\n    )\n\n    lower = df_filtered[target_parameter].quantile(lower_quantile)\n    top = df_filtered[target_parameter].quantile(upper_quantile)\n    df_filtered = df_filtered[\n        (df_filtered[target_parameter] &gt;= lower)\n        &amp; (df_filtered[target_parameter] &lt;= top)\n    ].reset_index(drop=True)\n\n    print(\n        f\"Number of samples after removing {target_parameter} quantiles [{lower_quantile}, {upper_quantile}]: {len(df_filtered)}\"\n    )\n\n    all_sample_ids = df_filtered[\"GLORIA_ID\"].astype(str).tolist()\n    Rrs_array = df_filtered[band_cols].values\n    param_array = df_filtered[[target_parameter]].values\n\n    if use_diff:\n        Rrs_array = np.diff(Rrs_array, axis=1)\n\n    scaler_Rrs = RobustMinMaxScaler(feature_range=Rrs_range)\n    scaler_Rrs.fit(torch.tensor(Rrs_array, dtype=torch.float32))\n    Rrs_normalized = scaler_Rrs.transform(\n        torch.tensor(Rrs_array, dtype=torch.float32)\n    ).numpy()\n\n    log_scaler = LogScaler(shift_min=False, safety_term=1e-8)\n    param_log = log_scaler.fit_transform(torch.tensor(param_array, dtype=torch.float32))\n    param_scaler = RobustMinMaxScaler(\n        feature_range=target_range, global_scale=True, robust=True\n    )\n    param_transformed = param_scaler.fit_transform(param_log).numpy()\n\n    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n    param_tensor = torch.tensor(param_transformed, dtype=torch.float32)\n    dataset = TensorDataset(Rrs_tensor, param_tensor)\n\n    num_samples = len(dataset)\n    indices = np.arange(num_samples)\n    np.random.seed(seed)\n    np.random.shuffle(indices)\n    train_size = int(split_ratio * num_samples)\n    train_indices = indices[:train_size]\n    test_indices = indices[train_size:]\n\n    train_dataset = Subset(dataset, train_indices)\n    test_dataset = Subset(dataset, test_indices)\n\n    train_ids = [all_sample_ids[i] for i in train_indices]\n    test_ids = [all_sample_ids[i] for i in test_indices]\n\n    train_dl = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)\n    test_dl = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = param_tensor.shape[1]\n    TSS_scalers_dict = {\"log\": log_scaler, \"robust\": param_scaler}\n\n    return (\n        train_dl,\n        test_dl,\n        input_dim,\n        output_dim,\n        train_ids,\n        test_ids,\n        scaler_Rrs,\n        TSS_scalers_dict,\n    )\n</code></pre>"},{"location":"emit_utils/data_loading/#hypercoast.emit_utils.data_loading.load_real_test","title":"<code>load_real_test(excel_path, selected_bands, max_allowed_diff=1.0, diff_before_norm=False, diff_after_norm=False, target_parameter='TSS', log_offset=0.01)</code>","text":"<p>Load and preprocess test data using sample-wise MinMax scaling.</p> <p>This function loads test data from Excel, performs band matching to the nearest available wavelengths, applies sample-wise MinMax scaling and log transformation, and returns a DataLoader for inference.</p> <p>Parameters:</p> Name Type Description Default <code>excel_path</code> <code>str</code> <p>Path to Excel file containing 'Rrs' and 'parameter' sheets.</p> required <code>selected_bands</code> <code>List[float]</code> <p>List of target wavelengths (nm) to extract.</p> required <code>max_allowed_diff</code> <code>float</code> <p>Maximum wavelength difference (nm) allowed for band matching.</p> <code>1.0</code> <code>diff_before_norm</code> <code>bool</code> <p>Whether to apply differencing before normalization.</p> <code>False</code> <code>diff_after_norm</code> <code>bool</code> <p>Whether to apply differencing after normalization.</p> <code>False</code> <code>target_parameter</code> <code>str</code> <p>Name of target parameter column (e.g., 'TSS', 'SPM').</p> <code>'TSS'</code> <code>log_offset</code> <code>float</code> <p>Offset added before log transformation to avoid log(0).</p> <code>0.01</code> <p>Returns:</p> Type Description <code>test_dl</code> <p>DataLoader for test data. input_dim: Number of input features. output_dim: Number of output features (always 1). sample_ids: List of sample IDs in test set. sample_dates: List of sample dates in test set.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If row counts don't match, no valid samples exist, or band matching fails.</p> Source code in <code>hypercoast/emit_utils/data_loading.py</code> <pre><code>def load_real_test(\n    excel_path: str,\n    selected_bands: List[float],\n    max_allowed_diff: float = 1.0,\n    diff_before_norm: bool = False,\n    diff_after_norm: bool = False,\n    target_parameter: str = \"TSS\",\n    log_offset: float = 0.01,\n) -&gt; Tuple[DataLoader, int, int, List[str], List[str]]:\n    \"\"\"Load and preprocess test data using sample-wise MinMax scaling.\n\n    This function loads test data from Excel, performs band matching to the\n    nearest available wavelengths, applies sample-wise MinMax scaling and log\n    transformation, and returns a DataLoader for inference.\n\n    Args:\n        excel_path: Path to Excel file containing 'Rrs' and 'parameter' sheets.\n        selected_bands: List of target wavelengths (nm) to extract.\n        max_allowed_diff: Maximum wavelength difference (nm) allowed for band matching.\n        diff_before_norm: Whether to apply differencing before normalization.\n        diff_after_norm: Whether to apply differencing after normalization.\n        target_parameter: Name of target parameter column (e.g., 'TSS', 'SPM').\n        log_offset: Offset added before log transformation to avoid log(0).\n\n    Returns:\n        test_dl: DataLoader for test data.\n        input_dim: Number of input features.\n        output_dim: Number of output features (always 1).\n        sample_ids: List of sample IDs in test set.\n        sample_dates: List of sample dates in test set.\n\n    Raises:\n        ValueError: If row counts don't match, no valid samples exist, or band\n            matching fails.\n    \"\"\"\n\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n\n    if df_rrs.shape[0] != df_param.shape[0]:\n        raise ValueError(\n            f\"\u274c The number of rows in the Rrs table and parameter table do not match. Rrs: {df_rrs.shape[0]}, parameter: {df_param.shape[0]}\"\n        )\n\n    # === Extract IDs and dates ===\n    sample_ids = df_rrs[\"Site Label\"].astype(str).tolist()\n    sample_dates = df_rrs[\"Date\"].astype(str).tolist()\n\n    # === Match target bands ===\n    rrs_wavelengths = []\n    rrs_cols = []\n    for col in df_rrs.columns:\n        try:\n            wl = float(col)\n            rrs_wavelengths.append(wl)\n            rrs_cols.append(col)\n        except Exception:\n            continue\n\n    band_cols = []\n    matched_bands = []\n    for target_band in selected_bands:\n        diffs = [abs(wl - target_band) for wl in rrs_wavelengths]\n        min_diff = min(diffs)\n        if min_diff &gt; max_allowed_diff:\n            raise ValueError(\n                f\"Target wavelength {target_band} nm cannot be matched, error {min_diff:.2f} nm exceeds the allowed range\"\n            )\n        best_idx = diffs.index(min_diff)\n        band_cols.append(rrs_cols[best_idx])\n        matched_bands.append(rrs_wavelengths[best_idx])\n\n    print(\n        f\"\\n\u2705 Band matching successful, {len(selected_bands)} target bands in total, {len(band_cols)} columns actually extracted\"\n    )\n    print(f\"Original number of test samples: {df_rrs.shape[0]}\\n\")\n\n    # === Extract Rrs and target parameter (without differencing for now) ===\n    Rrs_array = df_rrs[band_cols].values.astype(float)\n    target_array = df_param[[target_parameter]].values.astype(float).flatten()\n\n    # === Key: Remove rows with NaN/Inf before differencing ===\n    mask_inputs_ok = np.all(np.isfinite(Rrs_array), axis=1)\n    mask_target_ok = np.isfinite(target_array)\n    mask_ok = mask_inputs_ok &amp; mask_target_ok\n    if not np.any(mask_ok):\n        raise ValueError(\"\u274c No valid samples (NaN/Inf found in input or target).\")\n    dropped = int(len(mask_ok) - mask_ok.sum())\n    if dropped &gt; 0:\n        print(\n            f\"\u26a0\ufe0f Dropped {dropped} invalid samples (containing NaN/Inf) before differencing\"\n        )\n\n    Rrs_array = Rrs_array[mask_ok]\n    target_array = target_array[mask_ok]\n    sample_ids = [sid for sid, keep in zip(sample_ids, mask_ok) if keep]\n    sample_dates = [d for d, keep in zip(sample_dates, mask_ok) if keep]\n\n    # === Preprocessing before differencing (optional) ===\n    if diff_before_norm:\n        Rrs_array = np.diff(Rrs_array, axis=1)\n\n    # === Apply MinMaxScaler to [1, 10] for each sample ===\n    scalers_Rrs_test = [MinMaxScaler((1, 10)) for _ in range(Rrs_array.shape[0])]\n    Rrs_normalized = np.array(\n        [\n            scalers_Rrs_test[i].fit_transform(row.reshape(-1, 1)).flatten()\n            for i, row in enumerate(Rrs_array)\n        ]\n    )\n\n    # === Post-processing after differencing (optional) ===\n    if diff_after_norm:\n        Rrs_normalized = np.diff(Rrs_normalized, axis=1)\n\n    # === Transform target value to log10(x + log_offset) ===\n    target_transformed = np.log10(target_array + log_offset)\n\n    # === Construct DataLoader ===\n    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n    target_tensor = torch.tensor(target_transformed.reshape(-1, 1), dtype=torch.float32)\n\n    dataset = TensorDataset(Rrs_tensor, target_tensor)\n    test_dl = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = target_tensor.shape[1]\n\n    return test_dl, input_dim, output_dim, sample_ids, sample_dates\n</code></pre>"},{"location":"emit_utils/data_loading/#hypercoast.emit_utils.data_loading.load_real_test_Robust","title":"<code>load_real_test_Robust(excel_path, selected_bands, max_allowed_diff=1.0, scaler_Rrs=None, scalers_dict=None, use_diff=False, target_parameter='SPM')</code>","text":"<p>Load and preprocess test data using pre-fitted robust scalers.</p> <p>This function loads test data from Excel and applies the same preprocessing transformations used during training, including band matching, filtering, and scaling using pre-fitted scalers.</p> <p>Parameters:</p> Name Type Description Default <code>excel_path</code> <code>str</code> <p>Path to Excel file containing 'Rrs' and 'parameter' sheets.</p> required <code>selected_bands</code> <code>List[float]</code> <p>List of wavelengths (nm) to extract from the data.</p> required <code>max_allowed_diff</code> <code>float</code> <p>Maximum wavelength difference (nm) allowed for band matching.</p> <code>1.0</code> <code>scaler_Rrs</code> <code>Optional[Any]</code> <p>Pre-fitted scaler for Rrs data from training.</p> <code>None</code> <code>scalers_dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing 'log' and 'robust' scalers from training.</p> <code>None</code> <code>use_diff</code> <code>bool</code> <p>Whether to apply first-order differencing to Rrs spectra.</p> <code>False</code> <code>target_parameter</code> <code>str</code> <p>Name of target parameter column (e.g., 'SPM', 'TSS').</p> <code>'SPM'</code> <p>Returns:</p> Type Description <code>test_dl</code> <p>DataLoader for test data. input_dim: Number of input features. output_dim: Number of output features (always 1). sample_ids: List of sample IDs in test set. sample_dates: List of sample dates in test set.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If row counts don't match, no valid samples exist, or band matching fails.</p> Source code in <code>hypercoast/emit_utils/data_loading.py</code> <pre><code>def load_real_test_Robust(\n    excel_path: str,\n    selected_bands: List[float],\n    max_allowed_diff: float = 1.0,\n    scaler_Rrs: Optional[Any] = None,\n    scalers_dict: Optional[Dict[str, Any]] = None,\n    use_diff: bool = False,\n    target_parameter: str = \"SPM\",\n) -&gt; Tuple[DataLoader, int, int, List[str], List[str]]:\n    \"\"\"Load and preprocess test data using pre-fitted robust scalers.\n\n    This function loads test data from Excel and applies the same preprocessing\n    transformations used during training, including band matching, filtering,\n    and scaling using pre-fitted scalers.\n\n    Args:\n        excel_path: Path to Excel file containing 'Rrs' and 'parameter' sheets.\n        selected_bands: List of wavelengths (nm) to extract from the data.\n        max_allowed_diff: Maximum wavelength difference (nm) allowed for band matching.\n        scaler_Rrs: Pre-fitted scaler for Rrs data from training.\n        scalers_dict: Dictionary containing 'log' and 'robust' scalers from training.\n        use_diff: Whether to apply first-order differencing to Rrs spectra.\n        target_parameter: Name of target parameter column (e.g., 'SPM', 'TSS').\n\n    Returns:\n        test_dl: DataLoader for test data.\n        input_dim: Number of input features.\n        output_dim: Number of output features (always 1).\n        sample_ids: List of sample IDs in test set.\n        sample_dates: List of sample dates in test set.\n\n    Raises:\n        ValueError: If row counts don't match, no valid samples exist, or band\n            matching fails.\n    \"\"\"\n\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n\n    if df_rrs.shape[0] != df_param.shape[0]:\n        raise ValueError(\n            f\"\u274c The number of rows in the Rrs table and parameter table do not match. Rrs: {df_rrs.shape[0]}, parameter: {df_param.shape[0]}\"\n        )\n\n    sample_ids = df_rrs[\"Site Label\"].astype(str).tolist()\n    sample_dates = df_rrs[\"Date\"].astype(str).tolist()\n\n    # Match target bands\n    rrs_wavelengths = []\n    rrs_cols = []\n    for col in df_rrs.columns:\n        try:\n            wl = float(col)\n            rrs_wavelengths.append(wl)\n            rrs_cols.append(col)\n        except:\n            continue\n\n    band_cols = []\n    for target_band in selected_bands:\n        diffs = [abs(wl - target_band) for wl in rrs_wavelengths]\n        min_diff = min(diffs)\n        if min_diff &gt; max_allowed_diff:\n            raise ValueError(\n                f\"Target wavelength {target_band} nm cannot be matched, error {min_diff:.2f} nm exceeds the allowed range\"\n            )\n        best_idx = diffs.index(min_diff)\n        band_cols.append(rrs_cols[best_idx])\n\n    print(f\"\\n\u2705 Band matching successful, {len(selected_bands)} target bands in total\")\n    print(f\"Final number of valid test samples: {df_rrs.shape[0]}\\n\")\n\n    Rrs_array = df_rrs[band_cols].values\n    param_array = df_param[[target_parameter]].values.flatten()\n    # === Key: Remove rows with NaN/Inf before differencing ===\n    mask_inputs_ok = np.all(np.isfinite(Rrs_array), axis=1)\n    mask_target_ok = np.isfinite(param_array)\n    mask_ok = mask_inputs_ok &amp; mask_target_ok\n    if not np.any(mask_ok):\n        raise ValueError(\"\u274c Valid samples = 0 (NaN/Inf found in input or target).\")\n    dropped = int(len(mask_ok) - mask_ok.sum())\n    if dropped &gt; 0:\n        print(\n            f\"\u26a0\ufe0f Dropped {dropped} invalid samples (containing NaN/Inf) before differencing\"\n        )\n\n    Rrs_array = Rrs_array[mask_ok]\n    param_array = param_array[mask_ok]\n    sample_ids = [sid for sid, keep in zip(sample_ids, mask_ok) if keep]\n    sample_dates = [d for d, keep in zip(sample_dates, mask_ok) if keep]\n\n    if use_diff:\n        Rrs_array = np.diff(Rrs_array, axis=1)\n\n    Rrs_tensor = torch.tensor(Rrs_array, dtype=torch.float32)\n    Rrs_normalized = scaler_Rrs.transform(Rrs_tensor).numpy()\n\n    log_scaler = scalers_dict[\"log\"]\n    robust_scaler = scalers_dict[\"robust\"]\n    param_log = log_scaler.transform(\n        torch.tensor(param_array.reshape(-1, 1), dtype=torch.float32)\n    )\n    param_transformed = robust_scaler.transform(param_log).numpy()\n\n    dataset = TensorDataset(\n        torch.tensor(Rrs_normalized, dtype=torch.float32),\n        torch.tensor(param_transformed.reshape(-1, 1), dtype=torch.float32),\n    )\n    test_dl = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = 1\n\n    return test_dl, input_dim, output_dim, sample_ids, sample_dates\n</code></pre>"},{"location":"emit_utils/model_inference/","title":"model_inference module","text":"<p>Model inference and visualization utilities for EMIT data.</p> <p>This module provides functions for running model inference on EMIT hyperspectral data, visualizing results with RGB backgrounds, and converting predictions to GeoTIFF format.</p>"},{"location":"emit_utils/model_inference/#hypercoast.emit_utils.model_inference.infer_and_visualize_single_model_Robust","title":"<code>infer_and_visualize_single_model_Robust(model, test_loader, Rrs, mask, latitude, longitude, save_folder, rgb_nc_file, structure_name, TSS_scalers_dict, vmin=0, vmax=50, exposure_coefficient=5.0)</code>","text":"<p>Run model inference and visualize results with RGB background using robust scaling.</p> <p>This function performs model inference on preprocessed EMIT data, applies inverse transformations using robust scalers, creates a visualization overlaid on an RGB composite, and saves both array and image outputs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained PyTorch model for inference.</p> required <code>test_loader</code> <code>DataLoader</code> <p>DataLoader containing preprocessed test data.</p> required <code>Rrs</code> <code>ndarray</code> <p>Array of remote sensing reflectance data with shape (H, W, B).</p> required <code>mask</code> <code>ndarray</code> <p>Boolean mask indicating valid pixels with shape (H, W).</p> required <code>latitude</code> <code>ndarray</code> <p>Array of latitude values.</p> required <code>longitude</code> <code>ndarray</code> <p>Array of longitude values.</p> required <code>save_folder</code> <code>str</code> <p>Directory path to save outputs.</p> required <code>rgb_nc_file</code> <code>str</code> <p>Path to NetCDF file containing RGB bands for visualization.</p> required <code>structure_name</code> <code>str</code> <p>Name for output files.</p> required <code>TSS_scalers_dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing 'log' and 'robust' scalers for inverse transformation.</p> required <code>vmin</code> <code>float</code> <p>Minimum value for colormap scaling.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>Maximum value for colormap scaling.</p> <code>50</code> <code>exposure_coefficient</code> <code>float</code> <p>Coefficient for adjusting RGB brightness.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>final_output</code> <p>Array of shape (N, 3) containing [lat, lon, value] for each pixel.</p> Source code in <code>hypercoast/emit_utils/model_inference.py</code> <pre><code>def infer_and_visualize_single_model_Robust(\n    model: torch.nn.Module,\n    test_loader: DataLoader,\n    Rrs: np.ndarray,\n    mask: np.ndarray,\n    latitude: np.ndarray,\n    longitude: np.ndarray,\n    save_folder: str,\n    rgb_nc_file: str,\n    structure_name: str,\n    TSS_scalers_dict: Dict[str, Any],\n    vmin: float = 0,\n    vmax: float = 50,\n    exposure_coefficient: float = 5.0,\n) -&gt; np.ndarray:\n    \"\"\"Run model inference and visualize results with RGB background using robust scaling.\n\n    This function performs model inference on preprocessed EMIT data, applies\n    inverse transformations using robust scalers, creates a visualization overlaid\n    on an RGB composite, and saves both array and image outputs.\n\n    Args:\n        model: Trained PyTorch model for inference.\n        test_loader: DataLoader containing preprocessed test data.\n        Rrs: Array of remote sensing reflectance data with shape (H, W, B).\n        mask: Boolean mask indicating valid pixels with shape (H, W).\n        latitude: Array of latitude values.\n        longitude: Array of longitude values.\n        save_folder: Directory path to save outputs.\n        rgb_nc_file: Path to NetCDF file containing RGB bands for visualization.\n        structure_name: Name for output files.\n        TSS_scalers_dict: Dictionary containing 'log' and 'robust' scalers for\n            inverse transformation.\n        vmin: Minimum value for colormap scaling.\n        vmax: Maximum value for colormap scaling.\n        exposure_coefficient: Coefficient for adjusting RGB brightness.\n\n    Returns:\n        final_output: Array of shape (N, 3) containing [lat, lon, value] for each pixel.\n    \"\"\"\n    device = next(model.parameters()).device\n    predictions_all = []\n\n    # === Model inference ===\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch[0].to(device)\n            output_dict = model(batch)\n\n            # === Inverse transform using scalers ===\n            predictions_log = TSS_scalers_dict[\"robust\"].inverse_transform(\n                torch.tensor(output_dict[\"pred_y\"].cpu().numpy(), dtype=torch.float32)\n            )\n            predictions_real = (\n                TSS_scalers_dict[\"log\"].inverse_transform(predictions_log).numpy()\n            )\n            predictions_all.append(predictions_real)\n\n    predictions_all = np.vstack(predictions_all).squeeze(-1)\n\n    # Fill predictions into 2D array according to mask\n    outputs = np.full((Rrs.shape[0], Rrs.shape[1]), np.nan)\n    outputs[mask] = predictions_all\n\n    # Save as [lat, lon, value]\n    lat_flat = latitude.flatten()\n    lon_flat = longitude.flatten()\n    output_flat = outputs.flatten()\n    final_output = np.column_stack((lat_flat, lon_flat, output_flat))\n    if np.ma.isMaskedArray(final_output):\n        final_output = final_output.filled(np.nan)\n\n    os.makedirs(save_folder, exist_ok=True)\n    base_name = os.path.splitext(os.path.basename(structure_name))[0]\n    npy_path = os.path.join(save_folder, f\"{base_name}.npy\")\n    png_path = os.path.join(save_folder, f\"{base_name}.png\")\n    np.save(npy_path, final_output)\n\n    # === Construct RGB image from EMIT L2R .nc ===\n    with Dataset(rgb_nc_file) as ds:\n        # Latitude\n        if \"lat\" in ds.variables:\n            lat_var = ds.variables[\"lat\"][:]\n        elif \"latitude\" in ds.variables:\n            lat_var = ds.variables[\"latitude\"][:]\n        else:\n            raise KeyError(\"Latitude variable not found\")\n\n        # Longitude\n        if \"lon\" in ds.variables:\n            lon_var = ds.variables[\"lon\"][:]\n        elif \"longitude\" in ds.variables:\n            lon_var = ds.variables[\"longitude\"][:]\n        else:\n            raise KeyError(\"Longitude variable not found\")\n\n        # rhos band list\n        band_list = []\n        for name in ds.variables:\n            m = re.match(r\"^rhos_(\\d+(?:\\.\\d+)?)$\", name)\n            if m:\n                wl = float(m.group(1))\n                band_list.append((wl, name))\n        if not band_list:\n            raise ValueError(\"No rhos_* bands found\")\n\n        # Select nearest RGB bands\n        targets = {\"R\": 664.0, \"G\": 559.0, \"B\": 492.0}\n\n        def pick_nearest(target_nm):\n            return min(band_list, key=lambda x: abs(x[0] - target_nm))[1]\n\n        var_R = pick_nearest(targets[\"R\"])\n        var_G = pick_nearest(targets[\"G\"])\n        var_B = pick_nearest(targets[\"B\"])\n\n        R = ds.variables[var_R][:]\n        G = ds.variables[var_G][:]\n        B = ds.variables[var_B][:]\n\n        if isinstance(R, np.ma.MaskedArray):\n            R = R.filled(np.nan)\n        if isinstance(G, np.ma.MaskedArray):\n            G = G.filled(np.nan)\n        if isinstance(B, np.ma.MaskedArray):\n            B = B.filled(np.nan)\n\n    # Lat/lon grid\n    if lat_var.ndim == 1 and lon_var.ndim == 1:\n        lat2d, lon2d = np.meshgrid(lat_var, lon_var, indexing=\"ij\")\n    else:\n        lat2d, lon2d = lat_var, lon_var\n\n    H, W = R.shape\n    lat_flat = lat2d.reshape(-1)\n    lon_flat = lon2d.reshape(-1)\n    R_flat, G_flat, B_flat = R.reshape(-1), G.reshape(-1), B.reshape(-1)\n\n    lat_top, lat_bot = np.nanmax(lat2d), np.nanmin(lat2d)\n    lon_min, lon_max = np.nanmin(lon2d), np.nanmax(lon2d)\n    grid_lat = np.linspace(lat_top, lat_bot, H)\n    grid_lon = np.linspace(lon_min, lon_max, W)\n    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n\n    R_interp = griddata(\n        (lon_flat, lat_flat), R_flat, (grid_lon, grid_lat), method=\"linear\"\n    )\n    G_interp = griddata(\n        (lon_flat, lat_flat), G_flat, (grid_lon, grid_lat), method=\"linear\"\n    )\n    B_interp = griddata(\n        (lon_flat, lat_flat), B_flat, (grid_lon, grid_lat), method=\"linear\"\n    )\n\n    rgb_image = np.stack((R_interp, G_interp, B_interp), axis=-1)\n    rgb_max = np.nanmax(rgb_image)\n    if not np.isfinite(rgb_max) or rgb_max == 0:\n        rgb_max = 1.0\n    rgb_image = np.clip((rgb_image / rgb_max) * exposure_coefficient, 0, 1)\n    extent_raw = [lon_min, lon_max, lat_bot, lat_top]\n\n    # Interpolate predictions to same grid\n    interp_output = griddata(\n        (final_output[:, 1], final_output[:, 0]),  # lon, lat\n        final_output[:, 2],\n        (grid_lon, grid_lat),\n        method=\"linear\",\n    )\n    interp_output = np.ma.masked_invalid(interp_output)\n\n    # Plot and save PNG\n    plt.figure(figsize=(24, 6))\n    plt.imshow(rgb_image, extent=extent_raw, origin=\"upper\")\n    im = plt.imshow(\n        interp_output,\n        extent=extent_raw,\n        cmap=\"jet\",\n        alpha=1,\n        origin=\"upper\",\n        vmin=vmin,\n        vmax=vmax,\n    )\n    cbar = plt.colorbar(im)\n    # cbar.set_label('(mg m$^{-3}$)', fontsize=16)\n    plt.title(f\"{structure_name}\", loc=\"left\", fontsize=20)\n    plt.savefig(png_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n    plt.show()\n\n    print(f\"\u2705 Saved {png_path}\")\n    print(f\"\u2705 Saved {npy_path} (for npy_to_tif)\")\n\n    # Return numpy array for direct use\n    return final_output\n</code></pre>"},{"location":"emit_utils/model_inference/#hypercoast.emit_utils.model_inference.infer_and_visualize_single_model_minmax","title":"<code>infer_and_visualize_single_model_minmax(model, test_loader, Rrs, mask, latitude, longitude, save_folder, rgb_nc_file, structure_name, vmin=0, vmax=50, log_offset=0.01, exposure_coefficient=5.0)</code>","text":"<p>Run model inference and visualize results with RGB background using MinMax scaling.</p> <p>This function performs model inference on preprocessed EMIT data, applies inverse log transformation, creates a visualization overlaid on an RGB composite, and saves both array and image outputs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained PyTorch model for inference.</p> required <code>test_loader</code> <code>DataLoader</code> <p>DataLoader containing preprocessed test data.</p> required <code>Rrs</code> <code>ndarray</code> <p>Array of remote sensing reflectance data with shape (H, W, B).</p> required <code>mask</code> <code>ndarray</code> <p>Boolean mask indicating valid pixels with shape (H, W).</p> required <code>latitude</code> <code>ndarray</code> <p>Array of latitude values.</p> required <code>longitude</code> <code>ndarray</code> <p>Array of longitude values.</p> required <code>save_folder</code> <code>str</code> <p>Directory path to save outputs.</p> required <code>rgb_nc_file</code> <code>str</code> <p>Path to NetCDF file containing RGB bands for visualization.</p> required <code>structure_name</code> <code>str</code> <p>Name for output files.</p> required <code>vmin</code> <code>float</code> <p>Minimum value for colormap scaling.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>Maximum value for colormap scaling.</p> <code>50</code> <code>log_offset</code> <code>float</code> <p>Offset used in log transformation during preprocessing.</p> <code>0.01</code> <code>exposure_coefficient</code> <code>float</code> <p>Coefficient for adjusting RGB brightness.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>final_output</code> <p>Array of shape (N, 3) containing [lat, lon, value] for each pixel.</p> Source code in <code>hypercoast/emit_utils/model_inference.py</code> <pre><code>def infer_and_visualize_single_model_minmax(\n    model: torch.nn.Module,\n    test_loader: DataLoader,\n    Rrs: np.ndarray,\n    mask: np.ndarray,\n    latitude: np.ndarray,\n    longitude: np.ndarray,\n    save_folder: str,\n    rgb_nc_file: str,\n    structure_name: str,\n    vmin: float = 0,\n    vmax: float = 50,\n    log_offset: float = 0.01,\n    exposure_coefficient: float = 5.0,\n) -&gt; np.ndarray:\n    \"\"\"Run model inference and visualize results with RGB background using MinMax scaling.\n\n    This function performs model inference on preprocessed EMIT data, applies\n    inverse log transformation, creates a visualization overlaid on an RGB\n    composite, and saves both array and image outputs.\n\n    Args:\n        model: Trained PyTorch model for inference.\n        test_loader: DataLoader containing preprocessed test data.\n        Rrs: Array of remote sensing reflectance data with shape (H, W, B).\n        mask: Boolean mask indicating valid pixels with shape (H, W).\n        latitude: Array of latitude values.\n        longitude: Array of longitude values.\n        save_folder: Directory path to save outputs.\n        rgb_nc_file: Path to NetCDF file containing RGB bands for visualization.\n        structure_name: Name for output files.\n        vmin: Minimum value for colormap scaling.\n        vmax: Maximum value for colormap scaling.\n        log_offset: Offset used in log transformation during preprocessing.\n        exposure_coefficient: Coefficient for adjusting RGB brightness.\n\n    Returns:\n        final_output: Array of shape (N, 3) containing [lat, lon, value] for each pixel.\n    \"\"\"\n    device = next(model.parameters()).device\n    predictions_all = []\n\n    # === Model inference ===\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch[0].to(device)\n            output_dict = model(batch)\n            predictions = output_dict[\"pred_y\"]\n            predictions_np = predictions.cpu().numpy()\n            predictions_original = (10**predictions_np) - log_offset\n            predictions_all.append(predictions_original)\n\n    predictions_all = np.vstack(predictions_all).squeeze(-1)\n\n    # Fill predictions into 2D array according to mask\n    outputs = np.full((Rrs.shape[0], Rrs.shape[1]), np.nan)\n    outputs[mask] = predictions_all\n\n    # Flatten lat/lon and combine with predictions\n    lat_flat = latitude.flatten()\n    lon_flat = longitude.flatten()\n    output_flat = outputs.flatten()\n    final_output = np.column_stack((lat_flat, lon_flat, output_flat))\n    if np.ma.isMaskedArray(final_output):\n        final_output = final_output.filled(np.nan)\n\n    # Save .npy file (lat, lon, value)\n    os.makedirs(save_folder, exist_ok=True)\n    base_name = os.path.splitext(os.path.basename(structure_name))[0]\n    npy_path = os.path.join(save_folder, f\"{base_name}.npy\")\n    png_path = os.path.join(save_folder, f\"{base_name}.png\")\n    np.save(npy_path, final_output)\n\n    # === Read RGB bands from .nc file ===\n    with Dataset(rgb_nc_file) as ds:\n        if \"lat\" in ds.variables:\n            lat_var = ds.variables[\"lat\"][:]\n        elif \"latitude\" in ds.variables:\n            lat_var = ds.variables[\"latitude\"][:]\n        else:\n            raise KeyError(\"Latitude variable not found (lat/latitude)\")\n\n        if \"lon\" in ds.variables:\n            lon_var = ds.variables[\"lon\"][:]\n        elif \"longitude\" in ds.variables:\n            lon_var = ds.variables[\"longitude\"][:]\n        else:\n            raise KeyError(\"Longitude variable not found (lon/longitude)\")\n\n        band_list = []\n        for name in ds.variables.keys():\n            m = re.match(r\"^rhos_(\\d+(?:\\.\\d+)?)$\", name)\n            if m:\n                wl = float(m.group(1))\n                band_list.append((wl, name))\n        if not band_list:\n            raise ValueError(\"No rhos_* bands found in file\")\n\n        targets = {\"R\": 664.0, \"G\": 559.0, \"B\": 492.0}\n\n        def pick_nearest(target_nm):\n            idx = int(np.argmin([abs(w - target_nm) for w, _ in band_list]))\n            wl_sel, name_sel = band_list[idx]\n            return wl_sel, name_sel\n\n        wl_R, var_R = pick_nearest(targets[\"R\"])\n        wl_G, var_G = pick_nearest(targets[\"G\"])\n        wl_B, var_B = pick_nearest(targets[\"B\"])\n\n        print(\n            f\"RGB band selection: R\u2192{var_R} (\u0394{wl_R - targets['R']:+.1f}nm), \"\n            f\"G\u2192{var_G} (\u0394{wl_G - targets['G']:+.1f}nm), \"\n            f\"B\u2192{var_B} (\u0394{wl_B - targets['B']:+.1f}nm)\"\n        )\n\n        R = ds.variables[var_R][:]\n        G = ds.variables[var_G][:]\n        B = ds.variables[var_B][:]\n        if isinstance(R, np.ma.MaskedArray):\n            R = R.filled(np.nan)\n        if isinstance(G, np.ma.MaskedArray):\n            G = G.filled(np.nan)\n        if isinstance(B, np.ma.MaskedArray):\n            B = B.filled(np.nan)\n\n    if lat_var.ndim == 1 and lon_var.ndim == 1:\n        lat2d, lon2d = np.meshgrid(\n            np.asarray(lat_var), np.asarray(lon_var), indexing=\"ij\"\n        )\n    else:\n        lat2d, lon2d = np.asarray(lat_var), np.asarray(lon_var)\n\n    H, W = R.shape\n    lat_flat = lat2d.reshape(-1)\n    lon_flat = lon2d.reshape(-1)\n    R_flat, G_flat, B_flat = R.reshape(-1), G.reshape(-1), B.reshape(-1)\n\n    lat_top, lat_bot = np.nanmax(lat2d), np.nanmin(lat2d)\n    lon_min, lon_max = np.nanmin(lon2d), np.nanmax(lon2d)\n    grid_lat = np.linspace(lat_top, lat_bot, H)\n    grid_lon = np.linspace(lon_min, lon_max, W)\n    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n\n    R_interp = griddata(\n        (lon_flat, lat_flat), R_flat, (grid_lon, grid_lat), method=\"linear\"\n    )\n    G_interp = griddata(\n        (lon_flat, lat_flat), G_flat, (grid_lon, grid_lat), method=\"linear\"\n    )\n    B_interp = griddata(\n        (lon_flat, lat_flat), B_flat, (grid_lon, grid_lat), method=\"linear\"\n    )\n\n    rgb_image = np.stack((R_interp, G_interp, B_interp), axis=-1)\n    rgb_max = np.nanmax(rgb_image)\n    if not np.isfinite(rgb_max) or rgb_max == 0:\n        rgb_max = 1.0\n    rgb_image = np.clip((rgb_image / rgb_max) * exposure_coefficient, 0, 1)\n    extent_raw = [lon_min, lon_max, lat_bot, lat_top]\n\n    interp_output = griddata(\n        (final_output[:, 1], final_output[:, 0]),\n        final_output[:, 2],\n        (grid_lon, grid_lat),\n        method=\"linear\",\n    )\n    interp_output = np.ma.masked_invalid(interp_output)\n\n    plt.figure(figsize=(24, 6))\n    plt.imshow(rgb_image, extent=extent_raw, origin=\"upper\")\n    im = plt.imshow(\n        interp_output,\n        extent=extent_raw,\n        cmap=\"jet\",\n        alpha=1,\n        origin=\"upper\",\n        vmin=vmin,\n        vmax=vmax,\n    )\n    cbar = plt.colorbar(im)\n    # cbar.set_label('(mg m$^{-3}$)', fontsize=16)\n    plt.title(f\"{structure_name}\", loc=\"left\", fontsize=20)\n    plt.savefig(png_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n    plt.show()\n\n    print(f\"\u2705 Saved {png_path}\")\n    print(f\"\u2705 Saved {npy_path} (for npy_to_tif)\")\n\n    # Return numpy array for direct use\n    return final_output\n</code></pre>"},{"location":"emit_utils/model_inference/#hypercoast.emit_utils.model_inference.npy_to_tif","title":"<code>npy_to_tif(npy_input, out_tif, resolution_m=30, method='linear', nodata_val=-9999.0, bbox_padding=0.0, lat_col=0, lon_col=1, band_cols=None, band_names=None, wavelengths=None, crs='EPSG:4326', compress='deflate', bigtiff='IF_SAFER')</code>","text":"<p>Convert scattered point data to a multi-band GeoTIFF.</p> <p>This function takes point data in the format [lat, lon, band1, band2, ...] and interpolates it onto a regular grid to create a georeferenced GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>npy_input</code> <code>Union[str, numpy.ndarray]</code> <p>Path to .npy file or numpy array of shape [N, M] where N is the number of points and M includes lat, lon, and band values.</p> required <code>out_tif</code> <code>str</code> <p>Output path for the GeoTIFF file.</p> required <code>resolution_m</code> <code>float</code> <p>Spatial resolution in meters.</p> <code>30</code> <code>method</code> <code>str</code> <p>Interpolation method ('linear', 'nearest', 'cubic').</p> <code>'linear'</code> <code>nodata_val</code> <code>float</code> <p>Value to use for NoData pixels.</p> <code>-9999.0</code> <code>bbox_padding</code> <code>float</code> <p>Padding to add to bounding box in degrees.</p> <code>0.0</code> <code>lat_col</code> <code>int</code> <p>Column index containing latitude values.</p> <code>0</code> <code>lon_col</code> <code>int</code> <p>Column index containing longitude values.</p> <code>1</code> <code>band_cols</code> <code>Union[int, List[int]]</code> <p>Column indices to rasterize as bands. If None, uses all columns except lat/lon.</p> <code>None</code> <code>band_names</code> <code>Optional[List[str]]</code> <p>Optional list of band description names.</p> <code>None</code> <code>wavelengths</code> <code>Optional[List[float]]</code> <p>Optional list of wavelengths for band descriptions (e.g., [440, 619]).</p> <code>None</code> <code>crs</code> <code>str</code> <p>Coordinate reference system string.</p> <code>'EPSG:4326'</code> <code>compress</code> <code>str</code> <p>Compression method for GeoTIFF.</p> <code>'deflate'</code> <code>bigtiff</code> <code>str</code> <p>BigTIFF creation option ('YES', 'NO', 'IF_NEEDED', 'IF_SAFER').</p> <code>'IF_SAFER'</code> <p>Exceptions:</p> Type Description <code>TypeError</code> <p>If npy_input is neither a path string nor numpy array.</p> <code>ValueError</code> <p>If input array has fewer than 3 columns or no bands are selected.</p> Source code in <code>hypercoast/emit_utils/model_inference.py</code> <pre><code>def npy_to_tif(\n    npy_input: Union[str, np.ndarray],\n    out_tif: str,\n    resolution_m: float = 30,\n    method: str = \"linear\",\n    nodata_val: float = -9999.0,\n    bbox_padding: float = 0.0,\n    lat_col: int = 0,\n    lon_col: int = 1,\n    band_cols: Optional[Union[int, List[int]]] = None,\n    band_names: Optional[List[str]] = None,\n    wavelengths: Optional[List[float]] = None,\n    crs: str = \"EPSG:4326\",\n    compress: str = \"deflate\",\n    bigtiff: str = \"IF_SAFER\",\n) -&gt; None:\n    \"\"\"Convert scattered point data to a multi-band GeoTIFF.\n\n    This function takes point data in the format [lat, lon, band1, band2, ...]\n    and interpolates it onto a regular grid to create a georeferenced GeoTIFF file.\n\n    Args:\n        npy_input: Path to .npy file or numpy array of shape [N, M] where N is\n            the number of points and M includes lat, lon, and band values.\n        out_tif: Output path for the GeoTIFF file.\n        resolution_m: Spatial resolution in meters.\n        method: Interpolation method ('linear', 'nearest', 'cubic').\n        nodata_val: Value to use for NoData pixels.\n        bbox_padding: Padding to add to bounding box in degrees.\n        lat_col: Column index containing latitude values.\n        lon_col: Column index containing longitude values.\n        band_cols: Column indices to rasterize as bands. If None, uses all columns\n            except lat/lon.\n        band_names: Optional list of band description names.\n        wavelengths: Optional list of wavelengths for band descriptions (e.g., [440, 619]).\n        crs: Coordinate reference system string.\n        compress: Compression method for GeoTIFF.\n        bigtiff: BigTIFF creation option ('YES', 'NO', 'IF_NEEDED', 'IF_SAFER').\n\n    Raises:\n        TypeError: If npy_input is neither a path string nor numpy array.\n        ValueError: If input array has fewer than 3 columns or no bands are selected.\n    \"\"\"\n\n    # --- 1) Load data ---\n    if isinstance(npy_input, str):\n        arr = np.load(npy_input)\n    elif isinstance(npy_input, np.ndarray):\n        arr = npy_input\n    else:\n        raise TypeError(\"npy_input must be either a path string or a numpy.ndarray.\")\n\n    if arr.ndim != 2 or arr.shape[1] &lt; 3:\n        raise ValueError(\"Input must be 2D with &gt;=3 columns (lat, lon, values...).\")\n\n    lat = arr[:, lat_col].astype(float)\n    lon = arr[:, lon_col].astype(float)\n\n    # --- 2) Band selection ---\n    if band_cols is None:\n        band_cols = [i for i in range(arr.shape[1]) if i not in (lat_col, lon_col)]\n    if isinstance(band_cols, (int, np.integer)):\n        band_cols = [int(band_cols)]\n    if len(band_cols) == 0:\n        raise ValueError(\"No value columns selected for bands.\")\n\n    # --- 3) Bounds (+ padding) ---\n    lat_min, lat_max = np.nanmin(lat), np.nanmax(lat)\n    lon_min, lon_max = np.nanmin(lon), np.nanmax(lon)\n    lat_min -= bbox_padding\n    lat_max += bbox_padding\n    lon_min -= bbox_padding\n    lon_max += bbox_padding\n\n    # --- 4) Resolution conversion ---\n    lat_center = (lat_min + lat_max) / 2.0\n    deg_per_m_lat = 1.0 / 111000.0\n    deg_per_m_lon = 1.0 / (111000.0 * np.cos(np.radians(lat_center)))\n    res_lat_deg = resolution_m * deg_per_m_lat\n    res_lon_deg = resolution_m * deg_per_m_lon\n\n    # --- 5) Grid ---\n    lon_axis = np.arange(lon_min, lon_max + res_lon_deg, res_lon_deg)\n    lat_axis = np.arange(lat_min, lat_max + res_lat_deg, res_lat_deg)\n    Lon, Lat = np.meshgrid(lon_axis, lat_axis)\n\n    transform = from_origin(lon_axis.min(), lat_axis.max(), res_lon_deg, res_lat_deg)\n\n    # --- 6) Interpolation ---\n    grids = []\n    for idx in band_cols:\n        vals = arr[:, idx].astype(float)\n\n        g = griddata(points=(lon, lat), values=vals, xi=(Lon, Lat), method=method)\n        if np.isnan(g).any():\n            g_near = griddata(\n                points=(lon, lat), values=vals, xi=(Lon, Lat), method=method\n            )\n            g = np.where(np.isnan(g), g_near, g)\n\n        grids.append(np.flipud(g).astype(np.float32))\n\n    data_stack = np.stack(grids, axis=0)\n\n    # --- 7) Write GeoTIFF ---\n    profile = {\n        \"driver\": \"GTiff\",\n        \"height\": data_stack.shape[1],\n        \"width\": data_stack.shape[2],\n        \"count\": data_stack.shape[0],\n        \"dtype\": rasterio.float32,\n        \"crs\": crs,\n        \"transform\": transform,\n        \"nodata\": nodata_val,\n        \"compress\": compress,\n        \"tiled\": True,\n        \"blockxsize\": 256,\n        \"blockysize\": 256,\n        \"BIGTIFF\": bigtiff,\n    }\n\n    os.makedirs(os.path.dirname(out_tif) or \".\", exist_ok=True)\n    with rasterio.open(out_tif, \"w\", **profile) as dst:\n        for b in range(data_stack.shape[0]):\n            band = data_stack[b]\n            band[~np.isfinite(band)] = nodata_val\n            dst.write(band, b + 1)\n\n        # Descriptions\n        n_bands = data_stack.shape[0]\n        if band_names is not None and len(band_names) == n_bands:\n            descriptions = list(map(str, band_names))\n        elif wavelengths is not None and len(wavelengths) == n_bands:\n            descriptions = [f\"aphy_{int(wl)}\" for wl in wavelengths]\n        else:\n            descriptions = [f\"band_{band_cols[b]}\" for b in range(n_bands)]\n\n        for b in range(1, n_bands + 1):\n            dst.set_band_description(b, descriptions[b - 1])\n\n    print(f\"\u2705 GeoTIFF saved: {out_tif}\")\n</code></pre>"},{"location":"emit_utils/model_inference/#hypercoast.emit_utils.model_inference.preprocess_emit_data_Robust","title":"<code>preprocess_emit_data_Robust(nc_path, scaler_Rrs, use_diff=False, full_band_wavelengths=None)</code>","text":"<p>Preprocess EMIT NetCDF data using robust scaling for model inference.</p> <p>This function reads EMIT L2 data from NetCDF, extracts specified wavelength bands, applies filtering and masking, optionally performs spectral smoothing and differencing, and returns a DataLoader ready for inference.</p> <p>Parameters:</p> Name Type Description Default <code>nc_path</code> <code>str</code> <p>Path to EMIT L2 NetCDF file.</p> required <code>scaler_Rrs</code> <code>Any</code> <p>Pre-fitted robust scaler from training.</p> required <code>use_diff</code> <code>bool</code> <p>Whether to apply Gaussian smoothing and first-order differencing.</p> <code>False</code> <code>full_band_wavelengths</code> <code>Optional[List[float]]</code> <p>List of wavelengths (nm) to extract. If a band is not present, the closest available band is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>test_loader</code> <p>DataLoader containing preprocessed data for inference. filtered_Rrs: Array of extracted Rrs bands with shape (H, W, B). mask: Boolean mask indicating valid pixels with shape (H, W). latitude: Array of latitude values. longitude: Array of longitude values.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If full_band_wavelengths is not provided or no Rrs bands are found.</p> Source code in <code>hypercoast/emit_utils/model_inference.py</code> <pre><code>def preprocess_emit_data_Robust(\n    nc_path: str,\n    scaler_Rrs: Any,\n    use_diff: bool = False,\n    full_band_wavelengths: Optional[List[float]] = None,\n) -&gt; Tuple[DataLoader, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Preprocess EMIT NetCDF data using robust scaling for model inference.\n\n    This function reads EMIT L2 data from NetCDF, extracts specified wavelength\n    bands, applies filtering and masking, optionally performs spectral smoothing\n    and differencing, and returns a DataLoader ready for inference.\n\n    Args:\n        nc_path: Path to EMIT L2 NetCDF file.\n        scaler_Rrs: Pre-fitted robust scaler from training.\n        use_diff: Whether to apply Gaussian smoothing and first-order differencing.\n        full_band_wavelengths: List of wavelengths (nm) to extract. If a band is\n            not present, the closest available band is used.\n\n    Returns:\n        test_loader: DataLoader containing preprocessed data for inference.\n        filtered_Rrs: Array of extracted Rrs bands with shape (H, W, B).\n        mask: Boolean mask indicating valid pixels with shape (H, W).\n        latitude: Array of latitude values.\n        longitude: Array of longitude values.\n\n    Raises:\n        ValueError: If full_band_wavelengths is not provided or no Rrs bands are found.\n    \"\"\"\n\n    if full_band_wavelengths is None:\n        raise ValueError(\n            \"full_band_wavelengths must be provided to match EMIT Rrs bands\"\n        )\n\n    def find_closest_band(target, available_bands):\n        rrs_bands = [b for b in available_bands if b.startswith(\"Rrs_\")]\n        available_waves = [int(b.split(\"_\")[1]) for b in rrs_bands]\n        if not available_waves:\n            raise ValueError(\"\u274c No Rrs_* bands found in dataset\")\n        closest_wave = min(available_waves, key=lambda w: abs(w - target))\n        return f\"Rrs_{closest_wave}\"\n\n    dataset = Dataset(nc_path)\n    latitude = dataset.variables[\"lat\"][:]\n    longitude = dataset.variables[\"lon\"][:]\n\n    all_vars = dataset.variables.keys()\n\n    bands_to_extract = []\n    for w in full_band_wavelengths:\n        band_name = f\"Rrs_{int(w)}\"\n        if band_name in all_vars:\n            bands_to_extract.append(band_name)\n        else:\n            closest = find_closest_band(int(w), all_vars)\n            print(f\"\u26a0\ufe0f {band_name} does not exist, using the closest band {closest}\")\n            bands_to_extract.append(closest)\n    filtered_Rrs = np.array([dataset.variables[band][:] for band in bands_to_extract])\n    filtered_Rrs = np.moveaxis(filtered_Rrs, 0, -1)\n\n    mask = np.all(~np.isnan(filtered_Rrs), axis=2)\n\n    target_443 = (\n        f\"Rrs_443\"\n        if \"Rrs_443\" in bands_to_extract\n        else find_closest_band(443, bands_to_extract)\n    )\n    target_560 = (\n        f\"Rrs_560\"\n        if \"Rrs_560\" in bands_to_extract\n        else find_closest_band(560, bands_to_extract)\n    )\n\n    print(f\"Using {target_443} and {target_560} for mask check.\")\n\n    idx_443 = bands_to_extract.index(target_443)\n    idx_560 = bands_to_extract.index(target_560)\n    mask &amp;= filtered_Rrs[:, :, idx_443] &lt;= filtered_Rrs[:, :, idx_560]\n\n    valid_test_data = filtered_Rrs[mask]\n\n    # ---- smooth + diff\n    if use_diff:\n        from scipy.ndimage import gaussian_filter1d\n\n        Rrs_smoothed = np.array(\n            [gaussian_filter1d(spectrum, sigma=1) for spectrum in valid_test_data]\n        )\n        Rrs_processed = np.diff(Rrs_smoothed, axis=1)\n        print(\"\u2705 [5] Performed Gaussian smoothing + first-order differencing\")\n    else:\n        Rrs_processed = valid_test_data\n        print(\"\u2705 [5] Smoothing and differencing not enabled\")\n\n    # ---- normalize\n    Rrs_normalized = scaler_Rrs.transform(\n        torch.tensor(Rrs_processed, dtype=torch.float32)\n    ).numpy()\n\n    # ---- DataLoader\n    test_tensor = TensorDataset(torch.tensor(Rrs_normalized).float())\n    test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n    print(\"\u2705 [6] DataLoader construction completed\")\n\n    return test_loader, filtered_Rrs, mask, latitude, longitude\n</code></pre>"},{"location":"emit_utils/model_inference/#hypercoast.emit_utils.model_inference.preprocess_emit_data_minmax","title":"<code>preprocess_emit_data_minmax(nc_path, full_band_wavelengths=None, diff_before_norm=False, diff_after_norm=False)</code>","text":"<p>Preprocess EMIT NetCDF data using sample-wise MinMax scaling for inference.</p> <p>This function reads EMIT L2 data from NetCDF, extracts specified wavelength bands, applies filtering and masking, optionally performs spectral smoothing and differencing, applies sample-wise MinMax normalization, and returns a DataLoader ready for inference.</p> <p>Parameters:</p> Name Type Description Default <code>nc_path</code> <code>str</code> <p>Path to EMIT L2 NetCDF file.</p> required <code>full_band_wavelengths</code> <code>Optional[List[float]]</code> <p>List of wavelengths (nm) to extract. If a band is not present, the closest available band is used.</p> <code>None</code> <code>diff_before_norm</code> <code>bool</code> <p>Whether to apply differencing before normalization.</p> <code>False</code> <code>diff_after_norm</code> <code>bool</code> <p>Whether to apply differencing after normalization.</p> <code>False</code> <p>Returns:</p> Type Description <code>test_loader</code> <p>DataLoader containing preprocessed data for inference. Rrs: Array of extracted Rrs bands with shape (H, W, B). mask: Boolean mask indicating valid pixels with shape (H, W). latitude: Array of latitude values. longitude: Array of longitude values. Returns None if an error occurs during processing.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If full_band_wavelengths is empty, no Rrs bands are found, or no valid pixels pass filtering.</p> Source code in <code>hypercoast/emit_utils/model_inference.py</code> <pre><code>def preprocess_emit_data_minmax(\n    nc_path: str,\n    full_band_wavelengths: Optional[List[float]] = None,\n    diff_before_norm: bool = False,\n    diff_after_norm: bool = False,\n) -&gt; Optional[Tuple[DataLoader, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"Preprocess EMIT NetCDF data using sample-wise MinMax scaling for inference.\n\n    This function reads EMIT L2 data from NetCDF, extracts specified wavelength\n    bands, applies filtering and masking, optionally performs spectral smoothing\n    and differencing, applies sample-wise MinMax normalization, and returns a\n    DataLoader ready for inference.\n\n    Args:\n        nc_path: Path to EMIT L2 NetCDF file.\n        full_band_wavelengths: List of wavelengths (nm) to extract. If a band is\n            not present, the closest available band is used.\n        diff_before_norm: Whether to apply differencing before normalization.\n        diff_after_norm: Whether to apply differencing after normalization.\n\n    Returns:\n        test_loader: DataLoader containing preprocessed data for inference.\n        Rrs: Array of extracted Rrs bands with shape (H, W, B).\n        mask: Boolean mask indicating valid pixels with shape (H, W).\n        latitude: Array of latitude values.\n        longitude: Array of longitude values.\n        Returns None if an error occurs during processing.\n\n    Raises:\n        ValueError: If full_band_wavelengths is empty, no Rrs bands are found,\n            or no valid pixels pass filtering.\n    \"\"\"\n    print(f\"\ud83d\udce5 Start processing: {nc_path}\")\n\n    # ---- sanity checks\n    if full_band_wavelengths is None or len(full_band_wavelengths) == 0:\n        raise ValueError(\n            \"A non-empty full_band_wavelengths must be provided (e.g., [400, 402, ...]).\"\n        )\n\n    full_band_wavelengths = [int(w) for w in full_band_wavelengths]\n\n    try:\n        with Dataset(nc_path) as dataset:\n            latitude = dataset.variables[\"lat\"][:]\n            longitude = dataset.variables[\"lon\"][:]\n            all_vars = set(dataset.variables.keys())\n            available_wavelengths = [\n                float(v.split(\"_\")[1]) for v in all_vars if v.startswith(\"Rrs_\")\n            ]\n\n            def find_closest_band(target_nm: float):\n                nearest = min(available_wavelengths, key=lambda w: abs(w - target_nm))\n                return f\"Rrs_{int(nearest)}\"\n\n            # Search according to full_band_wavelengths\n            bands_to_extract = []\n            for w in full_band_wavelengths:\n                band_name = f\"Rrs_{w}\"\n                if band_name in all_vars:\n                    bands_to_extract.append(band_name)\n                else:\n                    closest = find_closest_band(w)\n                    print(\n                        f\"\u26a0\ufe0f {band_name} does not exist, using the closest band {closest}\"\n                    )\n                    bands_to_extract.append(closest)\n\n            seen = set()\n            bands_to_extract = [\n                b for b in bands_to_extract if not (b in seen or seen.add(b))\n            ]\n\n            if len(bands_to_extract) == 0:\n                raise ValueError(\"\u274c No usable Rrs_* bands found in the file.\")\n            # ---- read and stack to (H, W, B)\n            # Each variable expected shape: (lat, lon) or (y, x)\n            Rrs_stack = []\n            for band in bands_to_extract:\n                arr = dataset.variables[band][:]  # (H, W)\n                Rrs_stack.append(arr)\n\n            Rrs = np.array(Rrs_stack)  # (B, H, W)\n            Rrs = np.moveaxis(Rrs, 0, -1)  # (H, W, B)\n            filtered_Rrs = Rrs  # keep naming consistent with your previous return\n\n            # ---- build mask using 440 &amp; 560 (or nearest present within your requested list)\n            have_waves = [int(b.split(\"_\")[1]) for b in bands_to_extract]\n\n            def nearest_idx(target_nm: int):\n                # find nearest *among bands_to_extract*\n                nearest_w = min(have_waves, key=lambda w: abs(w - target_nm))\n                return bands_to_extract.index(f\"Rrs_{nearest_w}\")\n\n            # Prefer exact if available; otherwise nearest in the user-requested set\n            idx_440 = (\n                bands_to_extract.index(\"Rrs_440\")\n                if \"Rrs_440\" in bands_to_extract\n                else nearest_idx(440)\n            )\n            idx_560 = (\n                bands_to_extract.index(\"Rrs_560\")\n                if \"Rrs_560\" in bands_to_extract\n                else nearest_idx(560)\n            )\n\n            print(\n                f\"\u2705 Bands used for mask check: {bands_to_extract[idx_440]} and {bands_to_extract[idx_560]}\"\n            )\n\n            mask_nanfree = np.all(~np.isnan(filtered_Rrs), axis=2)\n            mask_condition = filtered_Rrs[:, :, idx_560] &gt;= filtered_Rrs[:, :, idx_440]\n            mask = mask_nanfree &amp; mask_condition\n            print(f\"\u2705 [4] Built valid mask, remaining pixels: {int(np.sum(mask))}\")\n\n            if not np.any(mask):\n                raise ValueError(\"\u274c No valid pixels passed the filtering.\")\n\n            valid_test_data = filtered_Rrs[mask]  # (N, B)\n\n        # === Check whether smoothing is needed (only executed if any differencing is enabled) ===\n        if diff_before_norm or diff_after_norm:\n            from scipy.ndimage import gaussian_filter1d\n\n            Rrs_smoothed = np.array(\n                [gaussian_filter1d(spectrum, sigma=1) for spectrum in valid_test_data]\n            )\n            print(\"\u2705 Gaussian smoothing applied\")\n        else:\n            Rrs_smoothed = valid_test_data\n            print(\"\u2705 Smoothing not enabled\")\n\n        # === Preprocessing before differencing ===\n        if diff_before_norm:\n            Rrs_preprocessed = np.diff(Rrs_smoothed, axis=1)\n            print(\"\u2705 Preprocessing before differencing completed\")\n        else:\n            Rrs_preprocessed = Rrs_smoothed\n            print(\"\u2705 Preprocessing before differencing not enabled\")\n\n        # === MinMax normalization to [1, 10] ===\n        scalers = [MinMaxScaler((1, 10)) for _ in range(Rrs_preprocessed.shape[0])]\n        Rrs_normalized = np.array(\n            [\n                scalers[i].fit_transform(row.reshape(-1, 1)).flatten()\n                for i, row in enumerate(Rrs_preprocessed)\n            ]\n        )\n\n        # === Post-processing after differencing ===\n        if diff_after_norm:\n            Rrs_normalized = np.diff(Rrs_normalized, axis=1)\n            print(\"\u2705 Post-processing after differencing completed\")\n        else:\n            print(\"\u2705 Post-processing after differencing not enabled\")\n\n        # === Construct DataLoader\n        test_tensor = TensorDataset(torch.tensor(Rrs_normalized).float())\n        test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n\n        return test_loader, Rrs, mask, latitude, longitude\n\n    except Exception as e:\n        print(f\"\u274c [ERROR] Failed to process file {nc_path}: {e}\")\n        return None\n</code></pre>"},{"location":"emit_utils/moe_vae/","title":"moe_vae module","text":"<p>Mixture of Experts Variational Autoencoder (MoE-VAE) models.</p> <p>This module implements VAE and MoE-VAE architectures for water quality parameter estimation from hyperspectral remote sensing data. It includes sparse dispatching for efficient expert routing and training/evaluation utilities.</p>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.MoE_VAE","title":"<code> MoE_VAE            (LightningModule)         </code>","text":"<p>Mixture of Experts Variational Autoencoder for water quality estimation.</p> <p>This class implements a sparsely-gated MoE architecture where multiple VAE experts specialize in different regions of the input space. A learned gating network dynamically routes each input to the top-k most relevant experts.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features (spectral bands).</p> required <code>output_dim</code> <code>int</code> <p>Number of output features (water quality parameters).</p> required <code>latent_dim</code> <code>int</code> <p>Dimension of the latent space for each expert VAE.</p> required <code>encoder_hidden_dims</code> <code>List[int]</code> <p>List of hidden layer dimensions for encoders.</p> required <code>decoder_hidden_dims</code> <code>List[int]</code> <p>List of hidden layer dimensions for decoders.</p> required <code>num_experts</code> <code>int</code> <p>Total number of expert VAEs.</p> required <code>k</code> <code>int</code> <p>Number of experts to activate for each input sample.</p> <code>4</code> <code>activation</code> <code>str</code> <p>Activation function ('relu', 'tanh', 'sigmoid', 'leakyrelu').</p> <code>'leakyrelu'</code> <code>noisy_gating</code> <code>bool</code> <p>Whether to add noise to gating for exploration during training.</p> <code>True</code> <code>use_norm</code> <code>Union[str, bool]</code> <p>Type of normalization ('batch', 'layer', 'group', or False).</p> <code>False</code> <code>use_dropout</code> <code>bool</code> <p>Whether to use dropout regularization.</p> <code>False</code> <code>use_softplus_output</code> <code>bool</code> <p>Whether to apply softplus activation to output.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>class MoE_VAE(LightningModule):\n    \"\"\"Mixture of Experts Variational Autoencoder for water quality estimation.\n\n    This class implements a sparsely-gated MoE architecture where multiple VAE\n    experts specialize in different regions of the input space. A learned gating\n    network dynamically routes each input to the top-k most relevant experts.\n\n    Args:\n        input_dim: Number of input features (spectral bands).\n        output_dim: Number of output features (water quality parameters).\n        latent_dim: Dimension of the latent space for each expert VAE.\n        encoder_hidden_dims: List of hidden layer dimensions for encoders.\n        decoder_hidden_dims: List of hidden layer dimensions for decoders.\n        num_experts: Total number of expert VAEs.\n        k: Number of experts to activate for each input sample.\n        activation: Activation function ('relu', 'tanh', 'sigmoid', 'leakyrelu').\n        noisy_gating: Whether to add noise to gating for exploration during training.\n        use_norm: Type of normalization ('batch', 'layer', 'group', or False).\n        use_dropout: Whether to use dropout regularization.\n        use_softplus_output: Whether to apply softplus activation to output.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        latent_dim: int,\n        encoder_hidden_dims: List[int],\n        decoder_hidden_dims: List[int],\n        num_experts: int,\n        k: int = 4,\n        activation: str = \"leakyrelu\",\n        noisy_gating: bool = True,\n        use_norm: Union[str, bool] = False,\n        use_dropout: bool = False,\n        use_softplus_output: bool = False,\n        **kwargs,\n    ):\n        super(MoE_VAE, self).__init__()\n        self.noisy_gating = noisy_gating\n        self.num_experts = num_experts\n        self.output_dim = output_dim\n        self.input_dim = input_dim\n        self.latent_dim = latent_dim\n        self.encoder_hidden_dims = encoder_hidden_dims\n        self.decoder_hidden_dims = decoder_hidden_dims\n        self.num_experts = num_experts\n        self.k = k\n        self.activation = activation\n        self.use_norm = use_norm\n        self.use_dropout = use_dropout\n        self.use_softplus_output = use_softplus_output\n\n        # instantiate experts\n        self.experts = nn.ModuleList(\n            [\n                VAE(\n                    self.input_dim,\n                    self.output_dim,\n                    self.latent_dim,\n                    self.encoder_hidden_dims,\n                    self.decoder_hidden_dims,\n                    self.activation,\n                    use_norm=self.use_norm,\n                    use_dropout=self.use_dropout,\n                    use_softplus_output=self.use_softplus_output,\n                )\n                for i in range(self.num_experts)\n            ]\n        )\n\n        self.w_gate = nn.Parameter(\n            torch.zeros(input_dim, num_experts, dtype=self.dtype), requires_grad=True\n        )\n        self.w_noise = nn.Parameter(\n            torch.zeros(input_dim, num_experts, dtype=self.dtype), requires_grad=True\n        )\n\n        self.softplus = nn.Softplus()\n        self.softmax = nn.Softmax(1)\n        self.register_buffer(\"mean\", torch.tensor([0.0]))\n        self.register_buffer(\"std\", torch.tensor([1.0]))\n        self.batch_gates = None\n\n        assert self.k &lt;= self.num_experts\n\n    def forward(\n        self, x: torch.Tensor, moe_weight: float = 1e-2\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Forward pass through the MoE-VAE model.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim).\n            moe_weight: Weight for the MoE load balancing loss.\n\n        Returns:\n            Dictionary containing:\n                - 'pred_y': Predicted output tensor (batch_size, output_dim).\n                - 'moe_loss': Load balancing loss to encourage uniform expert usage.\n        \"\"\"\n        gates, load = self.noisy_top_k_gating(x, self.training)\n        self.batch_gates = gates\n        # calculate importance loss\n        importance = gates.sum(0)\n\n        moe_loss = moe_weight * self.cv_squared(\n            importance\n        ) + moe_weight * self.cv_squared(load)\n\n        dispatcher = SparseDispatcher(self.num_experts, gates)\n        expert_inputs = dispatcher.dispatch(x)\n        gates = dispatcher.expert_to_gates()\n        expert_outputs = []\n        for i in range(self.num_experts):\n            input_i = expert_inputs[i]\n            if input_i.shape[0] &gt; 1:\n                expert_outputs.append(self.experts[i](input_i)[\"pred_y\"])\n            else:\n                expert_outputs.append(\n                    torch.zeros(\n                        (input_i.shape[0], self.output_dim), device=input_i.device\n                    )\n                )\n        pred_y = dispatcher.combine(expert_outputs)\n        return {\"pred_y\": pred_y, \"moe_loss\": moe_loss}\n\n    def loss_fn(self, output_dict: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Compute MoE-VAE loss including reconstruction and load balancing terms.\n\n        Args:\n            output_dict: Dictionary containing 'pred_y', 'y', and 'moe_loss'.\n\n        Returns:\n            Dictionary containing 'total_loss', 'mae_loss', 'mse_loss', and 'moe_loss'.\n        \"\"\"\n        pred_y = output_dict[\"pred_y\"]\n        y = output_dict[\"y\"]\n        batch_size = y.shape[0]\n        MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n        mse_losss = F.mse_loss(pred_y, y, reduction=\"mean\")\n        moe_loss = output_dict.get(\n            \"moe_loss\", torch.tensor(0.0, device=pred_y.device, dtype=pred_y.dtype)\n        )\n        total_loss = MAE + moe_loss\n        return {\n            \"total_loss\": total_loss,\n            \"mae_loss\": MAE,\n            \"mse_loss\": mse_losss,\n            \"moe_loss\": moe_loss,\n        }\n\n    def get_batch_gates(self) -&gt; Optional[torch.Tensor]:\n        \"\"\"Get the gating weights from the most recent forward pass.\n\n        Returns:\n            Tensor of shape (batch_size, num_experts) or None if forward hasn't been called.\n        \"\"\"\n        return self.batch_gates\n\n    def cv_squared(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute squared coefficient of variation for load balancing.\n\n        This metric encourages uniform distribution across experts by penalizing\n        high variance in expert usage.\n\n        Args:\n            x: Input tensor (e.g., expert loads or importance weights).\n\n        Returns:\n            Squared coefficient of variation scalar.\n        \"\"\"\n        eps = 1e-10\n        # if only num_experts = 1\n\n        if x.shape[0] == 1:\n            return torch.tensor([0], device=x.device, dtype=x.dtype)\n        return x.float().var() / (x.float().mean() ** 2 + eps)\n\n    def _gates_to_load(self, gates: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the load (number of samples) assigned to each expert.\n\n        Args:\n            gates: Gating tensor of shape (batch_size, num_experts).\n\n        Returns:\n            Load tensor of shape (num_experts,) with counts of assigned samples.\n        \"\"\"\n        return (gates &gt; 0).sum(0)\n\n    def _prob_in_top_k(\n        self,\n        clean_values: torch.Tensor,\n        noisy_values: torch.Tensor,\n        noise_stddev: torch.Tensor,\n        noisy_top_values: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute probability that each value is in top-k after adding noise.\n\n        This is used for load balancing during training by computing differentiable\n        probabilities of expert selection.\n\n        Args:\n            clean_values: Clean logits of shape (batch, num_experts).\n            noisy_values: Noisy logits of shape (batch, num_experts).\n            noise_stddev: Noise standard deviations of shape (batch, num_experts).\n            noisy_top_values: Top-k noisy values of shape (batch, k+1).\n\n        Returns:\n            Probabilities of shape (batch, num_experts).\n        \"\"\"\n        batch = clean_values.size(0)\n        m = noisy_top_values.size(1)\n        top_values_flat = noisy_top_values.flatten()\n\n        threshold_positions_if_in = (\n            torch.arange(batch, device=clean_values.device) * m + self.k\n        )\n        threshold_if_in = torch.unsqueeze(\n            torch.gather(top_values_flat, 0, threshold_positions_if_in), 1\n        )\n        is_in = torch.gt(noisy_values, threshold_if_in)\n        threshold_positions_if_out = threshold_positions_if_in - 1\n        threshold_if_out = torch.unsqueeze(\n            torch.gather(top_values_flat, 0, threshold_positions_if_out), 1\n        )\n        # is each value currently in the top k.\n        normal = Normal(self.mean, self.std)\n        prob_if_in = normal.cdf((clean_values - threshold_if_in) / noise_stddev)\n        prob_if_out = normal.cdf((clean_values - threshold_if_out) / noise_stddev)\n        prob = torch.where(is_in, prob_if_in, prob_if_out)\n        return prob\n\n    def noisy_top_k_gating(\n        self, x: torch.Tensor, train: bool, noise_epsilon: float = 1e-2\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Compute noisy top-k gating for expert selection.\n\n        Implements the gating mechanism from \"Outrageously Large Neural Networks:\n        The Sparsely-Gated Mixture-of-Experts Layer\" (https://arxiv.org/abs/1701.06538).\n        Adds tunable noise during training for exploration.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim).\n            train: Whether model is in training mode (adds noise if True).\n            noise_epsilon: Small constant for numerical stability.\n\n        Returns:\n            gates: Sparse gating weights of shape (batch_size, num_experts).\n            load: Load assigned to each expert of shape (num_experts,).\n        \"\"\"\n        clean_logits = x @ self.w_gate\n        if self.noisy_gating and train:\n            raw_noise_stddev = x @ self.w_noise\n            noise_stddev = self.softplus(raw_noise_stddev) + noise_epsilon\n            noisy_logits = clean_logits + (\n                torch.randn_like(clean_logits) * noise_stddev\n            )\n            logits = noisy_logits\n        else:\n            logits = clean_logits\n            # Add this safety check to ensure we always have at least one expert selected\n        if (logits.sum(dim=1) == 0).any():\n            # Add a small positive value to ensure we have non-zero logits\n            logits = logits + 1e-5\n\n        # calculate topk + 1 that will be needed for the noisy gates\n        top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n        top_k_logits = top_logits[:, : self.k]\n        top_k_indices = top_indices[:, : self.k]\n        top_k_gates = self.softmax(top_k_logits)\n\n        zeros = torch.zeros_like(logits, requires_grad=True, dtype=self.dtype)\n        gates = zeros.scatter(1, top_k_indices, top_k_gates)\n\n        # Safety check - ensure at least one expert is selected per sample\n        if (gates.sum(dim=1) &lt; 1e-6).any():\n            # Force selection of the top expert for samples with no experts\n            problematic_samples = (gates.sum(dim=1) &lt; 1e-6).nonzero().squeeze(1)\n            if problematic_samples.numel() &gt; 0:  # If there are problematic samples\n                # Select the top expert for these samples\n                top_expert = top_indices[problematic_samples, 0]\n                # Set a minimum value for the gate\n                gates[problematic_samples, top_expert] = 0.1\n\n        if self.noisy_gating and self.k &lt; self.num_experts and train:\n            load = (\n                self._prob_in_top_k(\n                    clean_logits, noisy_logits, noise_stddev, top_logits\n                )\n            ).sum(0)\n        else:\n            load = self._gates_to_load(gates)\n        return gates, load\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.MoE_VAE.cv_squared","title":"<code>cv_squared(self, x)</code>","text":"<p>Compute squared coefficient of variation for load balancing.</p> <p>This metric encourages uniform distribution across experts by penalizing high variance in expert usage.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor (e.g., expert loads or importance weights).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Squared coefficient of variation scalar.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def cv_squared(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute squared coefficient of variation for load balancing.\n\n    This metric encourages uniform distribution across experts by penalizing\n    high variance in expert usage.\n\n    Args:\n        x: Input tensor (e.g., expert loads or importance weights).\n\n    Returns:\n        Squared coefficient of variation scalar.\n    \"\"\"\n    eps = 1e-10\n    # if only num_experts = 1\n\n    if x.shape[0] == 1:\n        return torch.tensor([0], device=x.device, dtype=x.dtype)\n    return x.float().var() / (x.float().mean() ** 2 + eps)\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.MoE_VAE.forward","title":"<code>forward(self, x, moe_weight=0.01)</code>","text":"<p>Forward pass through the MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <code>moe_weight</code> <code>float</code> <p>Weight for the MoE load balancing loss.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Dictionary containing</code> <ul> <li>'pred_y': Predicted output tensor (batch_size, output_dim).<ul> <li>'moe_loss': Load balancing loss to encourage uniform expert usage.</li> </ul> </li> </ul> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, moe_weight: float = 1e-2\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Forward pass through the MoE-VAE model.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_dim).\n        moe_weight: Weight for the MoE load balancing loss.\n\n    Returns:\n        Dictionary containing:\n            - 'pred_y': Predicted output tensor (batch_size, output_dim).\n            - 'moe_loss': Load balancing loss to encourage uniform expert usage.\n    \"\"\"\n    gates, load = self.noisy_top_k_gating(x, self.training)\n    self.batch_gates = gates\n    # calculate importance loss\n    importance = gates.sum(0)\n\n    moe_loss = moe_weight * self.cv_squared(\n        importance\n    ) + moe_weight * self.cv_squared(load)\n\n    dispatcher = SparseDispatcher(self.num_experts, gates)\n    expert_inputs = dispatcher.dispatch(x)\n    gates = dispatcher.expert_to_gates()\n    expert_outputs = []\n    for i in range(self.num_experts):\n        input_i = expert_inputs[i]\n        if input_i.shape[0] &gt; 1:\n            expert_outputs.append(self.experts[i](input_i)[\"pred_y\"])\n        else:\n            expert_outputs.append(\n                torch.zeros(\n                    (input_i.shape[0], self.output_dim), device=input_i.device\n                )\n            )\n    pred_y = dispatcher.combine(expert_outputs)\n    return {\"pred_y\": pred_y, \"moe_loss\": moe_loss}\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.MoE_VAE.get_batch_gates","title":"<code>get_batch_gates(self)</code>","text":"<p>Get the gating weights from the most recent forward pass.</p> <p>Returns:</p> Type Description <code>Optional[torch.Tensor]</code> <p>Tensor of shape (batch_size, num_experts) or None if forward hasn't been called.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def get_batch_gates(self) -&gt; Optional[torch.Tensor]:\n    \"\"\"Get the gating weights from the most recent forward pass.\n\n    Returns:\n        Tensor of shape (batch_size, num_experts) or None if forward hasn't been called.\n    \"\"\"\n    return self.batch_gates\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.MoE_VAE.loss_fn","title":"<code>loss_fn(self, output_dict)</code>","text":"<p>Compute MoE-VAE loss including reconstruction and load balancing terms.</p> <p>Parameters:</p> Name Type Description Default <code>output_dict</code> <code>Dict[str, torch.Tensor]</code> <p>Dictionary containing 'pred_y', 'y', and 'moe_loss'.</p> required <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Dictionary containing 'total_loss', 'mae_loss', 'mse_loss', and 'moe_loss'.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def loss_fn(self, output_dict: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Compute MoE-VAE loss including reconstruction and load balancing terms.\n\n    Args:\n        output_dict: Dictionary containing 'pred_y', 'y', and 'moe_loss'.\n\n    Returns:\n        Dictionary containing 'total_loss', 'mae_loss', 'mse_loss', and 'moe_loss'.\n    \"\"\"\n    pred_y = output_dict[\"pred_y\"]\n    y = output_dict[\"y\"]\n    batch_size = y.shape[0]\n    MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n    mse_losss = F.mse_loss(pred_y, y, reduction=\"mean\")\n    moe_loss = output_dict.get(\n        \"moe_loss\", torch.tensor(0.0, device=pred_y.device, dtype=pred_y.dtype)\n    )\n    total_loss = MAE + moe_loss\n    return {\n        \"total_loss\": total_loss,\n        \"mae_loss\": MAE,\n        \"mse_loss\": mse_losss,\n        \"moe_loss\": moe_loss,\n    }\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.MoE_VAE.noisy_top_k_gating","title":"<code>noisy_top_k_gating(self, x, train, noise_epsilon=0.01)</code>","text":"<p>Compute noisy top-k gating for expert selection.</p> <p>Implements the gating mechanism from \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\" (https://arxiv.org/abs/1701.06538). Adds tunable noise during training for exploration.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <code>train</code> <code>bool</code> <p>Whether model is in training mode (adds noise if True).</p> required <code>noise_epsilon</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>gates</code> <p>Sparse gating weights of shape (batch_size, num_experts). load: Load assigned to each expert of shape (num_experts,).</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def noisy_top_k_gating(\n    self, x: torch.Tensor, train: bool, noise_epsilon: float = 1e-2\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute noisy top-k gating for expert selection.\n\n    Implements the gating mechanism from \"Outrageously Large Neural Networks:\n    The Sparsely-Gated Mixture-of-Experts Layer\" (https://arxiv.org/abs/1701.06538).\n    Adds tunable noise during training for exploration.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_dim).\n        train: Whether model is in training mode (adds noise if True).\n        noise_epsilon: Small constant for numerical stability.\n\n    Returns:\n        gates: Sparse gating weights of shape (batch_size, num_experts).\n        load: Load assigned to each expert of shape (num_experts,).\n    \"\"\"\n    clean_logits = x @ self.w_gate\n    if self.noisy_gating and train:\n        raw_noise_stddev = x @ self.w_noise\n        noise_stddev = self.softplus(raw_noise_stddev) + noise_epsilon\n        noisy_logits = clean_logits + (\n            torch.randn_like(clean_logits) * noise_stddev\n        )\n        logits = noisy_logits\n    else:\n        logits = clean_logits\n        # Add this safety check to ensure we always have at least one expert selected\n    if (logits.sum(dim=1) == 0).any():\n        # Add a small positive value to ensure we have non-zero logits\n        logits = logits + 1e-5\n\n    # calculate topk + 1 that will be needed for the noisy gates\n    top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n    top_k_logits = top_logits[:, : self.k]\n    top_k_indices = top_indices[:, : self.k]\n    top_k_gates = self.softmax(top_k_logits)\n\n    zeros = torch.zeros_like(logits, requires_grad=True, dtype=self.dtype)\n    gates = zeros.scatter(1, top_k_indices, top_k_gates)\n\n    # Safety check - ensure at least one expert is selected per sample\n    if (gates.sum(dim=1) &lt; 1e-6).any():\n        # Force selection of the top expert for samples with no experts\n        problematic_samples = (gates.sum(dim=1) &lt; 1e-6).nonzero().squeeze(1)\n        if problematic_samples.numel() &gt; 0:  # If there are problematic samples\n            # Select the top expert for these samples\n            top_expert = top_indices[problematic_samples, 0]\n            # Set a minimum value for the gate\n            gates[problematic_samples, top_expert] = 0.1\n\n    if self.noisy_gating and self.k &lt; self.num_experts and train:\n        load = (\n            self._prob_in_top_k(\n                clean_logits, noisy_logits, noise_stddev, top_logits\n            )\n        ).sum(0)\n    else:\n        load = self._gates_to_load(gates)\n    return gates, load\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.SparseDispatcher","title":"<code> SparseDispatcher        </code>","text":"<p>Helper for implementing sparse mixture of experts routing.</p> <p>This class efficiently dispatches inputs to different experts based on gating weights and combines their outputs. It leverages sparsity by only sending batch elements to experts with non-zero gate values.</p> <p>The dispatcher performs two key operations: 1. dispatch(): Distribute input samples to appropriate experts. 2. combine(): Aggregate expert outputs weighted by gate values.</p> <p>Examples:</p> <p>gates = torch.tensor([[0.7, 0.3, 0.0], [0.0, 0.5, 0.5]])  # (batch=2, experts=3) dispatcher = SparseDispatcher(num_experts=3, gates=gates) expert_inputs = dispatcher.dispatch(inputs) expert_outputs = [expert(expert_inputs[i]) for i, expert in enumerate(experts)] combined_output = dispatcher.combine(expert_outputs)</p> <p>Parameters:</p> Name Type Description Default <code>num_experts</code> <code>int</code> <p>Total number of experts.</p> required <code>gates</code> <code>Tensor</code> <p>Tensor of shape (batch_size, num_experts) with gating weights. Non-zero values indicate which experts receive which samples.</p> required Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>class SparseDispatcher(object):\n    \"\"\"Helper for implementing sparse mixture of experts routing.\n\n    This class efficiently dispatches inputs to different experts based on\n    gating weights and combines their outputs. It leverages sparsity by only\n    sending batch elements to experts with non-zero gate values.\n\n    The dispatcher performs two key operations:\n    1. dispatch(): Distribute input samples to appropriate experts.\n    2. combine(): Aggregate expert outputs weighted by gate values.\n\n    Example:\n        gates = torch.tensor([[0.7, 0.3, 0.0], [0.0, 0.5, 0.5]])  # (batch=2, experts=3)\n        dispatcher = SparseDispatcher(num_experts=3, gates=gates)\n        expert_inputs = dispatcher.dispatch(inputs)\n        expert_outputs = [expert(expert_inputs[i]) for i, expert in enumerate(experts)]\n        combined_output = dispatcher.combine(expert_outputs)\n\n    Args:\n        num_experts: Total number of experts.\n        gates: Tensor of shape (batch_size, num_experts) with gating weights.\n            Non-zero values indicate which experts receive which samples.\n    \"\"\"\n\n    def __init__(self, num_experts: int, gates: torch.Tensor):\n        \"\"\"Create a SparseDispatcher.\"\"\"\n        self._gates = gates\n        self._num_experts = num_experts\n\n        # Safety check: ensure at least one example per expert\n        if (gates.sum(dim=0) == 0).any():\n            # Find experts with no assignments and create dummy assignments\n            empty_experts = (gates.sum(dim=0) == 0).nonzero().squeeze(1)\n            if empty_experts.numel() &gt; 0:\n                # Assign the first example to all empty experts with a small weight\n                for expert_idx in empty_experts:\n                    gates[0, expert_idx] = 1e-5\n\n        # Sort experts\n        sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n        # Drop indices\n        _, self._expert_index = sorted_experts.split(1, dim=1)\n        # Get according batch index for each expert\n        self._batch_index = torch.nonzero(gates)[index_sorted_experts[:, 1], 0]\n        # Calculate num samples that each expert gets\n        self._part_sizes = (gates &gt; 0).sum(0).tolist()\n\n        # Safety check: ensure no expert has 0 examples\n        for i, size in enumerate(self._part_sizes):\n            if size == 0:\n                # Add a dummy example to this expert\n                self._part_sizes[i] = 1\n                if i &gt;= len(self._expert_index):\n                    # Add a new dummy index if needed\n                    self._expert_index = torch.cat(\n                        [self._expert_index, torch.tensor([[i]], device=gates.device)]\n                    )\n                    self._batch_index = torch.cat(\n                        [self._batch_index, torch.tensor([0], device=gates.device)]\n                    )\n\n        # Expand gates to match with self._batch_index\n        gates_exp = gates[self._batch_index.flatten()]\n        self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)\n\n        # Safety check for nonzero gates\n        if (self._nonzero_gates &lt;= 0).any():\n            self._nonzero_gates = torch.clamp(self._nonzero_gates, min=1e-5)\n\n    def dispatch(self, inp: torch.Tensor) -&gt; List[torch.Tensor]:\n        \"\"\"Dispatch input samples to their assigned experts.\n\n        Args:\n            inp: Input tensor of shape (batch_size, input_dim).\n\n        Returns:\n            List of tensors, one for each expert, containing the inputs\n            assigned to that expert.\n        \"\"\"\n        # assigns samples to experts whose gate is nonzero\n\n        # expand according to batch index so we can just split by _part_sizes\n        inp_exp = inp[self._batch_index].squeeze(1)\n        return torch.split(inp_exp, self._part_sizes, dim=0)\n\n    def combine(\n        self, expert_out: List[torch.Tensor], multiply_by_gates: bool = True\n    ) -&gt; torch.Tensor:\n        \"\"\"Combine expert outputs weighted by gate values.\n\n        Args:\n            expert_out: List of expert output tensors, each with shape\n                (expert_batch_size_i, output_dim).\n            multiply_by_gates: Whether to weight outputs by gate values.\n\n        Returns:\n            Combined output tensor of shape (batch_size, output_dim).\n        \"\"\"\n        # apply exp to expert outputs, so we are not longer in log space\n        stitched = torch.cat(expert_out, 0)\n\n        if multiply_by_gates:\n            stitched = stitched.mul(self._nonzero_gates)\n        zeros = torch.zeros(\n            self._gates.size(0),\n            expert_out[-1].size(1),\n            requires_grad=True,\n            device=stitched.device,\n        )\n        # combine samples that have been processed by the same k experts\n        combined = zeros.index_add(0, self._batch_index, stitched.float())\n        return combined\n\n    def expert_to_gates(self) -&gt; List[torch.Tensor]:\n        \"\"\"Get gate values for each expert's assigned samples.\n\n        Returns:\n            List of tensors containing gate values for each expert's samples.\n        \"\"\"\n        # split nonzero gates for each expert\n        return torch.split(self._nonzero_gates, self._part_sizes, dim=0)\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.SparseDispatcher.__init__","title":"<code>__init__(self, num_experts, gates)</code>  <code>special</code>","text":"<p>Create a SparseDispatcher.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def __init__(self, num_experts: int, gates: torch.Tensor):\n    \"\"\"Create a SparseDispatcher.\"\"\"\n    self._gates = gates\n    self._num_experts = num_experts\n\n    # Safety check: ensure at least one example per expert\n    if (gates.sum(dim=0) == 0).any():\n        # Find experts with no assignments and create dummy assignments\n        empty_experts = (gates.sum(dim=0) == 0).nonzero().squeeze(1)\n        if empty_experts.numel() &gt; 0:\n            # Assign the first example to all empty experts with a small weight\n            for expert_idx in empty_experts:\n                gates[0, expert_idx] = 1e-5\n\n    # Sort experts\n    sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n    # Drop indices\n    _, self._expert_index = sorted_experts.split(1, dim=1)\n    # Get according batch index for each expert\n    self._batch_index = torch.nonzero(gates)[index_sorted_experts[:, 1], 0]\n    # Calculate num samples that each expert gets\n    self._part_sizes = (gates &gt; 0).sum(0).tolist()\n\n    # Safety check: ensure no expert has 0 examples\n    for i, size in enumerate(self._part_sizes):\n        if size == 0:\n            # Add a dummy example to this expert\n            self._part_sizes[i] = 1\n            if i &gt;= len(self._expert_index):\n                # Add a new dummy index if needed\n                self._expert_index = torch.cat(\n                    [self._expert_index, torch.tensor([[i]], device=gates.device)]\n                )\n                self._batch_index = torch.cat(\n                    [self._batch_index, torch.tensor([0], device=gates.device)]\n                )\n\n    # Expand gates to match with self._batch_index\n    gates_exp = gates[self._batch_index.flatten()]\n    self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)\n\n    # Safety check for nonzero gates\n    if (self._nonzero_gates &lt;= 0).any():\n        self._nonzero_gates = torch.clamp(self._nonzero_gates, min=1e-5)\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.SparseDispatcher.combine","title":"<code>combine(self, expert_out, multiply_by_gates=True)</code>","text":"<p>Combine expert outputs weighted by gate values.</p> <p>Parameters:</p> Name Type Description Default <code>expert_out</code> <code>List[torch.Tensor]</code> <p>List of expert output tensors, each with shape (expert_batch_size_i, output_dim).</p> required <code>multiply_by_gates</code> <code>bool</code> <p>Whether to weight outputs by gate values.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Combined output tensor of shape (batch_size, output_dim).</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def combine(\n    self, expert_out: List[torch.Tensor], multiply_by_gates: bool = True\n) -&gt; torch.Tensor:\n    \"\"\"Combine expert outputs weighted by gate values.\n\n    Args:\n        expert_out: List of expert output tensors, each with shape\n            (expert_batch_size_i, output_dim).\n        multiply_by_gates: Whether to weight outputs by gate values.\n\n    Returns:\n        Combined output tensor of shape (batch_size, output_dim).\n    \"\"\"\n    # apply exp to expert outputs, so we are not longer in log space\n    stitched = torch.cat(expert_out, 0)\n\n    if multiply_by_gates:\n        stitched = stitched.mul(self._nonzero_gates)\n    zeros = torch.zeros(\n        self._gates.size(0),\n        expert_out[-1].size(1),\n        requires_grad=True,\n        device=stitched.device,\n    )\n    # combine samples that have been processed by the same k experts\n    combined = zeros.index_add(0, self._batch_index, stitched.float())\n    return combined\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.SparseDispatcher.dispatch","title":"<code>dispatch(self, inp)</code>","text":"<p>Dispatch input samples to their assigned experts.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <p>Returns:</p> Type Description <code>List[torch.Tensor]</code> <p>List of tensors, one for each expert, containing the inputs assigned to that expert.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def dispatch(self, inp: torch.Tensor) -&gt; List[torch.Tensor]:\n    \"\"\"Dispatch input samples to their assigned experts.\n\n    Args:\n        inp: Input tensor of shape (batch_size, input_dim).\n\n    Returns:\n        List of tensors, one for each expert, containing the inputs\n        assigned to that expert.\n    \"\"\"\n    # assigns samples to experts whose gate is nonzero\n\n    # expand according to batch index so we can just split by _part_sizes\n    inp_exp = inp[self._batch_index].squeeze(1)\n    return torch.split(inp_exp, self._part_sizes, dim=0)\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.SparseDispatcher.expert_to_gates","title":"<code>expert_to_gates(self)</code>","text":"<p>Get gate values for each expert's assigned samples.</p> <p>Returns:</p> Type Description <code>List[torch.Tensor]</code> <p>List of tensors containing gate values for each expert's samples.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def expert_to_gates(self) -&gt; List[torch.Tensor]:\n    \"\"\"Get gate values for each expert's assigned samples.\n\n    Returns:\n        List of tensors containing gate values for each expert's samples.\n    \"\"\"\n    # split nonzero gates for each expert\n    return torch.split(self._nonzero_gates, self._part_sizes, dim=0)\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.VAE","title":"<code> VAE            (LightningModule)         </code>","text":"<p>Variational Autoencoder for water quality parameter estimation.</p> <p>This class implements a standard VAE architecture with configurable encoder and decoder networks for estimating water quality parameters from hyperspectral remote sensing reflectance data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features (spectral bands).</p> required <code>output_dim</code> <code>int</code> <p>Number of output features (water quality parameters).</p> required <code>latent_dim</code> <code>int</code> <p>Dimension of the latent space.</p> required <code>encoder_hidden_dims</code> <code>List[int]</code> <p>List of hidden layer dimensions for the encoder.</p> required <code>decoder_hidden_dims</code> <code>List[int]</code> <p>List of hidden layer dimensions for the decoder.</p> required <code>activation</code> <code>str</code> <p>Activation function ('relu', 'tanh', 'sigmoid', 'leakyrelu').</p> <code>'leakyrelu'</code> <code>use_norm</code> <code>Union[str, bool]</code> <p>Type of normalization ('batch', 'layer', 'group', or False).</p> <code>False</code> <code>use_dropout</code> <code>bool</code> <p>Whether to use dropout regularization.</p> <code>False</code> <code>use_softplus_output</code> <code>bool</code> <p>Whether to apply softplus activation to output.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>class VAE(LightningModule):\n    \"\"\"Variational Autoencoder for water quality parameter estimation.\n\n    This class implements a standard VAE architecture with configurable encoder\n    and decoder networks for estimating water quality parameters from hyperspectral\n    remote sensing reflectance data.\n\n    Args:\n        input_dim: Number of input features (spectral bands).\n        output_dim: Number of output features (water quality parameters).\n        latent_dim: Dimension of the latent space.\n        encoder_hidden_dims: List of hidden layer dimensions for the encoder.\n        decoder_hidden_dims: List of hidden layer dimensions for the decoder.\n        activation: Activation function ('relu', 'tanh', 'sigmoid', 'leakyrelu').\n        use_norm: Type of normalization ('batch', 'layer', 'group', or False).\n        use_dropout: Whether to use dropout regularization.\n        use_softplus_output: Whether to apply softplus activation to output.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        latent_dim: int,\n        encoder_hidden_dims: List[int],\n        decoder_hidden_dims: List[int],\n        activation: str = \"leakyrelu\",\n        use_norm: Union[str, bool] = False,\n        use_dropout: bool = False,\n        use_softplus_output: bool = False,\n        **kwargs,\n    ):\n        super().__init__()\n        # Define the activation function\n        self.use_softplus_output = use_softplus_output\n        if activation == \"relu\":\n            self.activation = nn.ReLU()\n        elif activation == \"tanh\":\n            self.activation = nn.Tanh()\n        elif activation == \"sigmoid\":\n            self.activation = nn.Sigmoid()\n        elif activation == \"leakyrelu\":\n            self.activation = nn.LeakyReLU(0.2)\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation}\")\n\n        # Encoder layers\n        self.encoder_layers = self.build_layers(\n            input_dim, encoder_hidden_dims, use_norm, use_dropout\n        )\n        self.fc_mu = nn.Linear(encoder_hidden_dims[-1], latent_dim)\n        self.fc_log_var = nn.Linear(encoder_hidden_dims[-1], latent_dim)\n\n        # Decoder layers\n        self.decoder_layers = self.build_layers(\n            latent_dim, decoder_hidden_dims, use_norm, use_dropout\n        )\n        # self.decoder_layers.add_module('softplus', nn.Softplus())\n        self.decoder_layers.add_module(\n            \"output_layer\", nn.Linear(decoder_hidden_dims[-1], output_dim)\n        )\n        if self.use_softplus_output:\n            self.decoder_layers.add_module(\"output_activation\", nn.Softplus())\n        # self.decoder_layers.add_module('output_activation', nn.Tanh())  # Assuming output is in range [-1, 1]\n        # with the classic robust preprocessing method it is -1 to 1, but for others it may not.\n\n    def build_layers(\n        self,\n        input_dim: int,\n        hidden_dims: List[int],\n        use_norm: Union[str, bool],\n        use_dropout: bool = False,\n    ) -&gt; nn.Sequential:\n        \"\"\"Build a sequential network with specified layers and normalization.\n\n        Args:\n            input_dim: Number of input features.\n            hidden_dims: List of hidden layer dimensions.\n            use_norm: Type of normalization ('batch', 'layer', 'group', or False).\n            use_dropout: Whether to add dropout layers.\n\n        Returns:\n            Sequential module containing the network layers.\n        \"\"\"\n        layers = []\n        current_size = input_dim\n        for hidden_dim in hidden_dims:\n            next_size = hidden_dim\n            layers.append(nn.Linear(current_size, next_size))\n            if use_norm == \"batch\":\n                layers.append(nn.BatchNorm1d(hidden_dim))\n            elif use_norm == \"layer\":\n                layers.append(nn.LayerNorm(hidden_dim))\n            elif use_norm == \"group\":\n                num_groups = max(1, hidden_dim // 4)\n                layers.append(\n                    nn.GroupNorm(num_groups=num_groups, num_channels=hidden_dim)\n                )\n            layers.append(self.activation)\n            if use_dropout:\n                layers.append(nn.Dropout(0.1))\n            current_size = next_size\n        return nn.Sequential(*layers)\n\n    def encode(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Encode input to latent distribution parameters.\n\n        Args:\n            x: Input tensor of shape (batch, input_dim).\n\n        Returns:\n            mu: Mean of latent distribution.\n            log_var: Log variance of latent distribution.\n        \"\"\"\n        x = self.encoder_layers(x)\n        mu = self.fc_mu(x)\n        log_var = self.fc_log_var(x)\n        return mu, log_var\n\n    def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply reparameterization trick for sampling from latent distribution.\n\n        Args:\n            mu: Mean of latent distribution.\n            log_var: Log variance of latent distribution.\n\n        Returns:\n            z: Sampled latent vector.\n        \"\"\"\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        z = mu + eps * std\n        return z\n\n    def decode(self, z: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decode latent vector to output.\n\n        Args:\n            z: Latent vector.\n\n        Returns:\n            Decoded output tensor.\n        \"\"\"\n        return self.decoder_layers(z)\n\n    def forward(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Forward pass through the VAE.\n\n        Args:\n            x: Input tensor of shape (batch, input_dim).\n\n        Returns:\n            Dictionary containing 'pred_y', 'mu', and 'log_var'.\n        \"\"\"\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        pred_y = self.decode(z)\n        return {\"pred_y\": pred_y, \"mu\": mu, \"log_var\": log_var}\n\n    def loss_fn(\n        self, output_dict: Dict[str, torch.Tensor], kld_weight: float = 0.0\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Compute VAE loss including reconstruction and KL divergence terms.\n\n        Args:\n            output_dict: Dictionary containing 'pred_y', 'y', 'mu', and 'log_var'.\n            kld_weight: Weight for the KL divergence term.\n\n        Returns:\n            Dictionary containing 'total_loss', 'mae_loss', 'mse_loss', and 'kld_loss'.\n        \"\"\"\n        pred_y, y, mu, log_var = (\n            output_dict[\"pred_y\"],\n            output_dict[\"y\"],\n            output_dict[\"mu\"],\n            output_dict[\"log_var\"],\n        )\n        batch_size = y.shape[0]\n        MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n        # Reconstruction loss (MSE)\n        MSE = F.mse_loss(pred_y, y, reduction=\"mean\")\n        # KL divergence\n        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch_size\n        # Return combined loss\n        return {\n            \"total_loss\": MAE + kld_weight * KLD,\n            \"mae_loss\": MAE,\n            \"mse_loss\": MSE,\n            \"kld_loss\": KLD,\n        }\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.VAE.build_layers","title":"<code>build_layers(self, input_dim, hidden_dims, use_norm, use_dropout=False)</code>","text":"<p>Build a sequential network with specified layers and normalization.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_dims</code> <code>List[int]</code> <p>List of hidden layer dimensions.</p> required <code>use_norm</code> <code>Union[str, bool]</code> <p>Type of normalization ('batch', 'layer', 'group', or False).</p> required <code>use_dropout</code> <code>bool</code> <p>Whether to add dropout layers.</p> <code>False</code> <p>Returns:</p> Type Description <code>Sequential</code> <p>Sequential module containing the network layers.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def build_layers(\n    self,\n    input_dim: int,\n    hidden_dims: List[int],\n    use_norm: Union[str, bool],\n    use_dropout: bool = False,\n) -&gt; nn.Sequential:\n    \"\"\"Build a sequential network with specified layers and normalization.\n\n    Args:\n        input_dim: Number of input features.\n        hidden_dims: List of hidden layer dimensions.\n        use_norm: Type of normalization ('batch', 'layer', 'group', or False).\n        use_dropout: Whether to add dropout layers.\n\n    Returns:\n        Sequential module containing the network layers.\n    \"\"\"\n    layers = []\n    current_size = input_dim\n    for hidden_dim in hidden_dims:\n        next_size = hidden_dim\n        layers.append(nn.Linear(current_size, next_size))\n        if use_norm == \"batch\":\n            layers.append(nn.BatchNorm1d(hidden_dim))\n        elif use_norm == \"layer\":\n            layers.append(nn.LayerNorm(hidden_dim))\n        elif use_norm == \"group\":\n            num_groups = max(1, hidden_dim // 4)\n            layers.append(\n                nn.GroupNorm(num_groups=num_groups, num_channels=hidden_dim)\n            )\n        layers.append(self.activation)\n        if use_dropout:\n            layers.append(nn.Dropout(0.1))\n        current_size = next_size\n    return nn.Sequential(*layers)\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.VAE.decode","title":"<code>decode(self, z)</code>","text":"<p>Decode latent vector to output.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>Latent vector.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded output tensor.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def decode(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decode latent vector to output.\n\n    Args:\n        z: Latent vector.\n\n    Returns:\n        Decoded output tensor.\n    \"\"\"\n    return self.decoder_layers(z)\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.VAE.encode","title":"<code>encode(self, x)</code>","text":"<p>Encode input to latent distribution parameters.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch, input_dim).</p> required <p>Returns:</p> Type Description <code>mu</code> <p>Mean of latent distribution. log_var: Log variance of latent distribution.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Encode input to latent distribution parameters.\n\n    Args:\n        x: Input tensor of shape (batch, input_dim).\n\n    Returns:\n        mu: Mean of latent distribution.\n        log_var: Log variance of latent distribution.\n    \"\"\"\n    x = self.encoder_layers(x)\n    mu = self.fc_mu(x)\n    log_var = self.fc_log_var(x)\n    return mu, log_var\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.VAE.forward","title":"<code>forward(self, x)</code>","text":"<p>Forward pass through the VAE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch, input_dim).</p> required <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Dictionary containing 'pred_y', 'mu', and 'log_var'.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Forward pass through the VAE.\n\n    Args:\n        x: Input tensor of shape (batch, input_dim).\n\n    Returns:\n        Dictionary containing 'pred_y', 'mu', and 'log_var'.\n    \"\"\"\n    mu, log_var = self.encode(x)\n    z = self.reparameterize(mu, log_var)\n    pred_y = self.decode(z)\n    return {\"pred_y\": pred_y, \"mu\": mu, \"log_var\": log_var}\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.VAE.loss_fn","title":"<code>loss_fn(self, output_dict, kld_weight=0.0)</code>","text":"<p>Compute VAE loss including reconstruction and KL divergence terms.</p> <p>Parameters:</p> Name Type Description Default <code>output_dict</code> <code>Dict[str, torch.Tensor]</code> <p>Dictionary containing 'pred_y', 'y', 'mu', and 'log_var'.</p> required <code>kld_weight</code> <code>float</code> <p>Weight for the KL divergence term.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>Dictionary containing 'total_loss', 'mae_loss', 'mse_loss', and 'kld_loss'.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def loss_fn(\n    self, output_dict: Dict[str, torch.Tensor], kld_weight: float = 0.0\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Compute VAE loss including reconstruction and KL divergence terms.\n\n    Args:\n        output_dict: Dictionary containing 'pred_y', 'y', 'mu', and 'log_var'.\n        kld_weight: Weight for the KL divergence term.\n\n    Returns:\n        Dictionary containing 'total_loss', 'mae_loss', 'mse_loss', and 'kld_loss'.\n    \"\"\"\n    pred_y, y, mu, log_var = (\n        output_dict[\"pred_y\"],\n        output_dict[\"y\"],\n        output_dict[\"mu\"],\n        output_dict[\"log_var\"],\n    )\n    batch_size = y.shape[0]\n    MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n    # Reconstruction loss (MSE)\n    MSE = F.mse_loss(pred_y, y, reduction=\"mean\")\n    # KL divergence\n    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch_size\n    # Return combined loss\n    return {\n        \"total_loss\": MAE + kld_weight * KLD,\n        \"mae_loss\": MAE,\n        \"mse_loss\": MSE,\n        \"kld_loss\": KLD,\n    }\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.VAE.reparameterize","title":"<code>reparameterize(self, mu, log_var)</code>","text":"<p>Apply reparameterization trick for sampling from latent distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean of latent distribution.</p> required <code>log_var</code> <code>Tensor</code> <p>Log variance of latent distribution.</p> required <p>Returns:</p> Type Description <code>z</code> <p>Sampled latent vector.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply reparameterization trick for sampling from latent distribution.\n\n    Args:\n        mu: Mean of latent distribution.\n        log_var: Log variance of latent distribution.\n\n    Returns:\n        z: Sampled latent vector.\n    \"\"\"\n    std = torch.exp(0.5 * log_var)\n    eps = torch.randn_like(std)\n    z = mu + eps * std\n    return z\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.evaluate","title":"<code>evaluate(model, test_dl, device, TSS_scalers_dict=None, log_offset=0.01)</code>","text":"<p>Evaluate a VAE or MoE-VAE model on test data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[hypercoast.emit_utils.MoE_VAE.VAE, hypercoast.emit_utils.MoE_VAE.MoE_VAE]</code> <p>Trained VAE or MoE_VAE model.</p> required <code>test_dl</code> <code>DataLoader</code> <p>DataLoader providing test batches.</p> required <code>device</code> <code>device</code> <p>Device to run evaluation on (CPU or CUDA).</p> required <code>TSS_scalers_dict</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary with 'log' and 'robust' scalers for inverse transformation. If None, uses simple log offset.</p> <code>None</code> <code>log_offset</code> <code>float</code> <p>Offset for inverse log transformation if TSS_scalers_dict is None.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>predictions_inverse</code> <p>Predictions in original scale. actuals_inverse: Ground truth values in original scale.</p> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def evaluate(\n    model: Union[VAE, MoE_VAE],\n    test_dl: DataLoader,\n    device: torch.device,\n    TSS_scalers_dict: Optional[Dict[str, Any]] = None,\n    log_offset: float = 0.01,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Evaluate a VAE or MoE-VAE model on test data.\n\n    Args:\n        model: Trained VAE or MoE_VAE model.\n        test_dl: DataLoader providing test batches.\n        device: Device to run evaluation on (CPU or CUDA).\n        TSS_scalers_dict: Optional dictionary with 'log' and 'robust' scalers\n            for inverse transformation. If None, uses simple log offset.\n        log_offset: Offset for inverse log transformation if TSS_scalers_dict is None.\n\n    Returns:\n        predictions_inverse: Predictions in original scale.\n        actuals_inverse: Ground truth values in original scale.\n    \"\"\"\n    model.eval()\n    predictions, actuals = [], []\n\n    with torch.no_grad():\n        for x, y in test_dl:\n            x, y = x.to(device), y.to(device)\n            output_dict = model(x)\n            y_pred = output_dict[\"pred_y\"]\n            predictions.append(y_pred.cpu().numpy())\n            actuals.append(y.cpu().numpy())\n\n    predictions = np.vstack(predictions)\n    actuals = np.vstack(actuals)\n\n    # === Inverse transformation ===\n    if TSS_scalers_dict is not None:\n        log_scaler = TSS_scalers_dict[\"log\"]\n        robust_scaler = TSS_scalers_dict[\"robust\"]\n\n        # First reverse min-max, then reverse log\n        predictions_inverse = (\n            log_scaler.inverse_transform(\n                torch.tensor(\n                    robust_scaler.inverse_transform(\n                        torch.tensor(predictions, dtype=torch.float32)\n                    ),\n                    dtype=torch.float32,\n                )\n            )\n            .numpy()\n            .flatten()\n        )\n\n        actuals_inverse = (\n            log_scaler.inverse_transform(\n                torch.tensor(\n                    robust_scaler.inverse_transform(\n                        torch.tensor(actuals, dtype=torch.float32)\n                    ),\n                    dtype=torch.float32,\n                )\n            )\n            .numpy()\n            .flatten()\n        )\n    else:\n        predictions_inverse = (10 ** predictions.flatten()) - log_offset\n        actuals_inverse = (10 ** actuals.flatten()) - log_offset\n\n    return predictions_inverse, actuals_inverse\n</code></pre>"},{"location":"emit_utils/moe_vae/#hypercoast.emit_utils.MoE_VAE.train","title":"<code>train(model, train_dl, device, epochs=200, optimizer=None, save_dir=None)</code>","text":"<p>Train a VAE or MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[hypercoast.emit_utils.MoE_VAE.VAE, hypercoast.emit_utils.MoE_VAE.MoE_VAE]</code> <p>VAE or MoE_VAE model to train.</p> required <code>train_dl</code> <code>DataLoader</code> <p>DataLoader providing training batches.</p> required <code>device</code> <code>device</code> <p>Device to train on (CPU or CUDA).</p> required <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>optimizer</code> <code>Optional[torch.optim.optimizer.Optimizer]</code> <p>PyTorch optimizer. If None, must be configured externally.</p> <code>None</code> <code>save_dir</code> <code>Optional[str]</code> <p>Directory to save the best model checkpoint.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dictionary containing</code> <ul> <li>'total_loss': List of total losses per epoch.<ul> <li>'l1_loss': List of L1 losses per epoch.</li> <li>'best_loss': Best total loss achieved.</li> </ul> </li> </ul> Source code in <code>hypercoast/emit_utils/MoE_VAE.py</code> <pre><code>def train(\n    model: Union[VAE, MoE_VAE],\n    train_dl: DataLoader,\n    device: torch.device,\n    epochs: int = 200,\n    optimizer: Optional[torch.optim.Optimizer] = None,\n    save_dir: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Train a VAE or MoE-VAE model.\n\n    Args:\n        model: VAE or MoE_VAE model to train.\n        train_dl: DataLoader providing training batches.\n        device: Device to train on (CPU or CUDA).\n        epochs: Number of training epochs.\n        optimizer: PyTorch optimizer. If None, must be configured externally.\n        save_dir: Directory to save the best model checkpoint.\n\n    Returns:\n        Dictionary containing:\n            - 'total_loss': List of total losses per epoch.\n            - 'l1_loss': List of L1 losses per epoch.\n            - 'best_loss': Best total loss achieved.\n    \"\"\"\n    model.train()\n    min_total_loss = float(\"inf\")\n    best_model_path = os.path.join(save_dir, \"best_model_minloss.pth\")\n\n    total_list = []\n    l1_list = []\n\n    for epoch in range(epochs):\n        total_loss_epoch = 0.0\n        l1_epoch = 0.0\n\n        for x, y in train_dl:\n            x, y = x.to(device), y.to(device)\n\n            output_dict = model(x)\n            output_dict[\"y\"] = y\n\n            loss_dict = model.loss_fn(output_dict)\n\n            loss = loss_dict[\"total_loss\"]\n            l1 = loss_dict[\"mae_loss\"]\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss_epoch += loss.item()\n            l1_epoch += l1.item()\n\n        avg_total_loss = total_loss_epoch / len(train_dl)\n        avg_l1 = l1_epoch / len(train_dl)\n\n        print(f\"[Epoch {epoch+1}] Total: {avg_total_loss:.4f} | L1: {avg_l1:.4f}\")\n        total_list.append(avg_total_loss)\n        l1_list.append(avg_l1)\n\n        if avg_total_loss &lt; min_total_loss:\n            min_total_loss = avg_total_loss\n            torch.save(model.state_dict(), best_model_path)\n\n    return {\"total_loss\": total_list, \"l1_loss\": l1_list, \"best_loss\": min_total_loss}\n</code></pre>"},{"location":"emit_utils/plot_and_save/","title":"plot_and_save module","text":"<p>Plotting and result saving utilities for model evaluation.</p> <p>This module provides functions for visualizing model predictions, computing performance metrics, and saving results to Excel files.</p>"},{"location":"emit_utils/plot_and_save/#hypercoast.emit_utils.plot_and_save.calculate_metrics","title":"<code>calculate_metrics(predictions, actuals, threshold=0.8)</code>","text":"<p>Calculate performance metrics for water quality parameter predictions.</p> <p>Computes various metrics including epsilon, beta (from IOCCG protocols), NRMSE, RMSLE, MAPE, bias, and MAE for evaluating model performance.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Array of predicted values.</p> required <code>actuals</code> <code>ndarray</code> <p>Array of actual (ground truth) values.</p> required <code>threshold</code> <code>float</code> <p>Relative error threshold (not currently used in filtering).</p> <code>0.8</code> <p>Returns:</p> Type Description <code>epsilon</code> <p>Symmetric signed percentage difference (IOCCG). beta: Bias percentage (IOCCG). nrmse: Normalized root mean squared error. rmsle: Root mean squared logarithmic error. mape: Median absolute percentage error. bias: Multiplicative bias. mae: Median absolute error in log space (antilog).</p> Source code in <code>hypercoast/emit_utils/plot_and_save.py</code> <pre><code>def calculate_metrics(\n    predictions: np.ndarray, actuals: np.ndarray, threshold: float = 0.8\n) -&gt; Tuple[float, float, float, float, float, float, float]:\n    \"\"\"Calculate performance metrics for water quality parameter predictions.\n\n    Computes various metrics including epsilon, beta (from IOCCG protocols),\n    NRMSE, RMSLE, MAPE, bias, and MAE for evaluating model performance.\n\n    Args:\n        predictions: Array of predicted values.\n        actuals: Array of actual (ground truth) values.\n        threshold: Relative error threshold (not currently used in filtering).\n\n    Returns:\n        epsilon: Symmetric signed percentage difference (IOCCG).\n        beta: Bias percentage (IOCCG).\n        nrmse: Normalized root mean squared error.\n        rmsle: Root mean squared logarithmic error.\n        mape: Median absolute percentage error.\n        bias: Multiplicative bias.\n        mae: Median absolute error in log space (antilog).\n    \"\"\"\n    eps = 1e-10  # small constant to avoid division by zero\n\n    predictions = np.where(predictions &lt;= eps, eps, predictions)\n    actuals = np.where(actuals &lt;= eps, eps, actuals)\n    filtered_predictions = predictions\n    filtered_actuals = actuals\n\n    # Calculate epsilon and beta\n    log_ratios = np.log10(filtered_predictions / filtered_actuals)\n    Y = np.median(np.abs(log_ratios))\n    Z = np.median(log_ratios)\n    epsilon = 100 * (10**Y - 1)\n    beta = 50 * np.sign(Z) * (10 ** np.abs(Z) - 1)\n\n    # NRMSE: RMSE normalized by range (max - min)\n    rmse = np.sqrt(np.mean((filtered_predictions - filtered_actuals) ** 2))\n    nrmse = rmse / (np.max(filtered_actuals) - np.min(filtered_actuals) + eps)\n\n    rmsle = np.sqrt(\n        np.mean(\n            (np.log10(filtered_predictions + 1) - np.log10(filtered_actuals + 1)) ** 2\n        )\n    )\n    mape = 50 * np.median(\n        np.abs((filtered_predictions - filtered_actuals) / filtered_actuals)\n    )\n    bias = 10 ** (np.mean(np.log10(filtered_predictions) - np.log10(filtered_actuals)))\n    mae = 10 ** np.mean(\n        np.abs(np.log10(filtered_predictions) - np.log10(filtered_actuals))\n    )\n\n    return epsilon, beta, nrmse, rmsle, mape, bias, mae\n</code></pre>"},{"location":"emit_utils/plot_and_save/#hypercoast.emit_utils.plot_and_save.plot_results","title":"<code>plot_results(predictions_rescaled, actuals_rescaled, save_dir, threshold=10, mode='test', xlim=(-4, 4), ylim=(-4, 4))</code>","text":"<p>Create scatter plot with KDE contours comparing predictions vs actuals.</p> <p>Generates a log-log scatter plot with regression line, 1:1 reference line, KDE density contours, and performance metrics in the legend.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_rescaled</code> <code>ndarray</code> <p>Array of predicted values.</p> required <code>actuals_rescaled</code> <code>ndarray</code> <p>Array of actual (ground truth) values.</p> required <code>save_dir</code> <code>str</code> <p>Directory to save the output plot.</p> required <code>threshold</code> <code>float</code> <p>Threshold for filtering outliers in log space.</p> <code>10</code> <code>mode</code> <code>str</code> <p>Name prefix for the output file (e.g., 'test', 'train').</p> <code>'test'</code> <code>xlim</code> <code>Tuple[float, float]</code> <p>X-axis limits in log space (e.g., (-4, 4) for 10^-4 to 10^4).</p> <code>(-4, 4)</code> <code>ylim</code> <code>Tuple[float, float]</code> <p>Y-axis limits in log space.</p> <code>(-4, 4)</code> Source code in <code>hypercoast/emit_utils/plot_and_save.py</code> <pre><code>def plot_results(\n    predictions_rescaled: np.ndarray,\n    actuals_rescaled: np.ndarray,\n    save_dir: str,\n    threshold: float = 10,\n    mode: str = \"test\",\n    xlim: Tuple[float, float] = (-4, 4),\n    ylim: Tuple[float, float] = (-4, 4),\n) -&gt; None:\n    \"\"\"Create scatter plot with KDE contours comparing predictions vs actuals.\n\n    Generates a log-log scatter plot with regression line, 1:1 reference line,\n    KDE density contours, and performance metrics in the legend.\n\n    Args:\n        predictions_rescaled: Array of predicted values.\n        actuals_rescaled: Array of actual (ground truth) values.\n        save_dir: Directory to save the output plot.\n        threshold: Threshold for filtering outliers in log space.\n        mode: Name prefix for the output file (e.g., 'test', 'train').\n        xlim: X-axis limits in log space (e.g., (-4, 4) for 10^-4 to 10^4).\n        ylim: Y-axis limits in log space.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    actuals = actuals_rescaled.flatten()\n    predictions = predictions_rescaled.flatten()\n\n    log_actuals = np.log10(np.where(actuals == 0, 1e-10, actuals))\n    log_predictions = np.log10(np.where(predictions == 0, 1e-10, predictions))\n\n    mask = np.abs(log_predictions - log_actuals) &lt; threshold\n    filtered_predictions = predictions[mask]\n    filtered_actuals = actuals[mask]\n\n    filtered_log_actual = np.log10(\n        np.where(filtered_actuals == 0, 1e-10, filtered_actuals)\n    )\n    filtered_log_prediction = np.log10(\n        np.where(filtered_predictions == 0, 1e-10, filtered_predictions)\n    )\n\n    epsilon, beta, nrmse, rmsle, mape, bias, mae = calculate_metrics(\n        filtered_predictions, filtered_actuals, threshold\n    )\n\n    valid_mask = np.isfinite(filtered_log_actual) &amp; np.isfinite(filtered_log_prediction)\n    slope, intercept = np.polyfit(\n        filtered_log_actual[valid_mask], filtered_log_prediction[valid_mask], 1\n    )\n    x = np.array([xlim[0], xlim[1]])\n    y = slope * x + intercept\n\n    plt.figure(figsize=(6, 6))\n\n    # Regression line\n    plt.plot(x, y, linestyle=\"--\", color=\"blue\", linewidth=0.8)\n    # 1:1 line\n    plt.plot(xlim, ylim, linestyle=\"-\", color=\"black\", linewidth=0.8)\n\n    # Scatter &amp; KDE\n    sns.scatterplot(x=log_actuals, y=log_predictions, alpha=0.5)\n    sns.kdeplot(\n        x=filtered_log_actual,\n        y=filtered_log_prediction,\n        levels=3,\n        color=\"black\",\n        fill=False,\n        linewidths=0.8,\n    )\n\n    plt.xlabel(\"Actual Values\", fontsize=16, fontname=\"Ubuntu\")\n    plt.ylabel(\"Predicted Values\", fontsize=16, fontname=\"Ubuntu\")\n    plt.xlim(*xlim)\n    plt.ylim(*ylim)\n    plt.grid(True, which=\"both\", ls=\"--\")\n\n    plt.legend(\n        title=(\n            f\"MAE = {mae:.2f}, NRMSE = {nrmse:.2f}, RMSLE = {rmsle:.2f}\\n\"\n            f\"Bias = {bias:.2f}, Slope = {slope:.2f}\\n\"\n            f\"MAPE = {mape:.2f}%, \u03b5 = {epsilon:.2f}%, \u03b2 = {beta:.2f}%\"\n        ),\n        fontsize=12,\n        title_fontsize=10,\n        prop={\"family\": \"Ubuntu\"},\n    )\n\n    plt.xticks(fontsize=14, fontname=\"Ubuntu\")\n    plt.yticks(fontsize=14, fontname=\"Ubuntu\")\n\n    png_path = os.path.join(save_dir, f\"{mode}_plot.png\")\n    plt.tight_layout()\n    plt.savefig(png_path, bbox_inches=\"tight\", dpi=300)\n    plt.show()\n\n    print(f\"\u2705 Saved and displayed: {png_path}\")\n</code></pre>"},{"location":"emit_utils/plot_and_save/#hypercoast.emit_utils.plot_and_save.plot_results_with_density","title":"<code>plot_results_with_density(predictions_rescaled, actuals_rescaled, save_dir, threshold=10, mode='test_density', xlim=(-4, 4), ylim=(-4, 4), cmap='viridis', tick_min=-4, tick_max=4, tick_step=1)</code>","text":"<p>Create density-colored scatter plot comparing predictions vs actuals.</p> <p>Generates a log-log scatter plot where points are colored by local density estimated using Gaussian KDE. Includes regression line, 1:1 reference line, and performance metrics.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_rescaled</code> <code>ndarray</code> <p>Array of predicted values.</p> required <code>actuals_rescaled</code> <code>ndarray</code> <p>Array of actual (ground truth) values.</p> required <code>save_dir</code> <code>str</code> <p>Directory to save the output plot.</p> required <code>threshold</code> <code>float</code> <p>Threshold for filtering outliers in log space.</p> <code>10</code> <code>mode</code> <code>str</code> <p>Name prefix for the output file.</p> <code>'test_density'</code> <code>xlim</code> <code>Tuple[float, float]</code> <p>X-axis limits in log space.</p> <code>(-4, 4)</code> <code>ylim</code> <code>Tuple[float, float]</code> <p>Y-axis limits in log space.</p> <code>(-4, 4)</code> <code>cmap</code> <code>str</code> <p>Colormap name for density visualization.</p> <code>'viridis'</code> <code>tick_min</code> <code>float</code> <p>Minimum tick value in log space.</p> <code>-4</code> <code>tick_max</code> <code>float</code> <p>Maximum tick value in log space.</p> <code>4</code> <code>tick_step</code> <code>float</code> <p>Step between ticks in log space.</p> <code>1</code> Source code in <code>hypercoast/emit_utils/plot_and_save.py</code> <pre><code>def plot_results_with_density(\n    predictions_rescaled: np.ndarray,\n    actuals_rescaled: np.ndarray,\n    save_dir: str,\n    threshold: float = 10,\n    mode: str = \"test_density\",\n    xlim: Tuple[float, float] = (-4, 4),\n    ylim: Tuple[float, float] = (-4, 4),\n    cmap: str = \"viridis\",\n    tick_min: float = -4,\n    tick_max: float = 4,\n    tick_step: float = 1,\n) -&gt; None:\n    \"\"\"Create density-colored scatter plot comparing predictions vs actuals.\n\n    Generates a log-log scatter plot where points are colored by local density\n    estimated using Gaussian KDE. Includes regression line, 1:1 reference line,\n    and performance metrics.\n\n    Args:\n        predictions_rescaled: Array of predicted values.\n        actuals_rescaled: Array of actual (ground truth) values.\n        save_dir: Directory to save the output plot.\n        threshold: Threshold for filtering outliers in log space.\n        mode: Name prefix for the output file.\n        xlim: X-axis limits in log space.\n        ylim: Y-axis limits in log space.\n        cmap: Colormap name for density visualization.\n        tick_min: Minimum tick value in log space.\n        tick_max: Maximum tick value in log space.\n        tick_step: Step between ticks in log space.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    actuals = actuals_rescaled.flatten()\n    predictions = predictions_rescaled.flatten()\n\n    # Log10 transform (avoid non-positive values)\n    eps = 1e-10\n    log_actuals = np.log10(np.where(actuals &lt;= 0, eps, actuals))\n    log_predictions = np.log10(np.where(predictions &lt;= 0, eps, predictions))\n\n    # Optional threshold filtering\n    mask = np.abs(log_predictions - log_actuals) &lt; threshold\n    log_a_f, log_p_f = log_actuals[mask], log_predictions[mask]\n    a_f, p_f = actuals[mask], predictions[mask]\n\n    # Density estimation with Gaussian KDE\n    xy = np.vstack([log_a_f, log_p_f])\n    z = gaussian_kde(xy)(xy)\n    idx = z.argsort()\n    log_a_f, log_p_f, z = log_a_f[idx], log_p_f[idx], z[idx]\n    a_f, p_f = a_f[idx], p_f[idx]\n\n    # Calculate metrics\n    epsilon, beta, nrmse, rmsle, mape, bias, mae = calculate_metrics(\n        p_f, a_f, threshold\n    )\n\n    # Linear regression (in log-log space)\n    valid_mask = np.isfinite(log_a_f) &amp; np.isfinite(log_p_f)\n    slope, intercept = np.polyfit(log_a_f[valid_mask], log_p_f[valid_mask], 1)\n\n    # === Plot ===\n    plt.figure(figsize=(8, 6), dpi=300)\n\n    # 1:1 reference line (do not add to legend)\n    plt.plot(\n        [xlim[0], xlim[1]],\n        [xlim[0], xlim[1]],\n        linestyle=\"-\",\n        color=\"black\",\n        linewidth=0.9,\n    )\n\n    # Regression line (do not add to legend)\n    xs = np.array([xlim[0], xlim[1]])\n    ys = slope * xs + intercept\n    plt.plot(xs, ys, linestyle=\"--\", color=\"blue\", linewidth=0.9)\n\n    # Scatter points colored by density\n    sc = plt.scatter(log_a_f, log_p_f, c=z, s=30, cmap=cmap, alpha=1, edgecolors=\"none\")\n\n    # Colorbar for density\n    cbar = plt.colorbar(sc, fraction=0.06, pad=0.02)\n    cbar.ax.tick_params(labelsize=14)\n\n    # Axis ticks (shown as powers of 10)\n    ax = plt.gca()\n    ticks = np.arange(tick_min, tick_max + 1e-9, tick_step)\n    ax.set_xticks(ticks)\n    ax.set_yticks(ticks)\n    formatter = FuncFormatter(lambda val, pos: f\"$10^{{{val:.1f}}}$\")\n    ax.xaxis.set_major_formatter(formatter)\n    ax.yaxis.set_major_formatter(formatter)\n\n    ax.tick_params(axis=\"both\", labelsize=16)\n    plt.xlim(*xlim)\n    plt.ylim(*ylim)\n    plt.grid(True, ls=\"--\", alpha=0.5)\n\n    # Metrics in legend (only title shown)\n    plt.legend(\n        title=(\n            f\"MAE = {mae:.2f}, NRMSE = {nrmse:.2f}\\n\"\n            f\"RMSLE = {rmsle:.2f}, Bias = {bias:.2f}\\n\"\n            f\"MAPE = {mape:.2f}%, Slope = {slope:.2f}\\n\"\n            f\"\u03b5 = {epsilon:.2f}%, \u03b2 = {beta:.2f}%\"\n        ),\n        fontsize=12,\n        title_fontsize=10,\n        frameon=True,\n    )\n\n    # Save figure as PNG\n    png_path = os.path.join(save_dir, f\"{mode}.png\")\n    plt.tight_layout()\n    plt.savefig(png_path, bbox_inches=\"tight\", dpi=300)\n    plt.show()\n\n    print(f\"\u2705 Saved and displayed PNG: {png_path}\")\n</code></pre>"},{"location":"emit_utils/plot_and_save/#hypercoast.emit_utils.plot_and_save.save_results_from_excel_for_test","title":"<code>save_results_from_excel_for_test(predictions, actuals, sample_ids, dates, original_excel_path, save_dir)</code>","text":"<p>Save test results to Excel file with dataset name from original file.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Array of predicted values.</p> required <code>actuals</code> <code>ndarray</code> <p>Array of actual (ground truth) values.</p> required <code>sample_ids</code> <code>List[str]</code> <p>List of sample identifiers.</p> required <code>dates</code> <code>List[str]</code> <p>List of date strings.</p> required <code>original_excel_path</code> <code>str</code> <p>Path to original Excel file (used for naming output).</p> required <code>save_dir</code> <code>str</code> <p>Directory to save the output Excel file.</p> required Source code in <code>hypercoast/emit_utils/plot_and_save.py</code> <pre><code>def save_results_from_excel_for_test(\n    predictions: np.ndarray,\n    actuals: np.ndarray,\n    sample_ids: List[str],\n    dates: List[str],\n    original_excel_path: str,\n    save_dir: str,\n) -&gt; None:\n    \"\"\"Save test results to Excel file with dataset name from original file.\n\n    Args:\n        predictions: Array of predicted values.\n        actuals: Array of actual (ground truth) values.\n        sample_ids: List of sample identifiers.\n        dates: List of date strings.\n        original_excel_path: Path to original Excel file (used for naming output).\n        save_dir: Directory to save the output Excel file.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    filename = os.path.basename(original_excel_path)\n    dataset_name = os.path.splitext(filename)[0]\n\n    save_results_to_excel(\n        sample_ids,\n        actuals,\n        predictions,\n        os.path.join(save_dir, f\"{dataset_name}.xlsx\"),\n        dates=dates,\n    )\n</code></pre>"},{"location":"emit_utils/plot_and_save/#hypercoast.emit_utils.plot_and_save.save_results_to_excel","title":"<code>save_results_to_excel(ids, actuals, predictions, file_path, dates=None)</code>","text":"<p>Save prediction results to an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List of sample identifiers.</p> required <code>actuals</code> <code>ndarray</code> <p>Array of actual (ground truth) values.</p> required <code>predictions</code> <code>ndarray</code> <p>Array of predicted values.</p> required <code>file_path</code> <code>str</code> <p>Output path for the Excel file.</p> required <code>dates</code> <code>Optional[List[str]]</code> <p>Optional list of date strings to include in output.</p> <code>None</code> Source code in <code>hypercoast/emit_utils/plot_and_save.py</code> <pre><code>def save_results_to_excel(\n    ids: List[str],\n    actuals: np.ndarray,\n    predictions: np.ndarray,\n    file_path: str,\n    dates: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"Save prediction results to an Excel file.\n\n    Args:\n        ids: List of sample identifiers.\n        actuals: Array of actual (ground truth) values.\n        predictions: Array of predicted values.\n        file_path: Output path for the Excel file.\n        dates: Optional list of date strings to include in output.\n    \"\"\"\n    if dates is not None:\n        df = pd.DataFrame(\n            {\"ID\": ids, \"Date\": dates, \"Actual\": actuals, \"Predicted\": predictions}\n        )\n    else:\n        df = pd.DataFrame({\"ID\": ids, \"Actual\": actuals, \"Predicted\": predictions})\n\n    df.to_excel(file_path, index=False)\n</code></pre>"},{"location":"emit_utils/preprocess/","title":"preprocess module","text":"<p>Preprocessing and scaling utilities for hyperspectral data.</p> <p>This module provides custom scalers for preprocessing hyperspectral remote sensing data, including robust quantile-based scaling and logarithmic transformations.</p>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.LogScaler","title":"<code> LogScaler        </code>","text":"<p>Logarithmic scaler with optional shifting for non-positive values.</p> <p>This scaler applies log10 transformation after optionally shifting data to ensure all values are positive. Useful for compressing the dynamic range of water quality parameters that span multiple orders of magnitude.</p> <p>The transformation steps are: 1. Optionally shift values so minimum becomes 0 (if shift_min=True). 2. Add a small safety term to avoid log(0). 3. Apply log10 transformation.</p> <p>Parameters:</p> Name Type Description Default <code>shift_min</code> <code>bool</code> <p>Whether to shift data so the minimum value becomes 0.</p> <code>False</code> <code>safety_term</code> <code>float</code> <p>Small constant added before log to avoid log(0).</p> <code>1e-08</code> <p>Attributes:</p> Name Type Description <code>global_min</code> <p>Minimum value observed during fitting.</p> <code>shift_value</code> <p>Amount to shift data (computed from global_min).</p> <code>fitted</code> <p>Whether the scaler has been fitted to data.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>class LogScaler:\n    \"\"\"Logarithmic scaler with optional shifting for non-positive values.\n\n    This scaler applies log10 transformation after optionally shifting data to\n    ensure all values are positive. Useful for compressing the dynamic range of\n    water quality parameters that span multiple orders of magnitude.\n\n    The transformation steps are:\n    1. Optionally shift values so minimum becomes 0 (if shift_min=True).\n    2. Add a small safety term to avoid log(0).\n    3. Apply log10 transformation.\n\n    Args:\n        shift_min: Whether to shift data so the minimum value becomes 0.\n        safety_term: Small constant added before log to avoid log(0).\n\n    Attributes:\n        global_min: Minimum value observed during fitting.\n        shift_value: Amount to shift data (computed from global_min).\n        fitted: Whether the scaler has been fitted to data.\n    \"\"\"\n\n    def __init__(self, shift_min: bool = False, safety_term: float = 1e-8):\n        self.safety_term = safety_term\n        self.shift_min = shift_min  # Whether to shift minimum value to 0\n        self.global_min = None\n        self.shift_value = None\n        self.fitted = False\n\n    def fit(self, y: Union[torch.Tensor, \"np.ndarray\"]) -&gt; \"LogScaler\":\n        \"\"\"Fit the scaler by computing the global minimum.\n\n        Args:\n            y: Input values (can be tensor or array).\n\n        Returns:\n            self: Fitted scaler instance.\n        \"\"\"\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float32)\n\n        self.global_min = torch.min(y).item()\n        # Calculate shift value to make minimum = 0\n        self.shift_value = abs(self.global_min) if self.global_min &lt; 0 else 0\n        self.fitted = True\n        return self\n\n    def transform(self, y: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n        \"\"\"Transform data by applying log10 transformation.\n\n        Args:\n            y: Input values (can be tensor or array).\n\n        Returns:\n            Log-transformed values.\n\n        Raises:\n            ValueError: If scaler has not been fitted.\n        \"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler must be fitted before transform\")\n\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float32)\n\n        # Step 1: Shift values so minimum becomes 0\n        if self.shift_min:\n            # Shift to make minimum = 0\n            shifted = y - self.global_min\n        else:\n            # Use pre-calculated shift value\n            shifted = torch.clamp(y, min=0)  # Ensure no negative values\n\n        # Step 2: Add safety term to avoid log(0)\n        safe_values = shifted + self.safety_term\n\n        # Step 3: Apply log10\n        log_values = torch.log10(safe_values)\n\n        return log_values\n\n    def fit_transform(self, y: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n        \"\"\"Fit the scaler and transform data in one step.\n\n        Args:\n            y: Input values (can be tensor or array).\n\n        Returns:\n            Log-transformed values.\n        \"\"\"\n        return self.fit(y).transform(y)\n\n    def inverse_transform(\n        self, y_log: Union[torch.Tensor, \"np.ndarray\"]\n    ) -&gt; torch.Tensor:\n        \"\"\"Inverse transform log-transformed data back to original scale.\n\n        Args:\n            y_log: Log-transformed values.\n\n        Returns:\n            Values in original scale.\n\n        Raises:\n            ValueError: If scaler has not been fitted.\n        \"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler must be fitted before inverse_transform\")\n\n        if not isinstance(y_log, torch.Tensor):\n            y_log = torch.tensor(y_log, dtype=torch.float32)\n\n        # Step 1: Apply 10^y\n        exp_values = torch.pow(10, y_log)\n\n        # Step 2: Remove safety term\n        safe_removed = exp_values - self.safety_term\n\n        # Step 3: Remove shift to restore original range\n        if self.shift_min:\n            original_values = safe_removed - self.global_min\n        else:\n            original_values = safe_removed\n\n        return original_values\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.LogScaler.fit","title":"<code>fit(self, y)</code>","text":"<p>Fit the scaler by computing the global minimum.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Union[torch.Tensor, np.ndarray]</code> <p>Input values (can be tensor or array).</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted scaler instance.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>def fit(self, y: Union[torch.Tensor, \"np.ndarray\"]) -&gt; \"LogScaler\":\n    \"\"\"Fit the scaler by computing the global minimum.\n\n    Args:\n        y: Input values (can be tensor or array).\n\n    Returns:\n        self: Fitted scaler instance.\n    \"\"\"\n    if not isinstance(y, torch.Tensor):\n        y = torch.tensor(y, dtype=torch.float32)\n\n    self.global_min = torch.min(y).item()\n    # Calculate shift value to make minimum = 0\n    self.shift_value = abs(self.global_min) if self.global_min &lt; 0 else 0\n    self.fitted = True\n    return self\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.LogScaler.fit_transform","title":"<code>fit_transform(self, y)</code>","text":"<p>Fit the scaler and transform data in one step.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Union[torch.Tensor, np.ndarray]</code> <p>Input values (can be tensor or array).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Log-transformed values.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>def fit_transform(self, y: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n    \"\"\"Fit the scaler and transform data in one step.\n\n    Args:\n        y: Input values (can be tensor or array).\n\n    Returns:\n        Log-transformed values.\n    \"\"\"\n    return self.fit(y).transform(y)\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.LogScaler.inverse_transform","title":"<code>inverse_transform(self, y_log)</code>","text":"<p>Inverse transform log-transformed data back to original scale.</p> <p>Parameters:</p> Name Type Description Default <code>y_log</code> <code>Union[torch.Tensor, np.ndarray]</code> <p>Log-transformed values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Values in original scale.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If scaler has not been fitted.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>def inverse_transform(\n    self, y_log: Union[torch.Tensor, \"np.ndarray\"]\n) -&gt; torch.Tensor:\n    \"\"\"Inverse transform log-transformed data back to original scale.\n\n    Args:\n        y_log: Log-transformed values.\n\n    Returns:\n        Values in original scale.\n\n    Raises:\n        ValueError: If scaler has not been fitted.\n    \"\"\"\n    if not self.fitted:\n        raise ValueError(\"Scaler must be fitted before inverse_transform\")\n\n    if not isinstance(y_log, torch.Tensor):\n        y_log = torch.tensor(y_log, dtype=torch.float32)\n\n    # Step 1: Apply 10^y\n    exp_values = torch.pow(10, y_log)\n\n    # Step 2: Remove safety term\n    safe_removed = exp_values - self.safety_term\n\n    # Step 3: Remove shift to restore original range\n    if self.shift_min:\n        original_values = safe_removed - self.global_min\n    else:\n        original_values = safe_removed\n\n    return original_values\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.LogScaler.transform","title":"<code>transform(self, y)</code>","text":"<p>Transform data by applying log10 transformation.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Union[torch.Tensor, np.ndarray]</code> <p>Input values (can be tensor or array).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Log-transformed values.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If scaler has not been fitted.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>def transform(self, y: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n    \"\"\"Transform data by applying log10 transformation.\n\n    Args:\n        y: Input values (can be tensor or array).\n\n    Returns:\n        Log-transformed values.\n\n    Raises:\n        ValueError: If scaler has not been fitted.\n    \"\"\"\n    if not self.fitted:\n        raise ValueError(\"Scaler must be fitted before transform\")\n\n    if not isinstance(y, torch.Tensor):\n        y = torch.tensor(y, dtype=torch.float32)\n\n    # Step 1: Shift values so minimum becomes 0\n    if self.shift_min:\n        # Shift to make minimum = 0\n        shifted = y - self.global_min\n    else:\n        # Use pre-calculated shift value\n        shifted = torch.clamp(y, min=0)  # Ensure no negative values\n\n    # Step 2: Add safety term to avoid log(0)\n    safe_values = shifted + self.safety_term\n\n    # Step 3: Apply log10\n    log_values = torch.log10(safe_values)\n\n    return log_values\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.RobustMinMaxScaler","title":"<code> RobustMinMaxScaler        </code>","text":"<p>Robust MinMax scaler using quantiles for outlier-resistant normalization.</p> <p>This scaler provides robust scaling by using quantiles instead of min/max values, making it less sensitive to outliers. It can operate in global or feature-wise mode.</p> <p>Parameters:</p> Name Type Description Default <code>feature_range</code> <code>Tuple[float, float]</code> <p>Target range (min, max) for scaled values.</p> <code>(0, 1)</code> <code>global_scale</code> <code>bool</code> <p>If True, compute quantiles across all features globally. If False, compute quantiles independently for each feature.</p> <code>True</code> <code>robust</code> <code>bool</code> <p>If True, use quantiles for scaling. If False, use traditional min/max.</p> <code>True</code> <code>quantile_range</code> <code>Tuple[float, float]</code> <p>Tuple of (lower, upper) quantiles (e.g., (0.25, 0.75) for IQR).</p> <code>(0.25, 0.75)</code> <code>clip_outliers</code> <code>bool</code> <p>If True, clip values outside quantile range before scaling.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>min_val</code> <p>Fitted minimum (or lower quantile) value(s).</p> <code>max_val</code> <p>Fitted maximum (or upper quantile) value(s).</p> <code>fitted</code> <p>Whether the scaler has been fitted to data.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>class RobustMinMaxScaler:\n    \"\"\"Robust MinMax scaler using quantiles for outlier-resistant normalization.\n\n    This scaler provides robust scaling by using quantiles instead of min/max values,\n    making it less sensitive to outliers. It can operate in global or feature-wise mode.\n\n    Args:\n        feature_range: Target range (min, max) for scaled values.\n        global_scale: If True, compute quantiles across all features globally.\n            If False, compute quantiles independently for each feature.\n        robust: If True, use quantiles for scaling. If False, use traditional min/max.\n        quantile_range: Tuple of (lower, upper) quantiles (e.g., (0.25, 0.75) for IQR).\n        clip_outliers: If True, clip values outside quantile range before scaling.\n\n    Attributes:\n        min_val: Fitted minimum (or lower quantile) value(s).\n        max_val: Fitted maximum (or upper quantile) value(s).\n        fitted: Whether the scaler has been fitted to data.\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_range: Tuple[float, float] = (0, 1),\n        global_scale: bool = True,\n        robust: bool = True,\n        quantile_range: Tuple[float, float] = (0.25, 0.75),\n        clip_outliers: bool = False,\n    ):\n        self.feature_range = feature_range\n        self.global_scale = global_scale\n        self.robust = robust\n        self.quantile_range = quantile_range\n        self.clip_outliers = clip_outliers\n        self.min_val = None\n        self.max_val = None\n        self.fitted = False\n\n    def fit(self, X: Union[torch.Tensor, \"np.ndarray\"]) -&gt; \"RobustMinMaxScaler\":\n        \"\"\"Fit the scaler to the data by computing quantiles or min/max values.\n\n        Args:\n            X: Input tensor of shape (batch_size, features).\n\n        Returns:\n            self: Fitted scaler instance.\n        \"\"\"\n        if not isinstance(X, torch.Tensor):\n            X = torch.tensor(X, dtype=torch.float32)\n\n        if self.robust:\n            # Use quantiles for robust scaling\n            if self.global_scale:\n                # Global quantiles across all values\n                flat_X = X.flatten()\n                # If tensor is too large, use sampling for quantile calculation\n                max_samples = 1000000  # 1M samples max\n                if len(flat_X) &gt; max_samples:\n                    # Randomly sample from the tensor\n                    indices = torch.randperm(len(flat_X))[:max_samples]\n                    sampled_X = flat_X[indices]\n                    self.min_val = torch.quantile(sampled_X, self.quantile_range[0])\n                    self.max_val = torch.quantile(sampled_X, self.quantile_range[1])\n                else:\n                    self.min_val = torch.quantile(flat_X, self.quantile_range[0])\n                    self.max_val = torch.quantile(flat_X, self.quantile_range[1])\n            else:\n                # Feature-wise quantiles\n                self.min_val = torch.quantile(\n                    X, self.quantile_range[0], dim=0, keepdim=True\n                )\n                self.max_val = torch.quantile(\n                    X, self.quantile_range[1], dim=0, keepdim=True\n                )\n        else:\n            # Use traditional min/max\n            if self.global_scale:\n                # Global min/max across all values\n                self.min_val = torch.min(X)\n                self.max_val = torch.max(X)\n            else:\n                # Feature-wise min/max\n                self.min_val = torch.min(X, dim=0, keepdim=True)[0]\n                self.max_val = torch.max(X, dim=0, keepdim=True)[0]\n\n        self.fitted = True\n        return self\n\n    def transform(self, X: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n        \"\"\"Transform the data using fitted scaling parameters.\n\n        Args:\n            X: Input tensor of shape (batch_size, features).\n\n        Returns:\n            Scaled tensor in the target feature_range.\n\n        Raises:\n            ValueError: If scaler has not been fitted.\n        \"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler must be fitted before transform\")\n\n        if not isinstance(X, torch.Tensor):\n            X = torch.tensor(X, dtype=torch.float32)\n\n        # Clip outliers if using robust scaling\n        if self.robust and self.clip_outliers:\n            X = torch.clamp(X, min=self.min_val, max=self.max_val)\n\n        # Avoid division by zero\n        range_val = self.max_val - self.min_val\n        range_val = torch.where(range_val == 0, torch.ones_like(range_val), range_val)\n\n        # Scale to [0, 1]\n        scaled = (X - self.min_val) / range_val\n\n        # Scale to desired range\n        min_target, max_target = self.feature_range\n        return scaled * (max_target - min_target) + min_target\n\n    def fit_transform(self, X: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n        \"\"\"Fit the scaler and transform the data in one step.\n\n        Args:\n            X: Input tensor of shape (batch_size, features).\n\n        Returns:\n            Scaled tensor in the target feature_range.\n        \"\"\"\n        return self.fit(X).transform(X)\n\n    def inverse_transform(self, X: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n        \"\"\"Inverse transform scaled data back to original scale.\n\n        Args:\n            X: Scaled tensor.\n\n        Returns:\n            Tensor in original scale.\n\n        Raises:\n            ValueError: If scaler has not been fitted.\n        \"\"\"\n        if not self.fitted:\n            raise ValueError(\"Scaler must be fitted before inverse_transform\")\n\n        if not isinstance(X, torch.Tensor):\n            X = torch.tensor(X, dtype=torch.float32)\n\n        min_target, max_target = self.feature_range\n\n        # Scale back to [0, 1]\n        normalized = (X - min_target) / (max_target - min_target)\n\n        # Scale back to original range\n        range_val = self.max_val - self.min_val\n        range_val = torch.where(range_val == 0, torch.ones_like(range_val), range_val)\n\n        return normalized * range_val + self.min_val\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.RobustMinMaxScaler.fit","title":"<code>fit(self, X)</code>","text":"<p>Fit the scaler to the data by computing quantiles or min/max values.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[torch.Tensor, np.ndarray]</code> <p>Input tensor of shape (batch_size, features).</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted scaler instance.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>def fit(self, X: Union[torch.Tensor, \"np.ndarray\"]) -&gt; \"RobustMinMaxScaler\":\n    \"\"\"Fit the scaler to the data by computing quantiles or min/max values.\n\n    Args:\n        X: Input tensor of shape (batch_size, features).\n\n    Returns:\n        self: Fitted scaler instance.\n    \"\"\"\n    if not isinstance(X, torch.Tensor):\n        X = torch.tensor(X, dtype=torch.float32)\n\n    if self.robust:\n        # Use quantiles for robust scaling\n        if self.global_scale:\n            # Global quantiles across all values\n            flat_X = X.flatten()\n            # If tensor is too large, use sampling for quantile calculation\n            max_samples = 1000000  # 1M samples max\n            if len(flat_X) &gt; max_samples:\n                # Randomly sample from the tensor\n                indices = torch.randperm(len(flat_X))[:max_samples]\n                sampled_X = flat_X[indices]\n                self.min_val = torch.quantile(sampled_X, self.quantile_range[0])\n                self.max_val = torch.quantile(sampled_X, self.quantile_range[1])\n            else:\n                self.min_val = torch.quantile(flat_X, self.quantile_range[0])\n                self.max_val = torch.quantile(flat_X, self.quantile_range[1])\n        else:\n            # Feature-wise quantiles\n            self.min_val = torch.quantile(\n                X, self.quantile_range[0], dim=0, keepdim=True\n            )\n            self.max_val = torch.quantile(\n                X, self.quantile_range[1], dim=0, keepdim=True\n            )\n    else:\n        # Use traditional min/max\n        if self.global_scale:\n            # Global min/max across all values\n            self.min_val = torch.min(X)\n            self.max_val = torch.max(X)\n        else:\n            # Feature-wise min/max\n            self.min_val = torch.min(X, dim=0, keepdim=True)[0]\n            self.max_val = torch.max(X, dim=0, keepdim=True)[0]\n\n    self.fitted = True\n    return self\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.RobustMinMaxScaler.fit_transform","title":"<code>fit_transform(self, X)</code>","text":"<p>Fit the scaler and transform the data in one step.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[torch.Tensor, np.ndarray]</code> <p>Input tensor of shape (batch_size, features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scaled tensor in the target feature_range.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>def fit_transform(self, X: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n    \"\"\"Fit the scaler and transform the data in one step.\n\n    Args:\n        X: Input tensor of shape (batch_size, features).\n\n    Returns:\n        Scaled tensor in the target feature_range.\n    \"\"\"\n    return self.fit(X).transform(X)\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.RobustMinMaxScaler.inverse_transform","title":"<code>inverse_transform(self, X)</code>","text":"<p>Inverse transform scaled data back to original scale.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[torch.Tensor, np.ndarray]</code> <p>Scaled tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor in original scale.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If scaler has not been fitted.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>def inverse_transform(self, X: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n    \"\"\"Inverse transform scaled data back to original scale.\n\n    Args:\n        X: Scaled tensor.\n\n    Returns:\n        Tensor in original scale.\n\n    Raises:\n        ValueError: If scaler has not been fitted.\n    \"\"\"\n    if not self.fitted:\n        raise ValueError(\"Scaler must be fitted before inverse_transform\")\n\n    if not isinstance(X, torch.Tensor):\n        X = torch.tensor(X, dtype=torch.float32)\n\n    min_target, max_target = self.feature_range\n\n    # Scale back to [0, 1]\n    normalized = (X - min_target) / (max_target - min_target)\n\n    # Scale back to original range\n    range_val = self.max_val - self.min_val\n    range_val = torch.where(range_val == 0, torch.ones_like(range_val), range_val)\n\n    return normalized * range_val + self.min_val\n</code></pre>"},{"location":"emit_utils/preprocess/#hypercoast.emit_utils.preprocess.RobustMinMaxScaler.transform","title":"<code>transform(self, X)</code>","text":"<p>Transform the data using fitted scaling parameters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[torch.Tensor, np.ndarray]</code> <p>Input tensor of shape (batch_size, features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scaled tensor in the target feature_range.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If scaler has not been fitted.</p> Source code in <code>hypercoast/emit_utils/preprocess.py</code> <pre><code>def transform(self, X: Union[torch.Tensor, \"np.ndarray\"]) -&gt; torch.Tensor:\n    \"\"\"Transform the data using fitted scaling parameters.\n\n    Args:\n        X: Input tensor of shape (batch_size, features).\n\n    Returns:\n        Scaled tensor in the target feature_range.\n\n    Raises:\n        ValueError: If scaler has not been fitted.\n    \"\"\"\n    if not self.fitted:\n        raise ValueError(\"Scaler must be fitted before transform\")\n\n    if not isinstance(X, torch.Tensor):\n        X = torch.tensor(X, dtype=torch.float32)\n\n    # Clip outliers if using robust scaling\n    if self.robust and self.clip_outliers:\n        X = torch.clamp(X, min=self.min_val, max=self.max_val)\n\n    # Avoid division by zero\n    range_val = self.max_val - self.min_val\n    range_val = torch.where(range_val == 0, torch.ones_like(range_val), range_val)\n\n    # Scale to [0, 1]\n    scaled = (X - self.min_val) / range_val\n\n    # Scale to desired range\n    min_target, max_target = self.feature_range\n    return scaled * (max_target - min_target) + min_target\n</code></pre>"},{"location":"examples/EMIT_TSS/","title":"EMIT TSS","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[all]\"\n</pre> # %pip install \"hypercoast[all]\" In\u00a0[\u00a0]: Copied! <pre>import torch\nimport numpy as np\nimport os\nimport pickle\n\nfrom hypercoast import download_file\nfrom hypercoast.emit_utils import *\n</pre> import torch import numpy as np import os import pickle  from hypercoast import download_file from hypercoast.emit_utils import * <p>First, download the sample data from Google Drive. The file size is about 3.0 GB. It may take a while to download. Please be patient. The downloaded zip file will be saved as <code>EMIT-sample-data.zip</code> in the current working directory and automatically unzipped to <code>data</code> folder.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://drive.google.com/file/d/1q80PhA_vrLgIjRHuyxunVoWXmVUNgggV/view\"\n</pre> url = \"https://drive.google.com/file/d/1q80PhA_vrLgIjRHuyxunVoWXmVUNgggV/view\" In\u00a0[\u00a0]: Copied! <pre>download_file(url, output=\"EMIT-sample-data.zip\")\n</pre> download_file(url, output=\"EMIT-sample-data.zip\") In\u00a0[\u00a0]: Copied! <pre># === Device ===\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# === Parameters ===\nselected_bands = [\n    403,\n    411,\n    418,\n    425,\n    433,\n    440,\n    448,\n    455,\n    463,\n    470,\n    477,\n    485,\n    492,\n    500,\n    507,\n    515,\n    522,\n    530,\n    537,\n    544,\n    552,\n    559,\n    567,\n    574,\n    582,\n    589,\n    597,\n    604,\n    611,\n    619,\n    626,\n    634,\n    641,\n    649,\n    656,\n    664,\n    671,\n    679,\n    686,\n    693,\n    701,\n    708,\n    716,\n    723,\n    731,\n    738,\n    746,\n    753,\n    768,\n    776,\n    783,\n    790,\n    798,\n    805,\n    813,\n    820,\n    828,\n    835,\n    843,\n    850,\n    858,\n    865,\n    873,\n    880,\n    887,\n    895,\n]\n\nexcel_path = \"data/Gloria_updated_07242025.xlsx\"\ntest_files = [\n    \"data/GreatLake_all_data.xlsx\",\n    \"data/GOA_insitu_data_09052025.xlsx\",\n    \"data/satellite_for_EMIT_09052025.xlsx\",\n]\nnc_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2W.nc\"\nrgb_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2R.nc\"\nsave_dir = \"./EMIT/test_TSS\"\nos.makedirs(save_dir, exist_ok=True)\n</pre> # === Device === device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\")  # === Parameters === selected_bands = [     403,     411,     418,     425,     433,     440,     448,     455,     463,     470,     477,     485,     492,     500,     507,     515,     522,     530,     537,     544,     552,     559,     567,     574,     582,     589,     597,     604,     611,     619,     626,     634,     641,     649,     656,     664,     671,     679,     686,     693,     701,     708,     716,     723,     731,     738,     746,     753,     768,     776,     783,     790,     798,     805,     813,     820,     828,     835,     843,     850,     858,     865,     873,     880,     887,     895, ]  excel_path = \"data/Gloria_updated_07242025.xlsx\" test_files = [     \"data/GreatLake_all_data.xlsx\",     \"data/GOA_insitu_data_09052025.xlsx\",     \"data/satellite_for_EMIT_09052025.xlsx\", ] nc_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2W.nc\" rgb_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2R.nc\" save_dir = \"./EMIT/test_TSS\" os.makedirs(save_dir, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre># === Training dataset ===\n(\n    train_real_dl,\n    test_real_dl,\n    input_dim,\n    output_dim,\n    train_ids,\n    test_ids,\n    scalers_Rrs_real,\n    scalers_dict,\n) = load_real_data_Robust(\n    excel_path=excel_path,\n    selected_bands=selected_bands,\n    target_parameter=\"TSS\",\n    lower_quantile=0,\n    upper_quantile=1,\n)\n</pre> # === Training dataset === (     train_real_dl,     test_real_dl,     input_dim,     output_dim,     train_ids,     test_ids,     scalers_Rrs_real,     scalers_dict, ) = load_real_data_Robust(     excel_path=excel_path,     selected_bands=selected_bands,     target_parameter=\"TSS\",     lower_quantile=0,     upper_quantile=1, ) In\u00a0[\u00a0]: Copied! <pre># === External test datasets ===\ntest_dls, test_ids_list, test_dates_list = [], [], []\n\nfor file in test_files:\n    dl, _, _, ids, dates = load_real_test_Robust(\n        excel_path=file,\n        selected_bands=selected_bands,\n        scaler_Rrs=scalers_Rrs_real,\n        scalers_dict=scalers_dict,\n        target_parameter=\"TSS\",\n    )\n    test_dls.append(dl)\n    test_ids_list.append(ids)\n    test_dates_list.append(dates)\n</pre> # === External test datasets === test_dls, test_ids_list, test_dates_list = [], [], []  for file in test_files:     dl, _, _, ids, dates = load_real_test_Robust(         excel_path=file,         selected_bands=selected_bands,         scaler_Rrs=scalers_Rrs_real,         scalers_dict=scalers_dict,         target_parameter=\"TSS\",     )     test_dls.append(dl)     test_ids_list.append(ids)     test_dates_list.append(dates) In\u00a0[\u00a0]: Copied! <pre># === EMIT image preprocess ===\ntest_loader, Rrs, mask, latitude, longitude = preprocess_emit_data_Robust(\n    nc_path=nc_path,\n    scaler_Rrs=scalers_Rrs_real,\n    full_band_wavelengths=np.array(selected_bands),\n)\n</pre> # === EMIT image preprocess === test_loader, Rrs, mask, latitude, longitude = preprocess_emit_data_Robust(     nc_path=nc_path,     scaler_Rrs=scalers_Rrs_real,     full_band_wavelengths=np.array(selected_bands), ) In\u00a0[\u00a0]: Copied! <pre># Define the Mixture-of-Experts Variational Autoencoder (MoE-VAE) model\nmodel = MoE_VAE(\n    input_dim=input_dim,\n    output_dim=output_dim,\n    latent_dim=16,\n    encoder_hidden_dims=[128, 64, 32],\n    decoder_hidden_dims=[32, 64, 128],\n    activation=\"leakyrelu\",\n    use_norm=\"layer\",\n    use_dropout=False,\n    use_softplus_output=False,\n    num_experts=4,\n    k=2,\n    noisy_gating=True,\n).to(device)\n</pre> # Define the Mixture-of-Experts Variational Autoencoder (MoE-VAE) model model = MoE_VAE(     input_dim=input_dim,     output_dim=output_dim,     latent_dim=16,     encoder_hidden_dims=[128, 64, 32],     decoder_hidden_dims=[32, 64, 128],     activation=\"leakyrelu\",     use_norm=\"layer\",     use_dropout=False,     use_softplus_output=False,     num_experts=4,     k=2,     noisy_gating=True, ).to(device) In\u00a0[\u00a0]: Copied! <pre># Define optimizer and Train the MoE-VAE model\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ntrain_log = train(\n    model, train_real_dl, device, epochs=400, optimizer=optimizer, save_dir=save_dir\n)\n</pre> # Define optimizer and Train the MoE-VAE model optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) train_log = train(     model, train_real_dl, device, epochs=400, optimizer=optimizer, save_dir=save_dir ) In\u00a0[\u00a0]: Copied! <pre># Evaluate model on test set, compute metrics, save and plot results\npredictions, actuals = evaluate(model, test_real_dl, device, scalers_dict)\nepsilon, beta, nrmse, rmsle, mape, bias, mae = calculate_metrics(predictions, actuals)\ntest_loss = mae\n\nsave_results_to_excel(\n    test_ids, actuals, predictions, os.path.join(save_dir, \"test_results.xlsx\")\n)\nplot_results_with_density(\n    predictions, actuals, save_dir, mode=\"test_results\", xlim=(-2, 4), ylim=(-2, 4)\n)\n</pre> # Evaluate model on test set, compute metrics, save and plot results predictions, actuals = evaluate(model, test_real_dl, device, scalers_dict) epsilon, beta, nrmse, rmsle, mape, bias, mae = calculate_metrics(predictions, actuals) test_loss = mae  save_results_to_excel(     test_ids, actuals, predictions, os.path.join(save_dir, \"test_results.xlsx\") ) plot_results_with_density(     predictions, actuals, save_dir, mode=\"test_results\", xlim=(-2, 4), ylim=(-2, 4) ) In\u00a0[\u00a0]: Copied! <pre># Evaluate on external test datasets\nfor dl, ids, dates, path in zip(test_dls, test_ids_list, test_dates_list, test_files):\n    preds, acts = evaluate(model, dl, device, scalers_dict)\n    save_results_from_excel_for_test(preds, acts, ids, dates, path, save_dir)\n    # Get unique mode name from file name\n    mode_name = os.path.splitext(os.path.basename(path))[0]\n    # Save individual plot\n    plot_results(\n        preds,\n        acts,\n        save_dir,\n        threshold=10,\n        mode=f\"test_{mode_name}\",\n        xlim=(-2, 4),\n        ylim=(-2, 4),\n    )\n</pre> # Evaluate on external test datasets for dl, ids, dates, path in zip(test_dls, test_ids_list, test_dates_list, test_files):     preds, acts = evaluate(model, dl, device, scalers_dict)     save_results_from_excel_for_test(preds, acts, ids, dates, path, save_dir)     # Get unique mode name from file name     mode_name = os.path.splitext(os.path.basename(path))[0]     # Save individual plot     plot_results(         preds,         acts,         save_dir,         threshold=10,         mode=f\"test_{mode_name}\",         xlim=(-2, 4),         ylim=(-2, 4),     ) In\u00a0[\u00a0]: Copied! <pre># Save scalers for Rrs and other features\ntorch.save(scalers_dict, os.path.join(save_dir, \"scaler.pt\"))\nwith open(os.path.join(save_dir, \"scalers_Rrs_real.pkl\"), \"wb\") as f:\n    pickle.dump(scalers_Rrs_real, f)\n</pre> # Save scalers for Rrs and other features torch.save(scalers_dict, os.path.join(save_dir, \"scaler.pt\")) with open(os.path.join(save_dir, \"scalers_Rrs_real.pkl\"), \"wb\") as f:     pickle.dump(scalers_Rrs_real, f) In\u00a0[\u00a0]: Copied! <pre># Perform model inference and generate a spatial map of predicted TSS concentration\nOutput = infer_and_visualize_single_model_Robust(\n    model=model,\n    test_loader=test_loader,\n    Rrs=Rrs,\n    mask=mask,\n    latitude=latitude,\n    longitude=longitude,\n    save_folder=save_dir,\n    rgb_nc_file=rgb_path,\n    structure_name=\"TSS\",\n    TSS_scalers_dict=scalers_dict,\n    vmin=0,\n    vmax=50,\n)\n</pre> # Perform model inference and generate a spatial map of predicted TSS concentration Output = infer_and_visualize_single_model_Robust(     model=model,     test_loader=test_loader,     Rrs=Rrs,     mask=mask,     latitude=latitude,     longitude=longitude,     save_folder=save_dir,     rgb_nc_file=rgb_path,     structure_name=\"TSS\",     TSS_scalers_dict=scalers_dict,     vmin=0,     vmax=50, ) In\u00a0[\u00a0]: Copied! <pre># Save predicted Chlorophyll-a as GeoTIFF and visualize the spatial distribution on map\ntif_path = os.path.join(save_dir, \"TSS.tif\")\n\nnpy_to_tif(npy_input=Output, out_tif=tif_path, resolution_m=30)\n\nwith rasterio.open(tif_path) as src:\n    img = src.read(1)\n    transform = src.transform\n    bounds = src.bounds\nimg_masked = np.where(img &lt; 0, np.nan, img)\nextent = [bounds.left, bounds.right, bounds.bottom, bounds.top]\nplt.figure(figsize=(8, 6))\nim = plt.imshow(img_masked, cmap=\"jet\", vmin=0, vmax=50, extent=extent, origin=\"upper\")\nplt.colorbar(im, label=\"TSS\")\nplt.title(\"TSS\")\nplt.show()\n</pre> # Save predicted Chlorophyll-a as GeoTIFF and visualize the spatial distribution on map tif_path = os.path.join(save_dir, \"TSS.tif\")  npy_to_tif(npy_input=Output, out_tif=tif_path, resolution_m=30)  with rasterio.open(tif_path) as src:     img = src.read(1)     transform = src.transform     bounds = src.bounds img_masked = np.where(img &lt; 0, np.nan, img) extent = [bounds.left, bounds.right, bounds.bottom, bounds.top] plt.figure(figsize=(8, 6)) im = plt.imshow(img_masked, cmap=\"jet\", vmin=0, vmax=50, extent=extent, origin=\"upper\") plt.colorbar(im, label=\"TSS\") plt.title(\"TSS\") plt.show()"},{"location":"examples/EMIT_TSS/#tss-prediction-using-emit-data","title":"TSS Prediction using EMIT Data\u00b6","text":"<p>This notebook demonstrates how to predict Total Suspended Solids (TSS) concentrations using EMIT hyperspectral imagery and a Mixture-of-Experts Variational Autoencoder (MoE-VAE), including key steps such as:</p> <ul> <li>\u2705 Model Training and Testing</li> <li>\u2705 Inference</li> <li>\u2705 Visualization</li> </ul>"},{"location":"examples/EMIT_aCDOM/","title":"EMIT aCDOM","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[all]\"\n</pre> # %pip install \"hypercoast[all]\" In\u00a0[\u00a0]: Copied! <pre>import torch\nimport numpy as np\nimport os\nimport pickle\n\nfrom hypercoast import download_file\nfrom hypercoast.emit_utils import *\n</pre> import torch import numpy as np import os import pickle  from hypercoast import download_file from hypercoast.emit_utils import * <p>First, download the sample data from Google Drive. The file size is about 3.0 GB. It may take a while to download. Please be patient. The downloaded zip file will be saved as <code>EMIT-sample-data.zip</code> in the current working directory and automatically unzipped to <code>data</code> folder.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://drive.google.com/file/d/1q80PhA_vrLgIjRHuyxunVoWXmVUNgggV/view\"\n</pre> url = \"https://drive.google.com/file/d/1q80PhA_vrLgIjRHuyxunVoWXmVUNgggV/view\" In\u00a0[\u00a0]: Copied! <pre>download_file(url, output=\"EMIT-sample-data.zip\")\n</pre> download_file(url, output=\"EMIT-sample-data.zip\") In\u00a0[\u00a0]: Copied! <pre># === Device ===\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# === Parameters ===\nselected_bands = [\n    403,\n    411,\n    418,\n    425,\n    433,\n    440,\n    448,\n    455,\n    463,\n    470,\n    477,\n    485,\n    492,\n    500,\n    507,\n    515,\n    522,\n    530,\n    537,\n    544,\n    552,\n    559,\n    567,\n    574,\n    582,\n    589,\n    597,\n    604,\n    611,\n    619,\n    626,\n    634,\n    641,\n    649,\n    656,\n    664,\n    671,\n    679,\n    686,\n    693,\n    700,\n]\n\nexcel_path = \"data/Gloria_updated_07242025.xlsx\"\ntest_files = [\n    \"data/GreatLake_all_data.xlsx\",\n    \"data/GOA_insitu_data_09052025.xlsx\",\n    \"data/satellite_for_EMIT_09052025.xlsx\",\n]\nnc_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2W.nc\"\nrgb_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2R.nc\"\nsave_dir = \"./EMIT/test_aCDOM\"\nos.makedirs(save_dir, exist_ok=True)\n</pre> # === Device === device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\")  # === Parameters === selected_bands = [     403,     411,     418,     425,     433,     440,     448,     455,     463,     470,     477,     485,     492,     500,     507,     515,     522,     530,     537,     544,     552,     559,     567,     574,     582,     589,     597,     604,     611,     619,     626,     634,     641,     649,     656,     664,     671,     679,     686,     693,     700, ]  excel_path = \"data/Gloria_updated_07242025.xlsx\" test_files = [     \"data/GreatLake_all_data.xlsx\",     \"data/GOA_insitu_data_09052025.xlsx\",     \"data/satellite_for_EMIT_09052025.xlsx\", ] nc_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2W.nc\" rgb_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2R.nc\" save_dir = \"./EMIT/test_aCDOM\" os.makedirs(save_dir, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre># === Training dataset ===\n(\n    train_real_dl,\n    test_real_dl,\n    input_dim,\n    output_dim,\n    train_ids,\n    test_ids,\n    scalers_Rrs_real,\n    scalers_dict,\n) = load_real_data_Robust(\n    excel_path=excel_path,\n    selected_bands=selected_bands,\n    target_parameter=\"aCDOM440\",\n    lower_quantile=0,\n    upper_quantile=1,\n)\n</pre> # === Training dataset === (     train_real_dl,     test_real_dl,     input_dim,     output_dim,     train_ids,     test_ids,     scalers_Rrs_real,     scalers_dict, ) = load_real_data_Robust(     excel_path=excel_path,     selected_bands=selected_bands,     target_parameter=\"aCDOM440\",     lower_quantile=0,     upper_quantile=1, ) In\u00a0[\u00a0]: Copied! <pre># === External test datasets ===\ntest_dls, test_ids_list, test_dates_list = [], [], []\n\nfor file in test_files:\n    dl, _, _, ids, dates = load_real_test_Robust(\n        excel_path=file,\n        selected_bands=selected_bands,\n        scaler_Rrs=scalers_Rrs_real,\n        scalers_dict=scalers_dict,\n        target_parameter=\"aCDOM440\",\n    )\n    test_dls.append(dl)\n    test_ids_list.append(ids)\n    test_dates_list.append(dates)\n</pre> # === External test datasets === test_dls, test_ids_list, test_dates_list = [], [], []  for file in test_files:     dl, _, _, ids, dates = load_real_test_Robust(         excel_path=file,         selected_bands=selected_bands,         scaler_Rrs=scalers_Rrs_real,         scalers_dict=scalers_dict,         target_parameter=\"aCDOM440\",     )     test_dls.append(dl)     test_ids_list.append(ids)     test_dates_list.append(dates) In\u00a0[\u00a0]: Copied! <pre># === EMIT image preprocess ===\ntest_loader, Rrs, mask, latitude, longitude = preprocess_emit_data_Robust(\n    nc_path=nc_path,\n    scaler_Rrs=scalers_Rrs_real,\n    full_band_wavelengths=np.array(selected_bands),\n)\n</pre> # === EMIT image preprocess === test_loader, Rrs, mask, latitude, longitude = preprocess_emit_data_Robust(     nc_path=nc_path,     scaler_Rrs=scalers_Rrs_real,     full_band_wavelengths=np.array(selected_bands), ) In\u00a0[\u00a0]: Copied! <pre># Define the Mixture-of-Experts Variational Autoencoder (MoE-VAE) model\nmodel = MoE_VAE(\n    input_dim=input_dim,\n    output_dim=output_dim,\n    latent_dim=16,\n    encoder_hidden_dims=[64, 32],\n    decoder_hidden_dims=[32, 64],\n    activation=\"leakyrelu\",\n    use_norm=\"layer\",\n    use_dropout=False,\n    use_softplus_output=False,\n    num_experts=4,\n    k=2,\n    noisy_gating=True,\n).to(device)\n</pre> # Define the Mixture-of-Experts Variational Autoencoder (MoE-VAE) model model = MoE_VAE(     input_dim=input_dim,     output_dim=output_dim,     latent_dim=16,     encoder_hidden_dims=[64, 32],     decoder_hidden_dims=[32, 64],     activation=\"leakyrelu\",     use_norm=\"layer\",     use_dropout=False,     use_softplus_output=False,     num_experts=4,     k=2,     noisy_gating=True, ).to(device) In\u00a0[\u00a0]: Copied! <pre># Define optimizer and Train the MoE-VAE model\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ntrain_log = train(\n    model, train_real_dl, device, epochs=400, optimizer=optimizer, save_dir=save_dir\n)\n</pre> # Define optimizer and Train the MoE-VAE model optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) train_log = train(     model, train_real_dl, device, epochs=400, optimizer=optimizer, save_dir=save_dir ) In\u00a0[\u00a0]: Copied! <pre># Evaluate model on test set, compute metrics, save and plot results\npredictions, actuals = evaluate(model, test_real_dl, device, scalers_dict)\nepsilon, beta, nrmse, rmsle, mape, bias, mae = calculate_metrics(predictions, actuals)\ntest_loss = mae\n\nsave_results_to_excel(\n    test_ids, actuals, predictions, os.path.join(save_dir, \"test_results.xlsx\")\n)\nplot_results_with_density(\n    predictions, actuals, save_dir, mode=\"test_results\", xlim=(-4, 2), ylim=(-4, 2)\n)\n</pre> # Evaluate model on test set, compute metrics, save and plot results predictions, actuals = evaluate(model, test_real_dl, device, scalers_dict) epsilon, beta, nrmse, rmsle, mape, bias, mae = calculate_metrics(predictions, actuals) test_loss = mae  save_results_to_excel(     test_ids, actuals, predictions, os.path.join(save_dir, \"test_results.xlsx\") ) plot_results_with_density(     predictions, actuals, save_dir, mode=\"test_results\", xlim=(-4, 2), ylim=(-4, 2) ) In\u00a0[\u00a0]: Copied! <pre># Evaluate on external test datasets\nfor dl, ids, dates, path in zip(test_dls, test_ids_list, test_dates_list, test_files):\n    preds, acts = evaluate(model, dl, device, scalers_dict)\n    save_results_from_excel_for_test(preds, acts, ids, dates, path, save_dir)\n    # Get unique mode name from file name\n    mode_name = os.path.splitext(os.path.basename(path))[0]\n    # Save individual plot\n    plot_results(\n        preds,\n        acts,\n        save_dir,\n        threshold=10,\n        mode=f\"test_{mode_name}\",\n        xlim=(-4, 2),\n        ylim=(-4, 2),\n    )\n</pre> # Evaluate on external test datasets for dl, ids, dates, path in zip(test_dls, test_ids_list, test_dates_list, test_files):     preds, acts = evaluate(model, dl, device, scalers_dict)     save_results_from_excel_for_test(preds, acts, ids, dates, path, save_dir)     # Get unique mode name from file name     mode_name = os.path.splitext(os.path.basename(path))[0]     # Save individual plot     plot_results(         preds,         acts,         save_dir,         threshold=10,         mode=f\"test_{mode_name}\",         xlim=(-4, 2),         ylim=(-4, 2),     ) In\u00a0[\u00a0]: Copied! <pre># Save scalers for Rrs and other features\ntorch.save(scalers_dict, os.path.join(save_dir, \"scaler.pt\"))\nwith open(os.path.join(save_dir, \"scalers_Rrs_real.pkl\"), \"wb\") as f:\n    pickle.dump(scalers_Rrs_real, f)\n</pre> # Save scalers for Rrs and other features torch.save(scalers_dict, os.path.join(save_dir, \"scaler.pt\")) with open(os.path.join(save_dir, \"scalers_Rrs_real.pkl\"), \"wb\") as f:     pickle.dump(scalers_Rrs_real, f) In\u00a0[\u00a0]: Copied! <pre># Perform model inference and generate a spatial map of predicted aCDOM concentration\nOutput = infer_and_visualize_single_model_Robust(\n    model=model,\n    test_loader=test_loader,\n    Rrs=Rrs,\n    mask=mask,\n    latitude=latitude,\n    longitude=longitude,\n    save_folder=save_dir,\n    rgb_nc_file=rgb_path,\n    structure_name=\"aCDOM440\",\n    TSS_scalers_dict=scalers_dict,\n    vmin=0,\n    vmax=5,\n)\n</pre> # Perform model inference and generate a spatial map of predicted aCDOM concentration Output = infer_and_visualize_single_model_Robust(     model=model,     test_loader=test_loader,     Rrs=Rrs,     mask=mask,     latitude=latitude,     longitude=longitude,     save_folder=save_dir,     rgb_nc_file=rgb_path,     structure_name=\"aCDOM440\",     TSS_scalers_dict=scalers_dict,     vmin=0,     vmax=5, ) In\u00a0[\u00a0]: Copied! <pre># Save predicted Chlorophyll-a as GeoTIFF and visualize the spatial distribution on map\ntif_path = os.path.join(save_dir, \"aCDOM440.tif\")\n\nnpy_to_tif(npy_input=Output, out_tif=tif_path, resolution_m=30)\n\nwith rasterio.open(tif_path) as src:\n    img = src.read(1)\n    transform = src.transform\n    bounds = src.bounds\nimg_masked = np.where(img &lt; 0, np.nan, img)\nextent = [bounds.left, bounds.right, bounds.bottom, bounds.top]\nplt.figure(figsize=(8, 6))\nim = plt.imshow(img_masked, cmap=\"jet\", vmin=0, vmax=5, extent=extent, origin=\"upper\")\nplt.colorbar(im, label=\"aCDOM440\")\nplt.title(\"aCDOM440\")\nplt.show()\n</pre> # Save predicted Chlorophyll-a as GeoTIFF and visualize the spatial distribution on map tif_path = os.path.join(save_dir, \"aCDOM440.tif\")  npy_to_tif(npy_input=Output, out_tif=tif_path, resolution_m=30)  with rasterio.open(tif_path) as src:     img = src.read(1)     transform = src.transform     bounds = src.bounds img_masked = np.where(img &lt; 0, np.nan, img) extent = [bounds.left, bounds.right, bounds.bottom, bounds.top] plt.figure(figsize=(8, 6)) im = plt.imshow(img_masked, cmap=\"jet\", vmin=0, vmax=5, extent=extent, origin=\"upper\") plt.colorbar(im, label=\"aCDOM440\") plt.title(\"aCDOM440\") plt.show()"},{"location":"examples/EMIT_aCDOM/#acdom440-prediction-using-emit-data","title":"aCDOM440 Prediction using EMIT Data\u00b6","text":"<p>This notebook demonstrates how to Absorption by Colored Dissolved Organic Matter at 440 nm (aCDOM440) concentrations using EMIT hyperspectral imagery and a Mixture-of-Experts Variational Autoencoder (MoE-VAE), including key steps such as:</p> <ul> <li>\u2705 Model Training and Testing</li> <li>\u2705 Inference</li> <li>\u2705 Visualization</li> </ul>"},{"location":"examples/EMIT_chla/","title":"EMIT chla","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[all]\"\n</pre> # %pip install \"hypercoast[all]\" In\u00a0[\u00a0]: Copied! <pre>import torch\nimport numpy as np\nimport os\n\nfrom hypercoast import download_file\nfrom hypercoast.emit_utils import *\n</pre> import torch import numpy as np import os  from hypercoast import download_file from hypercoast.emit_utils import * <p>First, download the sample data from Google Drive. The file size is about 3.0 GB. It may take a while to download. Please be patient. The downloaded zip file will be saved as <code>EMIT-sample-data.zip</code> in the current working directory and automatically unzipped to <code>data</code> folder.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://drive.google.com/file/d/1q80PhA_vrLgIjRHuyxunVoWXmVUNgggV/view\"\n</pre> url = \"https://drive.google.com/file/d/1q80PhA_vrLgIjRHuyxunVoWXmVUNgggV/view\" In\u00a0[\u00a0]: Copied! <pre>download_file(url, output=\"EMIT-sample-data.zip\")\n</pre> download_file(url, output=\"EMIT-sample-data.zip\") In\u00a0[\u00a0]: Copied! <pre># === Device ===\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# === Parameters ===\nselected_bands = [\n    403,\n    411,\n    418,\n    425,\n    433,\n    440,\n    448,\n    455,\n    463,\n    470,\n    477,\n    485,\n    492,\n    500,\n    507,\n    515,\n    522,\n    530,\n    537,\n    544,\n    552,\n    559,\n    567,\n    574,\n    582,\n    589,\n    597,\n    604,\n    611,\n    619,\n    626,\n    634,\n    641,\n    649,\n    656,\n    664,\n    671,\n    679,\n    686,\n    693,\n    700,\n]\n\nexcel_path = \"data/Gloria_updated_07242025.xlsx\"\ntest_files = [\n    \"data/GreatLake_all_data.xlsx\",\n    \"data/GOA_insitu_data_09052025.xlsx\",\n    \"data/satellite_for_EMIT_09052025.xlsx\",\n]\nnc_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2W.nc\"\nrgb_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2R.nc\"\n\nsave_dir = os.path.join(\"./EMIT/test_Chla\")\nos.makedirs(save_dir, exist_ok=True)\n</pre> # === Device === device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\")  # === Parameters === selected_bands = [     403,     411,     418,     425,     433,     440,     448,     455,     463,     470,     477,     485,     492,     500,     507,     515,     522,     530,     537,     544,     552,     559,     567,     574,     582,     589,     597,     604,     611,     619,     626,     634,     641,     649,     656,     664,     671,     679,     686,     693,     700, ]  excel_path = \"data/Gloria_updated_07242025.xlsx\" test_files = [     \"data/GreatLake_all_data.xlsx\",     \"data/GOA_insitu_data_09052025.xlsx\",     \"data/satellite_for_EMIT_09052025.xlsx\", ] nc_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2W.nc\" rgb_path = \"data/ISS_EMIT_2024_09_29_17_42_42_L2R.nc\"  save_dir = os.path.join(\"./EMIT/test_Chla\") os.makedirs(save_dir, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre># === Training dataset ===\ntrain_real_dl, test_real_dl, input_dim, output_dim, train_ids, test_ids = (\n    load_real_data(\n        excel_path=excel_path,\n        selected_bands=selected_bands,\n        seed=42,\n        diff_before_norm=False,\n        diff_after_norm=False,\n        target_parameter=\"chl-a\",\n        lower_quantile=0,\n        upper_quantile=1,\n        log_offset=1,\n    )\n)\n</pre> # === Training dataset === train_real_dl, test_real_dl, input_dim, output_dim, train_ids, test_ids = (     load_real_data(         excel_path=excel_path,         selected_bands=selected_bands,         seed=42,         diff_before_norm=False,         diff_after_norm=False,         target_parameter=\"chl-a\",         lower_quantile=0,         upper_quantile=1,         log_offset=1,     ) ) In\u00a0[\u00a0]: Copied! <pre># === External test datasets ===\ntest_dls, test_ids_list, test_dates_list = [], [], []\nfor file in test_files:\n    dl, _, _, ids, dates = load_real_test(\n        excel_path=file,\n        selected_bands=selected_bands,\n        diff_before_norm=False,\n        diff_after_norm=False,\n        max_allowed_diff=1.0,\n        target_parameter=\"chl-a\",\n        log_offset=1,\n    )\n    test_dls.append(dl)\n    test_ids_list.append(ids)\n    test_dates_list.append(dates)\n</pre> # === External test datasets === test_dls, test_ids_list, test_dates_list = [], [], [] for file in test_files:     dl, _, _, ids, dates = load_real_test(         excel_path=file,         selected_bands=selected_bands,         diff_before_norm=False,         diff_after_norm=False,         max_allowed_diff=1.0,         target_parameter=\"chl-a\",         log_offset=1,     )     test_dls.append(dl)     test_ids_list.append(ids)     test_dates_list.append(dates) In\u00a0[\u00a0]: Copied! <pre># === EMIT image preprocess ===\ntest_loader, Rrs, mask, latitude, longitude = preprocess_emit_data_minmax(\n    nc_path=nc_path,\n    diff_before_norm=False,\n    diff_after_norm=False,\n    full_band_wavelengths=np.array(selected_bands),\n)\n</pre> # === EMIT image preprocess === test_loader, Rrs, mask, latitude, longitude = preprocess_emit_data_minmax(     nc_path=nc_path,     diff_before_norm=False,     diff_after_norm=False,     full_band_wavelengths=np.array(selected_bands), ) In\u00a0[\u00a0]: Copied! <pre># Define the Mixture-of-Experts Variational Autoencoder (MoE-VAE) model\nmodel = MoE_VAE(\n    input_dim=input_dim,\n    output_dim=output_dim,\n    latent_dim=16,\n    encoder_hidden_dims=[128, 64, 32],\n    decoder_hidden_dims=[32, 64, 128],\n    activation=\"leakyrelu\",\n    use_norm=\"layer\",\n    use_dropout=False,\n    use_softplus_output=True,\n    num_experts=4,\n    k=2,\n    noisy_gating=True,\n).to(device)\n</pre> # Define the Mixture-of-Experts Variational Autoencoder (MoE-VAE) model model = MoE_VAE(     input_dim=input_dim,     output_dim=output_dim,     latent_dim=16,     encoder_hidden_dims=[128, 64, 32],     decoder_hidden_dims=[32, 64, 128],     activation=\"leakyrelu\",     use_norm=\"layer\",     use_dropout=False,     use_softplus_output=True,     num_experts=4,     k=2,     noisy_gating=True, ).to(device) In\u00a0[\u00a0]: Copied! <pre># Define optimizer and Train the MoE-VAE model\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ntrain_log = train(\n    model, train_real_dl, device, epochs=400, optimizer=optimizer, save_dir=save_dir\n)\n</pre> # Define optimizer and Train the MoE-VAE model optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) train_log = train(     model, train_real_dl, device, epochs=400, optimizer=optimizer, save_dir=save_dir ) In\u00a0[\u00a0]: Copied! <pre># Evaluate model on test set, compute metrics, save and plot results\npredictions, actuals = evaluate(model, test_real_dl, device, log_offset=1)\nepsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(predictions, actuals)\ntest_loss = mae\n\nsave_results_to_excel(\n    test_ids, actuals, predictions, os.path.join(save_dir, \"test_results.xlsx\")\n)\nplot_results_with_density(\n    predictions, actuals, save_dir, mode=\"test_results\", xlim=(-2, 4), ylim=(-2, 4)\n)\n</pre> # Evaluate model on test set, compute metrics, save and plot results predictions, actuals = evaluate(model, test_real_dl, device, log_offset=1) epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(predictions, actuals) test_loss = mae  save_results_to_excel(     test_ids, actuals, predictions, os.path.join(save_dir, \"test_results.xlsx\") ) plot_results_with_density(     predictions, actuals, save_dir, mode=\"test_results\", xlim=(-2, 4), ylim=(-2, 4) ) In\u00a0[\u00a0]: Copied! <pre># Evaluate on external test datasets\nfor dl, ids, dates, path in zip(test_dls, test_ids_list, test_dates_list, test_files):\n    preds, acts = evaluate(model, dl, device, log_offset=1)\n    save_results_from_excel_for_test(preds, acts, ids, dates, path, save_dir)\n\n    # Get unique mode name from file name\n    mode_name = os.path.splitext(os.path.basename(path))[0]\n\n    # Save individual plot\n    plot_results(\n        preds,\n        acts,\n        save_dir,\n        threshold=10,\n        mode=f\"test_{mode_name}\",\n        xlim=(-2, 4),\n        ylim=(-2, 4),\n    )\n</pre> # Evaluate on external test datasets for dl, ids, dates, path in zip(test_dls, test_ids_list, test_dates_list, test_files):     preds, acts = evaluate(model, dl, device, log_offset=1)     save_results_from_excel_for_test(preds, acts, ids, dates, path, save_dir)      # Get unique mode name from file name     mode_name = os.path.splitext(os.path.basename(path))[0]      # Save individual plot     plot_results(         preds,         acts,         save_dir,         threshold=10,         mode=f\"test_{mode_name}\",         xlim=(-2, 4),         ylim=(-2, 4),     ) In\u00a0[\u00a0]: Copied! <pre># Perform model inference and generate a spatial map of predicted Chlorophyll-a concentration\n\nOutput = infer_and_visualize_single_model_minmax(\n    model=model,\n    test_loader=test_loader,\n    Rrs=Rrs,\n    mask=mask,\n    latitude=latitude,\n    longitude=longitude,\n    save_folder=save_dir,\n    rgb_nc_file=rgb_path,\n    structure_name=\"Chl-a\",\n    log_offset=1,\n    vmin=0,\n    vmax=30,\n)\n</pre> # Perform model inference and generate a spatial map of predicted Chlorophyll-a concentration  Output = infer_and_visualize_single_model_minmax(     model=model,     test_loader=test_loader,     Rrs=Rrs,     mask=mask,     latitude=latitude,     longitude=longitude,     save_folder=save_dir,     rgb_nc_file=rgb_path,     structure_name=\"Chl-a\",     log_offset=1,     vmin=0,     vmax=30, ) In\u00a0[\u00a0]: Copied! <pre># Save predicted Chlorophyll-a as GeoTIFF and visualize the spatial distribution on map\ntif_path = os.path.join(save_dir, \"chla.tif\")\n\nnpy_to_tif(npy_input=Output, out_tif=tif_path, resolution_m=30)\n\nwith rasterio.open(tif_path) as src:\n    img = src.read(1)\n    transform = src.transform\n    bounds = src.bounds\nimg_masked = np.where(img &lt; 0, np.nan, img)\nextent = [bounds.left, bounds.right, bounds.bottom, bounds.top]\nplt.figure(figsize=(8, 6))\nim = plt.imshow(img_masked, cmap=\"jet\", vmin=0, vmax=30, extent=extent, origin=\"upper\")\nplt.colorbar(im, label=\"Chl-a\")\nplt.title(\"Chl-a\")\nplt.show()\n</pre> # Save predicted Chlorophyll-a as GeoTIFF and visualize the spatial distribution on map tif_path = os.path.join(save_dir, \"chla.tif\")  npy_to_tif(npy_input=Output, out_tif=tif_path, resolution_m=30)  with rasterio.open(tif_path) as src:     img = src.read(1)     transform = src.transform     bounds = src.bounds img_masked = np.where(img &lt; 0, np.nan, img) extent = [bounds.left, bounds.right, bounds.bottom, bounds.top] plt.figure(figsize=(8, 6)) im = plt.imshow(img_masked, cmap=\"jet\", vmin=0, vmax=30, extent=extent, origin=\"upper\") plt.colorbar(im, label=\"Chl-a\") plt.title(\"Chl-a\") plt.show()"},{"location":"examples/EMIT_chla/#chl-a-prediction-using-emit-data","title":"Chl-a Prediction using EMIT Data\u00b6","text":"<p>This notebook demonstrates how to predict Chlorophyll-a (Chl-a) concentrations using EMIT hyperspectral imagery and a Mixture-of-Experts Variational Autoencoder (MoE-VAE), including key steps such as:</p> <ul> <li>\u2705 Model Training and Testing</li> <li>\u2705 Inference</li> <li>\u2705 Visualization</li> </ul>"},{"location":"examples/_earthaccess/","title":"earthaccess","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport earthaccess\nimport hypercoast\n</pre> import os import earthaccess import hypercoast In\u00a0[\u00a0]: Copied! <pre># os.environ[\"EARTHDATA_USERNAME\"] = \"your_username\"\n# os.environ[\"EARTHDATA_PASSWORD\"] = \"your_password\"\n</pre> # os.environ[\"EARTHDATA_USERNAME\"] = \"your_username\" # os.environ[\"EARTHDATA_PASSWORD\"] = \"your_password\" In\u00a0[\u00a0]: Copied! <pre>if os.environ.get(\"EARTHDATA_USERNAME\") is None:\n    raise ValueError(\"Please set the EARTHDATA_USERNAME environment variable\")\nif os.environ.get(\"EARTHDATA_PASSWORD\") is None:\n    raise ValueError(\"Please set the EARTHDATA_PASSWORD environment variable\")\n</pre> if os.environ.get(\"EARTHDATA_USERNAME\") is None:     raise ValueError(\"Please set the EARTHDATA_USERNAME environment variable\") if os.environ.get(\"EARTHDATA_PASSWORD\") is None:     raise ValueError(\"Please set the EARTHDATA_PASSWORD environment variable\") In\u00a0[\u00a0]: Copied! <pre>earthaccess.login(persist=True)\n</pre> earthaccess.login(persist=True) In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_pace(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-07-30\", \"2024-08-15\"),\n    short_name=\"PACE_OCI_L2_AOP_NRT\",\n    count=10,\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_pace(     bounding_box=(-83, 25, -81, 28),     temporal=(\"2024-07-30\", \"2024-08-15\"),     short_name=\"PACE_OCI_L2_AOP_NRT\",     count=10,     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_pace(results[:1], out_dir=\"data\")\n</pre> hypercoast.download_pace(results[:1], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nif not os.path.exists(filepath):\n    raise FileNotFoundError(filepath)\n</pre> filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" if not os.path.exists(filepath):     raise FileNotFoundError(filepath)"},{"location":"examples/_earthaccess/#testing-earthaccess-login","title":"Testing earthaccess login\u00b6","text":""},{"location":"examples/acolite/","title":"Acolite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport hypercoast\n</pre> import os import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/S2A_MSIL1C_20160920T164452_N0204_R083_T15RYN_20160920T164450.SAFE.zip\"\nwork_dir = os.path.expanduser(\"~/Downloads\")\nfilepath = os.path.join(work_dir, os.path.basename(url))\nhypercoast.download_file(url, filepath, quiet=True)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/S2A_MSIL1C_20160920T164452_N0204_R083_T15RYN_20160920T164450.SAFE.zip\" work_dir = os.path.expanduser(\"~/Downloads\") filepath = os.path.join(work_dir, os.path.basename(url)) hypercoast.download_file(url, filepath, quiet=True) In\u00a0[\u00a0]: Copied! <pre>input_dir = filepath.replace(\".zip\", \"\")\nif not os.path.exists(input_dir):\n    raise FileNotFoundError(f\"Directory {input_dir} not found\")\n</pre> input_dir = filepath.replace(\".zip\", \"\") if not os.path.exists(input_dir):     raise FileNotFoundError(f\"Directory {input_dir} not found\") In\u00a0[\u00a0]: Copied! <pre>acolite_dir = hypercoast.download_acolite(work_dir)\nprint(f\"Acolite directory: {acolite_dir}\")\n</pre> acolite_dir = hypercoast.download_acolite(work_dir) print(f\"Acolite directory: {acolite_dir}\") In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(work_dir, \"output\")\n</pre> out_dir = os.path.join(work_dir, \"output\") In\u00a0[\u00a0]: Copied! <pre>hypercoast.run_acolite(\n    acolite_dir=acolite_dir,\n    input_file=input_dir,\n    out_dir=out_dir,\n    l2w_parameters=\"Rrs_*,chl_oc3,chl_re_mishra,spm_nechad2016\",\n    rgb_rhot=True,\n    rgb_rhos=False,\n    map_l2w=True,\n)\n</pre> hypercoast.run_acolite(     acolite_dir=acolite_dir,     input_file=input_dir,     out_dir=out_dir,     l2w_parameters=\"Rrs_*,chl_oc3,chl_re_mishra,spm_nechad2016\",     rgb_rhot=True,     rgb_rhos=False,     map_l2w=True, ) In\u00a0[\u00a0]: Copied! <pre>input_dir = os.path.join(work_dir, \"data\")\ninput_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir)]\ninput_files\n</pre> input_dir = os.path.join(work_dir, \"data\") input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir)] input_files <p>Run the following code to process all images in the <code>data</code> folder.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.run_acolite(\n    acolite_dir=acolite_dir,\n    input_file=input_files,\n    out_dir=out_dir,\n    l2w_parameters=\"Rrs_*,chl_oc3,chl_re_mishra,spm_nechad2016\",\n    rgb_rhot=True,\n    rgb_rhos=False,\n    map_l2w=True,\n)\n</pre> hypercoast.run_acolite(     acolite_dir=acolite_dir,     input_file=input_files,     out_dir=out_dir,     l2w_parameters=\"Rrs_*,chl_oc3,chl_re_mishra,spm_nechad2016\",     rgb_rhot=True,     rgb_rhos=False,     map_l2w=True, )"},{"location":"examples/acolite/#atmospheric-correction-with-acolite","title":"Atmospheric Correction with Acolite\u00b6","text":"<p>Acolite can perform atmospheric correction on a variety of satellite sensors, including Landsat, Sentinel-2, PACE, EMIT, AVIRIS, among others. For more information on how to use Acolite, please refer to the Acolite manual</p> <p>In this example, we will use Acolite to perform atmospheric correction on a Sentinel-2 image.</p>"},{"location":"examples/acolite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/acolite/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/acolite/#download-acolite-software","title":"Download Acolite software\u00b6","text":""},{"location":"examples/acolite/#run-acolite","title":"Run Acolite\u00b6","text":""},{"location":"examples/acolite/#batch-processing","title":"Batch processing\u00b6","text":"<p>To process multiple images, put all the images in a folder. For example, unzip all the images in the <code>data</code> folder. Then, run the following code to make sure that all image folders are listed.</p>"},{"location":"examples/acolite_emit/","title":"Acolite emit","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport hypercoast\n</pre> import os import hypercoast In\u00a0[\u00a0]: Copied! <pre>work_dir = os.path.expanduser(\"~/Downloads\")\ninput_dir = os.path.join(work_dir, \"data\")\n</pre> work_dir = os.path.expanduser(\"~/Downloads\") input_dir = os.path.join(work_dir, \"data\") In\u00a0[\u00a0]: Copied! <pre>filepath = os.path.join(input_dir, \"EMIT_L1B_RAD_001_20230220T181144_2305112_013.nc\")\n</pre> filepath = os.path.join(input_dir, \"EMIT_L1B_RAD_001_20230220T181144_2305112_013.nc\") In\u00a0[\u00a0]: Copied! <pre>acolite_dir = hypercoast.download_acolite(work_dir)\nprint(f\"Acolite directory: {acolite_dir}\")\n</pre> acolite_dir = hypercoast.download_acolite(work_dir) print(f\"Acolite directory: {acolite_dir}\") In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(work_dir, \"output\")\n</pre> out_dir = os.path.join(work_dir, \"output\") In\u00a0[\u00a0]: Copied! <pre>hypercoast.run_acolite(\n    acolite_dir=acolite_dir,\n    input_file=filepath,\n    out_dir=out_dir,\n    l2w_parameters=\"Rrs_*\",\n    rgb_rhot=True,\n    rgb_rhos=True,\n    map_l2w=True,\n)\n</pre> hypercoast.run_acolite(     acolite_dir=acolite_dir,     input_file=filepath,     out_dir=out_dir,     l2w_parameters=\"Rrs_*\",     rgb_rhot=True,     rgb_rhos=True,     map_l2w=True, ) In\u00a0[\u00a0]: Copied! <pre>input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if \"RAD\" in f]\ninput_files\n</pre> input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if \"RAD\" in f] input_files <p>Run the following code to process all images in the <code>data</code> folder.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.run_acolite(\n    acolite_dir=acolite_dir,\n    input_file=input_files,\n    out_dir=out_dir,\n    l2w_parameters=\"Rrs_*\",\n    rgb_rhot=True,\n    rgb_rhos=True,\n    map_l2w=True,\n)\n</pre> hypercoast.run_acolite(     acolite_dir=acolite_dir,     input_file=input_files,     out_dir=out_dir,     l2w_parameters=\"Rrs_*\",     rgb_rhot=True,     rgb_rhos=True,     map_l2w=True, )"},{"location":"examples/acolite_emit/#emit-atmospheric-correction-with-acolite","title":"EMIT Atmospheric Correction with Acolite\u00b6","text":"<p>Acolite can perform atmospheric correction on a variety of satellite sensors, including Landsat, Sentinel-2, PACE, EMIT, AVIRIS, among others. For more information on how to use Acolite, please refer to the Acolite manual</p> <p>In this example, we will use Acolite to perform atmospheric correction on an EMIT image.</p>"},{"location":"examples/acolite_emit/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/acolite_emit/#specify-input-data","title":"Specify input data\u00b6","text":"<p>We will use the following input data:</p> <ul> <li>EMIT_L1B_RAD_001_20230220T181144_2305112_013.nc</li> <li>EMIT_L1B_OBS_001_20230220T181144_2305112_013.nc</li> </ul> <p>Put the input data in the <code>data</code> folder.</p>"},{"location":"examples/acolite_emit/#download-acolite-software","title":"Download Acolite software\u00b6","text":""},{"location":"examples/acolite_emit/#run-acolite","title":"Run Acolite\u00b6","text":""},{"location":"examples/acolite_emit/#batch-processing","title":"Batch processing\u00b6","text":"<p>To process multiple images, put all the images in a folder. For example, unzip all the images in the <code>data</code> folder. Then, run the following code to make sure that all image folders are listed.</p>"},{"location":"examples/aviris/","title":"Aviris","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Download sample dataset from Access the AVIRIS-NG L2 Surface Reflectance product page via ORNL DAAC https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1988</p> <p></p> <p>Download <code>ang20210401t150456_rfl_v2z1.zip</code> and unzip it.</p> <p></p> <p>The dataset contains 2 files: <code>ang20210401t150456_rfl_v2z1</code> and <code>ang20210401t150456_rfl_v2z1.hdr</code>. We will use the <code>ang20210401t150456_rfl_v2z1</code> file in this notebook.</p> In\u00a0[\u00a0]: Copied! <pre>filepath = \"ang20210401t150456_rfl_v2z1\"\n</pre> filepath = \"ang20210401t150456_rfl_v2z1\" <p>Read the AVIRIS data as an <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>ds = hypercoast.read_aviris(filepath)\nds\n</pre> ds = hypercoast.read_aviris(filepath) ds <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm\n</pre> m = hypercoast.Map() m <p>Add the AVIRIS data to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_aviris(ds, wavelengths=[1000, 700, 400], vmin=0, vmax=0.2)\nm.add(\"spectral\")\n</pre> m.add_aviris(ds, wavelengths=[1000, 700, 400], vmin=0, vmax=0.2) m.add(\"spectral\") <p></p>"},{"location":"examples/aviris/#visualizing-aviris-data-interactively-with-hypercoast","title":"Visualizing AVIRIS data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize AVIRIS hyperspectral data interactively with HyperCoast. For more information about AVIRIS, please visit the links below:</p> <ul> <li>https://aviris.jpl.nasa.gov/</li> <li>https://aviris.jpl.nasa.gov/dataportal/</li> <li>https://popo.jpl.nasa.gov/mmgis-aviris/?s=ujooa</li> <li>https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1988</li> <li>https://github.com/ornldaac/deltax_workshop_2022/tree/main</li> <li>https://github.com/jjmcnelis/aviris-ng-notebooks/tree/master</li> </ul>"},{"location":"examples/chla_predict/","title":"Chla predict","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[all]\"\n</pre> # %pip install \"hypercoast[all]\" In\u00a0[\u00a0]: Copied! <pre>import torch\nimport hypercoast\nfrom hypercoast.chla import *\n</pre> import torch import hypercoast from hypercoast.chla import * In\u00a0[\u00a0]: Copied! <pre>chla_data_url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/chla_test_data.zip\"\npace_data_url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20241024T182127.L2.OC_AOP.V2_0.NRT.nc\"\n</pre> chla_data_url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/chla_test_data.zip\" pace_data_url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20241024T182127.L2.OC_AOP.V2_0.NRT.nc\" In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_file(chla_data_url, quiet=True)\n</pre> hypercoast.download_file(chla_data_url, quiet=True) In\u00a0[\u00a0]: Copied! <pre>pace_filepath = hypercoast.download_file(pace_data_url, quiet=True)\n</pre> pace_filepath = hypercoast.download_file(pace_data_url, quiet=True) In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Load the training dataset\ntrain_real_dl, test_real_dl, input_dim, output_dim = load_real_data(\n    \"data/Chl_RC_PACE.csv\", \"data/Rrs_RC_PACE.csv\"\n)\n# Load the validation dataset\ntest_real_Sep, _, _ = load_real_test(\n    \"data/Chl_RC_PACE_Sep.csv\", \"data/Rrs_RC_PACE_Sep.csv\"\n)\n# Model output path.\nsave_dir = \"model/VAE_Chla_PACE\"\nos.makedirs(save_dir, exist_ok=True)\n\n# Create the VAE model and optimizer\nmodel = VAE(input_dim, output_dim).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n\nbest_model_path = \"model/vae_trans_model_best_Chl_PACE.pth\"\ntrain(model, train_real_dl, epochs=400, best_model_path=best_model_path)\n# Load the optimal model\nmodel.load_state_dict(torch.load(best_model_path, map_location=device))\n\npredictions, actuals = evaluate(model, test_real_dl)\n\npredictions_Sep, actuals_Sep = evaluate(model, test_real_Sep)\n\nsave_to_csv(predictions, os.path.join(save_dir, \"predictions.csv\"))\nsave_to_csv(actuals, os.path.join(save_dir, \"actuals.csv\"))\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Load the training dataset train_real_dl, test_real_dl, input_dim, output_dim = load_real_data(     \"data/Chl_RC_PACE.csv\", \"data/Rrs_RC_PACE.csv\" ) # Load the validation dataset test_real_Sep, _, _ = load_real_test(     \"data/Chl_RC_PACE_Sep.csv\", \"data/Rrs_RC_PACE_Sep.csv\" ) # Model output path. save_dir = \"model/VAE_Chla_PACE\" os.makedirs(save_dir, exist_ok=True)  # Create the VAE model and optimizer model = VAE(input_dim, output_dim).to(device) opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)  best_model_path = \"model/vae_trans_model_best_Chl_PACE.pth\" train(model, train_real_dl, epochs=400, best_model_path=best_model_path) # Load the optimal model model.load_state_dict(torch.load(best_model_path, map_location=device))  predictions, actuals = evaluate(model, test_real_dl)  predictions_Sep, actuals_Sep = evaluate(model, test_real_Sep)  save_to_csv(predictions, os.path.join(save_dir, \"predictions.csv\")) save_to_csv(actuals, os.path.join(save_dir, \"actuals.csv\")) In\u00a0[\u00a0]: Copied! <pre>plot_results(predictions, actuals, save_dir, mode=\"test\")\n</pre> plot_results(predictions, actuals, save_dir, mode=\"test\") In\u00a0[\u00a0]: Copied! <pre>plot_results(predictions_Sep, actuals_Sep, save_dir, mode=\"Sep\")\n</pre> plot_results(predictions_Sep, actuals_Sep, save_dir, mode=\"Sep\") In\u00a0[\u00a0]: Copied! <pre>chla_data_file = pace_filepath.replace(\".nc\", \".npy\")\n</pre> chla_data_file = pace_filepath.replace(\".nc\", \".npy\") In\u00a0[\u00a0]: Copied! <pre>chla_predict(pace_filepath, best_model_path, chla_data_file, device)\n</pre> chla_predict(pace_filepath, best_model_path, chla_data_file, device) In\u00a0[\u00a0]: Copied! <pre>geotiff_file = chla_data_file.replace(\".npy\", \".tif\")\nnpy_to_geotiff(chla_data_file, geotiff_file)\n</pre> geotiff_file = chla_data_file.replace(\".npy\", \".tif\") npy_to_geotiff(chla_data_file, geotiff_file) In\u00a0[\u00a0]: Copied! <pre>rgb_image_tif_file = \"data/snapshot-2024-08-10T00_00_00Z.tif\"\noutput_file = \"20241024-2.png\"\ntitle = \"PACE Chla Prediction\"\nfigsize = (12, 8)\ncmap = \"jet\"\n</pre> rgb_image_tif_file = \"data/snapshot-2024-08-10T00_00_00Z.tif\" output_file = \"20241024-2.png\" title = \"PACE Chla Prediction\" figsize = (12, 8) cmap = \"jet\" In\u00a0[\u00a0]: Copied! <pre>chla_viz(rgb_image_tif_file, chla_data_file, output_file, title, figsize, cmap)\n</pre> chla_viz(rgb_image_tif_file, chla_data_file, output_file, title, figsize, cmap) In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Esri.WorldImagery\")\nm.add_raster(\n    geotiff_file, colormap=\"jet\", nodata=-9999.0, vmin=0, vmax=35, layer_name=\"Chla\"\n)\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Esri.WorldImagery\") m.add_raster(     geotiff_file, colormap=\"jet\", nodata=-9999.0, vmin=0, vmax=35, layer_name=\"Chla\" ) m"},{"location":"examples/chla_predict/#chlorophyll-a-prediction-with-deep-learning","title":"Chlorophyll-a prediction with Deep Learning\u00b6","text":""},{"location":"examples/chla_predict/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following cell to install the required packages.</p>"},{"location":"examples/chla_predict/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/chla_predict/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/chla_predict/#train-the-model","title":"Train the model\u00b6","text":""},{"location":"examples/chla_predict/#predict-chlorophyll-a-concentration","title":"Predict chlorophyll-a concentration\u00b6","text":""},{"location":"examples/chla_predict/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/chlorophyll_a/","title":"Chlorophyll a","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>To download PACE data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, you can uncomment the code below to search and download the data.</p> In\u00a0[\u00a0]: Copied! <pre># hypercoast.nasa_earth_login()\n# temporal = (\"2024-06-01\", \"2024-07-01\")\n# results= hypercoast.search_pace_chla(temporal=temporal)\n# hypercoast.download_nasa_data(results, \"chla\")\n</pre> # hypercoast.nasa_earth_login() # temporal = (\"2024-06-01\", \"2024-07-01\") # results= hypercoast.search_pace_chla(temporal=temporal) # hypercoast.download_nasa_data(results, \"chla\") <p>Alternatively, you can download some sample data from here.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_chla.zip\"\nhypercoast.download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_chla.zip\" hypercoast.download_file(url) <p>The downloaded zip file is automatically extracted and saved in the <code>chla</code> directory, which contains 17 daily files of chlorophyll-a concentration data in the netCDF format. The date range of the data is from 2024-06-01 to 2024-06-17.</p> In\u00a0[\u00a0]: Copied! <pre>files = \"chla/*nc\"\n</pre> files = \"chla/*nc\" <p>Load all the data files in the <code>chla</code> directory as an xarray DataArray</p> In\u00a0[\u00a0]: Copied! <pre>array = hypercoast.read_pace_chla(files)\narray\n</pre> array = hypercoast.read_pace_chla(files) array <p>Select a date and visualize the chlorophyll-a concentration data with Matplotlib.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.viz_pace_chla(array, date=\"2024-06-01\", cmap=\"jet\", size=6)\n</pre> hypercoast.viz_pace_chla(array, date=\"2024-06-01\", cmap=\"jet\", size=6) <p>If the date is not specified, the data are averaged over the entire time range.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.viz_pace_chla(array, cmap=\"jet\", size=6)\n</pre> hypercoast.viz_pace_chla(array, cmap=\"jet\", size=6) <p>To visualize the data interactively, we can select either a single date or aggregate the data over a time range.</p> <p>First, let's select a single date from the data array:</p> In\u00a0[\u00a0]: Copied! <pre>single_array = array.sel(date=\"2024-06-01\")\nsingle_array\n</pre> single_array = array.sel(date=\"2024-06-01\") single_array <p>Convert the data array to an image that can be displayed on an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>single_image = hypercoast.pace_chla_to_image(single_array)\n</pre> single_image = hypercoast.pace_chla_to_image(single_array) <p>Create an interactive map and display the image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[40, -100], zoom=4)\nm.add_basemap(\"Hybrid\")\nm.add_raster(\n    single_image,\n    cmap=\"jet\",\n    vmin=-1,\n    vmax=2,\n    layer_name=\"Chlorophyll a\",\n    zoom_to_layer=False,\n)\nlabel = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\"\nm.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label)\nm\n</pre> m = hypercoast.Map(center=[40, -100], zoom=4) m.add_basemap(\"Hybrid\") m.add_raster(     single_image,     cmap=\"jet\",     vmin=-1,     vmax=2,     layer_name=\"Chlorophyll a\",     zoom_to_layer=False, ) label = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\" m.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label) m <p></p> <p>The daily image does not have a global coverage. To visualize the data globally, we can aggregate the data over a time range.</p> In\u00a0[\u00a0]: Copied! <pre>mean_array = array.mean(dim=\"date\")\n</pre> mean_array = array.mean(dim=\"date\") <p>Convert the aggregated data array to an image that can be displayed on an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>image = hypercoast.pace_chla_to_image(mean_array)\n</pre> image = hypercoast.pace_chla_to_image(mean_array) <p>Create an interactive map and display the image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[40, -100], zoom=4)\nm.add_basemap(\"Hybrid\")\nm.add_raster(\n    image, cmap=\"jet\", vmin=-1, vmax=2, layer_name=\"Chlorophyll a\", zoom_to_layer=False\n)\nlabel = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\"\nm.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label)\nm\n</pre> m = hypercoast.Map(center=[40, -100], zoom=4) m.add_basemap(\"Hybrid\") m.add_raster(     image, cmap=\"jet\", vmin=-1, vmax=2, layer_name=\"Chlorophyll a\", zoom_to_layer=False ) label = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\" m.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label) m <p></p>"},{"location":"examples/chlorophyll_a/#visualizing-pace-chlorophyll-a-data-interactively-with-hypercoast","title":"Visualizing PACE chlorophyll-a data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) data interactively with HyperCoast.</p>"},{"location":"examples/desis/","title":"Desis","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/desis.tif\"\nfilepath = \"data/desis.tif\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/desis.tif\" filepath = \"data/desis.tif\" hypercoast.download_file(url, filepath) <p>Load the dataset as a xarray.Dataset object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_desis(filepath)\n</pre> dataset = hypercoast.read_desis(filepath) <p>Plot the spectral signature of a pixel.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.filter_desis(dataset, lat=29.4315, lon=91.2927, return_plot=True)\n</pre> hypercoast.filter_desis(dataset, lat=29.4315, lon=91.2927, return_plot=True) <p>Visualize a single band of the hyperspectral image.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_desis(filepath, wavelengths=[1000], vmin=0, vmax=5000, nodata=0, colormap=\"jet\")\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=0.5, label=\"Reflectance\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_desis(filepath, wavelengths=[1000], vmin=0, vmax=5000, nodata=0, colormap=\"jet\") m.add_colormap(cmap=\"jet\", vmin=0, vmax=0.5, label=\"Reflectance\") m <p></p> <p>Plot the spectral signature of a pixel interactively.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_desis(filepath, wavelengths=[900, 600, 525], vmin=0, vmax=1000, nodata=0)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_desis(filepath, wavelengths=[900, 600, 525], vmin=0, vmax=1000, nodata=0) m.add(\"spectral\") m <p></p>"},{"location":"examples/desis/#visualizing-desis-data-interactively-with-hypercoast","title":"Visualizing DESIS data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize DESIS hyperspectral data interactively with HyperCoast.</p>"},{"location":"examples/ecostress/","title":"Ecostress","text":"In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_ecostress(\n    bbox=(-120.522, 34.4266, -120.2665, 34.5653),\n    temporal=(\"2023-04-01\", \"2023-04-02\"),\n    count=-1,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_ecostress(     bbox=(-120.522, 34.4266, -120.2665, 34.5653),     temporal=(\"2023-04-01\", \"2023-04-02\"),     count=-1,  # use -1 to return all datasets     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_ecostress(results[:5], out_dir=\"data\")\n</pre> hypercoast.download_ecostress(results[:5], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[34.5014, -120.4032], zoom=11)\nm.search_ecostress()\nm\n</pre> m = hypercoast.Map(center=[34.5014, -120.4032], zoom=11) m.search_ecostress() m In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_ecostress(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")\n</pre> # hypercoast.download_ecostress(m._NASA_DATA_RESULTS[:2], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\" In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\"\nhypercoast.download_file(url, filepath)\n</pre> filepath = \"data/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\" hypercoast.download_file(url, filepath) <p>Visualize the data with HyperCoast.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"HYBRID\")\nm.add_raster(filepath, colormap=\"jet\", layer_name=\"LST\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"HYBRID\") m.add_raster(filepath, colormap=\"jet\", layer_name=\"LST\") m.add(\"spectral\") m <p></p>"},{"location":"examples/ecostress/#search-and-download-nasa-ecostress-data-with-hypercoast","title":"Search and download NASA ECOSTRESS data with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to search and visualize NASA ECOSTRESS temperature data with HyperCoast.</p>"},{"location":"examples/ecostress/#search-for-ecostress-data-programmatically","title":"Search for ECOSTRESS data programmatically\u00b6","text":""},{"location":"examples/ecostress/#download-ecostress-data","title":"Download ECOSTRESS data\u00b6","text":""},{"location":"examples/ecostress/#search-for-ecostress-data-interactively","title":"Search for ECOSTRESS data interactively\u00b6","text":""},{"location":"examples/ecostress/#visualize-ecostress-data","title":"Visualize ECOSTRESS data\u00b6","text":"<p>Download a sample ECOSTRESS data file and visualize it with HyperCoast.</p>"},{"location":"examples/emit/","title":"Emit","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install hypercoast\n</pre> # %pip install hypercoast In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Download a sample EMIT data file from here.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nhypercoast.download_file(url, filepath)\n</pre> filepath = \"data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" hypercoast.download_file(url, filepath) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_emit(filepath)\n</pre> dataset = hypercoast.read_emit(filepath) <p>Visualize the data interactively with HyperCoast. By default, the plot will show all the bands in the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"SATELLITE\") m.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\") m.add(\"spectral\") m <p>To visualize a certain wavelength range, you can specify the <code>xlim</code> parameter as follows:</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\")\nm.add(\"spectral\", xlim=(400, 1200))\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"SATELLITE\") m.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\") m.add(\"spectral\", xlim=(400, 1200)) m <p>To access the selected spectral profiles from the mouse clicked location as a Pandas DataFrame, use the <code>Map.spectral_to_df()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>m.spectral_to_df()\n</pre> m.spectral_to_df() <p>To access the selected spectral profiles from the mouse clicked location as a GeoPandas GeoDataFrame, use the <code>Map.spectral_to_gdf()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>m.spectral_to_gdf()\n</pre> m.spectral_to_gdf() <p></p>"},{"location":"examples/emit/#visualizing-emit-data-interactively-with-hypercoast","title":"Visualizing EMIT data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Earth Surface Mineral Dust Source Investigation (EMIT) data interactively with HyperCoast. This notebook is inspired by the EMIT data visualization tutorial - Exploring_EMIT_L2A_Reflectance.ipynb. We have made it much easier to visualize the data interactively with HyperCoast.</p>"},{"location":"examples/enmap/","title":"Enmap","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Register and access EnMap hyperspectral data from their portal https://www.enmap.org/data_access/</p> <p>We primarily need 2 files for our usage: <code>ENMAP01-____L2A-DT0000000001_XXXX-SPECTRAL_IMAGE_COG.TIF</code> and <code>ENMAP01-____L2A-DT0000000001_XXXX-METADATA.XML</code>. Download and place them in the same folder.</p> In\u00a0[\u00a0]: Copied! <pre>filepath = \"/path/to/NMAP01-____L2A-DT0000000001_XXXX-SPECTRAL_IMAGE_COG.TIF\"\n</pre> filepath = \"/path/to/NMAP01-____L2A-DT0000000001_XXXX-SPECTRAL_IMAGE_COG.TIF\" <p>Read the EnMap data as an <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>ds = hypercoast.read_enmap(filepath)\nds\n</pre> ds = hypercoast.read_enmap(filepath) ds <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_enmap(ds, vmin=0, vmax=0.2)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_enmap(ds, vmin=0, vmax=0.2) m.add(\"spectral\") m <p>Add the EnMap data to the map.</p>"},{"location":"examples/enmap/#visualizing-enmap-data-interactively-with-hypercoast","title":"Visualizing EnMap data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize EnMap hyperspectral data interactively with HyperCoast. For more information about EnMap, please visit the links below:</p> <ul> <li>https://www.enmap.org/</li> <li>https://planning.enmap.org/</li> <li>https://en.wikipedia.org/wiki/EnMAP</li> </ul>"},{"location":"examples/field_data/","title":"Field data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\nimport pandas as pd\n</pre> import hypercoast import pandas as pd <p>Download a sample dataset.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_sample_points.csv\"\ndata = pd.read_csv(url)\ndata.head()\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_sample_points.csv\" data = pd.read_csv(url) data.head() <p>Download PACE data.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nfilepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" hypercoast.download_file(url, filepath) <p>Read the PACE dataset.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_pace(filepath)\n</pre> dataset = hypercoast.read_pace(filepath) <p>Run the following cell to show the map. Click on the markers to see the spectral data.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[27.235094, -87.791748], zoom=6)\n\nm.add_basemap(\"Hybrid\")\nwavelengths = [450, 550, 650]\nm.add_pace(\n    dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\"\n)\nm.add(\"spectral\")\n\nm.add_field_data(\n    data,\n    x_col=\"wavelength\",\n    y_col_prefix=\"(\",\n    x_label=\"Wavelength (nm)\",\n    y_label=\"Reflectance\",\n    use_marker_cluster=True,\n)\nm.set_center(-87.791748, 27.235094, zoom=6)\nm\n</pre> m = hypercoast.Map(center=[27.235094, -87.791748], zoom=6)  m.add_basemap(\"Hybrid\") wavelengths = [450, 550, 650] m.add_pace(     dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\" ) m.add(\"spectral\")  m.add_field_data(     data,     x_col=\"wavelength\",     y_col_prefix=\"(\",     x_label=\"Wavelength (nm)\",     y_label=\"Reflectance\",     use_marker_cluster=True, ) m.set_center(-87.791748, 27.235094, zoom=6) m <p></p>"},{"location":"examples/field_data/#visualizing-spectral-data-from-field-measurements","title":"Visualizing Spectral Data from Field Measurements\u00b6","text":"<p>This notebook demonstrates how to visualize spectral data from field measurements.</p>"},{"location":"examples/hypoxia/","title":"Hypoxia","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U openpyxl hypercoast\n</pre> # %pip install -U openpyxl hypercoast In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport hypercoast\n</pre> import pandas as pd import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/Hypoxia_Data_Sheet.xlsx\"\nxls_path = \"data/Hypoxia_Data_Sheet.xlsx\"\nhypercoast.download_file(url, xls_path, overwrite=True)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/Hypoxia_Data_Sheet.xlsx\" xls_path = \"data/Hypoxia_Data_Sheet.xlsx\" hypercoast.download_file(url, xls_path, overwrite=True) In\u00a0[\u00a0]: Copied! <pre>df = pd.read_excel(xls_path)\ndf.head()\n</pre> df = pd.read_excel(xls_path) df.head() In\u00a0[\u00a0]: Copied! <pre>df_filtered = df.dropna(subset=[\"Lon\", \"Lat\"]).reset_index(drop=True)\ndf_filtered.head()\n</pre> df_filtered = df.dropna(subset=[\"Lon\", \"Lat\"]).reset_index(drop=True) df_filtered.head() In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://github.com/opengeos/datasets/releases/download/hypercoast/Hypoxia_Path.kml\"\n)\nkml_path = \"data/Hypoxia_Path.kml\"\nhypercoast.download_file(url, kml_path)\n</pre> url = (     \"https://github.com/opengeos/datasets/releases/download/hypercoast/Hypoxia_Path.kml\" ) kml_path = \"data/Hypoxia_Path.kml\" hypercoast.download_file(url, kml_path) In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nfilepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" hypercoast.download_file(url, filepath) In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_pace(filepath)\n# dataset\n</pre> dataset = hypercoast.read_pace(filepath) # dataset In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450, 550, 650]\nm.add_pace(\n    dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\"\n)\nm.add(\"spectral\")\nstyle = {\"weight\": 2, \"color\": \"red\"}\nm.add_kml(kml_path, style=style, layer_name=\"Hypoxia Path\", info_mode=None)\nm.add_points_from_xy(\n    df_filtered,\n    x=\"Lon\",\n    y=\"Lat\",\n    max_cluster_radius=50,\n    layer_name=\"Hypoxia Data Points\",\n)\nm.set_center(-91.46118, 28.89758, zoom=8)\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") wavelengths = [450, 550, 650] m.add_pace(     dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\" ) m.add(\"spectral\") style = {\"weight\": 2, \"color\": \"red\"} m.add_kml(kml_path, style=style, layer_name=\"Hypoxia Path\", info_mode=None) m.add_points_from_xy(     df_filtered,     x=\"Lon\",     y=\"Lat\",     max_cluster_radius=50,     layer_name=\"Hypoxia Data Points\", ) m.set_center(-91.46118, 28.89758, zoom=8) m"},{"location":"examples/hypoxia/#visualizing-hypoxia-cruise-sampling-locations-in-the-gulf-of-mexico","title":"Visualizing Hypoxia Cruise Sampling Locations in the Gulf of Mexico\u00b6","text":""},{"location":"examples/image_cube/","title":"Image cube","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\"\nfilepath = \"data/neon.h5\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\" filepath = \"data/neon.h5\" hypercoast.download_file(url, filepath) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_neon(filepath)\ndataset\n</pre> dataset = hypercoast.read_neon(filepath) dataset <p>Visualize the NEON AOP hyperspectral data in 3D with a selected band overlaid on top of the 3D plot.</p> In\u00a0[\u00a0]: Copied! <pre>cube = hypercoast.image_cube(\n    dataset,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[700],\n    title=\"Reflectance\",\n)\ncube.show()\n</pre> cube = hypercoast.image_cube(     dataset,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[700],     title=\"Reflectance\", ) cube.show() <p>Visualize the NEON AOP hyperspectral data in 3D with an RGB image overlaid on top of the 3D plot.</p> In\u00a0[\u00a0]: Copied! <pre>cube2 = hypercoast.image_cube(\n    dataset,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"Reflectance\",\n)\ncube2.show()\n</pre> cube2 = hypercoast.image_cube(     dataset,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"Reflectance\", ) cube2.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nfilepath = \"EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nhypercoast.download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" filepath = \"EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" hypercoast.download_file(url) In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_emit(filepath)\ndataset\n</pre> dataset = hypercoast.read_emit(filepath) dataset <p>Select a subset of the data to avoid nodata areas.</p> In\u00a0[\u00a0]: Copied! <pre>ds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451))\nds\n</pre> ds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451)) ds <p>Visualize the EMIT data in 3D with an RGB image overlaid on top of the 3D plot.</p> In\u00a0[\u00a0]: Copied! <pre>cube = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.4),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n)\ncube.show()\n</pre> cube = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.4),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\", ) cube.show() <p></p>"},{"location":"examples/image_cube/#visualizing-hyperspectral-data-in-3d","title":"Visualizing Hyperspectral Data in 3D\u00b6","text":"<p>This notebook demonstrates how to visualize hyperspectral data in 3D using the PyVista plotting backend.</p>"},{"location":"examples/image_cube/#visualize-neon-aop-hyperspectral-data","title":"Visualize NEON AOP Hyperspectral Data\u00b6","text":"<p>Download a sample NEON AOP hyperspectral data.</p>"},{"location":"examples/image_cube/#visualize-nasa-emit-hyperspectral-data","title":"Visualize NASA EMIT Hyperspectral Data\u00b6","text":"<p>Download a sample EMIT data file from here.</p>"},{"location":"examples/image_slicing/","title":"Image slicing","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\"\nfilepath = \"data/neon.h5\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\" filepath = \"data/neon.h5\" hypercoast.download_file(url, filepath) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_neon(filepath)\ndataset\n</pre> dataset = hypercoast.read_neon(filepath) dataset <p>Extract a small subset of the dataset for demonstration purposes.</p> In\u00a0[\u00a0]: Copied! <pre>ds = dataset.isel(x=slice(100, 200), y=slice(100, 200))\nds\n</pre> ds = dataset.isel(x=slice(100, 200), y=slice(100, 200)) ds <p>Interactive slicing along the z-axis (band)</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"slice\",\n)\np.add_text(\"Band slicing \", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"slice\", ) p.add_text(\"Band slicing \", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Interactive slicing along the x-axis (longitude).</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"slice\",\n    normal=\"x\",\n)\np.add_text(\"X-axis slicing \", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"slice\",     normal=\"x\", ) p.add_text(\"X-axis slicing \", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Orthogonal slicing.</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"orthogonal\",\n)\np.add_text(\"Orthogonal slicing\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"orthogonal\", ) p.add_text(\"Orthogonal slicing\", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Clip the image cube with a plane (band slicing).</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"plane\", ) p.add_text(\"Band slicing\", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Interactive thresholding.</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"threshold\", ) p.add_text(\"Thresholding\", position=\"upper_right\", font_size=14) p.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nfilepath = \"data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" filepath = \"data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" hypercoast.download_file(url, filepath) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_emit(filepath)\ndataset\n</pre> dataset = hypercoast.read_emit(filepath) dataset <p>Select a subset of the data for demonstration purposes.</p> In\u00a0[\u00a0]: Copied! <pre>ds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93))\nds\n</pre> ds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93)) ds <p>Interactive slicing along the z-axis (band).</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\",     widget=\"plane\", ) p.add_text(\"Band slicing\", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Interactive thresholding.</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\",     widget=\"threshold\", ) p.add_text(\"Thresholding\", position=\"upper_right\", font_size=14) p.show() <p></p>"},{"location":"examples/image_slicing/#interactive-slicing-and-thresholding-of-hyperspectral-data-with-hypercoast","title":"Interactive slicing and thresholding of hyperspectral data with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to perform interactive slicing and thresholding of hyperspectral data with HyperCoast using the PyVista plotting backend.</p>"},{"location":"examples/image_slicing/#neon-aop","title":"NEON AOP\u00b6","text":"<p>Download a sample NEON AOP hyperspectral dataset.</p>"},{"location":"examples/image_slicing/#nasa-emit","title":"NASA EMIT\u00b6","text":"<p>Download a sample NASA EMIT hyperspectral dataset from here.</p>"},{"location":"examples/moe_vae/","title":"Moe vae","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[all]\"\n</pre> # %pip install \"hypercoast[all]\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nimport rasterio\nimport torch\n</pre> import os import numpy as np import pandas as pd import rasterio import torch In\u00a0[\u00a0]: Copied! <pre>from hypercoast import download_file\nfrom hypercoast.moe_vae import (\n    load_real_data,\n    load_real_test,\n    calculate_metrics,\n    plot_results,\n    save_results_to_excel,\n    save_and_plot_results_from_excel,\n    preprocess_pace_data_minmax,\n    infer_and_visualize_single_model_minmax,\n    MoE_VAE,\n    train,\n    evaluate,\n)\n</pre> from hypercoast import download_file from hypercoast.moe_vae import (     load_real_data,     load_real_test,     calculate_metrics,     plot_results,     save_results_to_excel,     save_and_plot_results_from_excel,     preprocess_pace_data_minmax,     infer_and_visualize_single_model_minmax,     MoE_VAE,     train,     evaluate, ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_moe_data.zip\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_moe_data.zip\" In\u00a0[\u00a0]: Copied! <pre>download_file(url, quiet=False)\n</pre> download_file(url, quiet=False) In\u00a0[\u00a0]: Copied! <pre>nc_path = \"./data/PACE_OCI.20240929T185124.L2.OC_AOP.V3_0.nc\"\npace_rgb_path = \"./data/snapshot-2024-08-10T00_00_00Z.tif\"\nwavelength_filepath = \"./data/pace_wavelengths.csv\"\n# === Dataset paths ===\nexcel_path_train = \"./data/Gloria_updated_07242025.xlsx\"\ntest_files = [\n    \"./data/GreatLake_all_data.xlsx\",\n    \"./data/GOA_insitu_data_07242025updated.xlsx\",\n    \"./data/satellite_for_PACE.xlsx\",\n    \"./data/satellite_for_PACE_LE.xlsx\",\n]\nbase_save_dir = \"./test\"\n</pre> nc_path = \"./data/PACE_OCI.20240929T185124.L2.OC_AOP.V3_0.nc\" pace_rgb_path = \"./data/snapshot-2024-08-10T00_00_00Z.tif\" wavelength_filepath = \"./data/pace_wavelengths.csv\" # === Dataset paths === excel_path_train = \"./data/Gloria_updated_07242025.xlsx\" test_files = [     \"./data/GreatLake_all_data.xlsx\",     \"./data/GOA_insitu_data_07242025updated.xlsx\",     \"./data/satellite_for_PACE.xlsx\",     \"./data/satellite_for_PACE_LE.xlsx\", ] base_save_dir = \"./test\" In\u00a0[\u00a0]: Copied! <pre>wv_PACE = pd.read_csv(wavelength_filepath)[\"wavelength\"].tolist()\n# wv_PACE\n</pre> wv_PACE = pd.read_csv(wavelength_filepath)[\"wavelength\"].tolist() # wv_PACE In\u00a0[\u00a0]: Copied! <pre>selected_bands = wv_PACE\n</pre> selected_bands = wv_PACE In\u00a0[\u00a0]: Copied! <pre>with rasterio.open(pace_rgb_path) as ds:\n    R, G, B = ds.read(1), ds.read(2), ds.read(3)\n    extent = [ds.bounds.left, ds.bounds.right, ds.bounds.bottom, ds.bounds.top]\n    rgb_image = np.stack((R, G, B), axis=-1)\nos.makedirs(base_save_dir, exist_ok=True)\n</pre> with rasterio.open(pace_rgb_path) as ds:     R, G, B = ds.read(1), ds.read(2), ds.read(3)     extent = [ds.bounds.left, ds.bounds.right, ds.bounds.bottom, ds.bounds.top]     rgb_image = np.stack((R, G, B), axis=-1) os.makedirs(base_save_dir, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</pre> device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\") In\u00a0[\u00a0]: Copied! <pre>train_real_dl, test_real_dl, input_dim, output_dim, train_ids, test_ids = (\n    load_real_data(\n        excel_path=excel_path_train,\n        selected_bands=selected_bands,\n        seed=42,\n        diff_before_norm=False,\n        diff_after_norm=False,\n        target_parameter=\"chl-a\",\n        lower_quantile=0,\n        upper_quantile=1,\n        log_offset=1,\n    )\n)\n</pre> train_real_dl, test_real_dl, input_dim, output_dim, train_ids, test_ids = (     load_real_data(         excel_path=excel_path_train,         selected_bands=selected_bands,         seed=42,         diff_before_norm=False,         diff_after_norm=False,         target_parameter=\"chl-a\",         lower_quantile=0,         upper_quantile=1,         log_offset=1,     ) ) In\u00a0[\u00a0]: Copied! <pre>test_dls, test_ids_list, test_dates_list = [], [], []\nfor file in test_files:\n    dl, _, _, ids, dates = load_real_test(\n        excel_path=file,\n        selected_bands=selected_bands,\n        diff_before_norm=False,\n        diff_after_norm=False,\n        max_allowed_diff=1.0,\n        target_parameter=\"chl-a\",\n        log_offset=1,\n    )\n    test_dls.append(dl)\n    test_ids_list.append(ids)\n    test_dates_list.append(dates)\n</pre> test_dls, test_ids_list, test_dates_list = [], [], [] for file in test_files:     dl, _, _, ids, dates = load_real_test(         excel_path=file,         selected_bands=selected_bands,         diff_before_norm=False,         diff_after_norm=False,         max_allowed_diff=1.0,         target_parameter=\"chl-a\",         log_offset=1,     )     test_dls.append(dl)     test_ids_list.append(ids)     test_dates_list.append(dates) In\u00a0[\u00a0]: Copied! <pre>test_loader, Rrs, mask, latitude, longitude = preprocess_pace_data_minmax(\n    nc_path=nc_path,\n    diff_before_norm=False,\n    diff_after_norm=False,\n    full_band_wavelengths=np.array(selected_bands),\n)\n</pre> test_loader, Rrs, mask, latitude, longitude = preprocess_pace_data_minmax(     nc_path=nc_path,     diff_before_norm=False,     diff_after_norm=False,     full_band_wavelengths=np.array(selected_bands), ) In\u00a0[\u00a0]: Copied! <pre>model = MoE_VAE(\n    input_dim=input_dim,\n    output_dim=output_dim,\n    latent_dim=32,\n    encoder_hidden_dims=[64, 64],\n    decoder_hidden_dims=[64, 64],\n    activation=\"leakyrelu\",\n    use_norm=\"layer\",\n    use_dropout=False,\n    use_softplus_output=True,\n    num_experts=4,\n    k=2,\n    noisy_gating=True,\n).to(device)\n</pre> model = MoE_VAE(     input_dim=input_dim,     output_dim=output_dim,     latent_dim=32,     encoder_hidden_dims=[64, 64],     decoder_hidden_dims=[64, 64],     activation=\"leakyrelu\",     use_norm=\"layer\",     use_dropout=False,     use_softplus_output=True,     num_experts=4,     k=2,     noisy_gating=True, ).to(device) In\u00a0[\u00a0]: Copied! <pre>optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ntrain_log = train(\n    model,\n    train_real_dl,\n    device,\n    epochs=400,\n    optimizer=optimizer,\n    save_dir=base_save_dir,\n)\nbest_train_loss = train_log[\"best_loss\"]\n</pre> optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) train_log = train(     model,     train_real_dl,     device,     epochs=400,     optimizer=optimizer,     save_dir=base_save_dir, ) best_train_loss = train_log[\"best_loss\"] In\u00a0[\u00a0]: Copied! <pre>predictions, actuals = evaluate(model, test_real_dl, device, log_offset=1)\nepsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(predictions, actuals)\ntest_loss = mae\n\nsave_results_to_excel(\n    test_ids, actuals, predictions, os.path.join(base_save_dir, \"test_results.xlsx\")\n)\nplot_results(predictions, actuals, base_save_dir, mode=\"test_results\")\n\nfor dl, ids, dates, path in zip(test_dls, test_ids_list, test_dates_list, test_files):\n    preds, acts = evaluate(model, dl, device, log_offset=1)\n    save_and_plot_results_from_excel(preds, acts, ids, dates, path, base_save_dir)\n</pre> predictions, actuals = evaluate(model, test_real_dl, device, log_offset=1) epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(predictions, actuals) test_loss = mae  save_results_to_excel(     test_ids, actuals, predictions, os.path.join(base_save_dir, \"test_results.xlsx\") ) plot_results(predictions, actuals, base_save_dir, mode=\"test_results\")  for dl, ids, dates, path in zip(test_dls, test_ids_list, test_dates_list, test_files):     preds, acts = evaluate(model, dl, device, log_offset=1)     save_and_plot_results_from_excel(preds, acts, ids, dates, path, base_save_dir) In\u00a0[\u00a0]: Copied! <pre>infer_and_visualize_single_model_minmax(\n    model=model,\n    test_loader=test_loader,\n    Rrs=Rrs,\n    mask=mask,\n    latitude=latitude,\n    longitude=longitude,\n    save_folder=base_save_dir,\n    extent=extent,\n    rgb_image=rgb_image,\n    structure_name=\"09292024\",\n    run=1,\n    vmin=0,\n    vmax=30,\n    log_offset=1,\n)\n\nprint(\n    f\"\u2705 Finished training, train loss: {best_train_loss:.4f}, test loss: {test_loss:.4f}\"\n)\n</pre> infer_and_visualize_single_model_minmax(     model=model,     test_loader=test_loader,     Rrs=Rrs,     mask=mask,     latitude=latitude,     longitude=longitude,     save_folder=base_save_dir,     extent=extent,     rgb_image=rgb_image,     structure_name=\"09292024\",     run=1,     vmin=0,     vmax=30,     log_offset=1, )  print(     f\"\u2705 Finished training, train loss: {best_train_loss:.4f}, test loss: {test_loss:.4f}\" )"},{"location":"examples/moe_vae/#mixture-of-experts-variational-autoencoder-moe-vae","title":"Mixture of Experts Variational Autoencoder (MoE-VAE)\u00b6","text":"<p>This example demonstrates how to use the Mixture of Experts Variational Autoencoder (MoE-VAE) to predict chlorophyll-a (chl-a) using PACE data.</p>"},{"location":"examples/moe_vae/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/moe_vae/#download-data","title":"Download data\u00b6","text":""},{"location":"examples/moe_vae/#set-data-paths","title":"Set data paths\u00b6","text":""},{"location":"examples/moe_vae/#set-pace-wavelengths","title":"Set PACE wavelengths\u00b6","text":""},{"location":"examples/moe_vae/#read-data","title":"Read data\u00b6","text":""},{"location":"examples/moe_vae/#load-training-data","title":"Load training data\u00b6","text":""},{"location":"examples/moe_vae/#load-test-data","title":"Load test data\u00b6","text":""},{"location":"examples/moe_vae/#load-pace-data","title":"Load PACE data\u00b6","text":""},{"location":"examples/moe_vae/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/moe_vae/#model-training","title":"Model training\u00b6","text":""},{"location":"examples/moe_vae/#model-evaluation","title":"Model evaluation\u00b6","text":""},{"location":"examples/moe_vae/#model-inference","title":"Model inference\u00b6","text":""},{"location":"examples/multispectral/","title":"Multispectral","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\n</pre> # %pip install \"hypercoast[extra] In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/cog.tif\"\nfilepath = \"data/cog.tif\"\nhypercoast.download_file(url, filepath, quiet=True)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/cog.tif\" filepath = \"data/cog.tif\" hypercoast.download_file(url, filepath, quiet=True) In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nfilepath = \"data/cog.tif\"  # replace it with your own raster data\nm.add_dataset(\n    filepath, indexes=[4, 1, 2], vmin=0, vmax=2500, layer_name=\"Landsat\", nodata=0\n)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() filepath = \"data/cog.tif\"  # replace it with your own raster data m.add_dataset(     filepath, indexes=[4, 1, 2], vmin=0, vmax=2500, layer_name=\"Landsat\", nodata=0 ) m.add(\"spectral\") m"},{"location":"examples/multispectral/#visualizing-multispectral-data-with-hypercoast","title":"Visualizing Multispectral Data with HyperCoast\u00b6","text":""},{"location":"examples/neon/","title":"Neon","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\"\nfilepath = \"data/neon.h5\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\" filepath = \"data/neon.h5\" hypercoast.download_file(url, filepath) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_neon(filepath)\ndataset\n</pre> dataset = hypercoast.read_neon(filepath) dataset <p>Visualize the data interactively with HyperCoast.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_neon(filepath, wavelengths=[1000, 700, 500], vmin=0, vmax=0.5)\nm\n</pre> m = hypercoast.Map() m.add_neon(filepath, wavelengths=[1000, 700, 500], vmin=0, vmax=0.5) m In\u00a0[\u00a0]: Copied! <pre>m.set_center(-76.5134, 38.8973, 16)\n</pre> m.set_center(-76.5134, 38.8973, 16) In\u00a0[\u00a0]: Copied! <pre>m.add(\"spectral\")\n</pre> m.add(\"spectral\") <p></p>"},{"location":"examples/neon/#visualizing-neon-aop-hyperspectral-data-interactively-with-hypercoast","title":"Visualizing NEON AOP hyperspectral data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize NEON AOP hyperspectral data interactively with HyperCoast.</p>"},{"location":"examples/overview/","title":"Overview","text":"<ul> <li>Search and download NASA hyperspectral data</li> <li>Search and download NASA ECOSTRESS data</li> <li>Visualize Hyperspectral Data in 3D</li> <li>Interactive slicing and thresholding of hyperspectral data</li> <li>Visualize AVIRIS data interactively</li> <li>Visualize DESIS data interactively</li> <li>Visualize EMIT data interactively</li> <li>Visualize NEON AOP hyperspectral data interactively</li> <li>Visualize PACE data interactively</li> <li>Visualize PACE chlorophyll-a data interactively</li> <li>Visualize ERA5 temperature data interactively</li> <li>Visualize PACE OCI L1 data products</li> <li>Visualize PACE OCI L2 data products</li> <li>Visualize Multispectral data interactively</li> </ul>"},{"location":"examples/pace/","title":"Pace","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Download a sample PACE data file from here.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nhypercoast.download_file(url, filepath)\n</pre> filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" hypercoast.download_file(url, filepath) <p>Let's make a scatter plot of the pixel locations so we can see the irregular spacing.</p> In\u00a0[\u00a0]: Copied! <pre>plot = hypercoast.view_pace_pixel_locations(filepath, step=20)\n</pre> plot = hypercoast.view_pace_pixel_locations(filepath, step=20) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_pace(filepath)\n</pre> dataset = hypercoast.read_pace(filepath) <p>Visualize selected bands of the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2)\n</pre> hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2) <p>Add projection.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2, crs=\"default\")\n</pre> hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2, crs=\"default\") <p>Plot a spectral signature.</p> In\u00a0[\u00a0]: Copied! <pre>latitude = 29.9307\nlongitude = -87.9106\nhypercoast.extract_pace(dataset, latitude, longitude, return_plot=True)\n</pre> latitude = 29.9307 longitude = -87.9106 hypercoast.extract_pace(dataset, latitude, longitude, return_plot=True) <p>Plot multiple spectral signatures.</p> In\u00a0[\u00a0]: Copied! <pre>latitude = (29.49, 29.50)\nlongitude = (-88.10, -88.00)\nhypercoast.filter_pace(dataset, latitude, longitude, return_plot=True)\n</pre> latitude = (29.49, 29.50) longitude = (-88.10, -88.00) hypercoast.filter_pace(dataset, latitude, longitude, return_plot=True) <p>Single-band visualization.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450]\nm.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\")\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") wavelengths = [450] m.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\") m.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\") m.add(\"spectral\") m <p></p> <p>Multiple-band visualization.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450, 550, 650]\nm.add_pace(\n    dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\"\n)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") wavelengths = [450, 550, 650] m.add_pace(     dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\" ) m.add(\"spectral\") m <p></p>"},{"location":"examples/pace/#visualizing-pace-data-interactively-with-hypercoast","title":"Visualizing PACE data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) data interactively with HyperCoast.</p>"},{"location":"examples/pace_cyano/","title":"Pace cyano","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U \"hypercoast[extra]\"\n</pre> # %pip install -U \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import earthaccess\nimport hypercoast\nfrom hypercoast.pace import (\n    cyano_band_ratios,\n    apply_kmeans,\n    apply_pca,\n    apply_sam,\n    apply_sam_spectral,\n)\n</pre> import earthaccess import hypercoast from hypercoast.pace import (     cyano_band_ratios,     apply_kmeans,     apply_pca,     apply_sam,     apply_sam_spectral, ) In\u00a0[\u00a0]: Copied! <pre>earthaccess.login(persist=True)\n</pre> earthaccess.login(persist=True) <p>Search for PACE AOP data:</p> In\u00a0[\u00a0]: Copied! <pre>results = hypercoast.search_pace(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-07-30\", \"2024-08-15\"),\n    short_name=\"PACE_OCI_L2_AOP_NRT\",\n    count=1,\n)\n</pre> results = hypercoast.search_pace(     bounding_box=(-83, 25, -81, 28),     temporal=(\"2024-07-30\", \"2024-08-15\"),     short_name=\"PACE_OCI_L2_AOP_NRT\",     count=1, ) <p>Download PACE AOP data:</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_pace(results[:1], out_dir=\"data\")\n</pre> hypercoast.download_pace(results[:1], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\ndataset = hypercoast.read_pace(filepath)\n# dataset\n</pre> filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" dataset = hypercoast.read_pace(filepath) # dataset <p></p> In\u00a0[\u00a0]: Copied! <pre>da = cyano_band_ratios(dataset, plot=True)\n</pre> da = cyano_band_ratios(dataset, plot=True) In\u00a0[\u00a0]: Copied! <pre>cluster_labels, latitudes, longitudes = apply_kmeans(dataset, n_clusters=6)\n</pre> cluster_labels, latitudes, longitudes = apply_kmeans(dataset, n_clusters=6) In\u00a0[\u00a0]: Copied! <pre>da = dataset[\"Rrs\"]\n\nfilter_condition = (\n    (da.sel(wavelength=650) &gt; da.sel(wavelength=620))\n    &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=681))\n    &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=450))\n)\nextent = [-95, -85, 27, 33]\ncolors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"]\n\ncluster_labels, latitudes, longitudes = apply_kmeans(\n    da, n_clusters=6, filter_condition=filter_condition, extent=extent, colors=colors\n)\n</pre> da = dataset[\"Rrs\"]  filter_condition = (     (da.sel(wavelength=650) &gt; da.sel(wavelength=620))     &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=681))     &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=450)) ) extent = [-95, -85, 27, 33] colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"]  cluster_labels, latitudes, longitudes = apply_kmeans(     da, n_clusters=6, filter_condition=filter_condition, extent=extent, colors=colors ) In\u00a0[\u00a0]: Copied! <pre>pca_data = apply_pca(dataset, n_components=3, x_component=0, y_component=1)\n</pre> pca_data = apply_pca(dataset, n_components=3, x_component=0, y_component=1) In\u00a0[\u00a0]: Copied! <pre>pca_data = apply_pca(dataset, n_components=3, x_component=1, y_component=2)\n</pre> pca_data = apply_pca(dataset, n_components=3, x_component=1, y_component=2) In\u00a0[\u00a0]: Copied! <pre>data, latitudes, longitudes = apply_sam(\n    dataset,\n    n_components=3,\n    n_clusters=6,\n)\n</pre> data, latitudes, longitudes = apply_sam(     dataset,     n_components=3,     n_clusters=6, ) In\u00a0[\u00a0]: Copied! <pre>extent = [-95, -85, 27, 33]\ncolors = [\"#377eb8\", \"#ff7f00\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"]\ndata, latitudes, longitudes = apply_sam(\n    dataset,\n    n_components=3,\n    n_clusters=6,\n    extent=extent,\n    colors=colors,\n)\n</pre> extent = [-95, -85, 27, 33] colors = [\"#377eb8\", \"#ff7f00\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"] data, latitudes, longitudes = apply_sam(     dataset,     n_components=3,     n_clusters=6,     extent=extent,     colors=colors, ) In\u00a0[\u00a0]: Copied! <pre>da = dataset[\"Rrs\"]\n\nfilter_condition = (\n    (da.sel(wavelength=650) &gt; da.sel(wavelength=620))\n    &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=681))\n    &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=450))\n)\nextent = [-95, -85, 27, 33]\ncolors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"]\n\ndata, latitudes, longitudes = apply_sam(\n    dataset,\n    n_components=3,\n    n_clusters=6,\n    filter_condition=filter_condition,\n    extent=extent,\n    colors=colors,\n)\n</pre> da = dataset[\"Rrs\"]  filter_condition = (     (da.sel(wavelength=650) &gt; da.sel(wavelength=620))     &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=681))     &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=450)) ) extent = [-95, -85, 27, 33] colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\"]  data, latitudes, longitudes = apply_sam(     dataset,     n_components=3,     n_clusters=6,     filter_condition=filter_condition,     extent=extent,     colors=colors, ) In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\ndataset = hypercoast.read_pace(filepath)\nurl = \"https://github.com/opengeos/datasets/releases/download/hypercoast/SAM_spectral_library.zip\"\nhypercoast.download_file(url)\nspectral_library = \"./SAM_spectral_library/*.csv\"\n</pre> filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" dataset = hypercoast.read_pace(filepath) url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/SAM_spectral_library.zip\" hypercoast.download_file(url) spectral_library = \"./SAM_spectral_library/*.csv\" In\u00a0[\u00a0]: Copied! <pre>extent = [-95, -85, 27, 33]\ndata, latitudes, longitudes = apply_sam_spectral(\n    dataset,\n    spectral_library=spectral_library,\n    extent=extent,\n)\n</pre> extent = [-95, -85, 27, 33] data, latitudes, longitudes = apply_sam_spectral(     dataset,     spectral_library=spectral_library,     extent=extent, ) In\u00a0[\u00a0]: Copied! <pre>da = dataset[\"Rrs\"]\nextent = [-95, -85, 27, 33]\nfilter_condition = (\n    (da.sel(wavelength=650) &gt; da.sel(wavelength=620))\n    &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=681))\n    &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=450))\n)\ndata, latitudes, longitudes = apply_sam_spectral(\n    da,\n    spectral_library=spectral_library,\n    filter_condition=filter_condition,\n    extent=extent,\n)\n</pre> da = dataset[\"Rrs\"] extent = [-95, -85, 27, 33] filter_condition = (     (da.sel(wavelength=650) &gt; da.sel(wavelength=620))     &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=681))     &amp; (da.sel(wavelength=701) &gt; da.sel(wavelength=450)) ) data, latitudes, longitudes = apply_sam_spectral(     da,     spectral_library=spectral_library,     filter_condition=filter_condition,     extent=extent, )"},{"location":"examples/pace_cyano/#mapping-cyanobacteria-with-pace-data","title":"Mapping Cyanobacteria with PACE data\u00b6","text":""},{"location":"examples/pace_cyano/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following cell to install the HyperCoast package.</p>"},{"location":"examples/pace_cyano/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/pace_cyano/#download-pace-data","title":"Download PACE data\u00b6","text":"<p>To download and access the PACE AOP data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, run the following cell and enter your NASA Earthdata login credentials.</p>"},{"location":"examples/pace_cyano/#read-pace-data","title":"Read PACE data\u00b6","text":"<p>Read PACE AOP data as an <code>xarray.Dataset</code>:</p>"},{"location":"examples/pace_cyano/#compute-band-ratios","title":"Compute band ratios\u00b6","text":""},{"location":"examples/pace_cyano/#the-spectra-of-cyanobacteria-bloom","title":"The spectra of cyanobacteria bloom:\u00b6","text":""},{"location":"examples/pace_cyano/#cyanobacteria-and-spectral-angle-mapper","title":"Cyanobacteria and Spectral Angle Mapper\u00b6","text":"<p>Spectral Angle Mapper: Spectral similarity Input: library of Cyanobacteria bloom Rrs spectra with Chla at different levels</p> <p>Spectral Mixture Analysis: unmix different cyanobacteria species based on spectral difference.</p> <p></p>"},{"location":"examples/pace_cyano/#k-means-applied-to-the-whole-image","title":"K-means applied to the whole image\u00b6","text":""},{"location":"examples/pace_cyano/#k-means-applied-to-selected-pixels","title":"K-means applied to selected pixels\u00b6","text":""},{"location":"examples/pace_cyano/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)\u00b6","text":""},{"location":"examples/pace_cyano/#spectral-angle-mapper-sam","title":"Spectral Angle Mapper (SAM)\u00b6","text":""},{"location":"examples/pace_cyano/#apply-sam-to-the-whole-image","title":"Apply SAM to the whole image\u00b6","text":""},{"location":"examples/pace_cyano/#apply-sam-to-selected-pixels","title":"Apply SAM to selected pixels\u00b6","text":""},{"location":"examples/pace_cyano/#apply-sam-with-a-filtering-condition","title":"Apply SAM with a filtering condition\u00b6","text":""},{"location":"examples/pace_cyano/#use-spectral-library","title":"Use spectral library\u00b6","text":""},{"location":"examples/pace_oci_l1/","title":"Pace oci l1","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\nimport xarray as xr\n</pre> import hypercoast import xarray as xr <p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, you can run the following cell to search and download PACE OCI L1 data products.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n\nshort_name = \"PACE_OCI_L1B_SCI\"\nresults, gdf = hypercoast.search_nasa_data(\n    short_name=short_name,\n    bbox=(-90.5642, 29.9749, -89.7143, 30.42),\n    temporal=(\"2024-06-15\", \"2024-06-16\"),\n    return_gdf=True,\n)\n</pre> hypercoast.nasa_earth_login()  short_name = \"PACE_OCI_L1B_SCI\" results, gdf = hypercoast.search_nasa_data(     short_name=short_name,     bbox=(-90.5642, 29.9749, -89.7143, 30.42),     temporal=(\"2024-06-15\", \"2024-06-16\"),     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p></p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_nasa_data(results[0], out_dir=\"data\")\n</pre> hypercoast.download_nasa_data(results[0], out_dir=\"data\") <p>Alternatively, use the following code block to download a sample dataset from here.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240615T182549.L1B.nc\"\nfilepath = \"data/PACE_OCI.20240615T182549.L1B.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240615T182549.L1B.nc\" filepath = \"data/PACE_OCI.20240615T182549.L1B.nc\" hypercoast.download_file(url, filepath) <p>Let's check the top-level groups in the sample dataset.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.netcdf_groups(filepath)\n</pre> hypercoast.netcdf_groups(filepath) <p>The top-level groups in the sample dataset are:</p> <pre>['sensor_band_parameters',\n 'scan_line_attributes',\n 'geolocation_data',\n 'navigation_data',\n 'observation_data']\n</pre> <p>Let's open the <code>observation_data</code> group, which contains the core science variables.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = xr.open_dataset(filepath, group=\"observation_data\")\nprint(list(dataset.variables))\n</pre> dataset = xr.open_dataset(filepath, group=\"observation_data\") print(list(dataset.variables)) <p>The data variables include:</p> <pre>['rhot_blue', 'qual_blue', 'rhot_red', 'qual_red', 'rhot_SWIR', 'qual_SWIR']\n</pre> <p>The dimensions of the <code>rhot_blue</code> variable are <code>(\"blue_bands\", \"number_of_scans\", \"ccd_pixels\")</code>, and it has shape <code>(119, 1710, 1272)</code>. The sizes attribute of a variable gives us that information as a dictionary.</p> In\u00a0[\u00a0]: Copied! <pre>dataset[\"rhot_blue\"].sizes\n</pre> dataset[\"rhot_blue\"].sizes <p>The dimensions of the <code>rhot_red</code> variable are <code>(\"red_bands\", \"number_of_scans\", \"ccd_pixels\")</code>, and it has shape <code>(163, 1710, 1272)</code></p> In\u00a0[\u00a0]: Copied! <pre>dataset[\"rhot_red\"].sizes\n</pre> dataset[\"rhot_red\"].sizes <p>The dimensions of the <code>rhot_SWIR</code> variable are <code>(\"SWIR_bands\", \"number_of_scans\", \"SWIR_pixels\")</code>, and it has shape <code>(9, 1710, 1272)</code></p> In\u00a0[\u00a0]: Copied! <pre>dataset[\"rhot_SWIR\"].sizes\n</pre> dataset[\"rhot_SWIR\"].sizes <p>Let's plot the reflectance at position <code>100</code> in the <code>blue_bands</code> dimension.</p> In\u00a0[\u00a0]: Copied! <pre>plot = dataset[\"rhot_blue\"].sel({\"blue_bands\": 100}).plot()\n</pre> plot = dataset[\"rhot_blue\"].sel({\"blue_bands\": 100}).plot() <p></p>"},{"location":"examples/pace_oci_l1/#visualizing-pace-oci-l1-data-products-with-hypercoast","title":"Visualizing PACE OCI L1 data products with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) OCI L1 data products. Part of the notebook is adapted from the NASA OB.DAAC tutorial - File Structure at Three Processing Levels for the Ocean Color Instrument (OCI). Credits to the NASA OB.DAAC team for the tutorial.</p>"},{"location":"examples/pace_oci_l2/","title":"Pace oci l2","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, you can uncomment and run the following cell to search and download PACE OCI L2 data products.</p> In\u00a0[\u00a0]: Copied! <pre># hypercoast.nasa_earth_login()\n\n# short_name = \"PACE_OCI_L2_BGC_NRT\"\n# results, gdf = hypercoast.search_nasa_data(\n#     short_name=short_name,\n#     bbox=(-90.5642, 29.9749, -89.7143, 30.42),\n#     temporal=(\"2024-06-15\", \"2024-06-16\"),\n#     return_gdf=True\n#     )\n# hypercoast.download_nasa_data(results, out_dir=\"bgc\")\n</pre> # hypercoast.nasa_earth_login()  # short_name = \"PACE_OCI_L2_BGC_NRT\" # results, gdf = hypercoast.search_nasa_data( #     short_name=short_name, #     bbox=(-90.5642, 29.9749, -89.7143, 30.42), #     temporal=(\"2024-06-15\", \"2024-06-16\"), #     return_gdf=True #     ) # hypercoast.download_nasa_data(results, out_dir=\"bgc\") <p>Alternatively, use the following code block to download a sample dataset from here.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc\"\nfilepath = \"data/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc\" filepath = \"data/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc\" hypercoast.download_file(url, filepath) <p>Load the downloaded dataset as an <code>xarray.Dataset</code>:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_pace_bgc(filepath)\n</pre> dataset = hypercoast.read_pace_bgc(filepath) <p>Let's inspect the data variables contained in the dataset:</p> In\u00a0[\u00a0]: Copied! <pre>dataset.variables\n</pre> dataset.variables <p>We can see that the dataset contains the following variables:</p> <ul> <li>Chlorophyll Concentration</li> <li>Phytoplankton Carbon</li> <li>Particulate Organic Carbon</li> </ul> <p>Transform the xarray dataset into gridded data.</p> <p>Plot the Chlorophyll Concentration.</p> In\u00a0[\u00a0]: Copied! <pre>chlor_a = hypercoast.grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"linear\")\nchlor_a.plot(vmin=0, vmax=20, cmap=\"jet\", size=6)\n</pre> chlor_a = hypercoast.grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"linear\") chlor_a.plot(vmin=0, vmax=20, cmap=\"jet\", size=6) <p>Plot the Phytoplankton Carbon.</p> In\u00a0[\u00a0]: Copied! <pre>carbon_phyto = hypercoast.grid_pace_bgc(\n    dataset, variable=\"carbon_phyto\", method=\"linear\"\n)\ncarbon_phyto.plot(vmin=0, vmax=120, cmap=\"jet\", size=6)\n</pre> carbon_phyto = hypercoast.grid_pace_bgc(     dataset, variable=\"carbon_phyto\", method=\"linear\" ) carbon_phyto.plot(vmin=0, vmax=120, cmap=\"jet\", size=6) <p>Particulate Organic Carbon.</p> In\u00a0[\u00a0]: Copied! <pre>poc = hypercoast.grid_pace_bgc(dataset, variable=\"poc\", method=\"linear\")\npoc.plot(vmin=0, vmax=1000, cmap=\"jet\")\n</pre> poc = hypercoast.grid_pace_bgc(dataset, variable=\"poc\", method=\"linear\") poc.plot(vmin=0, vmax=1000, cmap=\"jet\") <p>Plot the data on an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_raster(chlor_a, layer_name=\"Chlorophyll-a\", colormap=\"jet\", vmin=0, vmax=20)\nm.add_raster(\n    carbon_phyto, layer_name=\"Phytoplankton Carbon\", colormap=\"plasma\", vmin=0, vmax=120\n)\nm.add_raster(\n    poc, layer_name=\"Particulate Organic Carbon\", colormap=\"coolwarm\", vmin=0, vmax=1000\n)\nm.add_layer_manager()\n\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=20, label=\"Chlorophyll-a (mg/m3)\")\nm.add_colormap(cmap=\"plasma\", vmin=0, vmax=120, label=\"Phytoplankton Carbon (mg/m3)\")\nm.add_colormap(\n    cmap=\"coolwarm\", vmin=0, vmax=1000, label=\"Particulate Organic Carbon (mg/m3)\"\n)\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_raster(chlor_a, layer_name=\"Chlorophyll-a\", colormap=\"jet\", vmin=0, vmax=20) m.add_raster(     carbon_phyto, layer_name=\"Phytoplankton Carbon\", colormap=\"plasma\", vmin=0, vmax=120 ) m.add_raster(     poc, layer_name=\"Particulate Organic Carbon\", colormap=\"coolwarm\", vmin=0, vmax=1000 ) m.add_layer_manager()  m.add_colormap(cmap=\"jet\", vmin=0, vmax=20, label=\"Chlorophyll-a (mg/m3)\") m.add_colormap(cmap=\"plasma\", vmin=0, vmax=120, label=\"Phytoplankton Carbon (mg/m3)\") m.add_colormap(     cmap=\"coolwarm\", vmin=0, vmax=1000, label=\"Particulate Organic Carbon (mg/m3)\" ) m <p></p>"},{"location":"examples/pace_oci_l2/#visualizing-pace-oci-l2-data-products-with-hypercoast","title":"Visualizing PACE OCI L2 data products with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) OCI L2 data products, including the concentration of chlorophyll-a, concentration of phytoplankton carbon, and concentration of particulate organic carbon.</p>"},{"location":"examples/pca/","title":"Pca","text":"In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/Planet_2022-09-23.tif\"\nfilepath = \"data/Planet_2022-09-23.tif\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/Planet_2022-09-23.tif\" filepath = \"data/Planet_2022-09-23.tif\" hypercoast.download_file(url, filepath) In\u00a0[\u00a0]: Copied! <pre>output_file = filepath.replace(\".tif\", \"_pca.tif\")\n</pre> output_file = filepath.replace(\".tif\", \"_pca.tif\") In\u00a0[\u00a0]: Copied! <pre>hypercoast.pca(filepath, output_file, n_components=3)\n</pre> hypercoast.pca(filepath, output_file, n_components=3) In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_dataset(\n    filepath, indexes=[6, 4, 2], vmin=0, vmax=2500, layer_name=\"Planet Image\", nodata=0\n)\nm.add_dataset(\n    output_file, indexes=[0, 1, 2], vmin=0, vmax=2500, layer_name=\"PCA Image\", nodata=0\n)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_dataset(     filepath, indexes=[6, 4, 2], vmin=0, vmax=2500, layer_name=\"Planet Image\", nodata=0 ) m.add_dataset(     output_file, indexes=[0, 1, 2], vmin=0, vmax=2500, layer_name=\"PCA Image\", nodata=0 ) m.add(\"spectral\") m"},{"location":"examples/pca/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)\u00b6","text":"<p>PCA is a technique used to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize. In this notebook, we'll perform PCA on a multi-spectral imagery to reduce the dimensionality of the data and visualize the results.</p>"},{"location":"examples/prisma/","title":"Prisma","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Register and obtain the PRISMA datasets from the ASI PRISMA portal: https://www.asi.it/en/earth-science/prisma/</p> <p>The dataset contains <code>Prisma.he5</code>, which we will use in this notebook.</p> In\u00a0[\u00a0]: Copied! <pre>filepath = \"/path/to/Prisma.he5\"\n</pre> filepath = \"/path/to/Prisma.he5\" <p>Read the PRISMA data as an <code>xarray.Dataset</code> object.</p> <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>ds = hypercoast.read_prisma(filepath, method=\"nearest\")\nds\n</pre> ds = hypercoast.read_prisma(filepath, method=\"nearest\") ds In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_prisma(ds, wavelengths=[650.0, 550.0, 450.0], vmin=0, vmax=0.2)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_prisma(ds, wavelengths=[650.0, 550.0, 450.0], vmin=0, vmax=0.2) m.add(\"spectral\") m <p>Add the PRISMA data to the map.</p>"},{"location":"examples/prisma/#visualizing-prisma-data-interactively-with-hypercoast","title":"Visualizing PRISMA data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize PRISMA hyperspectral data interactively with HyperCoast. For more information about PRISMA, please visit the links below:</p> <ul> <li>https://www.asi.it/en/earth-science/prisma/</li> <li>https://en.wikipedia.org/wiki/PRISMA_(spacecraft)</li> </ul>"},{"location":"examples/search_data/","title":"Search data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>results = hypercoast.search_datasets(instrument=\"oci\")\n</pre> results = hypercoast.search_datasets(instrument=\"oci\") In\u00a0[\u00a0]: Copied! <pre>datasets = set()\nfor item in results:\n    summary = item.summary()\n    short_name = summary[\"short-name\"]\n    if short_name not in datasets:\n        print(short_name)\n    datasets.add(short_name)\nprint(f\"\\nFound {len(datasets)} unique datasets\")\n</pre> datasets = set() for item in results:     summary = item.summary()     short_name = summary[\"short-name\"]     if short_name not in datasets:         print(short_name)     datasets.add(short_name) print(f\"\\nFound {len(datasets)} unique datasets\") In\u00a0[\u00a0]: Copied! <pre>results = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    count=1,\n)\n</pre> results = hypercoast.search_nasa_data(     short_name=\"PACE_OCI_L2_BGC_NRT\",     count=1, ) <p>We can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the temporal parameter to request a date range and the bounding_box parameter to request granules that intersect with a bounding box. We can even provide a cloud_cover threshold to limit files that have a lower percentage of cloud cover. We do not provide a count, so we'll get all granules that satisfy the constraints.</p> In\u00a0[\u00a0]: Copied! <pre>tspan = (\"2024-04-01\", \"2024-04-16\")\nbbox = (-76.75, 36.97, -75.74, 39.01)\nclouds = (0, 50)\n\nresults, gdf = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    temporal=tspan,\n    bounding_box=bbox,\n    cloud_cover=clouds,\n    return_gdf=True,\n)\n</pre> tspan = (\"2024-04-01\", \"2024-04-16\") bbox = (-76.75, 36.97, -75.74, 39.01) clouds = (0, 50)  results, gdf = hypercoast.search_nasa_data(     short_name=\"PACE_OCI_L2_BGC_NRT\",     temporal=tspan,     bounding_box=bbox,     cloud_cover=clouds,     return_gdf=True, ) <p>Display the footprints of the granules that match the search criteria.</p> In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p>Displaying a single result shows a direct download link: try it! The link will download the granule to your local machine, which may or may not be what you want to do. Even if you are running the notebook on a remote host, this download link will open a new browser tab or window and offer to save a file to your local machine. If you are running the notebook locally, this may be of use.</p> In\u00a0[\u00a0]: Copied! <pre>results[0]\n</pre> results[0] <p>We can also download all the results with one command.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_nasa_data(results, out_dir=\"data\")\n</pre> hypercoast.download_nasa_data(results, out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_pace(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-05-10\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_pace(     bounding_box=(-83, 25, -81, 28),     temporal=(\"2024-05-10\", \"2024-05-16\"),     count=10,  # use -1 to return all datasets     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p></p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_pace(results[:2], out_dir=\"data\")\n</pre> hypercoast.download_pace(results[:2], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_emit(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-04-01\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_emit(     bounding_box=(-83, 25, -81, 28),     temporal=(\"2024-04-01\", \"2024-05-16\"),     count=10,  # use -1 to return all datasets     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p></p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_emit(results[:2], out_dir=\"data\")\n</pre> hypercoast.download_emit(results[:2], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_ecostress(\n    bbox=(-120.522, 34.4266, -120.2665, 34.5653),\n    temporal=(\"2023-04-01\", \"2023-04-02\"),\n    count=-1,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_ecostress(     bbox=(-120.522, 34.4266, -120.2665, 34.5653),     temporal=(\"2023-04-01\", \"2023-04-02\"),     count=-1,  # use -1 to return all datasets     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_ecostress(results[:5], out_dir=\"data\")\n</pre> hypercoast.download_ecostress(results[:5], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[27.25, -83.05], zoom=6)\nm.search_pace()\nm\n</pre> m = hypercoast.Map(center=[27.25, -83.05], zoom=6) m.search_pace() m In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_pace(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")\n</pre> # hypercoast.download_pace(m._NASA_DATA_RESULTS[:2], out_dir=\"data\") <p>Search for EMIT data interactively.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[27.25, -83.05], zoom=6)\nm.search_emit()\nm\n</pre> m = hypercoast.Map(center=[27.25, -83.05], zoom=6) m.search_emit() m In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_emit(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")\n</pre> # hypercoast.download_emit(m._NASA_DATA_RESULTS[:2], out_dir=\"data\") <p>Search for ECOSTRESS data interactively.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[34.5014, -120.4032], zoom=11)\nm.search_ecostress()\nm\n</pre> m = hypercoast.Map(center=[34.5014, -120.4032], zoom=11) m.search_ecostress() m In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_ecostress(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")\n</pre> # hypercoast.download_ecostress(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")"},{"location":"examples/search_data/#search-and-download-nasa-hyperspectral-data-with-hypercoast","title":"Search and download NASA hyperspectral data with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to search and download NASA hyperspectral data (e.g., EMIT, PACE) and ECOSTRESS temperature data with HyperCoast. Part of the source code is adapted from the NASA OB.DAAC tutorial - Access Data from the Ocean Color Instrument (OCI). Credits to the NASA OB.DAAC team.</p>"},{"location":"examples/search_data/#import-library","title":"Import library\u00b6","text":""},{"location":"examples/search_data/#login-to-earthdata","title":"Login to Earthdata\u00b6","text":"<p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov.</p>"},{"location":"examples/search_data/#search-for-datasets","title":"Search for datasets\u00b6","text":"<p>Collections on NASA Earthdata are discovered with the search_datasets function, which accepts an instrument filter as an easy way to get started. Each of the items in the list of collections returned has a \"short-name\".</p>"},{"location":"examples/search_data/#search-for-data-by-short-name","title":"Search for data by short name\u00b6","text":"<p>Next, we use the <code>search_nasa_data</code> function to find granules within a collection. Let's use the <code>short_name</code> for the PACE/OCI Level-2 data product for bio-optical and biogeochemical properties.</p>"},{"location":"examples/search_data/#search-for-pace-data","title":"Search for PACE data\u00b6","text":""},{"location":"examples/search_data/#download-pace-data","title":"Download PACE data\u00b6","text":"<p>Download the first 2 files</p>"},{"location":"examples/search_data/#search-for-emit-data","title":"Search for EMIT data\u00b6","text":""},{"location":"examples/search_data/#download-emit-data","title":"Download EMIT data\u00b6","text":"<p>Download the first 2 files</p>"},{"location":"examples/search_data/#download-ecostress-data","title":"Download ECOSTRESS data\u00b6","text":""},{"location":"examples/search_data/#interactive-search","title":"Interactive search\u00b6","text":"<p>Search for PACE data interactively.</p>"},{"location":"examples/tanager/","title":"Tanager","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install hypercoast\n</pre> # %pip install hypercoast In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://storage.googleapis.com/open-cogs/planet-stac/release1-basic-radiance/20250514_193937_64_4001_basic_radiance.h5\"\n</pre> url = \"https://storage.googleapis.com/open-cogs/planet-stac/release1-basic-radiance/20250514_193937_64_4001_basic_radiance.h5\" In\u00a0[\u00a0]: Copied! <pre>file_path = hypercoast.download_file(url)\n</pre> file_path = hypercoast.download_file(url) In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_tanager(file_path)\ndataset\n</pre> dataset = hypercoast.read_tanager(file_path) dataset In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_tanager(dataset, bands=[100, 60, 50], vmin=0, vmax=120, layer_name=\"Tanager\")\nm\n</pre> m = hypercoast.Map() m.add_tanager(dataset, bands=[100, 60, 50], vmin=0, vmax=120, layer_name=\"Tanager\") m <p>Alternatively, you can visualize a single band of the Tanager data and specify a colormap.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_tanager(dataset, bands=[100], colormap=\"jet\", layer_name=\"Tanager\")\nm\n</pre> m = hypercoast.Map() m.add_tanager(dataset, bands=[100], colormap=\"jet\", layer_name=\"Tanager\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_tanager(dataset, bands=[100, 60, 50])\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_tanager(dataset, bands=[100, 60, 50]) m.add(\"spectral\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_tanager(dataset, bands=[100, 60, 50])\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_tanager(dataset, bands=[100, 60, 50]) m.add(\"spectral\") m <p></p>"},{"location":"examples/tanager/#visualizing-planet-tanager-data-interactively-with-hypercoast","title":"Visualizing Planet Tanager Data Interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize the Planet Tanager hyperspectral data interactively with HyperCoast.</p> <p>Tanager-1 (launched August 2024) carries a high-precision Dyson imaging spectrometer onboard Planet\u2019s next-generation smallsat bus. Tanager provides high spectral resolution (~5 nm) across the full 380\u20132500 nm VSWIR spectral range. For more details, please refer to the Planet Tanager data release page.</p>"},{"location":"examples/tanager/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following line to install the packages.</p>"},{"location":"examples/tanager/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/tanager/#find-tanager-data","title":"Find Tanager data\u00b6","text":"<p>Browse the Tanager data on the Planet STAC browser. Find the data you want to visualize.</p> <p>For example, we want to visualize the data of the coastal water bodies in the San Francisco Bay area. Click on the \"Copy URL\" button to get the direct URL of the data.</p> <p></p>"},{"location":"examples/tanager/#download-tanager-data","title":"Download Tanager data\u00b6","text":"<p>Once you have the URL of the data, you can download the data using the following code:</p>"},{"location":"examples/tanager/#read-tanager-data","title":"Read Tanager data\u00b6","text":"<p>We can read the Tanager data using the <code>read_tanager</code> function. It will return a <code>xarray.Dataset</code> object. The <code>toa_radiance</code> variable is the top of atmosphere radiance. It has 426 spectral bands. Note that the dataset is not gridded. We will need to interpolate the data to a regular grid for visualization on an interactive map.</p>"},{"location":"examples/tanager/#visualize-tanager-data","title":"Visualize Tanager data\u00b6","text":"<p>Let's visualize the Tanager data on an interactive map. Specify the bands to visualize. You can visualize the data in the spectral space or the RGB space.</p>"},{"location":"examples/tanager/#change-band-combinations-interactively","title":"Change band combinations interactively\u00b6","text":"<p>To change the band combinations interactively, you can use spectral tool to select the bands you want to visualize.</p>"},{"location":"examples/tanager/#visualize-spectral-signatures","title":"Visualize spectral signatures\u00b6","text":"<p>To visualize the spectral signatures, you can use the <code>spectral</code> tool. Simply click on the map to visualize the spectral signatures.</p>"},{"location":"examples/tanager/#save-spectral-signatures","title":"Save spectral signatures\u00b6","text":"<p>To save the spectral signatures, click on the Save button in the spectral tool.</p> <p></p>"},{"location":"examples/tanager_3d/","title":"Tanager 3d","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install hypercoast\n</pre> # %pip install hypercoast In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://storage.googleapis.com/open-cogs/planet-stac/release1-basic-radiance/20250514_193937_64_4001_basic_radiance.h5\"\n</pre> url = \"https://storage.googleapis.com/open-cogs/planet-stac/release1-basic-radiance/20250514_193937_64_4001_basic_radiance.h5\" In\u00a0[\u00a0]: Copied! <pre>file_path = hypercoast.download_file(url)\n</pre> file_path = hypercoast.download_file(url) In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_tanager(file_path)\ndataset\n</pre> dataset = hypercoast.read_tanager(file_path) dataset In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_tanager(dataset, bands=[100, 60, 50], vmin=0, vmax=120, layer_name=\"Tanager\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_tanager(dataset, bands=[100, 60, 50], vmin=0, vmax=120, layer_name=\"Tanager\") m.add(\"spectral\") m In\u00a0[\u00a0]: Copied! <pre>gridded = hypercoast.grid_tanager(dataset, row_range=(200, 400), col_range=(200, 400))\ngridded = (gridded / 100).clip(0, 1)\ngridded\n</pre> gridded = hypercoast.grid_tanager(dataset, row_range=(200, 400), col_range=(200, 400)) gridded = (gridded / 100).clip(0, 1) gridded In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    gridded,\n    variable=\"toa_radiance\",\n    cmap=\"jet\",\n    clim=(0, 1),\n    rgb_wavelengths=[1000, 600, 500],\n    title=\"Radiance * 100\",\n)\np.show()\n</pre> p = hypercoast.image_cube(     gridded,     variable=\"toa_radiance\",     cmap=\"jet\",     clim=(0, 1),     rgb_wavelengths=[1000, 600, 500],     title=\"Radiance * 100\", ) p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    gridded,\n    variable=\"toa_radiance\",\n    cmap=\"jet\",\n    clim=(0, 1),\n    rgb_wavelengths=[1000, 700, 500],\n    title=\"Radiance * 100\",\n    widget=\"slice\",\n)\np.add_text(\"Band slicing \", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     gridded,     variable=\"toa_radiance\",     cmap=\"jet\",     clim=(0, 1),     rgb_wavelengths=[1000, 700, 500],     title=\"Radiance * 100\",     widget=\"slice\", ) p.add_text(\"Band slicing \", position=\"upper_right\", font_size=14) p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    gridded,\n    variable=\"toa_radiance\",\n    cmap=\"jet\",\n    clim=(0, 1),\n    rgb_wavelengths=[1000, 700, 500],\n    widget=\"slice\",\n    normal=\"x\",\n)\np.add_text(\"X-axis slicing \", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     gridded,     variable=\"toa_radiance\",     cmap=\"jet\",     clim=(0, 1),     rgb_wavelengths=[1000, 700, 500],     widget=\"slice\",     normal=\"x\", ) p.add_text(\"X-axis slicing \", position=\"upper_right\", font_size=14) p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    gridded,\n    variable=\"toa_radiance\",\n    cmap=\"jet\",\n    clim=(0, 1),\n    rgb_wavelengths=[1000, 700, 500],\n    title=\"Radiance * 100\",\n    widget=\"orthogonal\",\n)\np.add_text(\"Orthogonal slicing\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     gridded,     variable=\"toa_radiance\",     cmap=\"jet\",     clim=(0, 1),     rgb_wavelengths=[1000, 700, 500],     title=\"Radiance * 100\",     widget=\"orthogonal\", ) p.add_text(\"Orthogonal slicing\", position=\"upper_right\", font_size=14) p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    gridded,\n    variable=\"toa_radiance\",\n    cmap=\"jet\",\n    clim=(0, 1),\n    rgb_wavelengths=[1000, 700, 500],\n    title=\"Radiance * 100\",\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     gridded,     variable=\"toa_radiance\",     cmap=\"jet\",     clim=(0, 1),     rgb_wavelengths=[1000, 700, 500],     title=\"Radiance * 100\",     widget=\"plane\", ) p.add_text(\"Band slicing\", position=\"upper_right\", font_size=14) p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    gridded,\n    variable=\"toa_radiance\",\n    cmap=\"jet\",\n    clim=(0, 1),\n    rgb_wavelengths=[1000, 700, 500],\n    title=\"Radiance * 100\",\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     gridded,     variable=\"toa_radiance\",     cmap=\"jet\",     clim=(0, 1),     rgb_wavelengths=[1000, 700, 500],     title=\"Radiance * 100\",     widget=\"threshold\", ) p.add_text(\"Thresholding\", position=\"upper_right\", font_size=14) p.show()"},{"location":"examples/tanager_3d/#visualizing-planets-tanager-data-in-3d","title":"Visualizing Planet's Tanager Data in 3D\u00b6","text":"<p>This notebook demonstrates how to visualize Planet's Tanager hyperspectral data in 3D with HyperCoast.</p> <p>Tanager-1 (launched August 2024) carries a high-precision Dyson imaging spectrometer onboard Planet\u2019s next-generation smallsat bus. Tanager provides high spectral resolution (~5 nm) across the full 380\u20132500 nm VSWIR spectral range. For more details, please refer to the Planet Tanager data release page.</p>"},{"location":"examples/tanager_3d/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following line to install the packages.</p>"},{"location":"examples/tanager_3d/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/tanager_3d/#find-tanager-data","title":"Find Tanager data\u00b6","text":"<p>Browse the Tanager data on the Planet STAC browser. Find the data you want to visualize.</p> <p>For example, we want to visualize the data of the coastal water bodies in the San Francisco Bay area. Click on the \"Copy URL\" button to get the direct URL of the data.</p>"},{"location":"examples/tanager_3d/#download-tanager-data","title":"Download Tanager data\u00b6","text":"<p>Once you have the URL of the data, you can download the data using the following code:</p>"},{"location":"examples/tanager_3d/#read-tanager-data","title":"Read Tanager data\u00b6","text":"<p>We can read the Tanager data using the <code>read_tanager</code> function. It will return a <code>xarray.Dataset</code> object. The <code>toa_radiance</code> variable is the top of atmosphere radiance. It has 426 spectral bands. Note that the dataset is not gridded. We will need to interpolate the data to a regular grid for visualization on an interactive map.</p>"},{"location":"examples/tanager_3d/#visualize-tanager-data-in-2d","title":"Visualize Tanager data in 2D\u00b6","text":"<p>Let's visualize the Tanager data on an interactive map. Specify the bands to visualize. You can visualize the data in the spectral space or the RGB space.</p>"},{"location":"examples/tanager_3d/#create-a-rectangular-grid-of-tanager-data","title":"Create a rectangular grid of Tanager data\u00b6","text":""},{"location":"examples/tanager_3d/#create-a-3d-image-cube","title":"Create a 3D image cube\u00b6","text":""},{"location":"examples/tanager_3d/#interactive-slicing-along-the-z-axis-band","title":"Interactive slicing along the z-axis (band)\u00b6","text":""},{"location":"examples/tanager_3d/#interactive-slicing-along-the-x-axis-longitude","title":"Interactive slicing along the x-axis (longitude).\u00b6","text":""},{"location":"examples/tanager_3d/#orthogonal-slicing","title":"Orthogonal slicing\u00b6","text":""},{"location":"examples/tanager_3d/#clip-the-image-cube-with-a-plane-band-slicing","title":"Clip the image cube with a plane (band slicing)\u00b6","text":""},{"location":"examples/tanager_3d/#interactive-thresholding","title":"Interactive thresholding\u00b6","text":""},{"location":"examples/temperature/","title":"Temperature","text":"In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/ERA5_temperature_2023.nc\"\nfilepath = \"data/ERA5_temperature_2023.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/ERA5_temperature_2023.nc\" filepath = \"data/ERA5_temperature_2023.nc\" hypercoast.download_file(url, filepath) In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.open_dataset(filepath)\ndataset\n</pre> dataset = hypercoast.open_dataset(filepath) dataset In\u00a0[\u00a0]: Copied! <pre>camera_position = [(-479.09, -82.89, -444.45), (89.5, 179.5, 16.5), (0.58, 0.14, -0.80)]\n</pre> camera_position = [(-479.09, -82.89, -444.45), (89.5, 179.5, 16.5), (0.58, 0.14, -0.80)] In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    dataset,\n    variable=\"temperature_2m\",\n    clim=(270, 310),\n    title=\"Temperature\",\n    cmap=\"coolwarm\",\n    widget=\"plane\",\n    invert=False,\n    grid_spacing=(1, 1, 3),\n)\np.camera_position = camera_position\np.show()\n</pre> p = hypercoast.image_cube(     dataset,     variable=\"temperature_2m\",     clim=(270, 310),     title=\"Temperature\",     cmap=\"coolwarm\",     widget=\"plane\",     invert=False,     grid_spacing=(1, 1, 3), ) p.camera_position = camera_position p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    dataset,\n    variable=\"temperature_2m\",\n    clim=(270, 310),\n    title=\"Temperature\",\n    cmap=\"coolwarm\",\n    widget=\"slice\",\n    grid_spacing=(1, 1, 3),\n)\np.camera_position = camera_position\np.show()\n</pre> p = hypercoast.image_cube(     dataset,     variable=\"temperature_2m\",     clim=(270, 310),     title=\"Temperature\",     cmap=\"coolwarm\",     widget=\"slice\",     grid_spacing=(1, 1, 3), ) p.camera_position = camera_position p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    dataset,\n    variable=\"temperature_2m\",\n    clim=(270, 310),\n    title=\"Temperature\",\n    cmap=\"coolwarm\",\n    widget=\"threshold\",\n)\np.camera_position = camera_position\np.show()\n</pre> p = hypercoast.image_cube(     dataset,     variable=\"temperature_2m\",     clim=(270, 310),     title=\"Temperature\",     cmap=\"coolwarm\",     widget=\"threshold\", ) p.camera_position = camera_position p.show()"},{"location":"examples/temperature/#visualizing-era5-temperature-data-interactively-with-hypercoast","title":"Visualizing ERA5 temperature data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize ERA5 temperature data interactively with HyperCoast.</p>"},{"location":"examples/wyvern/","title":"Wyvern","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Download a sample Wyvern dataset. It is a subset of the full dataset.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/wyvern_dragonette-001_20240608T144036_fa4c4f71.tif\"\nfilepath = \"data/wyvern.tif\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/wyvern_dragonette-001_20240608T144036_fa4c4f71.tif\" filepath = \"data/wyvern.tif\" hypercoast.download_file(url, filepath) <p>Load the dataset as a xarray.Dataset object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_wyvern(filepath)\ndataset\n</pre> dataset = hypercoast.read_wyvern(filepath) dataset In\u00a0[\u00a0]: Copied! <pre>dataset.sel(wavelength=780, method=\"nearest\")\n</pre> dataset.sel(wavelength=780, method=\"nearest\") <p>Plot the spectral signature of a pixel.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.filter_wyvern(dataset, lat=40.72, lon=-73.95, return_plot=True)\n</pre> hypercoast.filter_wyvern(dataset, lat=40.72, lon=-73.95, return_plot=True) <p>Visualize a single band of the hyperspectral image.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_wyvern(\n    dataset,\n    wavelengths=[799],\n    indexes=[1],\n    vmin=0,\n    vmax=100,\n    colormap=\"jet\",\n    nodata=-9999,\n)\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=100, label=\"Reflectance\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_wyvern(     dataset,     wavelengths=[799],     indexes=[1],     vmin=0,     vmax=100,     colormap=\"jet\",     nodata=-9999, ) m.add_colormap(cmap=\"jet\", vmin=0, vmax=100, label=\"Reflectance\") m <p>Plot the spectral signature of a pixel interactively.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_wyvern(filepath, wavelengths=[799, 679, 570], vmin=0, vmax=100, nodata=-9999)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_wyvern(filepath, wavelengths=[799, 679, 570], vmin=0, vmax=100, nodata=-9999) m.add(\"spectral\") m <p></p>"},{"location":"examples/wyvern/#visualizing-wyvern-data-interactively-with-hypercoast","title":"Visualizing Wyvern data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Wyvern hyperspectral data interactively with HyperCoast.</p>"},{"location":"moe_vae/data_loading/","title":"data_loading module","text":"<p>Data loading and preprocessing utilities for MoE-VAE models.</p> <p>This module provides functions for loading, preprocessing, and preparing remote sensing data for training and inference with VAE and MoE-VAE models. It includes support for various data formats and preprocessing techniques.</p>"},{"location":"moe_vae/data_loading/#hypercoast.moe_vae.data_loading.load_real_data","title":"<code>load_real_data(excel_path, selected_bands, split_ratio=0.7, seed=42, diff_before_norm=False, diff_after_norm=False, target_parameter='TSS', lower_quantile=0.0, upper_quantile=1.0, log_offset=0.01)</code>","text":"<p>Load and preprocess real data using MinMax scaling.</p> <p>This function loads remote sensing data from Excel files and applies MinMax normalization with optional differencing operations. Each sample is normalized independently to the range [1, 10].</p> <p>Parameters:</p> Name Type Description Default <code>excel_path</code> <code>str</code> <p>Path to Excel file containing the data.</p> required <code>selected_bands</code> <code>list</code> <p>List of wavelength bands to extract.</p> required <code>split_ratio</code> <code>float</code> <p>Train/test split ratio. Defaults to 0.7.</p> <code>0.7</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible splits. Defaults to 42.</p> <code>42</code> <code>diff_before_norm</code> <code>bool</code> <p>Apply differencing before normalization. Defaults to False.</p> <code>False</code> <code>diff_after_norm</code> <code>bool</code> <p>Apply differencing after normalization. Defaults to False.</p> <code>False</code> <code>target_parameter</code> <code>str</code> <p>Target parameter column name. Defaults to \"TSS\".</p> <code>'TSS'</code> <code>lower_quantile</code> <code>float</code> <p>Lower quantile for outlier removal. Defaults to 0.0.</p> <code>0.0</code> <code>upper_quantile</code> <code>float</code> <p>Upper quantile for outlier removal. Defaults to 1.0.</p> <code>1.0</code> <code>log_offset</code> <code>float</code> <p>Offset for log transformation. Defaults to 0.01.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - train_dl (DataLoader): Training data loader     - test_dl (DataLoader): Test data loader     - input_dim (int): Input feature dimension     - output_dim (int): Output dimension     - train_ids (list): Training sample IDs     - test_ids (list): Test sample IDs</p> Source code in <code>hypercoast/moe_vae/data_loading.py</code> <pre><code>def load_real_data(\n    excel_path,\n    selected_bands,\n    split_ratio=0.7,\n    seed=42,\n    diff_before_norm=False,\n    diff_after_norm=False,\n    target_parameter=\"TSS\",\n    lower_quantile=0.0,\n    upper_quantile=1.0,\n    log_offset=0.01,\n):\n    \"\"\"Load and preprocess real data using MinMax scaling.\n\n    This function loads remote sensing data from Excel files and applies\n    MinMax normalization with optional differencing operations. Each sample\n    is normalized independently to the range [1, 10].\n\n    Args:\n        excel_path (str): Path to Excel file containing the data.\n        selected_bands (list): List of wavelength bands to extract.\n        split_ratio (float, optional): Train/test split ratio. Defaults to 0.7.\n        seed (int, optional): Random seed for reproducible splits. Defaults to 42.\n        diff_before_norm (bool, optional): Apply differencing before normalization.\n            Defaults to False.\n        diff_after_norm (bool, optional): Apply differencing after normalization.\n            Defaults to False.\n        target_parameter (str, optional): Target parameter column name.\n            Defaults to \"TSS\".\n        lower_quantile (float, optional): Lower quantile for outlier removal.\n            Defaults to 0.0.\n        upper_quantile (float, optional): Upper quantile for outlier removal.\n            Defaults to 1.0.\n        log_offset (float, optional): Offset for log transformation.\n            Defaults to 0.01.\n\n    Returns:\n        tuple: A tuple containing:\n            - train_dl (DataLoader): Training data loader\n            - test_dl (DataLoader): Test data loader\n            - input_dim (int): Input feature dimension\n            - output_dim (int): Output dimension\n            - train_ids (list): Training sample IDs\n            - test_ids (list): Test sample IDs\n    \"\"\"\n\n    rounded_bands = [int(round(b)) for b in selected_bands]\n    band_cols = [f\"Rrs_{b}\" for b in rounded_bands]\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n    df_rrs_selected = df_rrs[[\"GLORIA_ID\"] + band_cols]\n    df_param_selected = df_param[[\"GLORIA_ID\", target_parameter]]\n    df_merged = pd.merge(\n        df_rrs_selected, df_param_selected, on=\"GLORIA_ID\", how=\"inner\"\n    )\n\n    # === Filter valid samples ===\n    mask_rrs_valid = df_merged[band_cols].notna().all(axis=1)\n    mask_target_valid = df_merged[target_parameter].notna()\n    df_filtered = df_merged[mask_rrs_valid &amp; mask_target_valid].reset_index(drop=True)\n    print(\n        f\"\u2705 Number of samples after filtering Rrs and {target_parameter}: {len(df_filtered)}\"\n    )\n\n    # === Quantile clipping for target parameter ===\n    lower = df_filtered[target_parameter].quantile(lower_quantile)\n    upper = df_filtered[target_parameter].quantile(upper_quantile)\n    df_filtered = df_filtered[\n        (df_filtered[target_parameter] &gt;= lower)\n        &amp; (df_filtered[target_parameter] &lt;= upper)\n    ].reset_index(drop=True)\n    print(\n        f\"\u2705 Number of samples after removing {target_parameter} quantiles [{lower_quantile}, {upper_quantile}]: {len(df_filtered)}\"\n    )\n\n    # === Extract sample IDs, Rrs, and target parameter ===\n    all_sample_ids = df_filtered[\"GLORIA_ID\"].astype(str).tolist()\n    Rrs_array = df_filtered[band_cols].values\n    param_array = df_filtered[[target_parameter]].values\n\n    if diff_before_norm:\n        Rrs_array = np.diff(Rrs_array, axis=1)\n\n    # === Apply MinMax scaling to [1, 10] for each sample independently ===\n    scalers_Rrs_real = [MinMaxScaler((1, 10)) for _ in range(Rrs_array.shape[0])]\n    Rrs_normalized = np.array(\n        [\n            scalers_Rrs_real[i].fit_transform(row.reshape(-1, 1)).flatten()\n            for i, row in enumerate(Rrs_array)\n        ]\n    )\n\n    if diff_after_norm:\n        Rrs_normalized = np.diff(Rrs_normalized, axis=1)\n\n    # === Transform target parameter to log10(param + log_offset) ===\n    param_transformed = np.log10(param_array + log_offset)\n\n    # === Build Dataset ===\n    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n    param_tensor = torch.tensor(param_transformed, dtype=torch.float32)\n    dataset = TensorDataset(Rrs_tensor, param_tensor)\n\n    # === Split into training and testing sets ===\n    num_samples = len(dataset)\n    indices = np.arange(num_samples)\n    np.random.seed(seed)\n    np.random.shuffle(indices)\n    train_size = int(split_ratio * num_samples)\n    train_indices = indices[:train_size]\n    test_indices = indices[train_size:]\n\n    train_dataset = Subset(dataset, train_indices)\n    test_dataset = Subset(dataset, test_indices)\n\n    train_ids = [all_sample_ids[i] for i in train_indices]\n    test_ids = [all_sample_ids[i] for i in test_indices]\n\n    train_dl = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)\n    test_dl = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = param_tensor.shape[1]\n\n    return (train_dl, test_dl, input_dim, output_dim, train_ids, test_ids)\n</code></pre>"},{"location":"moe_vae/data_loading/#hypercoast.moe_vae.data_loading.load_real_data_Robust","title":"<code>load_real_data_Robust(excel_path, selected_bands, target_parameter='TSS', split_ratio=0.7, seed=42, use_diff=False, lower_quantile=0.0, upper_quantile=1.0, Rrs_range=(0, 0.25), target_range=(-0.5, 0.5))</code>","text":"<p>Load and preprocess real data using robust scaling methods.</p> <p>This function loads remote sensing reflectance (Rrs) and parameter data from Excel files, applies robust preprocessing including quantile filtering, normalization, and data splitting for training/testing.</p> <p>Parameters:</p> Name Type Description Default <code>excel_path</code> <code>str</code> <p>Path to Excel file containing Rrs and parameter data.</p> required <code>selected_bands</code> <code>list</code> <p>List of wavelength bands to extract from Rrs data.</p> required <code>target_parameter</code> <code>str</code> <p>Name of target parameter column. Defaults to \"TSS\".</p> <code>'TSS'</code> <code>split_ratio</code> <code>float</code> <p>Train/test split ratio. Defaults to 0.7.</p> <code>0.7</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible splits. Defaults to 42.</p> <code>42</code> <code>use_diff</code> <code>bool</code> <p>Whether to apply first difference to Rrs. Defaults to False.</p> <code>False</code> <code>lower_quantile</code> <code>float</code> <p>Lower quantile for outlier removal. Defaults to 0.0.</p> <code>0.0</code> <code>upper_quantile</code> <code>float</code> <p>Upper quantile for outlier removal. Defaults to 1.0.</p> <code>1.0</code> <code>Rrs_range</code> <code>tuple</code> <p>Target range for Rrs normalization. Defaults to (0, 0.25).</p> <code>(0, 0.25)</code> <code>target_range</code> <code>tuple</code> <p>Target range for parameter normalization. Defaults to (-0.5, 0.5).</p> <code>(-0.5, 0.5)</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - train_dl (DataLoader): Training data loader     - test_dl (DataLoader): Test data loader     - input_dim (int): Input feature dimension     - output_dim (int): Output dimension     - train_ids (list): Training sample IDs     - test_ids (list): Test sample IDs     - scaler_Rrs: Fitted Rrs scaler object     - TSS_scalers_dict (dict): Dictionary of fitted target scalers</p> Source code in <code>hypercoast/moe_vae/data_loading.py</code> <pre><code>def load_real_data_Robust(\n    excel_path,\n    selected_bands,\n    target_parameter=\"TSS\",\n    split_ratio=0.7,\n    seed=42,\n    use_diff=False,\n    lower_quantile=0.0,\n    upper_quantile=1.0,\n    Rrs_range=(0, 0.25),\n    target_range=(-0.5, 0.5),\n):\n    \"\"\"Load and preprocess real data using robust scaling methods.\n\n    This function loads remote sensing reflectance (Rrs) and parameter data from\n    Excel files, applies robust preprocessing including quantile filtering,\n    normalization, and data splitting for training/testing.\n\n    Args:\n        excel_path (str): Path to Excel file containing Rrs and parameter data.\n        selected_bands (list): List of wavelength bands to extract from Rrs data.\n        target_parameter (str, optional): Name of target parameter column.\n            Defaults to \"TSS\".\n        split_ratio (float, optional): Train/test split ratio. Defaults to 0.7.\n        seed (int, optional): Random seed for reproducible splits. Defaults to 42.\n        use_diff (bool, optional): Whether to apply first difference to Rrs.\n            Defaults to False.\n        lower_quantile (float, optional): Lower quantile for outlier removal.\n            Defaults to 0.0.\n        upper_quantile (float, optional): Upper quantile for outlier removal.\n            Defaults to 1.0.\n        Rrs_range (tuple, optional): Target range for Rrs normalization.\n            Defaults to (0, 0.25).\n        target_range (tuple, optional): Target range for parameter normalization.\n            Defaults to (-0.5, 0.5).\n\n    Returns:\n        tuple: A tuple containing:\n            - train_dl (DataLoader): Training data loader\n            - test_dl (DataLoader): Test data loader\n            - input_dim (int): Input feature dimension\n            - output_dim (int): Output dimension\n            - train_ids (list): Training sample IDs\n            - test_ids (list): Test sample IDs\n            - scaler_Rrs: Fitted Rrs scaler object\n            - TSS_scalers_dict (dict): Dictionary of fitted target scalers\n    \"\"\"\n\n    rounded_bands = [int(round(b)) for b in selected_bands]\n    band_cols = [f\"Rrs_{b}\" for b in rounded_bands]\n\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n\n    df_rrs_selected = df_rrs[[\"GLORIA_ID\"] + band_cols]\n    df_param_selected = df_param[[\"GLORIA_ID\", target_parameter]]\n    df_merged = pd.merge(\n        df_rrs_selected, df_param_selected, on=\"GLORIA_ID\", how=\"inner\"\n    )\n\n    mask_rrs_valid = df_merged[band_cols].notna().all(axis=1)\n    mask_param_valid = df_merged[target_parameter].notna()\n    df_filtered = df_merged[mask_rrs_valid &amp; mask_param_valid].reset_index(drop=True)\n\n    print(\n        f\"Number of samples after filtering Rrs and {target_parameter}: {len(df_filtered)}\"\n    )\n\n    lower = df_filtered[target_parameter].quantile(lower_quantile)\n    top = df_filtered[target_parameter].quantile(upper_quantile)\n    df_filtered = df_filtered[\n        (df_filtered[target_parameter] &gt;= lower)\n        &amp; (df_filtered[target_parameter] &lt;= top)\n    ].reset_index(drop=True)\n\n    print(\n        f\"Number of samples after removing {target_parameter} quantiles [{lower_quantile}, {upper_quantile}]: {len(df_filtered)}\"\n    )\n\n    all_sample_ids = df_filtered[\"GLORIA_ID\"].astype(str).tolist()\n    Rrs_array = df_filtered[band_cols].values\n    param_array = df_filtered[[target_parameter]].values\n\n    if use_diff:\n        Rrs_array = np.diff(Rrs_array, axis=1)\n\n    scaler_Rrs = RobustMinMaxScaler(feature_range=Rrs_range)\n    scaler_Rrs.fit(torch.tensor(Rrs_array, dtype=torch.float32))\n    Rrs_normalized = scaler_Rrs.transform(\n        torch.tensor(Rrs_array, dtype=torch.float32)\n    ).numpy()\n\n    log_scaler = LogScaler(shift_min=False, safety_term=1e-8)\n    param_log = log_scaler.fit_transform(torch.tensor(param_array, dtype=torch.float32))\n    param_scaler = RobustMinMaxScaler(\n        feature_range=target_range, global_scale=True, robust=True\n    )\n    param_transformed = param_scaler.fit_transform(param_log).numpy()\n\n    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n    param_tensor = torch.tensor(param_transformed, dtype=torch.float32)\n    dataset = TensorDataset(Rrs_tensor, param_tensor)\n\n    num_samples = len(dataset)\n    indices = np.arange(num_samples)\n    np.random.seed(seed)\n    np.random.shuffle(indices)\n    train_size = int(split_ratio * num_samples)\n    train_indices = indices[:train_size]\n    test_indices = indices[train_size:]\n\n    train_dataset = Subset(dataset, train_indices)\n    test_dataset = Subset(dataset, test_indices)\n\n    train_ids = [all_sample_ids[i] for i in train_indices]\n    test_ids = [all_sample_ids[i] for i in test_indices]\n\n    train_dl = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)\n    test_dl = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = param_tensor.shape[1]\n    TSS_scalers_dict = {\"log\": log_scaler, \"robust\": param_scaler}\n\n    return (\n        train_dl,\n        test_dl,\n        input_dim,\n        output_dim,\n        train_ids,\n        test_ids,\n        scaler_Rrs,\n        TSS_scalers_dict,\n    )\n</code></pre>"},{"location":"moe_vae/data_loading/#hypercoast.moe_vae.data_loading.load_real_test","title":"<code>load_real_test(excel_path, selected_bands, max_allowed_diff=1.0, diff_before_norm=False, diff_after_norm=False, target_parameter='TSS', log_offset=0.01)</code>","text":"<p>Load and preprocess real test data using MinMax scaling.</p> <p>This function loads test data from Excel files, matches wavelength bands to available bands, and applies MinMax normalization with optional differencing operations consistent with training preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>excel_path</code> <code>str</code> <p>Path to Excel file containing test data.</p> required <code>selected_bands</code> <code>list</code> <p>List of target wavelength bands.</p> required <code>max_allowed_diff</code> <code>float</code> <p>Maximum allowed wavelength difference for band matching in nm. Defaults to 1.0.</p> <code>1.0</code> <code>diff_before_norm</code> <code>bool</code> <p>Apply differencing before normalization. Defaults to False.</p> <code>False</code> <code>diff_after_norm</code> <code>bool</code> <p>Apply differencing after normalization. Defaults to False.</p> <code>False</code> <code>target_parameter</code> <code>str</code> <p>Target parameter column name. Defaults to \"TSS\".</p> <code>'TSS'</code> <code>log_offset</code> <code>float</code> <p>Offset for log transformation. Defaults to 0.01.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - test_dl (DataLoader): Test data loader     - input_dim (int): Input feature dimension     - output_dim (int): Output dimension     - sample_ids (list): Sample identifiers     - sample_dates (list): Sample dates</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If table row counts don't match or wavelengths can't be matched.</p> Source code in <code>hypercoast/moe_vae/data_loading.py</code> <pre><code>def load_real_test(\n    excel_path,\n    selected_bands,\n    max_allowed_diff=1.0,\n    diff_before_norm=False,\n    diff_after_norm=False,\n    target_parameter=\"TSS\",\n    log_offset=0.01,\n):\n    \"\"\"Load and preprocess real test data using MinMax scaling.\n\n    This function loads test data from Excel files, matches wavelength bands\n    to available bands, and applies MinMax normalization with optional\n    differencing operations consistent with training preprocessing.\n\n    Args:\n        excel_path (str): Path to Excel file containing test data.\n        selected_bands (list): List of target wavelength bands.\n        max_allowed_diff (float, optional): Maximum allowed wavelength difference\n            for band matching in nm. Defaults to 1.0.\n        diff_before_norm (bool, optional): Apply differencing before normalization.\n            Defaults to False.\n        diff_after_norm (bool, optional): Apply differencing after normalization.\n            Defaults to False.\n        target_parameter (str, optional): Target parameter column name.\n            Defaults to \"TSS\".\n        log_offset (float, optional): Offset for log transformation.\n            Defaults to 0.01.\n\n    Returns:\n        tuple: A tuple containing:\n            - test_dl (DataLoader): Test data loader\n            - input_dim (int): Input feature dimension\n            - output_dim (int): Output dimension\n            - sample_ids (list): Sample identifiers\n            - sample_dates (list): Sample dates\n\n    Raises:\n        ValueError: If table row counts don't match or wavelengths can't be matched.\n    \"\"\"\n\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n\n    if df_rrs.shape[0] != df_param.shape[0]:\n        raise ValueError(\n            f\"\u274c The number of rows in the Rrs table and parameter table do not match. Rrs: {df_rrs.shape[0]}, parameter: {df_param.shape[0]}\"\n        )\n\n    # === Extract IDs and dates ===\n    sample_ids = df_rrs[\"Site Label\"].astype(str).tolist()\n    sample_dates = df_rrs[\"Date\"].astype(str).tolist()\n\n    # === Match target bands ===\n    rrs_wavelengths = []\n    rrs_cols = []\n    for col in df_rrs.columns:\n        try:\n            wl = float(col)\n            rrs_wavelengths.append(wl)\n            rrs_cols.append(col)\n        except Exception:\n            continue\n\n    band_cols = []\n    matched_bands = []\n    for target_band in selected_bands:\n        diffs = [abs(wl - target_band) for wl in rrs_wavelengths]\n        min_diff = min(diffs)\n        if min_diff &gt; max_allowed_diff:\n            raise ValueError(\n                f\"Target wavelength {target_band} nm cannot be matched, error {min_diff:.2f} nm exceeds the allowed range\"\n            )\n        best_idx = diffs.index(min_diff)\n        band_cols.append(rrs_cols[best_idx])\n        matched_bands.append(rrs_wavelengths[best_idx])\n\n    print(\n        f\"\\n\u2705 Band matching successful, {len(selected_bands)} target bands in total, {len(band_cols)} columns actually extracted\"\n    )\n    print(f\"Original number of test samples: {df_rrs.shape[0]}\\n\")\n\n    # === Extract Rrs and target parameter (without differencing for now) ===\n    Rrs_array = df_rrs[band_cols].values.astype(float)\n    target_array = df_param[[target_parameter]].values.astype(float).flatten()\n\n    # === Key: Remove rows with NaN/Inf before differencing ===\n    mask_inputs_ok = np.all(np.isfinite(Rrs_array), axis=1)\n    mask_target_ok = np.isfinite(target_array)\n    mask_ok = mask_inputs_ok &amp; mask_target_ok\n    if not np.any(mask_ok):\n        raise ValueError(\"\u274c No valid samples (NaN/Inf found in input or target).\")\n    dropped = int(len(mask_ok) - mask_ok.sum())\n    if dropped &gt; 0:\n        print(\n            f\"\u26a0\ufe0f Dropped {dropped} invalid samples (containing NaN/Inf) before differencing\"\n        )\n\n    Rrs_array = Rrs_array[mask_ok]\n    target_array = target_array[mask_ok]\n    sample_ids = [sid for sid, keep in zip(sample_ids, mask_ok) if keep]\n    sample_dates = [d for d, keep in zip(sample_dates, mask_ok) if keep]\n\n    # === Preprocessing before differencing (optional) ===\n    if diff_before_norm:\n        Rrs_array = np.diff(Rrs_array, axis=1)\n\n    # === Apply MinMaxScaler to [1, 10] for each sample ===\n    scalers_Rrs_test = [MinMaxScaler((1, 10)) for _ in range(Rrs_array.shape[0])]\n    Rrs_normalized = np.array(\n        [\n            scalers_Rrs_test[i].fit_transform(row.reshape(-1, 1)).flatten()\n            for i, row in enumerate(Rrs_array)\n        ]\n    )\n\n    # === Post-processing after differencing (optional) ===\n    if diff_after_norm:\n        Rrs_normalized = np.diff(Rrs_normalized, axis=1)\n\n    # === Transform target value to log10(x + log_offset) ===\n    target_transformed = np.log10(target_array + log_offset)\n\n    # === Construct DataLoader ===\n    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n    target_tensor = torch.tensor(target_transformed.reshape(-1, 1), dtype=torch.float32)\n\n    dataset = TensorDataset(Rrs_tensor, target_tensor)\n    test_dl = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = target_tensor.shape[1]\n\n    return test_dl, input_dim, output_dim, sample_ids, sample_dates\n</code></pre>"},{"location":"moe_vae/data_loading/#hypercoast.moe_vae.data_loading.load_real_test_Robust","title":"<code>load_real_test_Robust(excel_path, selected_bands, max_allowed_diff=1.0, scaler_Rrs=None, scalers_dict=None, use_diff=False, target_parameter='SPM')</code>","text":"<p>Load and preprocess real test data using robust scaling methods.</p> <p>This function loads test data from Excel files, matches wavelength bands to the nearest available bands, and applies the same preprocessing transformations as used during training.</p> <p>Parameters:</p> Name Type Description Default <code>excel_path</code> <code>str</code> <p>Path to Excel file containing test data.</p> required <code>selected_bands</code> <code>list</code> <p>List of target wavelength bands.</p> required <code>max_allowed_diff</code> <code>float</code> <p>Maximum allowed wavelength difference for band matching. Defaults to 1.0.</p> <code>1.0</code> <code>scaler_Rrs</code> <p>Pre-fitted Rrs scaler from training data.</p> <code>None</code> <code>scalers_dict</code> <code>dict</code> <p>Dictionary of pre-fitted scalers from training.</p> <code>None</code> <code>use_diff</code> <code>bool</code> <p>Whether to apply first difference. Defaults to False.</p> <code>False</code> <code>target_parameter</code> <code>str</code> <p>Name of target parameter. Defaults to \"SPM\".</p> <code>'SPM'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - test_dl (DataLoader): Test data loader     - input_dim (int): Input feature dimension     - output_dim (int): Output dimension     - sample_ids (list): Sample identifiers     - sample_dates (list): Sample dates</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If number of rows in Rrs and parameter tables don't match, or if target wavelengths cannot be matched within tolerance.</p> Source code in <code>hypercoast/moe_vae/data_loading.py</code> <pre><code>def load_real_test_Robust(\n    excel_path,\n    selected_bands,\n    max_allowed_diff=1.0,\n    scaler_Rrs=None,\n    scalers_dict=None,\n    use_diff=False,\n    target_parameter=\"SPM\",\n):\n    \"\"\"Load and preprocess real test data using robust scaling methods.\n\n    This function loads test data from Excel files, matches wavelength bands\n    to the nearest available bands, and applies the same preprocessing\n    transformations as used during training.\n\n    Args:\n        excel_path (str): Path to Excel file containing test data.\n        selected_bands (list): List of target wavelength bands.\n        max_allowed_diff (float, optional): Maximum allowed wavelength difference\n            for band matching. Defaults to 1.0.\n        scaler_Rrs: Pre-fitted Rrs scaler from training data.\n        scalers_dict (dict): Dictionary of pre-fitted scalers from training.\n        use_diff (bool, optional): Whether to apply first difference.\n            Defaults to False.\n        target_parameter (str, optional): Name of target parameter.\n            Defaults to \"SPM\".\n\n    Returns:\n        tuple: A tuple containing:\n            - test_dl (DataLoader): Test data loader\n            - input_dim (int): Input feature dimension\n            - output_dim (int): Output dimension\n            - sample_ids (list): Sample identifiers\n            - sample_dates (list): Sample dates\n\n    Raises:\n        ValueError: If number of rows in Rrs and parameter tables don't match,\n            or if target wavelengths cannot be matched within tolerance.\n    \"\"\"\n\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n\n    if df_rrs.shape[0] != df_param.shape[0]:\n        raise ValueError(\n            f\"\u274c The number of rows in the Rrs table and parameter table do not match. Rrs: {df_rrs.shape[0]}, parameter: {df_param.shape[0]}\"\n        )\n\n    sample_ids = df_rrs[\"Site Label\"].astype(str).tolist()\n    sample_dates = df_rrs[\"Date\"].astype(str).tolist()\n\n    # Match target bands\n    rrs_wavelengths = []\n    rrs_cols = []\n    for col in df_rrs.columns:\n        try:\n            wl = float(col)\n            rrs_wavelengths.append(wl)\n            rrs_cols.append(col)\n        except:\n            continue\n\n    band_cols = []\n    for target_band in selected_bands:\n        diffs = [abs(wl - target_band) for wl in rrs_wavelengths]\n        min_diff = min(diffs)\n        if min_diff &gt; max_allowed_diff:\n            raise ValueError(\n                f\"Target wavelength {target_band} nm cannot be matched, error {min_diff:.2f} nm exceeds the allowed range\"\n            )\n        best_idx = diffs.index(min_diff)\n        band_cols.append(rrs_cols[best_idx])\n\n    print(f\"\\n\u2705 Band matching successful, {len(selected_bands)} target bands in total\")\n    print(f\"Final number of valid test samples: {df_rrs.shape[0]}\\n\")\n\n    Rrs_array = df_rrs[band_cols].values\n    param_array = df_param[[target_parameter]].values.flatten()\n    # === Key: Remove rows with NaN/Inf before differencing ===\n    mask_inputs_ok = np.all(np.isfinite(Rrs_array), axis=1)\n    mask_target_ok = np.isfinite(param_array)\n    mask_ok = mask_inputs_ok &amp; mask_target_ok\n    if not np.any(mask_ok):\n        raise ValueError(\"\u274c Valid samples = 0 (NaN/Inf found in input or target).\")\n    dropped = int(len(mask_ok) - mask_ok.sum())\n    if dropped &gt; 0:\n        print(\n            f\"\u26a0\ufe0f Dropped {dropped} invalid samples (containing NaN/Inf) before differencing\"\n        )\n\n    Rrs_array = Rrs_array[mask_ok]\n    param_array = param_array[mask_ok]\n    sample_ids = [sid for sid, keep in zip(sample_ids, mask_ok) if keep]\n    sample_dates = [d for d, keep in zip(sample_dates, mask_ok) if keep]\n\n    if use_diff:\n        Rrs_array = np.diff(Rrs_array, axis=1)\n\n    Rrs_tensor = torch.tensor(Rrs_array, dtype=torch.float32)\n    Rrs_normalized = scaler_Rrs.transform(Rrs_tensor).numpy()\n\n    log_scaler = scalers_dict[\"log\"]\n    robust_scaler = scalers_dict[\"robust\"]\n    param_log = log_scaler.transform(\n        torch.tensor(param_array.reshape(-1, 1), dtype=torch.float32)\n    )\n    param_transformed = robust_scaler.transform(param_log).numpy()\n\n    dataset = TensorDataset(\n        torch.tensor(Rrs_normalized, dtype=torch.float32),\n        torch.tensor(param_transformed.reshape(-1, 1), dtype=torch.float32),\n    )\n    test_dl = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = 1\n\n    return test_dl, input_dim, output_dim, sample_ids, sample_dates\n</code></pre>"},{"location":"moe_vae/model/","title":"model module","text":"<p>Variational Autoencoder and Mixture of Experts models.</p> <p>This module implements VAE and MoE-VAE architectures for remote sensing data analysis, including sparse gating mechanisms and training utilities.</p>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE","title":"<code> MoE_VAE            (LightningModule)         </code>","text":"<p>Call a Sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>class MoE_VAE(LightningModule):\n    \"\"\"Call a Sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts.\n    Args:\n    input_dim: integer - size of the input\n    output_dim: integer - size of the input\n    num_experts: an integer - number of experts\n    hidden_dims: an integer - hidden_dims size of the experts\n    noisy_gating: a boolean\n    k: an integer - how many experts to use for each batch element\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        latent_dim,\n        encoder_hidden_dims,\n        decoder_hidden_dims,\n        num_experts,\n        k=4,\n        activation=\"leakyrelu\",\n        noisy_gating=True,\n        use_norm=False,\n        use_dropout=False,\n        use_softplus_output=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the MoE-VAE model.\n\n        Args:\n            input_dim (int): Dimension of input data.\n            output_dim (int): Dimension of output/reconstructed data.\n            latent_dim (int): Dimension of latent space.\n            encoder_hidden_dims (list): List of hidden layer dimensions for encoder.\n            decoder_hidden_dims (list): List of hidden layer dimensions for decoder.\n            num_experts (int): Number of experts.\n            k (int, optional): Number of experts to use for each batch element.\n            activation (str, optional): Activation function name.\n            noisy_gating (bool, optional): Whether to use noisy gating.\n            use_norm (str or bool, optional): Normalization type. Can be 'batch',\n                'layer', or False. Defaults to False.\n            use_dropout (bool, optional): Whether to use dropout. Defaults to False.\n            use_softplus_output (bool, optional): Whether to apply softplus to output.\n                Defaults to False.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super(MoE_VAE, self).__init__()\n        self.noisy_gating = noisy_gating\n        self.num_experts = num_experts\n        self.output_dim = output_dim\n        self.input_dim = input_dim\n        self.latent_dim = latent_dim\n        self.encoder_hidden_dims = encoder_hidden_dims\n        self.decoder_hidden_dims = decoder_hidden_dims\n        self.num_experts = num_experts\n        self.k = k\n        self.activation = activation\n        self.use_norm = use_norm\n        self.use_dropout = use_dropout\n        self.use_softplus_output = use_softplus_output\n\n        # instantiate experts\n        self.experts = nn.ModuleList(\n            [\n                VAE(\n                    self.input_dim,\n                    self.output_dim,\n                    self.latent_dim,\n                    self.encoder_hidden_dims,\n                    self.decoder_hidden_dims,\n                    self.activation,\n                    use_norm=self.use_norm,\n                    use_dropout=self.use_dropout,\n                    use_softplus_output=self.use_softplus_output,\n                )\n                for i in range(self.num_experts)\n            ]\n        )\n\n        self.w_gate = nn.Parameter(\n            torch.zeros(input_dim, num_experts, dtype=self.dtype), requires_grad=True\n        )\n        self.w_noise = nn.Parameter(\n            torch.zeros(input_dim, num_experts, dtype=self.dtype), requires_grad=True\n        )\n\n        self.softplus = nn.Softplus()\n        self.softmax = nn.Softmax(1)\n        self.register_buffer(\"mean\", torch.tensor([0.0]))\n        self.register_buffer(\"std\", torch.tensor([1.0]))\n        self.batch_gates = None\n\n        assert self.k &lt;= self.num_experts\n\n    def forward(self, x, moe_weight=1e-2):\n        \"\"\"\n        Forward pass of the MoE-VAE model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape [batch_size, input_dim].\n            moe_weight (float, optional): Multiplier for load-balancing loss.\n                Defaults to 1e-2.\n\n        Returns:\n            dict: Dictionary containing:\n                - 'pred_y': Predicted output tensor of shape [batch_size, output_dim].\n                - 'moe_loss': Load-balancing loss.\n        \"\"\"\n        gates, load = self.noisy_top_k_gating(x, self.training)\n        self.batch_gates = gates\n        # calculate importance loss\n        importance = gates.sum(0)\n\n        moe_loss = moe_weight * self.cv_squared(\n            importance\n        ) + moe_weight * self.cv_squared(load)\n\n        dispatcher = SparseDispatcher(self.num_experts, gates)\n        expert_inputs = dispatcher.dispatch(x)\n        gates = dispatcher.expert_to_gates()\n        expert_outputs = []\n        for i in range(self.num_experts):\n            input_i = expert_inputs[i]\n            if input_i.shape[0] &gt; 1:\n                expert_outputs.append(self.experts[i](input_i)[\"pred_y\"])\n            else:\n                expert_outputs.append(\n                    torch.zeros(\n                        (input_i.shape[0], self.output_dim), device=input_i.device\n                    )\n                )\n        pred_y = dispatcher.combine(expert_outputs)\n        return {\"pred_y\": pred_y, \"moe_loss\": moe_loss}\n\n    def loss_fn(self, output_dict) -&gt; torch.Tensor:\n        \"\"\"\n        Compute loss between model output and target.\n\n        Args:\n            output: Model output tensor of shape (batch, output_dim)\n            target: Target tensor of shape (batch, output_dim)\n\n        Returns:\n            loss: Scalar tensor representing the loss\n        \"\"\"\n        pred_y = output_dict[\"pred_y\"]\n        y = output_dict[\"y\"]\n        batch_size = y.shape[0]\n        MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n        mse_losss = F.mse_loss(pred_y, y, reduction=\"mean\")\n        moe_loss = output_dict.get(\n            \"moe_loss\", torch.tensor(0.0, device=pred_y.device, dtype=pred_y.dtype)\n        )\n        total_loss = MAE + moe_loss\n        return {\n            \"total_loss\": total_loss,\n            \"mae_loss\": MAE,\n            \"mse_loss\": mse_losss,\n            \"moe_loss\": moe_loss,\n        }\n\n    def get_batch_gates(self):\n        \"\"\"Get the gating weights from the last forward pass.\n\n        Returns:\n            torch.Tensor: Gating weights of shape [batch_size, num_experts].\n        \"\"\"\n        return self.batch_gates\n\n    def cv_squared(self, x):\n        \"\"\"Compute squared coefficient of variation for load balancing.\n\n        Calculates the squared coefficient of variation (variance/mean\u00b2) which\n        serves as a loss term to encourage uniform distribution across experts.\n\n        Args:\n            x (torch.Tensor): Input tensor (typically expert loads or importance).\n\n        Returns:\n            torch.Tensor: Scalar tensor representing squared coefficient of variation.\n                Returns 0 for single-element tensors.\n        \"\"\"\n        eps = 1e-10\n        # if only num_experts = 1\n\n        if x.shape[0] == 1:\n            return torch.tensor([0], device=x.device, dtype=x.dtype)\n        return x.float().var() / (x.float().mean() ** 2 + eps)\n\n    def _gates_to_load(self, gates):\n        \"\"\"Convert gate weights to expert load counts.\n\n        Computes the number of examples assigned to each expert (with gate &gt; 0).\n\n        Args:\n            gates (torch.Tensor): Gate weights of shape [batch_size, num_experts].\n\n        Returns:\n            torch.Tensor: Load count per expert of shape [num_experts].\n        \"\"\"\n        return (gates &gt; 0).sum(0)\n\n    def _prob_in_top_k(\n        self, clean_values, noisy_values, noise_stddev, noisy_top_values\n    ):\n        \"\"\"Compute probability of expert being in top-k selection.\n\n        Helper function for noisy top-k gating that computes the probability\n        each expert would be selected given different noise realizations.\n        This enables differentiable load balancing.\n\n        Args:\n            clean_values (torch.Tensor): Clean logits of shape [batch, num_experts].\n            noisy_values (torch.Tensor): Noisy logits of shape [batch, num_experts].\n            noise_stddev (torch.Tensor): Noise standard deviation of same shape.\n            noisy_top_values (torch.Tensor): Top-k+1 noisy values for thresholding.\n\n        Returns:\n            torch.Tensor: Probability of each expert being in top-k,\n                shape [batch, num_experts].\n        \"\"\"\n        batch = clean_values.size(0)\n        m = noisy_top_values.size(1)\n        top_values_flat = noisy_top_values.flatten()\n\n        threshold_positions_if_in = (\n            torch.arange(batch, device=clean_values.device) * m + self.k\n        )\n        threshold_if_in = torch.unsqueeze(\n            torch.gather(top_values_flat, 0, threshold_positions_if_in), 1\n        )\n        is_in = torch.gt(noisy_values, threshold_if_in)\n        threshold_positions_if_out = threshold_positions_if_in - 1\n        threshold_if_out = torch.unsqueeze(\n            torch.gather(top_values_flat, 0, threshold_positions_if_out), 1\n        )\n        # is each value currently in the top k.\n        normal = Normal(self.mean, self.std)\n        prob_if_in = normal.cdf((clean_values - threshold_if_in) / noise_stddev)\n        prob_if_out = normal.cdf((clean_values - threshold_if_out) / noise_stddev)\n        prob = torch.where(is_in, prob_if_in, prob_if_out)\n        return prob\n\n    def noisy_top_k_gating(self, x, train, noise_epsilon=1e-2):\n        \"\"\"Noisy top-k gating mechanism for expert selection.\n\n        Implements the noisy top-k gating from \"Outrageously Large Neural Networks\"\n        (https://arxiv.org/abs/1701.06538). Adds controlled noise during training\n        to improve load balancing across experts.\n\n        Args:\n            x (torch.Tensor): Input features of shape [batch_size, input_dim].\n            train (bool): Whether model is in training mode (adds noise if True).\n            noise_epsilon (float, optional): Minimum noise standard deviation.\n                Defaults to 1e-2.\n\n        Returns:\n            tuple: A tuple containing:\n                - gates (torch.Tensor): Sparse gate weights [batch_size, num_experts]\n                - load (torch.Tensor): Expert load for balancing [num_experts]\n        \"\"\"\n        clean_logits = x @ self.w_gate\n        if self.noisy_gating and train:\n            raw_noise_stddev = x @ self.w_noise\n            noise_stddev = self.softplus(raw_noise_stddev) + noise_epsilon\n            noisy_logits = clean_logits + (\n                torch.randn_like(clean_logits) * noise_stddev\n            )\n            logits = noisy_logits\n        else:\n            logits = clean_logits\n            # Add this safety check to ensure we always have at least one expert selected\n        if (logits.sum(dim=1) == 0).any():\n            # Add a small positive value to ensure we have non-zero logits\n            logits = logits + 1e-5\n\n        # calculate topk + 1 that will be needed for the noisy gates\n        top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n        top_k_logits = top_logits[:, : self.k]\n        top_k_indices = top_indices[:, : self.k]\n        top_k_gates = self.softmax(top_k_logits)\n\n        zeros = torch.zeros_like(logits, requires_grad=True, dtype=self.dtype)\n        gates = zeros.scatter(1, top_k_indices, top_k_gates)\n\n        # Safety check - ensure at least one expert is selected per sample\n        if (gates.sum(dim=1) &lt; 1e-6).any():\n            # Force selection of the top expert for samples with no experts\n            problematic_samples = (gates.sum(dim=1) &lt; 1e-6).nonzero().squeeze(1)\n            if problematic_samples.numel() &gt; 0:  # If there are problematic samples\n                # Select the top expert for these samples\n                top_expert = top_indices[problematic_samples, 0]\n                # Set a minimum value for the gate\n                gates[problematic_samples, top_expert] = 0.1\n\n        if self.noisy_gating and self.k &lt; self.num_experts and train:\n            load = (\n                self._prob_in_top_k(\n                    clean_logits, noisy_logits, noise_stddev, top_logits\n                )\n            ).sum(0)\n        else:\n            load = self._gates_to_load(gates)\n        return gates, load\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE.__init__","title":"<code>__init__(self, input_dim, output_dim, latent_dim, encoder_hidden_dims, decoder_hidden_dims, num_experts, k=4, activation='leakyrelu', noisy_gating=True, use_norm=False, use_dropout=False, use_softplus_output=False, **kwargs)</code>  <code>special</code>","text":"<p>Initialize the MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of input data.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of output/reconstructed data.</p> required <code>latent_dim</code> <code>int</code> <p>Dimension of latent space.</p> required <code>encoder_hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions for encoder.</p> required <code>decoder_hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions for decoder.</p> required <code>num_experts</code> <code>int</code> <p>Number of experts.</p> required <code>k</code> <code>int</code> <p>Number of experts to use for each batch element.</p> <code>4</code> <code>activation</code> <code>str</code> <p>Activation function name.</p> <code>'leakyrelu'</code> <code>noisy_gating</code> <code>bool</code> <p>Whether to use noisy gating.</p> <code>True</code> <code>use_norm</code> <code>str or bool</code> <p>Normalization type. Can be 'batch', 'layer', or False. Defaults to False.</p> <code>False</code> <code>use_dropout</code> <code>bool</code> <p>Whether to use dropout. Defaults to False.</p> <code>False</code> <code>use_softplus_output</code> <code>bool</code> <p>Whether to apply softplus to output. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def __init__(\n    self,\n    input_dim,\n    output_dim,\n    latent_dim,\n    encoder_hidden_dims,\n    decoder_hidden_dims,\n    num_experts,\n    k=4,\n    activation=\"leakyrelu\",\n    noisy_gating=True,\n    use_norm=False,\n    use_dropout=False,\n    use_softplus_output=False,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the MoE-VAE model.\n\n    Args:\n        input_dim (int): Dimension of input data.\n        output_dim (int): Dimension of output/reconstructed data.\n        latent_dim (int): Dimension of latent space.\n        encoder_hidden_dims (list): List of hidden layer dimensions for encoder.\n        decoder_hidden_dims (list): List of hidden layer dimensions for decoder.\n        num_experts (int): Number of experts.\n        k (int, optional): Number of experts to use for each batch element.\n        activation (str, optional): Activation function name.\n        noisy_gating (bool, optional): Whether to use noisy gating.\n        use_norm (str or bool, optional): Normalization type. Can be 'batch',\n            'layer', or False. Defaults to False.\n        use_dropout (bool, optional): Whether to use dropout. Defaults to False.\n        use_softplus_output (bool, optional): Whether to apply softplus to output.\n            Defaults to False.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super(MoE_VAE, self).__init__()\n    self.noisy_gating = noisy_gating\n    self.num_experts = num_experts\n    self.output_dim = output_dim\n    self.input_dim = input_dim\n    self.latent_dim = latent_dim\n    self.encoder_hidden_dims = encoder_hidden_dims\n    self.decoder_hidden_dims = decoder_hidden_dims\n    self.num_experts = num_experts\n    self.k = k\n    self.activation = activation\n    self.use_norm = use_norm\n    self.use_dropout = use_dropout\n    self.use_softplus_output = use_softplus_output\n\n    # instantiate experts\n    self.experts = nn.ModuleList(\n        [\n            VAE(\n                self.input_dim,\n                self.output_dim,\n                self.latent_dim,\n                self.encoder_hidden_dims,\n                self.decoder_hidden_dims,\n                self.activation,\n                use_norm=self.use_norm,\n                use_dropout=self.use_dropout,\n                use_softplus_output=self.use_softplus_output,\n            )\n            for i in range(self.num_experts)\n        ]\n    )\n\n    self.w_gate = nn.Parameter(\n        torch.zeros(input_dim, num_experts, dtype=self.dtype), requires_grad=True\n    )\n    self.w_noise = nn.Parameter(\n        torch.zeros(input_dim, num_experts, dtype=self.dtype), requires_grad=True\n    )\n\n    self.softplus = nn.Softplus()\n    self.softmax = nn.Softmax(1)\n    self.register_buffer(\"mean\", torch.tensor([0.0]))\n    self.register_buffer(\"std\", torch.tensor([1.0]))\n    self.batch_gates = None\n\n    assert self.k &lt;= self.num_experts\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE.cv_squared","title":"<code>cv_squared(self, x)</code>","text":"<p>Compute squared coefficient of variation for load balancing.</p> <p>Calculates the squared coefficient of variation (variance/mean\u00b2) which serves as a loss term to encourage uniform distribution across experts.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor (typically expert loads or importance).</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Scalar tensor representing squared coefficient of variation.     Returns 0 for single-element tensors.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def cv_squared(self, x):\n    \"\"\"Compute squared coefficient of variation for load balancing.\n\n    Calculates the squared coefficient of variation (variance/mean\u00b2) which\n    serves as a loss term to encourage uniform distribution across experts.\n\n    Args:\n        x (torch.Tensor): Input tensor (typically expert loads or importance).\n\n    Returns:\n        torch.Tensor: Scalar tensor representing squared coefficient of variation.\n            Returns 0 for single-element tensors.\n    \"\"\"\n    eps = 1e-10\n    # if only num_experts = 1\n\n    if x.shape[0] == 1:\n        return torch.tensor([0], device=x.device, dtype=x.dtype)\n    return x.float().var() / (x.float().mean() ** 2 + eps)\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE.forward","title":"<code>forward(self, x, moe_weight=0.01)</code>","text":"<p>Forward pass of the MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor of shape [batch_size, input_dim].</p> required <code>moe_weight</code> <code>float</code> <p>Multiplier for load-balancing loss. Defaults to 1e-2.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing:     - 'pred_y': Predicted output tensor of shape [batch_size, output_dim].     - 'moe_loss': Load-balancing loss.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def forward(self, x, moe_weight=1e-2):\n    \"\"\"\n    Forward pass of the MoE-VAE model.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape [batch_size, input_dim].\n        moe_weight (float, optional): Multiplier for load-balancing loss.\n            Defaults to 1e-2.\n\n    Returns:\n        dict: Dictionary containing:\n            - 'pred_y': Predicted output tensor of shape [batch_size, output_dim].\n            - 'moe_loss': Load-balancing loss.\n    \"\"\"\n    gates, load = self.noisy_top_k_gating(x, self.training)\n    self.batch_gates = gates\n    # calculate importance loss\n    importance = gates.sum(0)\n\n    moe_loss = moe_weight * self.cv_squared(\n        importance\n    ) + moe_weight * self.cv_squared(load)\n\n    dispatcher = SparseDispatcher(self.num_experts, gates)\n    expert_inputs = dispatcher.dispatch(x)\n    gates = dispatcher.expert_to_gates()\n    expert_outputs = []\n    for i in range(self.num_experts):\n        input_i = expert_inputs[i]\n        if input_i.shape[0] &gt; 1:\n            expert_outputs.append(self.experts[i](input_i)[\"pred_y\"])\n        else:\n            expert_outputs.append(\n                torch.zeros(\n                    (input_i.shape[0], self.output_dim), device=input_i.device\n                )\n            )\n    pred_y = dispatcher.combine(expert_outputs)\n    return {\"pred_y\": pred_y, \"moe_loss\": moe_loss}\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE.get_batch_gates","title":"<code>get_batch_gates(self)</code>","text":"<p>Get the gating weights from the last forward pass.</p> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Gating weights of shape [batch_size, num_experts].</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def get_batch_gates(self):\n    \"\"\"Get the gating weights from the last forward pass.\n\n    Returns:\n        torch.Tensor: Gating weights of shape [batch_size, num_experts].\n    \"\"\"\n    return self.batch_gates\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE.loss_fn","title":"<code>loss_fn(self, output_dict)</code>","text":"<p>Compute loss between model output and target.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <p>Model output tensor of shape (batch, output_dim)</p> required <code>target</code> <p>Target tensor of shape (batch, output_dim)</p> required <p>Returns:</p> Type Description <code>loss</code> <p>Scalar tensor representing the loss</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def loss_fn(self, output_dict) -&gt; torch.Tensor:\n    \"\"\"\n    Compute loss between model output and target.\n\n    Args:\n        output: Model output tensor of shape (batch, output_dim)\n        target: Target tensor of shape (batch, output_dim)\n\n    Returns:\n        loss: Scalar tensor representing the loss\n    \"\"\"\n    pred_y = output_dict[\"pred_y\"]\n    y = output_dict[\"y\"]\n    batch_size = y.shape[0]\n    MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n    mse_losss = F.mse_loss(pred_y, y, reduction=\"mean\")\n    moe_loss = output_dict.get(\n        \"moe_loss\", torch.tensor(0.0, device=pred_y.device, dtype=pred_y.dtype)\n    )\n    total_loss = MAE + moe_loss\n    return {\n        \"total_loss\": total_loss,\n        \"mae_loss\": MAE,\n        \"mse_loss\": mse_losss,\n        \"moe_loss\": moe_loss,\n    }\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE.noisy_top_k_gating","title":"<code>noisy_top_k_gating(self, x, train, noise_epsilon=0.01)</code>","text":"<p>Noisy top-k gating mechanism for expert selection.</p> <p>Implements the noisy top-k gating from \"Outrageously Large Neural Networks\" (https://arxiv.org/abs/1701.06538). Adds controlled noise during training to improve load balancing across experts.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input features of shape [batch_size, input_dim].</p> required <code>train</code> <code>bool</code> <p>Whether model is in training mode (adds noise if True).</p> required <code>noise_epsilon</code> <code>float</code> <p>Minimum noise standard deviation. Defaults to 1e-2.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - gates (torch.Tensor): Sparse gate weights [batch_size, num_experts]     - load (torch.Tensor): Expert load for balancing [num_experts]</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def noisy_top_k_gating(self, x, train, noise_epsilon=1e-2):\n    \"\"\"Noisy top-k gating mechanism for expert selection.\n\n    Implements the noisy top-k gating from \"Outrageously Large Neural Networks\"\n    (https://arxiv.org/abs/1701.06538). Adds controlled noise during training\n    to improve load balancing across experts.\n\n    Args:\n        x (torch.Tensor): Input features of shape [batch_size, input_dim].\n        train (bool): Whether model is in training mode (adds noise if True).\n        noise_epsilon (float, optional): Minimum noise standard deviation.\n            Defaults to 1e-2.\n\n    Returns:\n        tuple: A tuple containing:\n            - gates (torch.Tensor): Sparse gate weights [batch_size, num_experts]\n            - load (torch.Tensor): Expert load for balancing [num_experts]\n    \"\"\"\n    clean_logits = x @ self.w_gate\n    if self.noisy_gating and train:\n        raw_noise_stddev = x @ self.w_noise\n        noise_stddev = self.softplus(raw_noise_stddev) + noise_epsilon\n        noisy_logits = clean_logits + (\n            torch.randn_like(clean_logits) * noise_stddev\n        )\n        logits = noisy_logits\n    else:\n        logits = clean_logits\n        # Add this safety check to ensure we always have at least one expert selected\n    if (logits.sum(dim=1) == 0).any():\n        # Add a small positive value to ensure we have non-zero logits\n        logits = logits + 1e-5\n\n    # calculate topk + 1 that will be needed for the noisy gates\n    top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n    top_k_logits = top_logits[:, : self.k]\n    top_k_indices = top_indices[:, : self.k]\n    top_k_gates = self.softmax(top_k_logits)\n\n    zeros = torch.zeros_like(logits, requires_grad=True, dtype=self.dtype)\n    gates = zeros.scatter(1, top_k_indices, top_k_gates)\n\n    # Safety check - ensure at least one expert is selected per sample\n    if (gates.sum(dim=1) &lt; 1e-6).any():\n        # Force selection of the top expert for samples with no experts\n        problematic_samples = (gates.sum(dim=1) &lt; 1e-6).nonzero().squeeze(1)\n        if problematic_samples.numel() &gt; 0:  # If there are problematic samples\n            # Select the top expert for these samples\n            top_expert = top_indices[problematic_samples, 0]\n            # Set a minimum value for the gate\n            gates[problematic_samples, top_expert] = 0.1\n\n    if self.noisy_gating and self.k &lt; self.num_experts and train:\n        load = (\n            self._prob_in_top_k(\n                clean_logits, noisy_logits, noise_stddev, top_logits\n            )\n        ).sum(0)\n    else:\n        load = self._gates_to_load(gates)\n    return gates, load\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE_Token","title":"<code> MoE_VAE_Token            (LightningModule)         </code>","text":"<p>Token-wise Mixture of Experts VAE for spectral data analysis.</p> <p>This variant of MoE-VAE divides the input spectral bands among different experts, with each expert processing a specific spectral segment. This is particularly useful for hyperspectral data where different spectral regions may have distinct characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Total dimension of input spectral data.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of output data.</p> required <code>latent_dim</code> <code>int</code> <p>Dimension of latent space for each VAE expert.</p> required <code>encoder_hidden_dims</code> <code>list</code> <p>Hidden layer dimensions for encoder networks.</p> required <code>decoder_hidden_dims</code> <code>list</code> <p>Hidden layer dimensions for decoder networks.</p> required <code>num_experts</code> <code>int</code> <p>Number of expert VAE models (spectral segments).</p> required <code>k</code> <code>int</code> <p>Kept for compatibility, unused in token-wise mode. Defaults to 4.</p> <code>4</code> <code>activation</code> <code>str</code> <p>Activation function name. Defaults to 'leakyrelu'.</p> <code>'leakyrelu'</code> <code>noisy_gating</code> <code>bool</code> <p>Kept for compatibility, unused in token-wise mode. Defaults to True.</p> <code>True</code> <code>use_norm</code> <code>str or bool</code> <p>Normalization type. Defaults to False.</p> <code>False</code> <code>use_dropout</code> <code>bool</code> <p>Whether to use dropout. Defaults to False.</p> <code>False</code> <code>use_softplus_output</code> <code>bool</code> <p>Whether to apply softplus to output. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>class MoE_VAE_Token(LightningModule):\n    \"\"\"Token-wise Mixture of Experts VAE for spectral data analysis.\n\n    This variant of MoE-VAE divides the input spectral bands among different\n    experts, with each expert processing a specific spectral segment. This is\n    particularly useful for hyperspectral data where different spectral regions\n    may have distinct characteristics.\n\n    Args:\n        input_dim (int): Total dimension of input spectral data.\n        output_dim (int): Dimension of output data.\n        latent_dim (int): Dimension of latent space for each VAE expert.\n        encoder_hidden_dims (list): Hidden layer dimensions for encoder networks.\n        decoder_hidden_dims (list): Hidden layer dimensions for decoder networks.\n        num_experts (int): Number of expert VAE models (spectral segments).\n        k (int, optional): Kept for compatibility, unused in token-wise mode.\n            Defaults to 4.\n        activation (str, optional): Activation function name. Defaults to 'leakyrelu'.\n        noisy_gating (bool, optional): Kept for compatibility, unused in token-wise mode.\n            Defaults to True.\n        use_norm (str or bool, optional): Normalization type. Defaults to False.\n        use_dropout (bool, optional): Whether to use dropout. Defaults to False.\n        use_softplus_output (bool, optional): Whether to apply softplus to output.\n            Defaults to False.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        latent_dim,\n        encoder_hidden_dims,\n        decoder_hidden_dims,\n        num_experts,\n        k=4,\n        activation=\"leakyrelu\",\n        noisy_gating=True,\n        use_norm=False,\n        use_dropout=False,\n        use_softplus_output=False,\n        **kwargs,\n    ):\n        super(MoE_VAE_Token, self).__init__()\n        self.num_experts = num_experts\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.latent_dim = latent_dim\n        self.encoder_hidden_dims = encoder_hidden_dims\n        self.decoder_hidden_dims = decoder_hidden_dims\n        self.activation = activation\n        self.use_norm = use_norm\n        self.use_dropout = use_dropout\n        self.use_softplus_output = use_softplus_output\n\n        # instantiate experts\n        self.sub_input_dims = [input_dim // num_experts] * (num_experts - 1)\n        self.sub_input_dims.append(input_dim - sum(self.sub_input_dims))\n\n        self.experts = nn.ModuleList(\n            [\n                VAE(\n                    sub_dim,\n                    sub_dim,\n                    self.latent_dim,\n                    self.encoder_hidden_dims,\n                    self.decoder_hidden_dims,\n                    self.activation,\n                    use_norm=self.use_norm,\n                    use_dropout=self.use_dropout,\n                    use_softplus_output=self.use_softplus_output,\n                )\n                for sub_dim in self.sub_input_dims\n            ]\n        )\n\n        self.w_gate = nn.Parameter(\n            torch.zeros(input_dim, num_experts, dtype=self.dtype), requires_grad=True\n        )\n        self.w_noise = nn.Parameter(\n            torch.zeros(input_dim, num_experts, dtype=self.dtype), requires_grad=True\n        )\n\n        self.softplus = nn.Softplus()\n        self.softmax = nn.Softmax(1)\n        self.register_buffer(\"mean\", torch.tensor([0.0]))\n        self.register_buffer(\"std\", torch.tensor([1.0]))\n        self.batch_gates = None\n        self.k = k\n        assert self.k &lt;= self.num_experts\n\n    def forward(self, x, moe_weight=0.0):\n        \"\"\"\n        Token-wise MoE forward pass:\n        Each expert processes a different spectral segment of the input.\n\n        Args:\n            x: Tensor of shape [batch_size, input_dim]\n            moe_weight: kept for compatibility, but unused in token-wise mode.\n\n        Returns:\n            A dict with:\n                'pred_y': reconstructed tensor of shape [batch_size, input_dim]\n                'moe_loss': dummy 0.0 (no gating loss in token-wise)\n        \"\"\"\n        # Split the input into band segments for each expert\n        x_chunks = torch.split(x, self.sub_input_dims, dim=1)\n\n        expert_outputs = []\n        for i in range(self.num_experts):\n            out_i = self.experts[i](x_chunks[i])[\"pred_y\"]\n            expert_outputs.append(out_i)\n\n        pred_y = torch.cat(expert_outputs, dim=1)\n        return {\n            \"pred_y\": pred_y,\n            \"moe_loss\": torch.tensor(0.0, device=x.device, dtype=x.dtype),\n        }\n\n    def loss_fn(self, output_dict):\n        \"\"\"Compute loss for token-wise MoE-VAE model.\n\n        Computes reconstruction loss without MoE-specific penalties since\n        no gating mechanism is used in the token-wise approach.\n\n        Args:\n            output_dict (dict): Dictionary containing model outputs and targets:\n                - 'pred_y': Model predictions\n                - 'y': Target values\n                - 'moe_loss': Always zero for token-wise model\n\n        Returns:\n            dict: Dictionary containing loss components:\n                - 'total_loss': MAE loss (no MoE penalty)\n                - 'mae_loss': Mean Absolute Error\n                - 'mse_loss': Mean Squared Error\n                - 'moe_loss': Zero tensor\n        \"\"\"\n        pred_y = output_dict[\"pred_y\"]\n        y = output_dict[\"y\"]\n        batch_size = y.shape[0]\n        MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n        mse_losss = F.mse_loss(pred_y, y, reduction=\"mean\")\n        moe_loss = output_dict.get(\n            \"moe_loss\", torch.tensor(0.0, device=pred_y.device, dtype=pred_y.dtype)\n        )\n        total_loss = MAE + moe_loss\n        return {\n            \"total_loss\": total_loss,\n            \"mae_loss\": MAE,\n            \"mse_loss\": mse_losss,\n            \"moe_loss\": moe_loss,\n        }\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE_Token.forward","title":"<code>forward(self, x, moe_weight=0.0)</code>","text":"<p>Token-wise MoE forward pass: Each expert processes a different spectral segment of the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Tensor of shape [batch_size, input_dim]</p> required <code>moe_weight</code> <p>kept for compatibility, but unused in token-wise mode.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>A dict with</code> <p>'pred_y': reconstructed tensor of shape [batch_size, input_dim]     'moe_loss': dummy 0.0 (no gating loss in token-wise)</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def forward(self, x, moe_weight=0.0):\n    \"\"\"\n    Token-wise MoE forward pass:\n    Each expert processes a different spectral segment of the input.\n\n    Args:\n        x: Tensor of shape [batch_size, input_dim]\n        moe_weight: kept for compatibility, but unused in token-wise mode.\n\n    Returns:\n        A dict with:\n            'pred_y': reconstructed tensor of shape [batch_size, input_dim]\n            'moe_loss': dummy 0.0 (no gating loss in token-wise)\n    \"\"\"\n    # Split the input into band segments for each expert\n    x_chunks = torch.split(x, self.sub_input_dims, dim=1)\n\n    expert_outputs = []\n    for i in range(self.num_experts):\n        out_i = self.experts[i](x_chunks[i])[\"pred_y\"]\n        expert_outputs.append(out_i)\n\n    pred_y = torch.cat(expert_outputs, dim=1)\n    return {\n        \"pred_y\": pred_y,\n        \"moe_loss\": torch.tensor(0.0, device=x.device, dtype=x.dtype),\n    }\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.MoE_VAE_Token.loss_fn","title":"<code>loss_fn(self, output_dict)</code>","text":"<p>Compute loss for token-wise MoE-VAE model.</p> <p>Computes reconstruction loss without MoE-specific penalties since no gating mechanism is used in the token-wise approach.</p> <p>Parameters:</p> Name Type Description Default <code>output_dict</code> <code>dict</code> <p>Dictionary containing model outputs and targets: - 'pred_y': Model predictions - 'y': Target values - 'moe_loss': Always zero for token-wise model</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing loss components:     - 'total_loss': MAE loss (no MoE penalty)     - 'mae_loss': Mean Absolute Error     - 'mse_loss': Mean Squared Error     - 'moe_loss': Zero tensor</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def loss_fn(self, output_dict):\n    \"\"\"Compute loss for token-wise MoE-VAE model.\n\n    Computes reconstruction loss without MoE-specific penalties since\n    no gating mechanism is used in the token-wise approach.\n\n    Args:\n        output_dict (dict): Dictionary containing model outputs and targets:\n            - 'pred_y': Model predictions\n            - 'y': Target values\n            - 'moe_loss': Always zero for token-wise model\n\n    Returns:\n        dict: Dictionary containing loss components:\n            - 'total_loss': MAE loss (no MoE penalty)\n            - 'mae_loss': Mean Absolute Error\n            - 'mse_loss': Mean Squared Error\n            - 'moe_loss': Zero tensor\n    \"\"\"\n    pred_y = output_dict[\"pred_y\"]\n    y = output_dict[\"y\"]\n    batch_size = y.shape[0]\n    MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n    mse_losss = F.mse_loss(pred_y, y, reduction=\"mean\")\n    moe_loss = output_dict.get(\n        \"moe_loss\", torch.tensor(0.0, device=pred_y.device, dtype=pred_y.dtype)\n    )\n    total_loss = MAE + moe_loss\n    return {\n        \"total_loss\": total_loss,\n        \"mae_loss\": MAE,\n        \"mse_loss\": mse_losss,\n        \"moe_loss\": moe_loss,\n    }\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.SparseDispatcher","title":"<code> SparseDispatcher        </code>","text":"<p>Helper for implementing a mixture of experts with sparse gating.</p> <p>This class handles the distribution of inputs to experts and combines their outputs based on gating weights. It optimizes computation by only processing inputs for experts with non-zero gates.</p> <p>The class provides two main functions: - dispatch: Creates input batches for each expert based on gating weights - combine: Combines expert outputs weighted by their respective gates</p> <p>Parameters:</p> Name Type Description Default <code>num_experts</code> <code>int</code> <p>Number of expert models.</p> required <code>gates</code> <code>torch.Tensor</code> <p>Gating weights of shape [batch_size, num_experts]. Element [b, e] represents the weight for sending batch element b to expert e.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; gates = torch.tensor([[0.8, 0.2, 0.0], [0.1, 0.0, 0.9]])\n&gt;&gt;&gt; dispatcher = SparseDispatcher(3, gates)\n&gt;&gt;&gt; expert_inputs = dispatcher.dispatch(inputs)\n&gt;&gt;&gt; expert_outputs = [experts[i](expert_inputs[i]) for i in range(3)]\n&gt;&gt;&gt; combined_output = dispatcher.combine(expert_outputs)\n</code></pre> <p>Note</p> <p>Input and output tensors are expected to be 2D [batch, depth]. Caller is responsible for reshaping higher-dimensional tensors before dispatch and after combine operations.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>class SparseDispatcher(object):\n    \"\"\"Helper for implementing a mixture of experts with sparse gating.\n\n    This class handles the distribution of inputs to experts and combines their\n    outputs based on gating weights. It optimizes computation by only processing\n    inputs for experts with non-zero gates.\n\n    The class provides two main functions:\n    - dispatch: Creates input batches for each expert based on gating weights\n    - combine: Combines expert outputs weighted by their respective gates\n\n    Args:\n        num_experts (int): Number of expert models.\n        gates (torch.Tensor): Gating weights of shape [batch_size, num_experts].\n            Element [b, e] represents the weight for sending batch element b\n            to expert e.\n\n    Example:\n        &gt;&gt;&gt; gates = torch.tensor([[0.8, 0.2, 0.0], [0.1, 0.0, 0.9]])\n        &gt;&gt;&gt; dispatcher = SparseDispatcher(3, gates)\n        &gt;&gt;&gt; expert_inputs = dispatcher.dispatch(inputs)\n        &gt;&gt;&gt; expert_outputs = [experts[i](expert_inputs[i]) for i in range(3)]\n        &gt;&gt;&gt; combined_output = dispatcher.combine(expert_outputs)\n\n    Note:\n        Input and output tensors are expected to be 2D [batch, depth]. Caller\n        is responsible for reshaping higher-dimensional tensors before dispatch\n        and after combine operations.\n    \"\"\"\n\n    def __init__(self, num_experts, gates):\n        \"\"\"Initialize the SparseDispatcher.\n\n        Args:\n            num_experts (int): Number of expert models.\n            gates (torch.Tensor): Gating weights of shape [batch_size, num_experts].\n        \"\"\"\n        self._gates = gates\n        self._num_experts = num_experts\n\n        # Safety check: ensure at least one example per expert\n        if (gates.sum(dim=0) == 0).any():\n            # Find experts with no assignments and create dummy assignments\n            empty_experts = (gates.sum(dim=0) == 0).nonzero().squeeze(1)\n            if empty_experts.numel() &gt; 0:\n                # Assign the first example to all empty experts with a small weight\n                for expert_idx in empty_experts:\n                    gates[0, expert_idx] = 1e-5\n\n        # Sort experts\n        sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n        # Drop indices\n        _, self._expert_index = sorted_experts.split(1, dim=1)\n        # Get according batch index for each expert\n        self._batch_index = torch.nonzero(gates)[index_sorted_experts[:, 1], 0]\n        # Calculate num samples that each expert gets\n        self._part_sizes = (gates &gt; 0).sum(0).tolist()\n\n        # Safety check: ensure no expert has 0 examples\n        for i, size in enumerate(self._part_sizes):\n            if size == 0:\n                # Add a dummy example to this expert\n                self._part_sizes[i] = 1\n                if i &gt;= len(self._expert_index):\n                    # Add a new dummy index if needed\n                    self._expert_index = torch.cat(\n                        [self._expert_index, torch.tensor([[i]], device=gates.device)]\n                    )\n                    self._batch_index = torch.cat(\n                        [self._batch_index, torch.tensor([0], device=gates.device)]\n                    )\n\n        # Expand gates to match with self._batch_index\n        gates_exp = gates[self._batch_index.flatten()]\n        self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)\n\n        # Safety check for nonzero gates\n        if (self._nonzero_gates &lt;= 0).any():\n            self._nonzero_gates = torch.clamp(self._nonzero_gates, min=1e-5)\n\n    def dispatch(self, inp):\n        \"\"\"Distribute input tensor to experts based on gating weights.\n\n        Creates separate input tensors for each expert containing only the\n        samples assigned to that expert (where gates[b, i] &gt; 0).\n\n        Args:\n            inp (torch.Tensor): Input tensor of shape [batch_size, input_dim].\n\n        Returns:\n            list: List of tensors, one for each expert. Each tensor contains\n                only the inputs assigned to that expert.\n        \"\"\"\n\n        # assigns samples to experts whose gate is nonzero\n\n        # expand according to batch index so we can just split by _part_sizes\n        inp_exp = inp[self._batch_index].squeeze(1)\n        return torch.split(inp_exp, self._part_sizes, dim=0)\n\n    def combine(self, expert_out, multiply_by_gates=True):\n        \"\"\"Combine expert outputs weighted by gating values.\n\n        Aggregates outputs from all experts for each batch element, weighted\n        by the corresponding gate values. The final output for batch element b\n        is the sum of expert outputs weighted by gates[b, i].\n\n        Args:\n            expert_out (list): List of expert output tensors, each with shape\n                [expert_batch_size_i, output_dim].\n            multiply_by_gates (bool, optional): Whether to weight outputs by\n                gate values. If False, outputs are simply summed. Defaults to True.\n\n        Returns:\n            torch.Tensor: Combined output tensor of shape [batch_size, output_dim].\n        \"\"\"\n        # apply exp to expert outputs, so we are not longer in log space\n        stitched = torch.cat(expert_out, 0)\n\n        if multiply_by_gates:\n            stitched = stitched.mul(self._nonzero_gates)\n        zeros = torch.zeros(\n            self._gates.size(0),\n            expert_out[-1].size(1),\n            requires_grad=True,\n            device=stitched.device,\n        )\n        # combine samples that have been processed by the same k experts\n        combined = zeros.index_add(0, self._batch_index, stitched.float())\n        return combined\n\n    def expert_to_gates(self):\n        \"\"\"Extract gate values for each expert's assigned samples.\n\n        Returns:\n            list: List of 1D tensors, one for each expert, containing the\n                gate values for samples assigned to that expert.\n        \"\"\"\n        # split nonzero gates for each expert\n        return torch.split(self._nonzero_gates, self._part_sizes, dim=0)\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.SparseDispatcher.__init__","title":"<code>__init__(self, num_experts, gates)</code>  <code>special</code>","text":"<p>Initialize the SparseDispatcher.</p> <p>Parameters:</p> Name Type Description Default <code>num_experts</code> <code>int</code> <p>Number of expert models.</p> required <code>gates</code> <code>torch.Tensor</code> <p>Gating weights of shape [batch_size, num_experts].</p> required Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def __init__(self, num_experts, gates):\n    \"\"\"Initialize the SparseDispatcher.\n\n    Args:\n        num_experts (int): Number of expert models.\n        gates (torch.Tensor): Gating weights of shape [batch_size, num_experts].\n    \"\"\"\n    self._gates = gates\n    self._num_experts = num_experts\n\n    # Safety check: ensure at least one example per expert\n    if (gates.sum(dim=0) == 0).any():\n        # Find experts with no assignments and create dummy assignments\n        empty_experts = (gates.sum(dim=0) == 0).nonzero().squeeze(1)\n        if empty_experts.numel() &gt; 0:\n            # Assign the first example to all empty experts with a small weight\n            for expert_idx in empty_experts:\n                gates[0, expert_idx] = 1e-5\n\n    # Sort experts\n    sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n    # Drop indices\n    _, self._expert_index = sorted_experts.split(1, dim=1)\n    # Get according batch index for each expert\n    self._batch_index = torch.nonzero(gates)[index_sorted_experts[:, 1], 0]\n    # Calculate num samples that each expert gets\n    self._part_sizes = (gates &gt; 0).sum(0).tolist()\n\n    # Safety check: ensure no expert has 0 examples\n    for i, size in enumerate(self._part_sizes):\n        if size == 0:\n            # Add a dummy example to this expert\n            self._part_sizes[i] = 1\n            if i &gt;= len(self._expert_index):\n                # Add a new dummy index if needed\n                self._expert_index = torch.cat(\n                    [self._expert_index, torch.tensor([[i]], device=gates.device)]\n                )\n                self._batch_index = torch.cat(\n                    [self._batch_index, torch.tensor([0], device=gates.device)]\n                )\n\n    # Expand gates to match with self._batch_index\n    gates_exp = gates[self._batch_index.flatten()]\n    self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)\n\n    # Safety check for nonzero gates\n    if (self._nonzero_gates &lt;= 0).any():\n        self._nonzero_gates = torch.clamp(self._nonzero_gates, min=1e-5)\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.SparseDispatcher.combine","title":"<code>combine(self, expert_out, multiply_by_gates=True)</code>","text":"<p>Combine expert outputs weighted by gating values.</p> <p>Aggregates outputs from all experts for each batch element, weighted by the corresponding gate values. The final output for batch element b is the sum of expert outputs weighted by gates[b, i].</p> <p>Parameters:</p> Name Type Description Default <code>expert_out</code> <code>list</code> <p>List of expert output tensors, each with shape [expert_batch_size_i, output_dim].</p> required <code>multiply_by_gates</code> <code>bool</code> <p>Whether to weight outputs by gate values. If False, outputs are simply summed. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Combined output tensor of shape [batch_size, output_dim].</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def combine(self, expert_out, multiply_by_gates=True):\n    \"\"\"Combine expert outputs weighted by gating values.\n\n    Aggregates outputs from all experts for each batch element, weighted\n    by the corresponding gate values. The final output for batch element b\n    is the sum of expert outputs weighted by gates[b, i].\n\n    Args:\n        expert_out (list): List of expert output tensors, each with shape\n            [expert_batch_size_i, output_dim].\n        multiply_by_gates (bool, optional): Whether to weight outputs by\n            gate values. If False, outputs are simply summed. Defaults to True.\n\n    Returns:\n        torch.Tensor: Combined output tensor of shape [batch_size, output_dim].\n    \"\"\"\n    # apply exp to expert outputs, so we are not longer in log space\n    stitched = torch.cat(expert_out, 0)\n\n    if multiply_by_gates:\n        stitched = stitched.mul(self._nonzero_gates)\n    zeros = torch.zeros(\n        self._gates.size(0),\n        expert_out[-1].size(1),\n        requires_grad=True,\n        device=stitched.device,\n    )\n    # combine samples that have been processed by the same k experts\n    combined = zeros.index_add(0, self._batch_index, stitched.float())\n    return combined\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.SparseDispatcher.dispatch","title":"<code>dispatch(self, inp)</code>","text":"<p>Distribute input tensor to experts based on gating weights.</p> <p>Creates separate input tensors for each expert containing only the samples assigned to that expert (where gates[b, i] &gt; 0).</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>torch.Tensor</code> <p>Input tensor of shape [batch_size, input_dim].</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of tensors, one for each expert. Each tensor contains     only the inputs assigned to that expert.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def dispatch(self, inp):\n    \"\"\"Distribute input tensor to experts based on gating weights.\n\n    Creates separate input tensors for each expert containing only the\n    samples assigned to that expert (where gates[b, i] &gt; 0).\n\n    Args:\n        inp (torch.Tensor): Input tensor of shape [batch_size, input_dim].\n\n    Returns:\n        list: List of tensors, one for each expert. Each tensor contains\n            only the inputs assigned to that expert.\n    \"\"\"\n\n    # assigns samples to experts whose gate is nonzero\n\n    # expand according to batch index so we can just split by _part_sizes\n    inp_exp = inp[self._batch_index].squeeze(1)\n    return torch.split(inp_exp, self._part_sizes, dim=0)\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.SparseDispatcher.expert_to_gates","title":"<code>expert_to_gates(self)</code>","text":"<p>Extract gate values for each expert's assigned samples.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of 1D tensors, one for each expert, containing the     gate values for samples assigned to that expert.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def expert_to_gates(self):\n    \"\"\"Extract gate values for each expert's assigned samples.\n\n    Returns:\n        list: List of 1D tensors, one for each expert, containing the\n            gate values for samples assigned to that expert.\n    \"\"\"\n    # split nonzero gates for each expert\n    return torch.split(self._nonzero_gates, self._part_sizes, dim=0)\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.VAE","title":"<code> VAE            (LightningModule)         </code>","text":"<p>Variational Autoencoder implementation using PyTorch Lightning.</p> <p>A standard VAE architecture with configurable encoder/decoder networks, support for various activation functions, normalization layers, and dropout regularization.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of input data.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of output/reconstructed data.</p> required <code>latent_dim</code> <code>int</code> <p>Dimension of latent space.</p> required <code>encoder_hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions for encoder.</p> required <code>decoder_hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions for decoder.</p> required <code>activation</code> <code>str</code> <p>Activation function name. Supports 'relu', 'tanh', 'sigmoid', 'leakyrelu'. Defaults to 'leakyrelu'.</p> <code>'leakyrelu'</code> <code>use_norm</code> <code>str or bool</code> <p>Normalization type. Can be 'batch', 'layer', or False. Defaults to False.</p> <code>False</code> <code>use_dropout</code> <code>bool</code> <p>Whether to use dropout. Defaults to False.</p> <code>False</code> <code>use_softplus_output</code> <code>bool</code> <p>Whether to apply softplus to output. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>class VAE(LightningModule):\n    \"\"\"Variational Autoencoder implementation using PyTorch Lightning.\n\n    A standard VAE architecture with configurable encoder/decoder networks,\n    support for various activation functions, normalization layers, and\n    dropout regularization.\n\n    Args:\n        input_dim (int): Dimension of input data.\n        output_dim (int): Dimension of output/reconstructed data.\n        latent_dim (int): Dimension of latent space.\n        encoder_hidden_dims (list): List of hidden layer dimensions for encoder.\n        decoder_hidden_dims (list): List of hidden layer dimensions for decoder.\n        activation (str, optional): Activation function name. Supports 'relu',\n            'tanh', 'sigmoid', 'leakyrelu'. Defaults to 'leakyrelu'.\n        use_norm (str or bool, optional): Normalization type. Can be 'batch',\n            'layer', or False. Defaults to False.\n        use_dropout (bool, optional): Whether to use dropout. Defaults to False.\n        use_softplus_output (bool, optional): Whether to apply softplus to output.\n            Defaults to False.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        latent_dim,\n        encoder_hidden_dims,\n        decoder_hidden_dims,\n        activation=\"leakyrelu\",\n        use_norm=False,\n        use_dropout=False,\n        use_softplus_output=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the VAE model.\n\n        Args:\n            input_dim (int): Dimension of input data.\n            output_dim (int): Dimension of output/reconstructed data.\n            latent_dim (int): Dimension of latent space.\n            encoder_hidden_dims (list): List of hidden layer dimensions for encoder.\n            decoder_hidden_dims (list): List of hidden layer dimensions for decoder.\n            activation (str, optional): Activation function name. Supports 'relu',\n                'tanh', 'sigmoid', 'leakyrelu'. Defaults to 'leakyrelu'.\n            use_norm (str or bool, optional): Normalization type. Can be 'batch',\n                'layer', or False. Defaults to False.\n            use_dropout (bool, optional): Whether to use dropout. Defaults to False.\n            use_softplus_output (bool, optional): Whether to apply softplus to output.\n                Defaults to False.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__()\n        # Define the activation function\n        self.use_softplus_output = use_softplus_output\n        if activation == \"relu\":\n            self.activation = nn.ReLU()\n        elif activation == \"tanh\":\n            self.activation = nn.Tanh()\n        elif activation == \"sigmoid\":\n            self.activation = nn.Sigmoid()\n        elif activation == \"leakyrelu\":\n            self.activation = nn.LeakyReLU(0.2)\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation}\")\n\n        # Encoder layers\n        self.encoder_layers = self.build_layers(\n            input_dim, encoder_hidden_dims, use_norm, use_dropout\n        )\n        self.fc_mu = nn.Linear(encoder_hidden_dims[-1], latent_dim)\n        self.fc_log_var = nn.Linear(encoder_hidden_dims[-1], latent_dim)\n\n        # Decoder layers\n        self.decoder_layers = self.build_layers(\n            latent_dim, decoder_hidden_dims, use_norm, use_dropout\n        )\n        # self.decoder_layers.add_module('softplus', nn.Softplus())\n        self.decoder_layers.add_module(\n            \"output_layer\", nn.Linear(decoder_hidden_dims[-1], output_dim)\n        )\n        if self.use_softplus_output:\n            self.decoder_layers.add_module(\"output_activation\", nn.Softplus())\n        # self.decoder_layers.add_module('output_activation', nn.Tanh())  # Assuming output is in range [-1, 1]\n        # with the classic robust preprocessing method it is -1 to 1, but for others it may not.\n\n    def build_layers(self, input_dim, hidden_dims, use_norm, use_dropout=False):\n        \"\"\"Build sequential neural network layers.\n\n        Args:\n            input_dim (int): Input dimension for the first layer.\n            hidden_dims (list): List of hidden layer dimensions.\n            use_norm (str or bool): Normalization type ('batch', 'layer', or False).\n            use_dropout (bool, optional): Whether to include dropout layers.\n                Defaults to False.\n\n        Returns:\n            nn.Sequential: Sequential container of network layers.\n        \"\"\"\n        layers = []\n        current_size = input_dim\n        for hidden_dim in hidden_dims:\n            next_size = hidden_dim\n            layers.append(nn.Linear(current_size, next_size))\n            if use_norm == \"batch\":\n                layers.append(nn.BatchNorm1d(hidden_dim))\n            elif use_norm == \"layer\":\n                layers.append(nn.LayerNorm(hidden_dim))\n            layers.append(self.activation)\n            if use_dropout:\n                layers.append(nn.Dropout(0.1))\n            current_size = next_size\n        return nn.Sequential(*layers)\n\n    def encode(self, x):\n        \"\"\"Encode input to latent space parameters.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n\n        Returns:\n            tuple: A tuple containing:\n                - mu (torch.Tensor): Mean of latent distribution.\n                - log_var (torch.Tensor): Log variance of latent distribution.\n        \"\"\"\n        x = self.encoder_layers(x)\n        mu = self.fc_mu(x)\n        log_var = self.fc_log_var(x)\n        return mu, log_var\n\n    def reparameterize(self, mu, log_var):\n        \"\"\"Reparameterization trick for sampling from latent distribution.\n\n        Args:\n            mu (torch.Tensor): Mean of latent distribution.\n            log_var (torch.Tensor): Log variance of latent distribution.\n\n        Returns:\n            torch.Tensor: Sampled latent vector.\n        \"\"\"\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        z = mu + eps * std\n        return z\n\n    def decode(self, z):\n        \"\"\"Decode latent representation to output space.\n\n        Args:\n            z (torch.Tensor): Latent representation.\n\n        Returns:\n            torch.Tensor: Reconstructed output.\n        \"\"\"\n        return self.decoder_layers(z)\n\n    def forward(self, x):\n        \"\"\"Forward pass through the VAE.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            dict: Dictionary containing:\n                - 'pred_y': Reconstructed output\n                - 'mu': Mean of latent distribution\n                - 'log_var': Log variance of latent distribution\n        \"\"\"\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        pred_y = self.decode(z)\n        return {\"pred_y\": pred_y, \"mu\": mu, \"log_var\": log_var}\n\n    def loss_fn(self, output_dict, kld_weight=0.0):\n        \"\"\"Compute VAE loss (reconstruction + KL divergence).\n\n        Args:\n            output_dict (dict): Dictionary containing model outputs and targets.\n            kld_weight (float, optional): Weight for KL divergence term.\n                Defaults to 0.0.\n\n        Returns:\n            dict: Dictionary containing different loss components:\n                - 'total_loss': Combined loss (MAE + weighted KLD)\n                - 'mae_loss': Mean Absolute Error\n                - 'mse_loss': Mean Squared Error\n                - 'kld_loss': KL Divergence loss\n        \"\"\"\n        pred_y, y, mu, log_var = (\n            output_dict[\"pred_y\"],\n            output_dict[\"y\"],\n            output_dict[\"mu\"],\n            output_dict[\"log_var\"],\n        )\n        batch_size = y.shape[0]\n        MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n        # Reconstruction loss (MSE)\n        MSE = F.mse_loss(pred_y, y, reduction=\"mean\")\n        # KL divergence\n        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch_size\n        # Return combined loss\n        return {\n            \"total_loss\": MAE + kld_weight * KLD,\n            \"mae_loss\": MAE,\n            \"mse_loss\": MSE,\n            \"kld_loss\": KLD,\n        }\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.VAE.__init__","title":"<code>__init__(self, input_dim, output_dim, latent_dim, encoder_hidden_dims, decoder_hidden_dims, activation='leakyrelu', use_norm=False, use_dropout=False, use_softplus_output=False, **kwargs)</code>  <code>special</code>","text":"<p>Initialize the VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of input data.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of output/reconstructed data.</p> required <code>latent_dim</code> <code>int</code> <p>Dimension of latent space.</p> required <code>encoder_hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions for encoder.</p> required <code>decoder_hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions for decoder.</p> required <code>activation</code> <code>str</code> <p>Activation function name. Supports 'relu', 'tanh', 'sigmoid', 'leakyrelu'. Defaults to 'leakyrelu'.</p> <code>'leakyrelu'</code> <code>use_norm</code> <code>str or bool</code> <p>Normalization type. Can be 'batch', 'layer', or False. Defaults to False.</p> <code>False</code> <code>use_dropout</code> <code>bool</code> <p>Whether to use dropout. Defaults to False.</p> <code>False</code> <code>use_softplus_output</code> <code>bool</code> <p>Whether to apply softplus to output. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def __init__(\n    self,\n    input_dim,\n    output_dim,\n    latent_dim,\n    encoder_hidden_dims,\n    decoder_hidden_dims,\n    activation=\"leakyrelu\",\n    use_norm=False,\n    use_dropout=False,\n    use_softplus_output=False,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the VAE model.\n\n    Args:\n        input_dim (int): Dimension of input data.\n        output_dim (int): Dimension of output/reconstructed data.\n        latent_dim (int): Dimension of latent space.\n        encoder_hidden_dims (list): List of hidden layer dimensions for encoder.\n        decoder_hidden_dims (list): List of hidden layer dimensions for decoder.\n        activation (str, optional): Activation function name. Supports 'relu',\n            'tanh', 'sigmoid', 'leakyrelu'. Defaults to 'leakyrelu'.\n        use_norm (str or bool, optional): Normalization type. Can be 'batch',\n            'layer', or False. Defaults to False.\n        use_dropout (bool, optional): Whether to use dropout. Defaults to False.\n        use_softplus_output (bool, optional): Whether to apply softplus to output.\n            Defaults to False.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__()\n    # Define the activation function\n    self.use_softplus_output = use_softplus_output\n    if activation == \"relu\":\n        self.activation = nn.ReLU()\n    elif activation == \"tanh\":\n        self.activation = nn.Tanh()\n    elif activation == \"sigmoid\":\n        self.activation = nn.Sigmoid()\n    elif activation == \"leakyrelu\":\n        self.activation = nn.LeakyReLU(0.2)\n    else:\n        raise ValueError(f\"Unsupported activation function: {activation}\")\n\n    # Encoder layers\n    self.encoder_layers = self.build_layers(\n        input_dim, encoder_hidden_dims, use_norm, use_dropout\n    )\n    self.fc_mu = nn.Linear(encoder_hidden_dims[-1], latent_dim)\n    self.fc_log_var = nn.Linear(encoder_hidden_dims[-1], latent_dim)\n\n    # Decoder layers\n    self.decoder_layers = self.build_layers(\n        latent_dim, decoder_hidden_dims, use_norm, use_dropout\n    )\n    # self.decoder_layers.add_module('softplus', nn.Softplus())\n    self.decoder_layers.add_module(\n        \"output_layer\", nn.Linear(decoder_hidden_dims[-1], output_dim)\n    )\n    if self.use_softplus_output:\n        self.decoder_layers.add_module(\"output_activation\", nn.Softplus())\n    # self.decoder_layers.add_module('output_activation', nn.Tanh())  # Assuming output is in range [-1, 1]\n    # with the classic robust preprocessing method it is -1 to 1, but for others it may not.\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.VAE.build_layers","title":"<code>build_layers(self, input_dim, hidden_dims, use_norm, use_dropout=False)</code>","text":"<p>Build sequential neural network layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension for the first layer.</p> required <code>hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions.</p> required <code>use_norm</code> <code>str or bool</code> <p>Normalization type ('batch', 'layer', or False).</p> required <code>use_dropout</code> <code>bool</code> <p>Whether to include dropout layers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>nn.Sequential</code> <p>Sequential container of network layers.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def build_layers(self, input_dim, hidden_dims, use_norm, use_dropout=False):\n    \"\"\"Build sequential neural network layers.\n\n    Args:\n        input_dim (int): Input dimension for the first layer.\n        hidden_dims (list): List of hidden layer dimensions.\n        use_norm (str or bool): Normalization type ('batch', 'layer', or False).\n        use_dropout (bool, optional): Whether to include dropout layers.\n            Defaults to False.\n\n    Returns:\n        nn.Sequential: Sequential container of network layers.\n    \"\"\"\n    layers = []\n    current_size = input_dim\n    for hidden_dim in hidden_dims:\n        next_size = hidden_dim\n        layers.append(nn.Linear(current_size, next_size))\n        if use_norm == \"batch\":\n            layers.append(nn.BatchNorm1d(hidden_dim))\n        elif use_norm == \"layer\":\n            layers.append(nn.LayerNorm(hidden_dim))\n        layers.append(self.activation)\n        if use_dropout:\n            layers.append(nn.Dropout(0.1))\n        current_size = next_size\n    return nn.Sequential(*layers)\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.VAE.decode","title":"<code>decode(self, z)</code>","text":"<p>Decode latent representation to output space.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>torch.Tensor</code> <p>Latent representation.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Reconstructed output.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def decode(self, z):\n    \"\"\"Decode latent representation to output space.\n\n    Args:\n        z (torch.Tensor): Latent representation.\n\n    Returns:\n        torch.Tensor: Reconstructed output.\n    \"\"\"\n    return self.decoder_layers(z)\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.VAE.encode","title":"<code>encode(self, x)</code>","text":"<p>Encode input to latent space parameters.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - mu (torch.Tensor): Mean of latent distribution.     - log_var (torch.Tensor): Log variance of latent distribution.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def encode(self, x):\n    \"\"\"Encode input to latent space parameters.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n\n    Returns:\n        tuple: A tuple containing:\n            - mu (torch.Tensor): Mean of latent distribution.\n            - log_var (torch.Tensor): Log variance of latent distribution.\n    \"\"\"\n    x = self.encoder_layers(x)\n    mu = self.fc_mu(x)\n    log_var = self.fc_log_var(x)\n    return mu, log_var\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.VAE.forward","title":"<code>forward(self, x)</code>","text":"<p>Forward pass through the VAE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing:     - 'pred_y': Reconstructed output     - 'mu': Mean of latent distribution     - 'log_var': Log variance of latent distribution</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass through the VAE.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        dict: Dictionary containing:\n            - 'pred_y': Reconstructed output\n            - 'mu': Mean of latent distribution\n            - 'log_var': Log variance of latent distribution\n    \"\"\"\n    mu, log_var = self.encode(x)\n    z = self.reparameterize(mu, log_var)\n    pred_y = self.decode(z)\n    return {\"pred_y\": pred_y, \"mu\": mu, \"log_var\": log_var}\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.VAE.loss_fn","title":"<code>loss_fn(self, output_dict, kld_weight=0.0)</code>","text":"<p>Compute VAE loss (reconstruction + KL divergence).</p> <p>Parameters:</p> Name Type Description Default <code>output_dict</code> <code>dict</code> <p>Dictionary containing model outputs and targets.</p> required <code>kld_weight</code> <code>float</code> <p>Weight for KL divergence term. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing different loss components:     - 'total_loss': Combined loss (MAE + weighted KLD)     - 'mae_loss': Mean Absolute Error     - 'mse_loss': Mean Squared Error     - 'kld_loss': KL Divergence loss</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def loss_fn(self, output_dict, kld_weight=0.0):\n    \"\"\"Compute VAE loss (reconstruction + KL divergence).\n\n    Args:\n        output_dict (dict): Dictionary containing model outputs and targets.\n        kld_weight (float, optional): Weight for KL divergence term.\n            Defaults to 0.0.\n\n    Returns:\n        dict: Dictionary containing different loss components:\n            - 'total_loss': Combined loss (MAE + weighted KLD)\n            - 'mae_loss': Mean Absolute Error\n            - 'mse_loss': Mean Squared Error\n            - 'kld_loss': KL Divergence loss\n    \"\"\"\n    pred_y, y, mu, log_var = (\n        output_dict[\"pred_y\"],\n        output_dict[\"y\"],\n        output_dict[\"mu\"],\n        output_dict[\"log_var\"],\n    )\n    batch_size = y.shape[0]\n    MAE = F.l1_loss(pred_y, y, reduction=\"mean\")\n    # Reconstruction loss (MSE)\n    MSE = F.mse_loss(pred_y, y, reduction=\"mean\")\n    # KL divergence\n    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch_size\n    # Return combined loss\n    return {\n        \"total_loss\": MAE + kld_weight * KLD,\n        \"mae_loss\": MAE,\n        \"mse_loss\": MSE,\n        \"kld_loss\": KLD,\n    }\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.VAE.reparameterize","title":"<code>reparameterize(self, mu, log_var)</code>","text":"<p>Reparameterization trick for sampling from latent distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>torch.Tensor</code> <p>Mean of latent distribution.</p> required <code>log_var</code> <code>torch.Tensor</code> <p>Log variance of latent distribution.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Sampled latent vector.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def reparameterize(self, mu, log_var):\n    \"\"\"Reparameterization trick for sampling from latent distribution.\n\n    Args:\n        mu (torch.Tensor): Mean of latent distribution.\n        log_var (torch.Tensor): Log variance of latent distribution.\n\n    Returns:\n        torch.Tensor: Sampled latent vector.\n    \"\"\"\n    std = torch.exp(0.5 * log_var)\n    eps = torch.randn_like(std)\n    z = mu + eps * std\n    return z\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.evaluate","title":"<code>evaluate(model, test_dl, device, TSS_scalers_dict=None, log_offset=0.01)</code>","text":"<p>Evaluate the MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>MoE-VAE model.</p> required <code>test_dl</code> <code>torch.utils.data.DataLoader</code> <p>DataLoader for test data.</p> required <code>device</code> <code>torch.device</code> <p>Device to use for evaluation.</p> required <code>TSS_scalers_dict</code> <code>dict</code> <p>Dictionary containing scalers for TSS.</p> <code>None</code> <code>log_offset</code> <code>float</code> <p>Log offset for predictions. Defaults to 0.01.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing:     - predictions_inverse (numpy.ndarray): Inverse transformed predictions.     - actuals_inverse (numpy.ndarray): Inverse transformed actuals.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def evaluate(model, test_dl, device, TSS_scalers_dict=None, log_offset=0.01):\n    \"\"\"Evaluate the MoE-VAE model.\n\n    Args:\n        model (torch.nn.Module): MoE-VAE model.\n        test_dl (torch.utils.data.DataLoader): DataLoader for test data.\n        device (torch.device): Device to use for evaluation.\n        TSS_scalers_dict (dict, optional): Dictionary containing scalers for TSS.\n        log_offset (float, optional): Log offset for predictions. Defaults to 0.01.\n\n    Returns:\n        tuple: Tuple containing:\n            - predictions_inverse (numpy.ndarray): Inverse transformed predictions.\n            - actuals_inverse (numpy.ndarray): Inverse transformed actuals.\n    \"\"\"\n    model.eval()\n    predictions, actuals = [], []\n\n    with torch.no_grad():\n        for x, y in test_dl:\n            x, y = x.to(device), y.to(device)\n            output_dict = model(x)\n            y_pred = output_dict[\"pred_y\"]\n            predictions.append(y_pred.cpu().numpy())\n            actuals.append(y.cpu().numpy())\n\n    predictions = np.vstack(predictions)\n    actuals = np.vstack(actuals)\n\n    # === Inverse transformation ===\n    if TSS_scalers_dict is not None:\n        log_scaler = TSS_scalers_dict[\"log\"]\n        robust_scaler = TSS_scalers_dict[\"robust\"]\n\n        # First reverse min-max, then reverse log\n        predictions_inverse = (\n            log_scaler.inverse_transform(\n                torch.tensor(\n                    robust_scaler.inverse_transform(\n                        torch.tensor(predictions, dtype=torch.float32)\n                    ),\n                    dtype=torch.float32,\n                )\n            )\n            .numpy()\n            .flatten()\n        )\n\n        actuals_inverse = (\n            log_scaler.inverse_transform(\n                torch.tensor(\n                    robust_scaler.inverse_transform(\n                        torch.tensor(actuals, dtype=torch.float32)\n                    ),\n                    dtype=torch.float32,\n                )\n            )\n            .numpy()\n            .flatten()\n        )\n    else:\n        predictions_inverse = (10 ** predictions.flatten()) - log_offset\n        actuals_inverse = (10 ** actuals.flatten()) - log_offset\n\n    return predictions_inverse, actuals_inverse\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.evaluate_token","title":"<code>evaluate_token(model, test_dl, device, TSS_scalers_dict=None, log_offset=0.01)</code>","text":"<p>Evaluate the token-wise MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>MoE-VAE model.</p> required <code>test_dl</code> <code>torch.utils.data.DataLoader</code> <p>DataLoader for test data.</p> required <code>device</code> <code>torch.device</code> <p>Device to use for evaluation.</p> required <code>TSS_scalers_dict</code> <code>dict</code> <p>Dictionary containing scalers for TSS.</p> <code>None</code> <code>log_offset</code> <code>float</code> <p>Log offset for predictions. Defaults to 0.01.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing:     - predictions_inverse (numpy.ndarray): Inverse transformed predictions.     - actuals_inverse (numpy.ndarray): Inverse transformed actuals.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def evaluate_token(model, test_dl, device, TSS_scalers_dict=None, log_offset=0.01):\n    \"\"\"Evaluate the token-wise MoE-VAE model.\n\n    Args:\n        model (torch.nn.Module): MoE-VAE model.\n        test_dl (torch.utils.data.DataLoader): DataLoader for test data.\n        device (torch.device): Device to use for evaluation.\n        TSS_scalers_dict (dict, optional): Dictionary containing scalers for TSS.\n        log_offset (float, optional): Log offset for predictions. Defaults to 0.01.\n\n    Returns:\n        tuple: Tuple containing:\n            - predictions_inverse (numpy.ndarray): Inverse transformed predictions.\n            - actuals_inverse (numpy.ndarray): Inverse transformed actuals.\n    \"\"\"\n    model.eval()\n    predictions, actuals = [], []\n\n    with torch.no_grad():\n        for x, y in test_dl:\n            x, y = x.to(device), y.to(device)\n            output_dict = model(x)\n            y_pred = output_dict[\"pred_y\"]  # [B, token_len]\n\n            if y_pred.ndim == 2:\n                y_pred = y_pred.mean(dim=1, keepdim=True)  # [B, 1]\n\n            predictions.append(y_pred.cpu().numpy())\n            actuals.append(y.cpu().numpy())\n\n    predictions = np.vstack(predictions)\n    actuals = np.vstack(actuals)\n\n    # === Inverse transformation ===\n    if TSS_scalers_dict is not None:\n        log_scaler = TSS_scalers_dict[\"log\"]\n        robust_scaler = TSS_scalers_dict[\"robust\"]\n\n        # First reverse min-max, then reverse log\n        predictions_inverse = (\n            log_scaler.inverse_transform(\n                torch.tensor(\n                    robust_scaler.inverse_transform(\n                        torch.tensor(predictions, dtype=torch.float32)\n                    ),\n                    dtype=torch.float32,\n                )\n            )\n            .numpy()\n            .flatten()\n        )\n\n        actuals_inverse = (\n            log_scaler.inverse_transform(\n                torch.tensor(\n                    robust_scaler.inverse_transform(\n                        torch.tensor(actuals, dtype=torch.float32)\n                    ),\n                    dtype=torch.float32,\n                )\n            )\n            .numpy()\n            .flatten()\n        )\n    else:\n        predictions_inverse = (10 ** predictions.flatten()) - log_offset\n        actuals_inverse = (10 ** actuals.flatten()) - log_offset\n\n    return predictions_inverse, actuals_inverse\n</code></pre>"},{"location":"moe_vae/model/#hypercoast.moe_vae.model.train","title":"<code>train(model, train_dl, device, epochs=200, optimizer=None, save_dir=None)</code>","text":"<p>Train the MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>MoE-VAE model.</p> required <code>train_dl</code> <code>torch.utils.data.DataLoader</code> <p>DataLoader for training data.</p> required <code>device</code> <code>torch.device</code> <p>Device to use for training.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train. Defaults to 200.</p> <code>200</code> <code>optimizer</code> <code>torch.optim.Optimizer</code> <p>Optimizer to use for training.</p> <code>None</code> <code>save_dir</code> <code>str</code> <p>Directory to save the model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing training metrics:     - 'total_loss': List of total loss values per epoch.     - 'l1_loss': List of L1 loss values per epoch.     - 'best_loss': Minimum total loss value.</p> Source code in <code>hypercoast/moe_vae/model.py</code> <pre><code>def train(model, train_dl, device, epochs=200, optimizer=None, save_dir=None):\n    \"\"\"Train the MoE-VAE model.\n\n    Args:\n        model (torch.nn.Module): MoE-VAE model.\n        train_dl (torch.utils.data.DataLoader): DataLoader for training data.\n        device (torch.device): Device to use for training.\n        epochs (int, optional): Number of epochs to train. Defaults to 200.\n        optimizer (torch.optim.Optimizer, optional): Optimizer to use for training.\n        save_dir (str, optional): Directory to save the model. Defaults to None.\n\n    Returns:\n        dict: Dictionary containing training metrics:\n            - 'total_loss': List of total loss values per epoch.\n            - 'l1_loss': List of L1 loss values per epoch.\n            - 'best_loss': Minimum total loss value.\n    \"\"\"\n    model.train()\n    min_total_loss = float(\"inf\")\n    best_model_path = os.path.join(save_dir, \"best_model_minloss.pth\")\n\n    total_list = []\n    l1_list = []\n\n    for epoch in range(epochs):\n        total_loss_epoch = 0.0\n        l1_epoch = 0.0\n\n        for x, y in train_dl:\n            x, y = x.to(device), y.to(device)\n\n            output_dict = model(x)\n            output_dict[\"y\"] = y\n\n            loss_dict = model.loss_fn(output_dict)\n\n            loss = loss_dict[\"total_loss\"]\n            l1 = loss_dict[\"mae_loss\"]\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss_epoch += loss.item()\n            l1_epoch += l1.item()\n\n        avg_total_loss = total_loss_epoch / len(train_dl)\n        avg_l1 = l1_epoch / len(train_dl)\n\n        print(f\"[Epoch {epoch+1}] Total: {avg_total_loss:.4f} | L1: {avg_l1:.4f}\")\n        total_list.append(avg_total_loss)\n        l1_list.append(avg_l1)\n\n        if avg_total_loss &lt; min_total_loss:\n            min_total_loss = avg_total_loss\n            torch.save(model.state_dict(), best_model_path)\n\n    return {\"total_loss\": total_list, \"l1_loss\": l1_list, \"best_loss\": min_total_loss}\n</code></pre>"},{"location":"moe_vae/model_inference/","title":"model_inference module","text":"<p>Model inference utilities for MoE-VAE.</p> <p>This module provides functions for preprocessing and inference using MoE-VAE models.</p>"},{"location":"moe_vae/model_inference/#hypercoast.moe_vae.model_inference.infer_and_visualize_single_model_Robust","title":"<code>infer_and_visualize_single_model_Robust(model, test_loader, Rrs, mask, latitude, longitude, save_folder, extent, rgb_image, structure_name, run, TSS_scalers_dict, vmin=0, vmax=50)</code>","text":"<p>Infer and visualize results from a single MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>MoE-VAE model.</p> required <code>test_loader</code> <code>DataLoader</code> <p>DataLoader for test data.</p> required <code>Rrs</code> <code>array</code> <p>Rrs data.</p> required <code>mask</code> <code>array</code> <p>Boolean mask indicating valid pixels.</p> required <code>latitude</code> <code>array</code> <p>Latitude values.</p> required <code>longitude</code> <code>array</code> <p>Longitude values.</p> required <code>save_folder</code> <code>str</code> <p>Path to save the results.</p> required <code>extent</code> <code>tuple</code> <p>Tuple containing the extent of the image.</p> required <code>rgb_image</code> <code>array</code> <p>RGB image.</p> required <code>structure_name</code> <code>str</code> <p>Name of the structure.</p> required <code>run</code> <code>int</code> <p>Run number.</p> required <code>TSS_scalers_dict</code> <code>dict</code> <p>Dictionary containing the scalers for TSS.</p> required <code>vmin</code> <code>float</code> <p>Minimum value for the colorbar.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>Maximum value for the colorbar.</p> <code>50</code> Source code in <code>hypercoast/moe_vae/model_inference.py</code> <pre><code>def infer_and_visualize_single_model_Robust(\n    model,\n    test_loader,\n    Rrs,\n    mask,\n    latitude,\n    longitude,\n    save_folder,\n    extent,\n    rgb_image,\n    structure_name,\n    run,\n    TSS_scalers_dict,\n    vmin=0,\n    vmax=50,\n):\n    \"\"\"\n    Infer and visualize results from a single MoE-VAE model.\n\n    Args:\n        model (torch.nn.Module): MoE-VAE model.\n        test_loader (DataLoader): DataLoader for test data.\n        Rrs (array): Rrs data.\n        mask (array): Boolean mask indicating valid pixels.\n        latitude (array): Latitude values.\n        longitude (array): Longitude values.\n        save_folder (str): Path to save the results.\n        extent (tuple): Tuple containing the extent of the image.\n        rgb_image (array): RGB image.\n        structure_name (str): Name of the structure.\n        run (int): Run number.\n        TSS_scalers_dict (dict): Dictionary containing the scalers for TSS.\n        vmin (float): Minimum value for the colorbar.\n        vmax (float): Maximum value for the colorbar.\n    \"\"\"\n    device = next(model.parameters()).device\n    predictions_all = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch[0].to(device)\n            output_dict = model(batch)\n            predictions = output_dict[\"pred_y\"]\n\n            # === Inverse transform using TSS_scalers_dict from training ===\n            predictions_log = TSS_scalers_dict[\"robust\"].inverse_transform(\n                torch.tensor(predictions.cpu().numpy(), dtype=torch.float32)\n            )\n            predictions_all.append(\n                TSS_scalers_dict[\"log\"].inverse_transform(predictions_log).numpy()\n            )\n\n    predictions_all = np.vstack(predictions_all).squeeze(-1)\n    outputs = np.full((Rrs.shape[0], Rrs.shape[1]), np.nan)\n    outputs[mask] = predictions_all\n    lat_flat = latitude.flatten()\n    lon_flat = longitude.flatten()\n    output_flat = outputs.flatten()\n    final_output = np.column_stack((lat_flat, lon_flat, output_flat))\n\n    if np.ma.isMaskedArray(final_output):\n        final_output = final_output.filled(np.nan)\n    os.makedirs(save_folder, exist_ok=True)\n    base_name = os.path.splitext(os.path.basename(structure_name))[0]\n    npy_path = os.path.join(save_folder, f\"{base_name}.npy\")\n    png_path = os.path.join(save_folder, f\"{base_name}.png\")\n    np.save(npy_path, final_output)\n\n    latitude_masked = final_output[:, 0]\n    longitude_masked = final_output[:, 1]\n    tss_values = final_output[:, 2]\n\n    mean_lat = (extent[2] + extent[3]) / 2\n    resolution_deg_lat = 1000 / 111000\n    resolution_deg_lon = 1000 / (111000 * np.cos(np.radians(mean_lat)))\n    grid_lon = np.arange(extent[0], extent[1], resolution_deg_lon)\n    grid_lat = np.arange(extent[3], extent[2], -resolution_deg_lat)\n    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n    tss_resampled = griddata(\n        (longitude_masked, latitude_masked),\n        tss_values,\n        (grid_lon, grid_lat),\n        method=\"linear\",\n    )\n    tss_resampled = np.ma.masked_invalid(tss_resampled)\n\n    plt.figure(figsize=(24, 6))\n    plt.imshow(rgb_image / 255.0, extent=extent, origin=\"upper\")\n    im = plt.imshow(\n        tss_resampled,\n        extent=extent,\n        cmap=\"jet\",\n        alpha=1,\n        origin=\"upper\",\n        vmin=vmin,\n        vmax=vmax,\n    )\n    cbar = plt.colorbar(im)\n    cbar.set_label(\"(mg m$^{-3}$)\", fontsize=16)\n    plt.title(f\"{structure_name} - Run {run}\", loc=\"left\", fontsize=20)\n    plt.savefig(png_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n    plt.close()\n</code></pre>"},{"location":"moe_vae/model_inference/#hypercoast.moe_vae.model_inference.infer_and_visualize_single_model_minmax","title":"<code>infer_and_visualize_single_model_minmax(model, test_loader, Rrs, mask, latitude, longitude, save_folder, extent, rgb_image, structure_name, run, vmin=0, vmax=50, log_offset=0.01)</code>","text":"<p>Infer and visualize results from a single MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>MoE-VAE model.</p> required <code>test_loader</code> <code>DataLoader</code> <p>DataLoader for test data.</p> required <code>Rrs</code> <code>array</code> <p>Rrs data.</p> required <code>mask</code> <code>array</code> <p>Boolean mask indicating valid pixels.</p> required <code>latitude</code> <code>array</code> <p>Latitude values.</p> required <code>longitude</code> <code>array</code> <p>Longitude values.</p> required <code>save_folder</code> <code>str</code> <p>Path to save the results.</p> required <code>extent</code> <code>tuple</code> <p>Tuple containing the extent of the image.</p> required <code>rgb_image</code> <code>array</code> <p>RGB image.</p> required <code>structure_name</code> <code>str</code> <p>Name of the structure.</p> required <code>run</code> <code>int</code> <p>Run number.</p> required <code>vmin</code> <code>float</code> <p>Minimum value for the colorbar.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>Maximum value for the colorbar.</p> <code>50</code> <code>log_offset</code> <code>float</code> <p>Log offset for predictions.</p> <code>0.01</code> Source code in <code>hypercoast/moe_vae/model_inference.py</code> <pre><code>def infer_and_visualize_single_model_minmax(\n    model,\n    test_loader,\n    Rrs,\n    mask,\n    latitude,\n    longitude,\n    save_folder,\n    extent,\n    rgb_image,\n    structure_name,\n    run,\n    vmin=0,\n    vmax=50,\n    log_offset=0.01,\n):\n    \"\"\"\n    Infer and visualize results from a single MoE-VAE model.\n\n    Args:\n        model (torch.nn.Module): MoE-VAE model.\n        test_loader (DataLoader): DataLoader for test data.\n        Rrs (array): Rrs data.\n        mask (array): Boolean mask indicating valid pixels.\n        latitude (array): Latitude values.\n        longitude (array): Longitude values.\n        save_folder (str): Path to save the results.\n        extent (tuple): Tuple containing the extent of the image.\n        rgb_image (array): RGB image.\n        structure_name (str): Name of the structure.\n        run (int): Run number.\n        vmin (float): Minimum value for the colorbar.\n        vmax (float): Maximum value for the colorbar.\n        log_offset (float): Log offset for predictions.\n    \"\"\"\n    device = next(model.parameters()).device\n    predictions_all = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch[0].to(device)\n            output_dict = model(batch)\n            predictions = output_dict[\"pred_y\"]\n\n            predictions_np = predictions.cpu().numpy()\n            predictions_original = (10**predictions_np) - log_offset\n            predictions_all.append(predictions_original)\n\n    predictions_all = np.vstack(predictions_all).squeeze(-1)\n\n    outputs = np.full((Rrs.shape[0], Rrs.shape[1]), np.nan)\n    outputs[mask] = predictions_all\n\n    lat_flat = latitude.flatten()\n    lon_flat = longitude.flatten()\n    output_flat = outputs.flatten()\n    final_output = np.column_stack((lat_flat, lon_flat, output_flat))\n    if np.ma.isMaskedArray(final_output):\n        final_output = final_output.filled(np.nan)\n    os.makedirs(save_folder, exist_ok=True)\n    base_name = os.path.splitext(os.path.basename(structure_name))[0]\n    npy_path = os.path.join(save_folder, f\"{base_name}.npy\")\n    png_path = os.path.join(save_folder, f\"{base_name}.png\")\n    np.save(npy_path, final_output)\n\n    latitude_masked = final_output[:, 0]\n    longitude_masked = final_output[:, 1]\n    tss_values = final_output[:, 2]\n\n    mean_lat = (extent[2] + extent[3]) / 2\n    resolution_deg_lat = 1000 / 111000\n    resolution_deg_lon = 1000 / (111000 * np.cos(np.radians(mean_lat)))\n    grid_lon = np.arange(extent[0], extent[1], resolution_deg_lon)\n    grid_lat = np.arange(extent[3], extent[2], -resolution_deg_lat)\n    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n\n    tss_resampled = griddata(\n        (longitude_masked, latitude_masked),\n        tss_values,\n        (grid_lon, grid_lat),\n        method=\"linear\",\n    )\n    tss_resampled = np.ma.masked_invalid(tss_resampled)\n\n    plt.figure(figsize=(24, 6))\n    plt.imshow(rgb_image / 255.0, extent=extent, origin=\"upper\")\n    im = plt.imshow(\n        tss_resampled,\n        extent=extent,\n        cmap=\"jet\",\n        alpha=1,\n        origin=\"upper\",\n        vmin=vmin,\n        vmax=vmax,\n    )\n    cbar = plt.colorbar(im)\n    cbar.set_label(\"(mg m$^{-3}$)\", fontsize=16)\n    plt.title(f\"{structure_name} - Run {run}\", loc=\"left\", fontsize=20)\n    plt.savefig(png_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n    plt.close()\n</code></pre>"},{"location":"moe_vae/model_inference/#hypercoast.moe_vae.model_inference.infer_and_visualize_token_model_Robust","title":"<code>infer_and_visualize_token_model_Robust(model, test_loader, Rrs, mask, latitude, longitude, save_folder, extent, rgb_image, structure_name, run, TSS_scalers_dict, vmin=0, vmax=50)</code>","text":"<p>Infer and visualize results from a token-based MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>MoE-VAE model.</p> required <code>test_loader</code> <code>DataLoader</code> <p>DataLoader for test data.</p> required <code>Rrs</code> <code>array</code> <p>Rrs data.</p> required <code>mask</code> <code>array</code> <p>Boolean mask indicating valid pixels.</p> required <code>latitude</code> <code>array</code> <p>Latitude values.</p> required <code>longitude</code> <code>array</code> <p>Longitude values.</p> required <code>save_folder</code> <code>str</code> <p>Path to save the results.</p> required <code>extent</code> <code>tuple</code> <p>Tuple containing the extent of the image.</p> required <code>rgb_image</code> <code>array</code> <p>RGB image.</p> required <code>structure_name</code> <code>str</code> <p>Name of the structure.</p> required <code>run</code> <code>int</code> <p>Run number.</p> required <code>TSS_scalers_dict</code> <code>dict</code> <p>Dictionary containing the scalers for TSS.</p> required <code>vmin</code> <code>float</code> <p>Minimum value for the colorbar.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>Maximum value for the colorbar.</p> <code>50</code> Source code in <code>hypercoast/moe_vae/model_inference.py</code> <pre><code>def infer_and_visualize_token_model_Robust(\n    model,\n    test_loader,\n    Rrs,\n    mask,\n    latitude,\n    longitude,\n    save_folder,\n    extent,\n    rgb_image,\n    structure_name,\n    run,\n    TSS_scalers_dict,\n    vmin=0,\n    vmax=50,\n):\n    \"\"\"\n    Infer and visualize results from a token-based MoE-VAE model.\n\n    Args:\n        model (torch.nn.Module): MoE-VAE model.\n        test_loader (DataLoader): DataLoader for test data.\n        Rrs (array): Rrs data.\n        mask (array): Boolean mask indicating valid pixels.\n        latitude (array): Latitude values.\n        longitude (array): Longitude values.\n        save_folder (str): Path to save the results.\n        extent (tuple): Tuple containing the extent of the image.\n        rgb_image (array): RGB image.\n        structure_name (str): Name of the structure.\n        run (int): Run number.\n        TSS_scalers_dict (dict): Dictionary containing the scalers for TSS.\n        vmin (float): Minimum value for the colorbar.\n        vmax (float): Maximum value for the colorbar.\n    \"\"\"\n    device = next(model.parameters()).device\n    predictions_all = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch[0].to(device)\n            output_dict = model(batch)\n            predictions = output_dict[\"pred_y\"]  # shape [B, token_len]\n\n            # === Aggregate by token ===\n            if predictions.ndim == 2:\n                predictions = predictions.mean(dim=1, keepdim=True)  # [B, 1]\n\n            # === Robust + log inverse transform ===\n            predictions_log = TSS_scalers_dict[\"robust\"].inverse_transform(\n                torch.tensor(predictions.cpu().numpy(), dtype=torch.float32)\n            )\n            predictions_all.append(\n                TSS_scalers_dict[\"log\"].inverse_transform(predictions_log).numpy()\n            )\n\n    # === Concatenate and remove extra dimensions ===\n    predictions_all = np.vstack(predictions_all).reshape(-1)\n\n    outputs = np.full((Rrs.shape[0], Rrs.shape[1]), np.nan)\n    outputs[mask] = predictions_all\n\n    lat_flat = latitude.flatten()\n    lon_flat = longitude.flatten()\n    output_flat = outputs.flatten()\n    final_output = np.column_stack((lat_flat, lon_flat, output_flat))\n\n    os.makedirs(save_folder, exist_ok=True)\n    base_name = os.path.splitext(os.path.basename(structure_name))[0]\n    npy_path = os.path.join(save_folder, f\"{base_name}.npy\")\n    png_path = os.path.join(save_folder, f\"{base_name}.png\")\n    np.save(npy_path, final_output)\n\n    latitude_masked = final_output[:, 0]\n    longitude_masked = final_output[:, 1]\n    tss_values = final_output[:, 2]\n\n    mean_lat = (extent[2] + extent[3]) / 2\n    resolution_deg_lat = 1000 / 111000\n    resolution_deg_lon = 1000 / (111000 * np.cos(np.radians(mean_lat)))\n    grid_lon = np.arange(extent[0], extent[1], resolution_deg_lon)\n    grid_lat = np.arange(extent[3], extent[2], -resolution_deg_lat)\n    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n\n    tss_resampled = griddata(\n        (longitude_masked, latitude_masked),\n        tss_values,\n        (grid_lon, grid_lat),\n        method=\"linear\",\n    )\n    tss_resampled = np.ma.masked_invalid(tss_resampled)\n\n    plt.figure(figsize=(24, 6))\n    plt.imshow(rgb_image / 255.0, extent=extent, origin=\"upper\")\n    im = plt.imshow(\n        tss_resampled,\n        extent=extent,\n        cmap=\"jet\",\n        alpha=1,\n        origin=\"upper\",\n        vmin=vmin,\n        vmax=vmax,\n    )\n    cbar = plt.colorbar(im)\n    cbar.set_label(\"(mg m$^{-3}$)\", fontsize=16)\n    plt.title(f\"{structure_name} - Run {run}\", loc=\"left\", fontsize=20)\n    plt.savefig(png_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n    plt.close()\n</code></pre>"},{"location":"moe_vae/model_inference/#hypercoast.moe_vae.model_inference.infer_and_visualize_token_model_minmax","title":"<code>infer_and_visualize_token_model_minmax(model, test_loader, Rrs, mask, latitude, longitude, save_folder, extent, rgb_image, structure_name, run, vmin=0, vmax=50, log_offset=0.01)</code>","text":"<p>Infer and visualize results from a token-based MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>MoE-VAE model.</p> required <code>test_loader</code> <code>DataLoader</code> <p>DataLoader for test data.</p> required <code>Rrs</code> <code>array</code> <p>Rrs data.</p> required <code>mask</code> <code>array</code> <p>Boolean mask indicating valid pixels.</p> required <code>latitude</code> <code>array</code> <p>Latitude values.</p> required <code>longitude</code> <code>array</code> <p>Longitude values.</p> required <code>save_folder</code> <code>str</code> <p>Path to save the results.</p> required <code>extent</code> <code>tuple</code> <p>Tuple containing the extent of the image.</p> required <code>rgb_image</code> <code>array</code> <p>RGB image.</p> required <code>structure_name</code> <code>str</code> <p>Name of the structure.</p> required <code>run</code> <code>int</code> <p>Run number.</p> required <code>vmin</code> <code>float</code> <p>Minimum value for the colorbar.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>Maximum value for the colorbar.</p> <code>50</code> <code>log_offset</code> <code>float</code> <p>Log offset for predictions.</p> <code>0.01</code> Source code in <code>hypercoast/moe_vae/model_inference.py</code> <pre><code>def infer_and_visualize_token_model_minmax(\n    model,\n    test_loader,\n    Rrs,\n    mask,\n    latitude,\n    longitude,\n    save_folder,\n    extent,\n    rgb_image,\n    structure_name,\n    run,\n    vmin=0,\n    vmax=50,\n    log_offset=0.01,\n):\n    \"\"\"\n    Infer and visualize results from a token-based MoE-VAE model.\n\n    Args:\n        model (torch.nn.Module): MoE-VAE model.\n        test_loader (DataLoader): DataLoader for test data.\n        Rrs (array): Rrs data.\n        mask (array): Boolean mask indicating valid pixels.\n        latitude (array): Latitude values.\n        longitude (array): Longitude values.\n        save_folder (str): Path to save the results.\n        extent (tuple): Tuple containing the extent of the image.\n        rgb_image (array): RGB image.\n        structure_name (str): Name of the structure.\n        run (int): Run number.\n        vmin (float): Minimum value for the colorbar.\n        vmax (float): Maximum value for the colorbar.\n        log_offset (float): Log offset for predictions.\n    \"\"\"\n    device = next(model.parameters()).device\n    predictions_all = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = batch[0].to(device)\n            output_dict = model(batch)\n            predictions = output_dict[\"pred_y\"]  # shape [B, token_len]\n\n            # === Aggregate along the token dimension ===\n            if predictions.ndim == 2:\n                predictions = predictions.mean(dim=1, keepdim=True)  # shape [B, 1]\n\n            predictions_np = predictions.cpu().numpy()\n            predictions_original = (10**predictions_np) - log_offset\n            predictions_all.append(predictions_original)\n\n    # === Concatenate and flatten ===\n    predictions_all = np.vstack(predictions_all).reshape(-1)\n\n    outputs = np.full((Rrs.shape[0], Rrs.shape[1]), np.nan)\n    outputs[mask] = predictions_all\n\n    lat_flat = latitude.flatten()\n    lon_flat = longitude.flatten()\n    output_flat = outputs.flatten()\n    final_output = np.column_stack((lat_flat, lon_flat, output_flat))\n\n    os.makedirs(save_folder, exist_ok=True)\n    base_name = os.path.splitext(os.path.basename(structure_name))[0]\n    npy_path = os.path.join(save_folder, f\"{base_name}.npy\")\n    png_path = os.path.join(save_folder, f\"{base_name}.png\")\n    np.save(npy_path, final_output)\n\n    latitude_masked = final_output[:, 0]\n    longitude_masked = final_output[:, 1]\n    tss_values = final_output[:, 2]\n\n    mean_lat = (extent[2] + extent[3]) / 2\n    resolution_deg_lat = 1000 / 111000\n    resolution_deg_lon = 1000 / (111000 * np.cos(np.radians(mean_lat)))\n    grid_lon = np.arange(extent[0], extent[1], resolution_deg_lon)\n    grid_lat = np.arange(extent[3], extent[2], -resolution_deg_lat)\n    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n\n    tss_resampled = griddata(\n        (longitude_masked, latitude_masked),\n        tss_values,\n        (grid_lon, grid_lat),\n        method=\"linear\",\n    )\n    tss_resampled = np.ma.masked_invalid(tss_resampled)\n\n    plt.figure(figsize=(24, 6))\n    plt.imshow(rgb_image / 255.0, extent=extent, origin=\"upper\")\n    im = plt.imshow(\n        tss_resampled,\n        extent=extent,\n        cmap=\"jet\",\n        alpha=1,\n        origin=\"upper\",\n        vmin=vmin,\n        vmax=vmax,\n    )\n    cbar = plt.colorbar(im)\n    cbar.set_label(\"(mg m$^{-3}$)\", fontsize=16)\n    plt.title(f\"{structure_name} - Run {run}\", loc=\"left\", fontsize=20)\n    plt.savefig(png_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.1)\n    plt.close()\n</code></pre>"},{"location":"moe_vae/model_inference/#hypercoast.moe_vae.model_inference.preprocess_emit_data_Robust","title":"<code>preprocess_emit_data_Robust(nc_path, scaler_Rrs, use_diff=True, full_band_wavelengths=None)</code>","text":"<p>Preprocess EMIT data for Robust scaling.</p> <p>Parameters:</p> Name Type Description Default <code>nc_path</code> <code>str</code> <p>Path to the NetCDF file containing EMIT data.</p> required <code>scaler_Rrs</code> <code>object</code> <p>RobustScaler object for Rrs normalization.</p> required <code>use_diff</code> <code>bool</code> <p>Whether to apply first-order differencing.</p> <code>True</code> <code>full_band_wavelengths</code> <code>list</code> <p>List of target wavelength bands.</p> <code>None</code> <p>Returns:</p> Type Description <code>test_loader (DataLoader)</code> <p>DataLoader for test data. filtered_Rrs (array): Filtered Rrs data. mask (array): Boolean mask indicating valid pixels. latitude (array): Latitude values. longitude (array): Longitude values.</p> Source code in <code>hypercoast/moe_vae/model_inference.py</code> <pre><code>def preprocess_emit_data_Robust(\n    nc_path, scaler_Rrs, use_diff=True, full_band_wavelengths=None\n):\n    \"\"\"\n    Preprocess EMIT data for Robust scaling.\n\n    Args:\n        nc_path (str): Path to the NetCDF file containing EMIT data.\n        scaler_Rrs (object): RobustScaler object for Rrs normalization.\n        use_diff (bool): Whether to apply first-order differencing.\n        full_band_wavelengths (list): List of target wavelength bands.\n\n    Returns:\n        test_loader (DataLoader): DataLoader for test data.\n        filtered_Rrs (array): Filtered Rrs data.\n        mask (array): Boolean mask indicating valid pixels.\n        latitude (array): Latitude values.\n        longitude (array): Longitude values.\n    \"\"\"\n\n    if full_band_wavelengths is None:\n        raise ValueError(\n            \"full_band_wavelengths must be provided to match EMIT Rrs bands\"\n        )\n\n    def find_closest_band(target, available_bands):\n        available_waves = [int(b.split(\"_\")[1]) for b in available_bands]\n        closest_wave = min(available_waves, key=lambda w: abs(w - target))\n        return f\"Rrs_{closest_wave}\"\n\n    dataset = nc(nc_path)\n    latitude = dataset.variables[\"lat\"][:]\n    longitude = dataset.variables[\"lon\"][:]\n\n    all_vars = dataset.variables.keys()\n\n    bands_to_extract = []\n    for w in full_band_wavelengths:\n        band_name = f\"Rrs_{int(w)}\"\n        if band_name in all_vars:\n            bands_to_extract.append(band_name)\n        else:\n            closest = find_closest_band(int(w))\n            print(f\"\u26a0\ufe0f {band_name} does not exist, using the closest band {closest}\")\n            bands_to_extract.append(closest)\n    filtered_Rrs = np.array([dataset.variables[band][:] for band in bands_to_extract])\n    filtered_Rrs = np.moveaxis(filtered_Rrs, 0, -1)\n\n    mask = np.all(~np.isnan(filtered_Rrs), axis=2)\n\n    target_443 = (\n        f\"Rrs_443\"\n        if \"Rrs_443\" in bands_to_extract\n        else find_closest_band(443, bands_to_extract)\n    )\n    target_560 = (\n        f\"Rrs_560\"\n        if \"Rrs_560\" in bands_to_extract\n        else find_closest_band(560, bands_to_extract)\n    )\n\n    print(f\"Using {target_443} and {target_560} for mask check.\")\n\n    idx_443 = bands_to_extract.index(target_443)\n    idx_560 = bands_to_extract.index(target_560)\n    mask &amp;= filtered_Rrs[:, :, idx_443] &lt;= filtered_Rrs[:, :, idx_560]\n\n    valid_test_data = filtered_Rrs[mask]\n\n    # ---- smooth + diff\n    if use_diff:\n        from scipy.ndimage import gaussian_filter1d\n\n        Rrs_smoothed = np.array(\n            [gaussian_filter1d(spectrum, sigma=1) for spectrum in valid_test_data]\n        )\n        Rrs_processed = np.diff(Rrs_smoothed, axis=1)\n        print(\"\u2705 [5] Performed Gaussian smoothing + first-order differencing\")\n    else:\n        Rrs_processed = valid_test_data\n        print(\"\u2705 [5] Smoothing and differencing not enabled\")\n\n    # ---- normalize\n    Rrs_normalized = scaler_Rrs.transform(\n        torch.tensor(Rrs_processed, dtype=torch.float32)\n    ).numpy()\n\n    # ---- DataLoader\n    test_tensor = TensorDataset(torch.tensor(Rrs_normalized).float())\n    test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n    print(\"\u2705 [6] DataLoader construction completed\")\n\n    return test_loader, filtered_Rrs, mask, latitude, longitude\n</code></pre>"},{"location":"moe_vae/model_inference/#hypercoast.moe_vae.model_inference.preprocess_emit_data_minmax","title":"<code>preprocess_emit_data_minmax(nc_path, full_band_wavelengths=None, diff_before_norm=False, diff_after_norm=False)</code>","text":"<p>Read EMIT NetCDF, extract Rrs_* bands according to full_band_wavelengths, apply (optional) smooth+diff and robust normalization, and return a DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>nc_path</code> <code>str</code> <p>Path to the NetCDF file containing EMIT data.</p> required <code>full_band_wavelengths</code> <code>list</code> <p>List of target wavelength bands.</p> <code>None</code> <code>diff_before_norm</code> <code>bool</code> <p>Whether to apply first-order differencing before normalization.</p> <code>False</code> <code>diff_after_norm</code> <code>bool</code> <p>Whether to apply first-order differencing after normalization.</p> <code>False</code> <p>Returns:</p> Type Description <p>test_loader, filtered_Rrs(H, W, B), mask(H, W), latitude(H), longitude(W)</p> Source code in <code>hypercoast/moe_vae/model_inference.py</code> <pre><code>def preprocess_emit_data_minmax(\n    nc_path, full_band_wavelengths=None, diff_before_norm=False, diff_after_norm=False\n):\n    \"\"\"\n    Read EMIT NetCDF, extract Rrs_* bands according to full_band_wavelengths,\n    apply (optional) smooth+diff and robust normalization, and return a DataLoader.\n\n    Args:\n        nc_path (str): Path to the NetCDF file containing EMIT data.\n        full_band_wavelengths (list): List of target wavelength bands.\n        diff_before_norm (bool): Whether to apply first-order differencing before normalization.\n        diff_after_norm (bool): Whether to apply first-order differencing after normalization.\n\n    Returns:\n        test_loader, filtered_Rrs(H, W, B), mask(H, W), latitude(H), longitude(W)\n    \"\"\"\n    print(f\"\ud83d\udce5 Start processing: {nc_path}\")\n\n    # ---- sanity checks\n    if full_band_wavelengths is None or len(full_band_wavelengths) == 0:\n        raise ValueError(\n            \"A non-empty full_band_wavelengths must be provided (e.g., [400, 402, ...]).\"\n        )\n\n    full_band_wavelengths = [int(w) for w in full_band_wavelengths]\n\n    try:\n        with nc(nc_path, \"r\") as dataset:\n            latitude = dataset.variables[\"lat\"][:]\n            longitude = dataset.variables[\"lon\"][:]\n            all_vars = set(dataset.variables.keys())\n            available_wavelengths = [\n                float(v.split(\"_\")[1]) for v in all_vars if v.startswith(\"Rrs_\")\n            ]\n\n            def find_closest_band(target_nm: float):\n                nearest = min(available_wavelengths, key=lambda w: abs(w - target_nm))\n                return f\"Rrs_{int(nearest)}\"\n\n            # Search according to full_band_wavelengths\n            bands_to_extract = []\n            for w in full_band_wavelengths:\n                band_name = f\"Rrs_{w}\"\n                if band_name in all_vars:\n                    bands_to_extract.append(band_name)\n                else:\n                    closest = find_closest_band(w)\n                    print(\n                        f\"\u26a0\ufe0f {band_name} does not exist, using the closest band {closest}\"\n                    )\n                    bands_to_extract.append(closest)\n\n            seen = set()\n            bands_to_extract = [\n                b for b in bands_to_extract if not (b in seen or seen.add(b))\n            ]\n\n            if len(bands_to_extract) == 0:\n                raise ValueError(\"\u274c No usable Rrs_* bands found in the file.\")\n            # ---- read and stack to (H, W, B)\n            # Each variable expected shape: (lat, lon) or (y, x)\n            Rrs_stack = []\n            for band in bands_to_extract:\n                arr = dataset.variables[band][:]  # (H, W)\n                Rrs_stack.append(arr)\n\n            Rrs = np.array(Rrs_stack)  # (B, H, W)\n            Rrs = np.moveaxis(Rrs, 0, -1)  # (H, W, B)\n            filtered_Rrs = Rrs  # keep naming consistent with your previous return\n\n            # ---- build mask using 440 &amp; 560 (or nearest present within your requested list)\n            have_waves = [int(b.split(\"_\")[1]) for b in bands_to_extract]\n\n            def nearest_idx(target_nm: int):\n                # find nearest *among bands_to_extract*\n                nearest_w = min(have_waves, key=lambda w: abs(w - target_nm))\n                return bands_to_extract.index(f\"Rrs_{nearest_w}\")\n\n            # Prefer exact if available; otherwise nearest in the user-requested set\n            idx_440 = (\n                bands_to_extract.index(\"Rrs_440\")\n                if \"Rrs_440\" in bands_to_extract\n                else nearest_idx(440)\n            )\n            idx_560 = (\n                bands_to_extract.index(\"Rrs_560\")\n                if \"Rrs_560\" in bands_to_extract\n                else nearest_idx(560)\n            )\n\n            print(\n                f\"\u2705 Bands used for mask check: {bands_to_extract[idx_440]} and {bands_to_extract[idx_560]}\"\n            )\n\n            mask_nanfree = np.all(~np.isnan(filtered_Rrs), axis=2)\n            mask_condition = filtered_Rrs[:, :, idx_560] &gt;= filtered_Rrs[:, :, idx_440]\n            mask = mask_nanfree &amp; mask_condition\n            print(f\"\u2705 [4] Built valid mask, remaining pixels: {int(np.sum(mask))}\")\n\n            if not np.any(mask):\n                raise ValueError(\"\u274c No valid pixels passed the filtering.\")\n\n            valid_test_data = filtered_Rrs[mask]  # (N, B)\n\n        # === Check whether smoothing is needed (only executed if any differencing is enabled) ===\n        if diff_before_norm or diff_after_norm:\n            from scipy.ndimage import gaussian_filter1d\n\n            Rrs_smoothed = np.array(\n                [gaussian_filter1d(spectrum, sigma=1) for spectrum in valid_test_data]\n            )\n            print(\"\u2705 Gaussian smoothing applied\")\n        else:\n            Rrs_smoothed = valid_test_data\n            print(\"\u2705 Smoothing not enabled\")\n\n        # === Preprocessing before differencing ===\n        if diff_before_norm:\n            Rrs_preprocessed = np.diff(Rrs_smoothed, axis=1)\n            print(\"\u2705 Preprocessing before differencing completed\")\n        else:\n            Rrs_preprocessed = Rrs_smoothed\n            print(\"\u2705 Preprocessing before differencing not enabled\")\n\n        # === MinMax normalization to [1, 10] ===\n        scalers = [MinMaxScaler((1, 10)) for _ in range(Rrs_preprocessed.shape[0])]\n        Rrs_normalized = np.array(\n            [\n                scalers[i].fit_transform(row.reshape(-1, 1)).flatten()\n                for i, row in enumerate(Rrs_preprocessed)\n            ]\n        )\n\n        # === Post-processing after differencing ===\n        if diff_after_norm:\n            Rrs_normalized = np.diff(Rrs_normalized, axis=1)\n            print(\"\u2705 Post-processing after differencing completed\")\n        else:\n            print(\"\u2705 Post-processing after differencing not enabled\")\n\n        # === Construct DataLoader\n        test_tensor = TensorDataset(torch.tensor(Rrs_normalized).float())\n        test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n\n        return test_loader, Rrs, mask, latitude, longitude\n\n    except Exception as e:\n        print(f\"\u274c [ERROR] Failed to process file {nc_path}: {e}\")\n        return None\n</code></pre>"},{"location":"moe_vae/model_inference/#hypercoast.moe_vae.model_inference.preprocess_pace_data_Robust","title":"<code>preprocess_pace_data_Robust(nc_path, scaler_Rrs, use_diff=True, full_band_wavelengths=None)</code>","text":"<p>Preprocess PACE data for Robust scaling.</p> <p>Parameters:</p> Name Type Description Default <code>nc_path</code> <code>str</code> <p>Path to the NetCDF file containing PACE data.</p> required <code>scaler_Rrs</code> <code>object</code> <p>RobustScaler object for Rrs normalization.</p> required <code>use_diff</code> <code>bool</code> <p>Whether to apply first-order differencing.</p> <code>True</code> <code>full_band_wavelengths</code> <code>list</code> <p>List of target wavelength bands.</p> <code>None</code> <p>Returns:</p> Type Description <code>test_loader (DataLoader)</code> <p>DataLoader for test data. filtered_Rrs (array): Filtered Rrs data. mask (array): Boolean mask indicating valid pixels. latitude (array): Latitude values. longitude (array): Longitude values.</p> Source code in <code>hypercoast/moe_vae/model_inference.py</code> <pre><code>def preprocess_pace_data_Robust(\n    nc_path, scaler_Rrs, use_diff=True, full_band_wavelengths=None\n):\n    \"\"\"\n    Preprocess PACE data for Robust scaling.\n\n    Args:\n        nc_path (str): Path to the NetCDF file containing PACE data.\n        scaler_Rrs (object): RobustScaler object for Rrs normalization.\n        use_diff (bool): Whether to apply first-order differencing.\n        full_band_wavelengths (list): List of target wavelength bands.\n\n    Returns:\n        test_loader (DataLoader): DataLoader for test data.\n        filtered_Rrs (array): Filtered Rrs data.\n        mask (array): Boolean mask indicating valid pixels.\n        latitude (array): Latitude values.\n        longitude (array): Longitude values.\n    \"\"\"\n    print(f\"\ud83d\udce5 Start processing: {nc_path}\")\n    try:\n        PACE_dataset = read_pace(nc_path)\n        print(\"\u2705 [1] Successfully read PACE data\")\n\n        da = PACE_dataset[\"Rrs\"]\n        Rrs = da.values  # [lat, lon, bands]\n        latitude = da.latitude.values\n        longitude = da.longitude.values\n        print(\"\u2705 [2] Successfully retrieved Rrs, lat, and lon\")\n\n        # \u2705 Extract wavelength\n        if full_band_wavelengths is None:\n            raise ValueError(\n                \"full_band_wavelengths must be provided to match PACE Rrs bands\"\n            )\n\n        if hasattr(da, \"wavelength\") or \"wavelength\" in da.coords:\n            pace_band_wavelengths = da.wavelength.values\n        else:\n            raise ValueError(\n                \"\u274c Unable to extract wavelength from PACE data. Please check the NetCDF file structure.\"\n            )\n\n        missing = [b for b in full_band_wavelengths if b not in pace_band_wavelengths]\n        if missing:\n            raise ValueError(\n                f\"\u274c The following wavelengths are not present in the PACE data: {missing}\"\n            )\n\n        indices = [\n            np.where(pace_band_wavelengths == b)[0][0] for b in full_band_wavelengths\n        ]\n        band_wavelengths = pace_band_wavelengths[indices]\n        assert (\n            band_wavelengths == np.array(full_band_wavelengths)\n        ).all(), \"\u274c Band order mismatch\"\n\n        filtered_Rrs = Rrs[:, :, indices]\n        print(\n            f\"\u2705 [3] Bands re-extracted based on full_band_wavelengths, total {len(indices)} bands\"\n        )\n\n        # \u2705 Build mask\n        idx_440 = np.where(band_wavelengths == 440)[0][0]\n        idx_560 = np.where(band_wavelengths == 560)[0][0]\n\n        Rrs_440 = filtered_Rrs[:, :, idx_440]\n        Rrs_560 = filtered_Rrs[:, :, idx_560]\n\n        mask_nanfree = np.all(~np.isnan(filtered_Rrs), axis=2)\n        mask_condition = Rrs_560 &gt;= Rrs_440\n        mask = mask_nanfree &amp; mask_condition\n        print(f\"\u2705 [4] Built valid mask, remaining pixels: {np.sum(mask)}\")\n\n        if not np.any(mask):\n            raise ValueError(\"\u274c No valid pixels passed the filtering.\")\n\n        valid_test_data = filtered_Rrs[mask]\n\n        # \u2705 Smoothing before differencing (enabled only if use_diff=True)\n        if use_diff:\n            from scipy.ndimage import gaussian_filter1d\n\n            Rrs_smoothed = np.array(\n                [gaussian_filter1d(spectrum, sigma=1) for spectrum in valid_test_data]\n            )\n            Rrs_processed = np.diff(Rrs_smoothed, axis=1)\n            print(\"\u2705 [5] Performed Gaussian smoothing + first-order differencing\")\n        else:\n            Rrs_processed = valid_test_data\n            print(\"\u2705 [5] Smoothing and differencing not enabled\")\n\n        # \u2705 Normalization (RobustScaler provided)\n        Rrs_normalized = scaler_Rrs.transform(\n            torch.tensor(Rrs_processed, dtype=torch.float32)\n        ).numpy()\n\n        # \u2705 Construct DataLoader\n        from torch.utils.data import DataLoader, TensorDataset\n\n        test_tensor = TensorDataset(torch.tensor(Rrs_normalized).float())\n        test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n        print(\"\u2705 [6] DataLoader construction completed\")\n\n        return test_loader, filtered_Rrs, mask, latitude, longitude\n\n    except Exception as e:\n        print(f\"\u274c [ERROR] Failed to process file {nc_path}: {e}\")\n        return None\n</code></pre>"},{"location":"moe_vae/model_inference/#hypercoast.moe_vae.model_inference.preprocess_pace_data_minmax","title":"<code>preprocess_pace_data_minmax(nc_path, full_band_wavelengths=None, diff_before_norm=False, diff_after_norm=False)</code>","text":"<p>Preprocess PACE data for MinMax scaling.</p> <p>Parameters:</p> Name Type Description Default <code>nc_path</code> <code>str</code> <p>Path to the NetCDF file containing PACE data.</p> required <code>full_band_wavelengths</code> <code>list</code> <p>List of target wavelength bands.</p> <code>None</code> <code>diff_before_norm</code> <code>bool</code> <p>Whether to apply first-order differencing before normalization.</p> <code>False</code> <code>diff_after_norm</code> <code>bool</code> <p>Whether to apply first-order differencing after normalization.</p> <code>False</code> <p>Returns:</p> Type Description <code>test_loader (DataLoader)</code> <p>DataLoader for test data. Rrs (array): Rrs data. mask (array): Boolean mask indicating valid pixels. latitude (array): Latitude values. longitude (array): Longitude values.</p> Source code in <code>hypercoast/moe_vae/model_inference.py</code> <pre><code>def preprocess_pace_data_minmax(\n    nc_path, full_band_wavelengths=None, diff_before_norm=False, diff_after_norm=False\n):\n    \"\"\"\n    Preprocess PACE data for MinMax scaling.\n\n    Args:\n        nc_path (str): Path to the NetCDF file containing PACE data.\n        full_band_wavelengths (list): List of target wavelength bands.\n        diff_before_norm (bool): Whether to apply first-order differencing before normalization.\n        diff_after_norm (bool): Whether to apply first-order differencing after normalization.\n\n    Returns:\n        test_loader (DataLoader): DataLoader for test data.\n        Rrs (array): Rrs data.\n        mask (array): Boolean mask indicating valid pixels.\n        latitude (array): Latitude values.\n        longitude (array): Longitude values.\n    \"\"\"\n    try:\n        # === Load data ===\n        PACE_dataset = read_pace(nc_path)\n        da = PACE_dataset[\"Rrs\"]\n        Rrs = da.values  # [lat, lon, bands]\n        latitude = da.latitude.values\n        longitude = da.longitude.values\n\n        # === Band check ===\n        if full_band_wavelengths is None:\n            raise ValueError(\n                \"full_band_wavelengths must be provided to match PACE Rrs bands\"\n            )\n\n        if hasattr(da, \"wavelength\") or \"wavelength\" in da.coords:\n            pace_band_wavelengths = da.wavelength.values\n        else:\n            raise ValueError(\n                \"\u274c Unable to extract wavelength from PACE data. Please check the NetCDF file structure.\"\n            )\n\n        # Check for missing bands\n        missing = [b for b in full_band_wavelengths if b not in pace_band_wavelengths]\n        if missing:\n            raise ValueError(\n                f\"\u274c The following wavelengths are not found in the PACE data: {missing}\"\n            )\n\n        # Extract indices in the order of full_band_wavelengths\n        indices = [\n            np.where(pace_band_wavelengths == b)[0][0] for b in full_band_wavelengths\n        ]\n        band_wavelengths = pace_band_wavelengths[indices]\n        assert (\n            band_wavelengths == np.array(full_band_wavelengths)\n        ).all(), \"\u274c Band order is inconsistent\"\n\n        # Extract Rrs for selected_bands\n        filtered_Rrs = Rrs[:, :, indices]\n\n        # === Mask construction ===\n        idx_440 = np.where(band_wavelengths == 440)[0][0]\n        idx_560 = np.where(band_wavelengths == 560)[0][0]\n        Rrs_440 = filtered_Rrs[:, :, idx_440]\n        Rrs_560 = filtered_Rrs[:, :, idx_560]\n\n        mask_nanfree = np.all(~np.isnan(filtered_Rrs), axis=2)\n        mask_condition = Rrs_560 &gt;= Rrs_440\n        mask = mask_nanfree &amp; mask_condition\n        if not np.any(mask):\n            raise ValueError(\"\u274c No valid pixels passed the filtering.\")\n\n        valid_data = filtered_Rrs[mask]  # [num_pixel, num_band]\n\n        # === Check if smoothing is needed (only executed when any differencing is enabled) ===\n        if diff_before_norm or diff_after_norm:\n            from scipy.ndimage import gaussian_filter1d\n\n            Rrs_smoothed = np.array(\n                [gaussian_filter1d(spectrum, sigma=1) for spectrum in valid_data]\n            )\n            print(\"\u2705 Gaussian smoothing applied\")\n        else:\n            Rrs_smoothed = valid_data\n            print(\"\u2705 Smoothing not enabled\")\n\n        # === Preprocessing before differencing ===\n        if diff_before_norm:\n            Rrs_preprocessed = np.diff(Rrs_smoothed, axis=1)\n            print(\"\u2705 Preprocessing before differencing completed\")\n        else:\n            Rrs_preprocessed = Rrs_smoothed\n            print(\"\u2705 Preprocessing before differencing not enabled\")\n\n        # === MinMax normalization to [1, 10] ===\n        scalers = [MinMaxScaler((1, 10)) for _ in range(Rrs_preprocessed.shape[0])]\n        Rrs_normalized = np.array(\n            [\n                scalers[i].fit_transform(row.reshape(-1, 1)).flatten()\n                for i, row in enumerate(Rrs_preprocessed)\n            ]\n        )\n\n        # === Post-processing after differencing ===\n        if diff_after_norm:\n            Rrs_normalized = np.diff(Rrs_normalized, axis=1)\n            print(\"\u2705 Post-processing after differencing completed\")\n        else:\n            print(\"\u2705 Post-processing after differencing not enabled\")\n\n        # === Construct DataLoader ===\n        test_tensor = TensorDataset(torch.tensor(Rrs_normalized).float())\n        test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n\n        return test_loader, Rrs, mask, latitude, longitude\n\n    except Exception as e:\n        print(f\"\u274c [ERROR] Failed to process file {nc_path}: {e}\")\n        return None\n</code></pre>"},{"location":"moe_vae/plot_and_save/","title":"plot_and_save module","text":""},{"location":"moe_vae/plot_and_save/#hypercoast.moe_vae.plot_and_save.calculate_metrics","title":"<code>calculate_metrics(predictions, actuals, threshold=0.8)</code>","text":"<p>Calculate epsilon, beta and additional metrics (RMSE, RMSLE, MAPE, Bias, MAE).</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array-like</code> <p>Predicted values.</p> required <code>actuals</code> <code>array-like</code> <p>Actual values.</p> required <code>threshold</code> <code>float</code> <p>Relative error threshold. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing:     - epsilon (float): Epsilon value.     - beta (float): Beta value.     - rmse (float): Root mean square error.     - rmsle (float): Root mean square log error.     - mape (float): Mean absolute percentage error.     - bias (float): Bias value.     - mae (float): Mean absolute error.</p> Source code in <code>hypercoast/moe_vae/plot_and_save.py</code> <pre><code>def calculate_metrics(predictions, actuals, threshold=0.8):\n    \"\"\"Calculate epsilon, beta and additional metrics (RMSE, RMSLE, MAPE, Bias, MAE).\n\n    Args:\n        predictions (array-like): Predicted values.\n        actuals (array-like): Actual values.\n        threshold (float, optional): Relative error threshold. Defaults to 0.8.\n\n    Returns:\n        tuple: Tuple containing:\n            - epsilon (float): Epsilon value.\n            - beta (float): Beta value.\n            - rmse (float): Root mean square error.\n            - rmsle (float): Root mean square log error.\n            - mape (float): Mean absolute percentage error.\n            - bias (float): Bias value.\n            - mae (float): Mean absolute error.\n    \"\"\"\n    # Apply the threshold to filter out predictions with large relative error\n    # mask = np.abs(predictions - actuals) / np.abs(actuals+1e-10) &lt; threshold\n    # filtered_predictions = predictions[mask]\n    # filtered_actuals = actuals[mask]\n    predictions = np.where(predictions &lt;= 1e-10, 1e-10, predictions)\n    actuals = np.where(actuals &lt;= 1e-10, 1e-10, actuals)\n    filtered_predictions = predictions\n    filtered_actuals = actuals\n\n    # Calculate epsilon and beta\n    log_ratios = np.log10(filtered_predictions / filtered_actuals)\n    Y = np.median(np.abs(log_ratios))\n    Z = np.median(log_ratios)\n    epsilon = 100 * (10**Y - 1)\n    beta = 50 * np.sign(Z) * (10 ** np.abs(Z) - 1)\n\n    # Calculate additional metrics\n    rmse = np.sqrt(np.mean((filtered_predictions - filtered_actuals) ** 2))\n    rmsle = np.sqrt(\n        np.mean(\n            (np.log10(filtered_predictions + 1) - np.log10(filtered_actuals + 1)) ** 2\n        )\n    )\n    mape = 50 * np.median(\n        np.abs((filtered_predictions - filtered_actuals) / filtered_actuals)\n    )\n    bias = 10 ** (np.mean(np.log10(filtered_predictions) - np.log10(filtered_actuals)))\n    mae = 10 ** np.mean(\n        np.abs(np.log10(filtered_predictions) - np.log10(filtered_actuals))\n    )\n\n    return epsilon, beta, rmse, rmsle, mape, bias, mae\n</code></pre>"},{"location":"moe_vae/plot_and_save/#hypercoast.moe_vae.plot_and_save.plot_results","title":"<code>plot_results(predictions_rescaled, actuals_rescaled, save_dir, threshold=10, mode='test')</code>","text":"<p>Plot the results of the MoE-VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_rescaled</code> <code>array-like</code> <p>Predicted values.</p> required <code>actuals_rescaled</code> <code>array-like</code> <p>Actual values.</p> required <code>save_dir</code> <code>str</code> <p>Directory to save the plot.</p> required <code>threshold</code> <code>float</code> <p>Relative error threshold. Defaults to 10.</p> <code>10</code> <code>mode</code> <code>str</code> <p>Mode of the plot. Defaults to \"test\".</p> <code>'test'</code> Source code in <code>hypercoast/moe_vae/plot_and_save.py</code> <pre><code>def plot_results(\n    predictions_rescaled, actuals_rescaled, save_dir, threshold=10, mode=\"test\"\n):\n    \"\"\"Plot the results of the MoE-VAE model.\n\n    Args:\n        predictions_rescaled (array-like): Predicted values.\n        actuals_rescaled (array-like): Actual values.\n        save_dir (str): Directory to save the plot.\n        threshold (float, optional): Relative error threshold. Defaults to 10.\n        mode (str, optional): Mode of the plot. Defaults to \"test\".\n    \"\"\"\n\n    actuals = actuals_rescaled.flatten()\n    predictions = predictions_rescaled.flatten()\n\n    log_actuals = np.log10(actuals)\n    log_predictions = np.log10(predictions)\n\n    # mask = np.abs(predictions - actuals) / np.abs(actuals+1e-10) &lt; threshold\n    mask = np.abs(log_predictions - log_actuals) &lt; threshold\n    filtered_predictions = predictions[mask]\n    filtered_actuals = actuals[mask]\n\n    log_actual = np.log10(np.where(actuals == 0, 1e-10, actuals))\n    log_prediction = np.log10(np.where(predictions == 0, 1e-10, predictions))\n\n    filtered_log_actual = np.log10(\n        np.where(filtered_actuals == 0, 1e-10, filtered_actuals)\n    )\n    filtered_log_prediction = np.log10(\n        np.where(filtered_predictions == 0, 1e-10, filtered_predictions)\n    )\n\n    epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(\n        filtered_predictions, filtered_actuals, threshold\n    )\n\n    valid_mask = np.isfinite(filtered_log_actual) &amp; np.isfinite(filtered_log_prediction)\n    slope, intercept = np.polyfit(\n        filtered_log_actual[valid_mask], filtered_log_prediction[valid_mask], 1\n    )\n    x = np.array([-4, 4])\n    y = slope * x + intercept\n\n    plt.figure(figsize=(6, 6))\n\n    plt.plot(x, y, linestyle=\"--\", color=\"blue\", linewidth=0.8)\n    lims = [-4, 4]\n    plt.plot(lims, lims, linestyle=\"-\", color=\"black\", linewidth=0.8)\n\n    sns.scatterplot(x=log_actual, y=log_prediction, alpha=0.5)\n\n    sns.kdeplot(\n        x=filtered_log_actual,\n        y=filtered_log_prediction,\n        levels=3,\n        color=\"black\",\n        fill=False,\n        linewidths=0.8,\n    )\n\n    plt.xlabel(\"Actual Values\", fontsize=16, fontname=\"Ubuntu\")\n    plt.ylabel(\"Predicted Values\", fontsize=16, fontname=\"Ubuntu\")\n    plt.xlim(-4, 4)\n    plt.ylim(-4, 4)\n    plt.grid(True, which=\"both\", ls=\"--\")\n\n    plt.legend(\n        title=(\n            f\"MAE = {mae:.2f}, RMSE = {rmse:.2f}, RMSLE = {rmsle:.2f} \\n\"\n            f\"Bias = {bias:.2f}, Slope = {slope:.2f} \\n\"\n            f\"MAPE = {mape:.2f}%, \u03b5 = {epsilon:.2f}%, \u03b2 = {beta:.2f}%\"\n        ),\n        fontsize=16,\n        title_fontsize=12,\n        prop={\"family\": \"Ubuntu\"},\n    )\n\n    plt.xticks(fontsize=20, fontname=\"Ubuntu\")\n    plt.yticks(fontsize=20, fontname=\"Ubuntu\")\n\n    plt.savefig(os.path.join(save_dir, f\"{mode}_plot.pdf\"), bbox_inches=\"tight\")\n    plt.close()\n</code></pre>"},{"location":"moe_vae/plot_and_save/#hypercoast.moe_vae.plot_and_save.save_and_plot_results_from_excel","title":"<code>save_and_plot_results_from_excel(predictions, actuals, sample_ids, dates, original_excel_path, save_dir)</code>","text":"<p>Save and plot the results from an Excel file.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array-like</code> <p>Predicted values.</p> required <code>actuals</code> <code>array-like</code> <p>Actual values.</p> required <code>sample_ids</code> <code>array-like</code> <p>IDs of the samples.</p> required <code>dates</code> <code>array-like</code> <p>Dates of the samples.</p> required <code>original_excel_path</code> <code>str</code> <p>Path to the original Excel file.</p> required <code>save_dir</code> <code>str</code> <p>Directory to save the results.</p> required Source code in <code>hypercoast/moe_vae/plot_and_save.py</code> <pre><code>def save_and_plot_results_from_excel(\n    predictions, actuals, sample_ids, dates, original_excel_path, save_dir\n):\n    \"\"\"Save and plot the results from an Excel file.\n\n    Args:\n        predictions (array-like): Predicted values.\n        actuals (array-like): Actual values.\n        sample_ids (array-like): IDs of the samples.\n        dates (array-like): Dates of the samples.\n        original_excel_path (str): Path to the original Excel file.\n        save_dir (str): Directory to save the results.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    filename = os.path.basename(original_excel_path)\n    dataset_name = os.path.splitext(filename)[0]\n\n    save_results_to_excel(\n        sample_ids,\n        actuals,\n        predictions,\n        os.path.join(save_dir, f\"{dataset_name}.xlsx\"),\n        dates=dates,\n    )\n    plot_results(predictions, actuals, save_dir, mode=dataset_name)\n</code></pre>"},{"location":"moe_vae/plot_and_save/#hypercoast.moe_vae.plot_and_save.save_model_structure_from_classes","title":"<code>save_model_structure_from_classes(save_path)</code>","text":"<p>Save the model structure from classes to a file.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the model structure.</p> required Source code in <code>hypercoast/moe_vae/plot_and_save.py</code> <pre><code>def save_model_structure_from_classes(save_path):\n    \"\"\"Save the model structure from classes to a file.\n\n    Args:\n        save_path (str): Path to save the model structure.\n    \"\"\"\n    classes = [SparseDispatcher, VAE, MoE_VAE]\n\n    with open(save_path, \"w\") as f:\n        for cls in classes:\n            f.write(f\"# === Source for {cls.__name__} ===\\n\")\n            f.write(inspect.getsource(cls))\n            f.write(\"\\n\\n\")\n</code></pre>"},{"location":"moe_vae/plot_and_save/#hypercoast.moe_vae.plot_and_save.save_results_to_excel","title":"<code>save_results_to_excel(ids, actuals, predictions, file_path, dates=None)</code>","text":"<p>Save prediction results to an Excel file. - If dates are included, output ID, Date, Actual, Predicted; - Otherwise, output ID, Actual, Predicted.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>array-like</code> <p>IDs of the samples.</p> required <code>actuals</code> <code>array-like</code> <p>Actual values.</p> required <code>predictions</code> <code>array-like</code> <p>Predicted values.</p> required <code>file_path</code> <code>str</code> <p>Path to save the Excel file.</p> required <code>dates</code> <code>array-like</code> <p>Dates of the samples. Defaults to None.</p> <code>None</code> Source code in <code>hypercoast/moe_vae/plot_and_save.py</code> <pre><code>def save_results_to_excel(ids, actuals, predictions, file_path, dates=None):\n    \"\"\"\n    Save prediction results to an Excel file.\n    - If dates are included, output ID, Date, Actual, Predicted;\n    - Otherwise, output ID, Actual, Predicted.\n\n    Args:\n        ids (array-like): IDs of the samples.\n        actuals (array-like): Actual values.\n        predictions (array-like): Predicted values.\n        file_path (str): Path to save the Excel file.\n        dates (array-like, optional): Dates of the samples. Defaults to None.\n    \"\"\"\n    if dates is not None:\n        df = pd.DataFrame(\n            {\"ID\": ids, \"Date\": dates, \"Actual\": actuals, \"Predicted\": predictions}\n        )\n    else:\n        df = pd.DataFrame({\"ID\": ids, \"Actual\": actuals, \"Predicted\": predictions})\n\n    df.to_excel(file_path, index=False)\n</code></pre>"},{"location":"publications/S2-MSI/Sentinel2MSI-chla/","title":"Sentinel2MSI chla","text":"<p>Part 1. Import the necessary libraries and define VAE model and functions</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport scipy.io\nimport pandas as pd\nimport netCDF4 as nc\nimport hypercoast\nimport rasterio\nfrom rasterio.transform import from_origin\nfrom rasterio.warp import reproject, Resampling\nfrom scipy.interpolate import griddata\nfrom datetime import datetime\nfrom torch.distributions.normal import Normal\nfrom torch.utils.data import DataLoader, TensorDataset, Subset\nfrom tqdm import tqdm\n\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n\n        # encoder\n        self.encoder_layer = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2),\n            nn.Linear(64, 64),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2),\n        )\n\n        self.fc1 = nn.Linear(64, 32)\n        self.fc2 = nn.Linear(64, 32)\n\n        # decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2),\n            nn.Linear(64, 64),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU(0.2),\n            nn.Linear(64, output_dim),\n            nn.Softplus(),\n        )\n\n    def encode(self, x):\n        x = self.encoder_layer(x)\n        mu = self.fc1(x)\n        log_var = self.fc2(x)\n        return mu, log_var\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        z = mu + eps * std\n        return z\n\n    def decode(self, z):\n        return self.decoder(z)\n\n    def forward(self, x):\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        x_reconstructed = self.decode(z)\n        return x_reconstructed, mu, log_var\n\n\ndef loss_function(recon_x, x, mu, log_var):\n    L1 = F.l1_loss(recon_x, x, reduction=\"mean\")\n    BCE = F.mse_loss(recon_x, x, reduction=\"mean\")\n    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    return L1\n\n\ndef load_real_data(excel_path, selected_bands, split_ratio=0.7):\n\n    rounded_bands = [int(round(b)) for b in selected_bands]\n    band_cols = [f\"Rrs_{b}\" for b in rounded_bands]\n\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n\n    df_rrs_selected = df_rrs[[\"GLORIA_ID\"] + band_cols]\n    df_param_selected = df_param[[\"GLORIA_ID\", \"chl-a\"]]\n    df_merged = pd.merge(\n        df_rrs_selected, df_param_selected, on=\"GLORIA_ID\", how=\"inner\"\n    )\n\n    mask_rrs_valid = df_merged[band_cols].notna().all(axis=1)\n    mask_tss_valid = df_merged[\"chl-a\"].notna()\n    df_filtered = df_merged[mask_rrs_valid &amp; mask_tss_valid].reset_index(drop=True)\n\n    lower = df_filtered[\"chl-a\"].quantile(0)\n    top = df_filtered[\"chl-a\"].quantile(1)\n    df_filtered = df_filtered[\n        (df_filtered[\"chl-a\"] &gt;= lower) &amp; (df_filtered[\"chl-a\"] &lt;= top)\n    ].reset_index(drop=True)\n    all_sample_ids = df_filtered[\"GLORIA_ID\"].astype(str).tolist()\n\n    Rrs_array = df_filtered[band_cols].values\n    Chl_array = df_filtered[[\"chl-a\"]].values\n\n    scalers_Rrs = [\n        MinMaxScaler(feature_range=(1, 10)) for _ in range(Rrs_array.shape[0])\n    ]\n    Rrs_normalized = np.array(\n        [\n            scalers_Rrs[i].fit_transform(row.reshape(-1, 1)).flatten()\n            for i, row in enumerate(Rrs_array)\n        ]\n    )\n    Chl_normalized = np.log10(Chl_array + 1)\n\n    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n    Chl_tensor = torch.tensor(Chl_normalized, dtype=torch.float32)\n    dataset = TensorDataset(Rrs_tensor, Chl_tensor)\n\n    num_samples = len(dataset)\n    indices = np.arange(num_samples)\n    np.random.seed(42)\n    np.random.shuffle(indices)\n\n    train_size = int(split_ratio * num_samples)\n    train_indices = indices[:train_size]\n    test_indices = indices[train_size:]\n\n    train_dataset = Subset(dataset, train_indices)\n    test_dataset = Subset(dataset, test_indices)\n\n    train_ids = [all_sample_ids[i] for i in train_indices]\n    test_ids = [all_sample_ids[i] for i in test_indices]\n\n    train_dl = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)\n    test_dl = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = Chl_tensor.shape[1]\n\n    return train_dl, test_dl, input_dim, output_dim, train_ids, test_ids\n\n\ndef load_real_test(excel_path, selected_bands, max_allowed_diff=1.0):\n\n    df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")\n    df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")\n\n    sample_ids = df_rrs[\"Site Label\"].astype(str).tolist()\n    sample_dates = df_rrs[\"Date\"].astype(str).tolist()\n\n    rrs_wavelengths = []\n    rrs_cols = []\n    for col in df_rrs.columns:\n        try:\n            wl = float(col)\n            rrs_wavelengths.append(wl)\n            rrs_cols.append(col)\n        except:\n            continue\n\n    band_cols = []\n    matched_bands = []\n    for target_band in selected_bands:\n        diffs = [abs(wl - target_band) for wl in rrs_wavelengths]\n        min_diff = min(diffs)\n        best_idx = diffs.index(min_diff)\n        band_cols.append(rrs_cols[best_idx])\n        matched_bands.append(rrs_wavelengths[best_idx])\n    Rrs_array = df_rrs[band_cols].values\n    Chl_array = df_param[[\"Chl-a\"]].values\n\n    scalers_Rrs = [\n        MinMaxScaler(feature_range=(1, 10)) for _ in range(Rrs_array.shape[0])\n    ]\n    Rrs_normalized = np.array(\n        [\n            scalers_Rrs[i].fit_transform(row.reshape(-1, 1)).flatten()\n            for i, row in enumerate(Rrs_array)\n        ]\n    )\n    Chl_normalized = np.log10(Chl_array + 1)\n    Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)\n    Chl_tensor = torch.tensor(Chl_normalized, dtype=torch.float32)\n    dataset = TensorDataset(Rrs_tensor, Chl_tensor)\n\n    dataset = TensorDataset(Rrs_tensor, Chl_tensor)\n    test_dl = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0)\n\n    input_dim = Rrs_tensor.shape[1]\n    output_dim = Chl_tensor.shape[1]\n\n    return test_dl, input_dim, output_dim, sample_ids, sample_dates\n\n\ndef calculate_metrics(predictions, actuals, threshold=100):\n    \"\"\"\n    Calculate epsilon, beta and additional metrics (RMSE, RMSLE, MAPE, Bias, MAE).\n\n    :param predictions: array-like, predicted values\n    :param actuals: array-like, actual values\n    :param threshold: float, relative error threshold\n    :return: epsilon, beta, rmse, rmsle, mape, bias, mae\n    \"\"\"\n    # Apply the threshold to filter out predictions with large relative error\n    # mask = np.abs(predictions - actuals) / np.abs(actuals+1e-10) &lt; threshold\n    # filtered_predictions = predictions[mask]\n    # filtered_actuals = actuals[mask]\n    # predictions = np.where(predictions &lt;= 1e-10, 1e-10, predictions)\n    # actuals = np.where(actuals &lt;= 1e-10, 1e-10, actuals)\n    filtered_predictions = predictions\n    filtered_actuals = actuals\n\n    # Calculate epsilon and beta\n    log_ratios = np.log10(filtered_predictions / filtered_actuals)\n    Y = np.median(np.abs(log_ratios))\n    Z = np.median(log_ratios)\n    epsilon = 100 * (10**Y - 1)\n    beta = 50 * np.sign(Z) * (10 ** np.abs(Z) - 1)\n\n    # Calculate additional metrics\n    rmse = np.sqrt(np.mean((filtered_predictions - filtered_actuals) ** 2))\n    rmsle = np.sqrt(\n        np.mean(\n            (np.log10(filtered_predictions + 1) - np.log10(filtered_actuals + 1)) ** 2\n        )\n    )\n    mape = 50 * np.median(\n        np.abs((filtered_predictions - filtered_actuals) / filtered_actuals)\n    )\n    bias = 10 ** (np.mean(np.log10(filtered_predictions) - np.log10(filtered_actuals)))\n    mae = 10 ** np.mean(\n        np.abs(np.log10(filtered_predictions) - np.log10(filtered_actuals))\n    )\n\n    return epsilon, beta, rmse, rmsle, mape, bias, mae\n\n\ndef plot_results(\n    predictions_rescaled, actuals_rescaled, save_dir, threshold=100, mode=\"test\"\n):\n\n    actuals = actuals_rescaled.flatten()\n    predictions = predictions_rescaled.flatten()\n\n    log_actuals = np.log10(actuals)\n    log_predictions = np.log10(predictions)\n\n    # mask = np.abs(predictions - actuals) / np.abs(actuals+1e-10) &lt; threshold\n    mask = np.abs(log_predictions - log_actuals) &lt; threshold\n    filtered_predictions = predictions[mask]\n    filtered_actuals = actuals[mask]\n\n    log_actual = np.log10(np.where(actuals == 0, 1e-10, actuals))\n    log_prediction = np.log10(np.where(predictions == 0, 1e-10, predictions))\n\n    filtered_log_actual = np.log10(\n        np.where(filtered_actuals == 0, 1e-10, filtered_actuals)\n    )\n    filtered_log_prediction = np.log10(\n        np.where(filtered_predictions == 0, 1e-10, filtered_predictions)\n    )\n\n    epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(\n        filtered_predictions, filtered_actuals, threshold\n    )\n\n    valid_mask = np.isfinite(filtered_log_actual) &amp; np.isfinite(filtered_log_prediction)\n    slope, intercept = np.polyfit(\n        filtered_log_actual[valid_mask], filtered_log_prediction[valid_mask], 1\n    )\n    x = np.array([-2, 4])\n    y = slope * x + intercept\n\n    plt.figure(figsize=(6, 6))\n\n    plt.plot(x, y, linestyle=\"--\", color=\"blue\", linewidth=0.8)\n    lims = [-2, 4]\n    plt.plot(lims, lims, linestyle=\"-\", color=\"black\", linewidth=0.8)\n\n    sns.scatterplot(x=log_actual, y=log_prediction, alpha=0.5)\n\n    sns.kdeplot(\n        x=filtered_log_actual,\n        y=filtered_log_prediction,\n        levels=3,\n        color=\"black\",\n        fill=False,\n        linewidths=0.8,\n    )\n\n    plt.xlabel(\"Actual Chl-a Values\", fontsize=16, fontname=\"Ubuntu\")\n    plt.ylabel(\"Predicted Chl-a Values\", fontsize=16, fontname=\"Ubuntu\")\n    plt.xlim(-2, 4)\n    plt.ylim(-2, 4)\n    plt.grid(True, which=\"both\", ls=\"--\")\n\n    plt.legend(\n        title=(\n            f\"MAE = {mae:.2f}, RMSE = {rmse:.2f}, RMSLE = {rmsle:.2f} \\n\"\n            f\"Bias = {bias:.2f}, Slope = {slope:.2f} \\n\"\n            f\"MAPE = {mape:.2f}%, \u03b5 = {epsilon:.2f}%, \u03b2 = {beta:.2f}%\"\n        ),\n        fontsize=16,\n        title_fontsize=12,\n        prop={\"family\": \"Ubuntu\"},\n    )\n\n    plt.xticks(fontsize=20, fontname=\"Ubuntu\")\n    plt.yticks(fontsize=20, fontname=\"Ubuntu\")\n\n    plt.savefig(os.path.join(save_dir, f\"{mode}_plot.pdf\"), bbox_inches=\"tight\")\n    plt.close()\n\n\ndef save_results_to_excel(ids, actuals, predictions, file_path, dates=None):\n\n    if dates is not None:\n        df = pd.DataFrame(\n            {\"ID\": ids, \"Date\": dates, \"Actual\": actuals, \"Predicted\": predictions}\n        )\n    else:\n        df = pd.DataFrame({\"ID\": ids, \"Actual\": actuals, \"Predicted\": predictions})\n\n    df.to_excel(file_path, index=False)\n</pre> import torch import torch.nn as nn import numpy as np from torch.utils.data import DataLoader, TensorDataset, random_split import torch.nn.functional as F from sklearn.preprocessing import MinMaxScaler, StandardScaler import matplotlib.pyplot as plt import seaborn as sns import os import scipy.io import pandas as pd import netCDF4 as nc import hypercoast import rasterio from rasterio.transform import from_origin from rasterio.warp import reproject, Resampling from scipy.interpolate import griddata from datetime import datetime from torch.distributions.normal import Normal from torch.utils.data import DataLoader, TensorDataset, Subset from tqdm import tqdm   class VAE(nn.Module):     def __init__(self, input_dim, output_dim):         super().__init__()          # encoder         self.encoder_layer = nn.Sequential(             nn.Linear(input_dim, 64),             nn.BatchNorm1d(64),             nn.LeakyReLU(0.2),             nn.Linear(64, 64),             nn.BatchNorm1d(64),             nn.LeakyReLU(0.2),         )          self.fc1 = nn.Linear(64, 32)         self.fc2 = nn.Linear(64, 32)          # decoder         self.decoder = nn.Sequential(             nn.Linear(32, 64),             nn.BatchNorm1d(64),             nn.LeakyReLU(0.2),             nn.Linear(64, 64),             nn.BatchNorm1d(64),             nn.LeakyReLU(0.2),             nn.Linear(64, output_dim),             nn.Softplus(),         )      def encode(self, x):         x = self.encoder_layer(x)         mu = self.fc1(x)         log_var = self.fc2(x)         return mu, log_var      def reparameterize(self, mu, log_var):         std = torch.exp(0.5 * log_var)         eps = torch.randn_like(std)         z = mu + eps * std         return z      def decode(self, z):         return self.decoder(z)      def forward(self, x):         mu, log_var = self.encode(x)         z = self.reparameterize(mu, log_var)         x_reconstructed = self.decode(z)         return x_reconstructed, mu, log_var   def loss_function(recon_x, x, mu, log_var):     L1 = F.l1_loss(recon_x, x, reduction=\"mean\")     BCE = F.mse_loss(recon_x, x, reduction=\"mean\")     KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())     return L1   def load_real_data(excel_path, selected_bands, split_ratio=0.7):      rounded_bands = [int(round(b)) for b in selected_bands]     band_cols = [f\"Rrs_{b}\" for b in rounded_bands]      df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")     df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")      df_rrs_selected = df_rrs[[\"GLORIA_ID\"] + band_cols]     df_param_selected = df_param[[\"GLORIA_ID\", \"chl-a\"]]     df_merged = pd.merge(         df_rrs_selected, df_param_selected, on=\"GLORIA_ID\", how=\"inner\"     )      mask_rrs_valid = df_merged[band_cols].notna().all(axis=1)     mask_tss_valid = df_merged[\"chl-a\"].notna()     df_filtered = df_merged[mask_rrs_valid &amp; mask_tss_valid].reset_index(drop=True)      lower = df_filtered[\"chl-a\"].quantile(0)     top = df_filtered[\"chl-a\"].quantile(1)     df_filtered = df_filtered[         (df_filtered[\"chl-a\"] &gt;= lower) &amp; (df_filtered[\"chl-a\"] &lt;= top)     ].reset_index(drop=True)     all_sample_ids = df_filtered[\"GLORIA_ID\"].astype(str).tolist()      Rrs_array = df_filtered[band_cols].values     Chl_array = df_filtered[[\"chl-a\"]].values      scalers_Rrs = [         MinMaxScaler(feature_range=(1, 10)) for _ in range(Rrs_array.shape[0])     ]     Rrs_normalized = np.array(         [             scalers_Rrs[i].fit_transform(row.reshape(-1, 1)).flatten()             for i, row in enumerate(Rrs_array)         ]     )     Chl_normalized = np.log10(Chl_array + 1)      Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)     Chl_tensor = torch.tensor(Chl_normalized, dtype=torch.float32)     dataset = TensorDataset(Rrs_tensor, Chl_tensor)      num_samples = len(dataset)     indices = np.arange(num_samples)     np.random.seed(42)     np.random.shuffle(indices)      train_size = int(split_ratio * num_samples)     train_indices = indices[:train_size]     test_indices = indices[train_size:]      train_dataset = Subset(dataset, train_indices)     test_dataset = Subset(dataset, test_indices)      train_ids = [all_sample_ids[i] for i in train_indices]     test_ids = [all_sample_ids[i] for i in test_indices]      train_dl = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)     test_dl = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)      input_dim = Rrs_tensor.shape[1]     output_dim = Chl_tensor.shape[1]      return train_dl, test_dl, input_dim, output_dim, train_ids, test_ids   def load_real_test(excel_path, selected_bands, max_allowed_diff=1.0):      df_rrs = pd.read_excel(excel_path, sheet_name=\"Rrs\")     df_param = pd.read_excel(excel_path, sheet_name=\"parameter\")      sample_ids = df_rrs[\"Site Label\"].astype(str).tolist()     sample_dates = df_rrs[\"Date\"].astype(str).tolist()      rrs_wavelengths = []     rrs_cols = []     for col in df_rrs.columns:         try:             wl = float(col)             rrs_wavelengths.append(wl)             rrs_cols.append(col)         except:             continue      band_cols = []     matched_bands = []     for target_band in selected_bands:         diffs = [abs(wl - target_band) for wl in rrs_wavelengths]         min_diff = min(diffs)         best_idx = diffs.index(min_diff)         band_cols.append(rrs_cols[best_idx])         matched_bands.append(rrs_wavelengths[best_idx])     Rrs_array = df_rrs[band_cols].values     Chl_array = df_param[[\"Chl-a\"]].values      scalers_Rrs = [         MinMaxScaler(feature_range=(1, 10)) for _ in range(Rrs_array.shape[0])     ]     Rrs_normalized = np.array(         [             scalers_Rrs[i].fit_transform(row.reshape(-1, 1)).flatten()             for i, row in enumerate(Rrs_array)         ]     )     Chl_normalized = np.log10(Chl_array + 1)     Rrs_tensor = torch.tensor(Rrs_normalized, dtype=torch.float32)     Chl_tensor = torch.tensor(Chl_normalized, dtype=torch.float32)     dataset = TensorDataset(Rrs_tensor, Chl_tensor)      dataset = TensorDataset(Rrs_tensor, Chl_tensor)     test_dl = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=0)      input_dim = Rrs_tensor.shape[1]     output_dim = Chl_tensor.shape[1]      return test_dl, input_dim, output_dim, sample_ids, sample_dates   def calculate_metrics(predictions, actuals, threshold=100):     \"\"\"     Calculate epsilon, beta and additional metrics (RMSE, RMSLE, MAPE, Bias, MAE).      :param predictions: array-like, predicted values     :param actuals: array-like, actual values     :param threshold: float, relative error threshold     :return: epsilon, beta, rmse, rmsle, mape, bias, mae     \"\"\"     # Apply the threshold to filter out predictions with large relative error     # mask = np.abs(predictions - actuals) / np.abs(actuals+1e-10) &lt; threshold     # filtered_predictions = predictions[mask]     # filtered_actuals = actuals[mask]     # predictions = np.where(predictions &lt;= 1e-10, 1e-10, predictions)     # actuals = np.where(actuals &lt;= 1e-10, 1e-10, actuals)     filtered_predictions = predictions     filtered_actuals = actuals      # Calculate epsilon and beta     log_ratios = np.log10(filtered_predictions / filtered_actuals)     Y = np.median(np.abs(log_ratios))     Z = np.median(log_ratios)     epsilon = 100 * (10**Y - 1)     beta = 50 * np.sign(Z) * (10 ** np.abs(Z) - 1)      # Calculate additional metrics     rmse = np.sqrt(np.mean((filtered_predictions - filtered_actuals) ** 2))     rmsle = np.sqrt(         np.mean(             (np.log10(filtered_predictions + 1) - np.log10(filtered_actuals + 1)) ** 2         )     )     mape = 50 * np.median(         np.abs((filtered_predictions - filtered_actuals) / filtered_actuals)     )     bias = 10 ** (np.mean(np.log10(filtered_predictions) - np.log10(filtered_actuals)))     mae = 10 ** np.mean(         np.abs(np.log10(filtered_predictions) - np.log10(filtered_actuals))     )      return epsilon, beta, rmse, rmsle, mape, bias, mae   def plot_results(     predictions_rescaled, actuals_rescaled, save_dir, threshold=100, mode=\"test\" ):      actuals = actuals_rescaled.flatten()     predictions = predictions_rescaled.flatten()      log_actuals = np.log10(actuals)     log_predictions = np.log10(predictions)      # mask = np.abs(predictions - actuals) / np.abs(actuals+1e-10) &lt; threshold     mask = np.abs(log_predictions - log_actuals) &lt; threshold     filtered_predictions = predictions[mask]     filtered_actuals = actuals[mask]      log_actual = np.log10(np.where(actuals == 0, 1e-10, actuals))     log_prediction = np.log10(np.where(predictions == 0, 1e-10, predictions))      filtered_log_actual = np.log10(         np.where(filtered_actuals == 0, 1e-10, filtered_actuals)     )     filtered_log_prediction = np.log10(         np.where(filtered_predictions == 0, 1e-10, filtered_predictions)     )      epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(         filtered_predictions, filtered_actuals, threshold     )      valid_mask = np.isfinite(filtered_log_actual) &amp; np.isfinite(filtered_log_prediction)     slope, intercept = np.polyfit(         filtered_log_actual[valid_mask], filtered_log_prediction[valid_mask], 1     )     x = np.array([-2, 4])     y = slope * x + intercept      plt.figure(figsize=(6, 6))      plt.plot(x, y, linestyle=\"--\", color=\"blue\", linewidth=0.8)     lims = [-2, 4]     plt.plot(lims, lims, linestyle=\"-\", color=\"black\", linewidth=0.8)      sns.scatterplot(x=log_actual, y=log_prediction, alpha=0.5)      sns.kdeplot(         x=filtered_log_actual,         y=filtered_log_prediction,         levels=3,         color=\"black\",         fill=False,         linewidths=0.8,     )      plt.xlabel(\"Actual Chl-a Values\", fontsize=16, fontname=\"Ubuntu\")     plt.ylabel(\"Predicted Chl-a Values\", fontsize=16, fontname=\"Ubuntu\")     plt.xlim(-2, 4)     plt.ylim(-2, 4)     plt.grid(True, which=\"both\", ls=\"--\")      plt.legend(         title=(             f\"MAE = {mae:.2f}, RMSE = {rmse:.2f}, RMSLE = {rmsle:.2f} \\n\"             f\"Bias = {bias:.2f}, Slope = {slope:.2f} \\n\"             f\"MAPE = {mape:.2f}%, \u03b5 = {epsilon:.2f}%, \u03b2 = {beta:.2f}%\"         ),         fontsize=16,         title_fontsize=12,         prop={\"family\": \"Ubuntu\"},     )      plt.xticks(fontsize=20, fontname=\"Ubuntu\")     plt.yticks(fontsize=20, fontname=\"Ubuntu\")      plt.savefig(os.path.join(save_dir, f\"{mode}_plot.pdf\"), bbox_inches=\"tight\")     plt.close()   def save_results_to_excel(ids, actuals, predictions, file_path, dates=None):      if dates is not None:         df = pd.DataFrame(             {\"ID\": ids, \"Date\": dates, \"Actual\": actuals, \"Predicted\": predictions}         )     else:         df = pd.DataFrame({\"ID\": ids, \"Actual\": actuals, \"Predicted\": predictions})      df.to_excel(file_path, index=False) <p>Part 2. In this section, we used the VAE deep learning algorithm to train a MSI-Chla model and evaluated its performance on both the training dataset and the validation dataset.</p> In\u00a0[\u00a0]: Copied! <pre>def train(model, train_dl, device, epochs=200, optimizer=None, save_dir=None):\n    model.train()\n\n    min_total_loss = float(\"inf\")\n    best_model_path = os.path.join(save_dir, \"best_model_minloss.pth\")\n\n    total_list = []\n    l1_list = []\n\n    for epoch in range(epochs):\n        total_loss_epoch = 0.0\n        l1_epoch = 0.0\n\n        for x, y in train_dl:\n            x, y = x.to(device), y.to(device)\n\n            y_pred, mu, log_var = model(x)\n            loss = loss_function(y_pred, y, mu, log_var)\n            l1 = loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss_epoch += loss.item()\n            l1_epoch += l1.item()\n\n        avg_total_loss = total_loss_epoch / len(train_dl)\n        avg_l1 = l1_epoch / len(train_dl)\n\n        print(f\"[Epoch {epoch+1}] Total: {avg_total_loss:.4f} | L1: {avg_l1:.4f}\")\n\n        total_list.append(avg_total_loss)\n        l1_list.append(avg_l1)\n\n        if avg_total_loss &lt; min_total_loss:\n            min_total_loss = avg_total_loss\n            torch.save(model.state_dict(), best_model_path)\n\n    return {\"total_loss\": total_list, \"l1_loss\": l1_list, \"best_loss\": min_total_loss}\n\n\ndef evaluate(model, test_dl, device):\n    model.eval()\n    predictions, actuals = [], []\n    with torch.no_grad():\n        for x, y in test_dl:\n            x, y = x.to(device), y.to(device)\n            y_pred, _, _ = model(x)\n            predictions.append(y_pred.cpu().numpy())\n            actuals.append(y.cpu().numpy())\n\n    predictions = np.vstack(predictions).flatten()\n    actuals = np.vstack(actuals).flatten()\n    predictions_inverse = np.power(10, predictions) - 1\n    actuals_inverse = np.power(10, actuals) - 1\n    return predictions_inverse, actuals_inverse\n\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    selected_bands = [443, 492, 560, 665, 704, 740]\n\n    train_real_dl, test_real_dl, input_dim, output_dim, train_ids, test_ids = (\n        load_real_data(\n            \"/home/data/20250727/data/Gloria_updated_07242025.xlsx\", selected_bands\n        )\n    )\n    test_dl1, _, _, test_ids1, test_dates = load_real_test(\n        \"/home/data/20250727/data/GreatLake_all_data.xlsx\", selected_bands\n    )\n    test_dl2, _, _, test_ids2, test_dates2 = load_real_test(\n        \"/home/data/20250727/data/GOA_insitu_data_07242025updated.xlsx\", selected_bands\n    )\n\n    save_dir = \"/home/data/20250727/vae/MSI01\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    model = VAE(input_dim, output_dim).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n\n    train_log = train(\n        model=model,\n        train_dl=train_real_dl,\n        device=device,\n        epochs=400,\n        optimizer=optimizer,\n        save_dir=save_dir,\n    )\n    best_train_loss = train_log[\"best_loss\"]\n\n    predictions, actuals = evaluate(model, test_real_dl, device)\n    epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(\n        predictions, actuals\n    )\n    test_loss = rmse\n\n    predictions_GL, actuals_GL = evaluate(model, test_dl1, device)\n    epsilon_GL, beta_GL, rmse_GL, rmsle_GL, mape_GL, bias_GL, mae_GL = (\n        calculate_metrics(predictions_GL, actuals_GL)\n    )\n    predictions_Field_work, actuals_Field_work = evaluate(model, test_dl2, device)\n\n    save_results_to_excel(\n        test_ids, actuals, predictions, os.path.join(save_dir, \"test.xlsx\")\n    )\n    save_results_to_excel(\n        test_ids1,\n        actuals_GL,\n        predictions_GL,\n        os.path.join(save_dir, \"GL.xlsx\"),\n        dates=test_dates,\n    )\n    save_results_to_excel(\n        test_ids2,\n        actuals_Field_work,\n        predictions_Field_work,\n        os.path.join(save_dir, \"GOA.xlsx\"),\n        dates=test_dates2,\n    )\n\n    train_Rrs, train_Chl = [], []\n    for x, y in train_real_dl:\n        train_Rrs.append(x.cpu().numpy())\n        train_Chl.append(y.cpu().numpy())\n    train_data = np.hstack((np.vstack(train_Rrs), np.vstack(train_Chl)))\n\n    test_Rrs, test_Chl = [], []\n    for x, y in test_real_dl:\n        test_Rrs.append(x.cpu().numpy())\n        test_Chl.append(y.cpu().numpy())\n    test_data = np.hstack((np.vstack(test_Rrs), np.vstack(test_Chl)))\n\n    np.savetxt(\n        os.path.join(save_dir, \"train_data.csv\"),\n        train_data,\n        delimiter=\",\",\n        header=\",\".join([f\"Rrs_{i}\" for i in range(input_dim)] + [\"Chla\"]),\n        comments=\"\",\n    )\n    np.savetxt(\n        os.path.join(save_dir, \"test_data.csv\"),\n        test_data,\n        delimiter=\",\",\n        header=\",\".join([f\"Rrs_{i}\" for i in range(input_dim)] + [\"Chla\"]),\n        comments=\"\",\n    )\n\n    plot_results(predictions, actuals, save_dir, mode=\"test\")\n    plot_results(predictions_GL, actuals_GL, save_dir, mode=\"GL\")\n    plot_results(predictions_Field_work, actuals_Field_work, save_dir, mode=\"GOA\")\n\n    print(\n        f\"Run completed with train loss: {best_train_loss:.4f}, test loss: {test_loss:.4f}\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</pre> def train(model, train_dl, device, epochs=200, optimizer=None, save_dir=None):     model.train()      min_total_loss = float(\"inf\")     best_model_path = os.path.join(save_dir, \"best_model_minloss.pth\")      total_list = []     l1_list = []      for epoch in range(epochs):         total_loss_epoch = 0.0         l1_epoch = 0.0          for x, y in train_dl:             x, y = x.to(device), y.to(device)              y_pred, mu, log_var = model(x)             loss = loss_function(y_pred, y, mu, log_var)             l1 = loss              optimizer.zero_grad()             loss.backward()             optimizer.step()              total_loss_epoch += loss.item()             l1_epoch += l1.item()          avg_total_loss = total_loss_epoch / len(train_dl)         avg_l1 = l1_epoch / len(train_dl)          print(f\"[Epoch {epoch+1}] Total: {avg_total_loss:.4f} | L1: {avg_l1:.4f}\")          total_list.append(avg_total_loss)         l1_list.append(avg_l1)          if avg_total_loss &lt; min_total_loss:             min_total_loss = avg_total_loss             torch.save(model.state_dict(), best_model_path)      return {\"total_loss\": total_list, \"l1_loss\": l1_list, \"best_loss\": min_total_loss}   def evaluate(model, test_dl, device):     model.eval()     predictions, actuals = [], []     with torch.no_grad():         for x, y in test_dl:             x, y = x.to(device), y.to(device)             y_pred, _, _ = model(x)             predictions.append(y_pred.cpu().numpy())             actuals.append(y.cpu().numpy())      predictions = np.vstack(predictions).flatten()     actuals = np.vstack(actuals).flatten()     predictions_inverse = np.power(10, predictions) - 1     actuals_inverse = np.power(10, actuals) - 1     return predictions_inverse, actuals_inverse   def main():     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     print(f\"Using device: {device}\")      selected_bands = [443, 492, 560, 665, 704, 740]      train_real_dl, test_real_dl, input_dim, output_dim, train_ids, test_ids = (         load_real_data(             \"/home/data/20250727/data/Gloria_updated_07242025.xlsx\", selected_bands         )     )     test_dl1, _, _, test_ids1, test_dates = load_real_test(         \"/home/data/20250727/data/GreatLake_all_data.xlsx\", selected_bands     )     test_dl2, _, _, test_ids2, test_dates2 = load_real_test(         \"/home/data/20250727/data/GOA_insitu_data_07242025updated.xlsx\", selected_bands     )      save_dir = \"/home/data/20250727/vae/MSI01\"     os.makedirs(save_dir, exist_ok=True)      model = VAE(input_dim, output_dim).to(device)     optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)      train_log = train(         model=model,         train_dl=train_real_dl,         device=device,         epochs=400,         optimizer=optimizer,         save_dir=save_dir,     )     best_train_loss = train_log[\"best_loss\"]      predictions, actuals = evaluate(model, test_real_dl, device)     epsilon, beta, rmse, rmsle, mape, bias, mae = calculate_metrics(         predictions, actuals     )     test_loss = rmse      predictions_GL, actuals_GL = evaluate(model, test_dl1, device)     epsilon_GL, beta_GL, rmse_GL, rmsle_GL, mape_GL, bias_GL, mae_GL = (         calculate_metrics(predictions_GL, actuals_GL)     )     predictions_Field_work, actuals_Field_work = evaluate(model, test_dl2, device)      save_results_to_excel(         test_ids, actuals, predictions, os.path.join(save_dir, \"test.xlsx\")     )     save_results_to_excel(         test_ids1,         actuals_GL,         predictions_GL,         os.path.join(save_dir, \"GL.xlsx\"),         dates=test_dates,     )     save_results_to_excel(         test_ids2,         actuals_Field_work,         predictions_Field_work,         os.path.join(save_dir, \"GOA.xlsx\"),         dates=test_dates2,     )      train_Rrs, train_Chl = [], []     for x, y in train_real_dl:         train_Rrs.append(x.cpu().numpy())         train_Chl.append(y.cpu().numpy())     train_data = np.hstack((np.vstack(train_Rrs), np.vstack(train_Chl)))      test_Rrs, test_Chl = [], []     for x, y in test_real_dl:         test_Rrs.append(x.cpu().numpy())         test_Chl.append(y.cpu().numpy())     test_data = np.hstack((np.vstack(test_Rrs), np.vstack(test_Chl)))      np.savetxt(         os.path.join(save_dir, \"train_data.csv\"),         train_data,         delimiter=\",\",         header=\",\".join([f\"Rrs_{i}\" for i in range(input_dim)] + [\"Chla\"]),         comments=\"\",     )     np.savetxt(         os.path.join(save_dir, \"test_data.csv\"),         test_data,         delimiter=\",\",         header=\",\".join([f\"Rrs_{i}\" for i in range(input_dim)] + [\"Chla\"]),         comments=\"\",     )      plot_results(predictions, actuals, save_dir, mode=\"test\")     plot_results(predictions_GL, actuals_GL, save_dir, mode=\"GL\")     plot_results(predictions_Field_work, actuals_Field_work, save_dir, mode=\"GOA\")      print(         f\"Run completed with train loss: {best_train_loss:.4f}, test loss: {test_loss:.4f}\"     )   if __name__ == \"__main__\":     main() <p>Part 3. We need to download the MSI Level-1 data, process it with ACOLITE to generate Level-2 water-leaving reflectance (L2W) products, and then perform model inference to generate chlorophyll-a (Chl-a) concentration maps.</p> In\u00a0[\u00a0]: Copied! <pre>input_path = \"/home/data/Chl-MSI/Harvey/Second/processed data/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.nc\"\noutput_path = (\n    \"/home/data/20250727/vae/MSI/0-100/run_7/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.npy\"\n)\nmodel_path = \"/home/data/20250727/vae/MSI/0-100/run_7/best_model_minloss.pth\"\n\noutput_dim = 1\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndataset = nc.Dataset(input_path)\nlatitude = dataset.variables[\"lat\"][:]\nlongitude = dataset.variables[\"lon\"][:]\n\nall_vars = dataset.variables.keys()\nbands_to_extract = [\n    f\"Rrs_{wavelength}\"\n    for wavelength in range(399, 755)\n    if f\"Rrs_{wavelength}\" in all_vars\n]\ninput_dim = len(bands_to_extract)\nif input_dim == 0:\n    raise ValueError(\"\u274c No valid bands found.\")\n\nmodel = VAE(input_dim, output_dim).to(device)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n\nfiltered_Rrs = np.array(\n    [dataset.variables[band][:] for band in bands_to_extract]\n)  # shape: (bands, H, W)\nfiltered_Rrs = np.moveaxis(filtered_Rrs, 0, -1)  # shape: (H, W, bands)\n\nmask = np.all(~np.isnan(filtered_Rrs), axis=2)\n\n\ndef find_closest_band(target, available_bands):\n    available_waves = [int(b.split(\"_\")[1]) for b in available_bands]\n    closest_wave = min(available_waves, key=lambda w: abs(w - target))\n    return f\"Rrs_{closest_wave}\"\n\n\ntarget_443 = (\n    \"Rrs_443\"\n    if \"Rrs_443\" in bands_to_extract\n    else find_closest_band(443, bands_to_extract)\n)\ntarget_560 = (\n    \"Rrs_560\"\n    if \"Rrs_560\" in bands_to_extract\n    else find_closest_band(560, bands_to_extract)\n)\n\nprint(f\"Using {target_443} and {target_560} for mask check.\")\n\nidx_443 = bands_to_extract.index(target_443)\nidx_560 = bands_to_extract.index(target_560)\nmask &amp;= filtered_Rrs[:, :, idx_443] &lt;= filtered_Rrs[:, :, idx_560]\n\n\nvalid_test_data = filtered_Rrs[mask]\nvalid_test_data = np.array(\n    [\n        MinMaxScaler(feature_range=(1, 10)).fit_transform(row.reshape(-1, 1)).flatten()\n        for row in valid_test_data\n    ]\n)\n\ntest_tensor = TensorDataset(torch.tensor(valid_test_data).float())\ntest_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)\n\npredictions_all = []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch[0].to(device)\n        predictions, _, _ = model(batch)\n        predictions = 10**predictions - 1\n        predictions_all.append(predictions.cpu().numpy())\n\npredictions_all = np.vstack(predictions_all)\nif predictions_all.shape[-1] == 1:\n    predictions_all = predictions_all.squeeze(-1)\n\noutputs = np.full((filtered_Rrs.shape[0], filtered_Rrs.shape[1]), np.nan)\noutputs[mask] = predictions_all\n\nlat_flat = latitude.flatten()\nlon_flat = longitude.flatten()\noutput_flat = outputs.flatten()\n\nfinal_output = np.column_stack((lat_flat, lon_flat, output_flat))\nif isinstance(final_output, np.ma.MaskedArray):\n    final_output = final_output.filled(np.nan)\n\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nnp.save(output_path, final_output)\nprint(f\"\u2705 Processed and saved to: {output_path}\")\n</pre> input_path = \"/home/data/Chl-MSI/Harvey/Second/processed data/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.nc\" output_path = (     \"/home/data/20250727/vae/MSI/0-100/run_7/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.npy\" ) model_path = \"/home/data/20250727/vae/MSI/0-100/run_7/best_model_minloss.pth\"  output_dim = 1 device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") dataset = nc.Dataset(input_path) latitude = dataset.variables[\"lat\"][:] longitude = dataset.variables[\"lon\"][:]  all_vars = dataset.variables.keys() bands_to_extract = [     f\"Rrs_{wavelength}\"     for wavelength in range(399, 755)     if f\"Rrs_{wavelength}\" in all_vars ] input_dim = len(bands_to_extract) if input_dim == 0:     raise ValueError(\"\u274c No valid bands found.\")  model = VAE(input_dim, output_dim).to(device) model.load_state_dict(torch.load(model_path, map_location=device)) model.eval()  filtered_Rrs = np.array(     [dataset.variables[band][:] for band in bands_to_extract] )  # shape: (bands, H, W) filtered_Rrs = np.moveaxis(filtered_Rrs, 0, -1)  # shape: (H, W, bands)  mask = np.all(~np.isnan(filtered_Rrs), axis=2)   def find_closest_band(target, available_bands):     available_waves = [int(b.split(\"_\")[1]) for b in available_bands]     closest_wave = min(available_waves, key=lambda w: abs(w - target))     return f\"Rrs_{closest_wave}\"   target_443 = (     \"Rrs_443\"     if \"Rrs_443\" in bands_to_extract     else find_closest_band(443, bands_to_extract) ) target_560 = (     \"Rrs_560\"     if \"Rrs_560\" in bands_to_extract     else find_closest_band(560, bands_to_extract) )  print(f\"Using {target_443} and {target_560} for mask check.\")  idx_443 = bands_to_extract.index(target_443) idx_560 = bands_to_extract.index(target_560) mask &amp;= filtered_Rrs[:, :, idx_443] &lt;= filtered_Rrs[:, :, idx_560]   valid_test_data = filtered_Rrs[mask] valid_test_data = np.array(     [         MinMaxScaler(feature_range=(1, 10)).fit_transform(row.reshape(-1, 1)).flatten()         for row in valid_test_data     ] )  test_tensor = TensorDataset(torch.tensor(valid_test_data).float()) test_loader = DataLoader(test_tensor, batch_size=2048, shuffle=False)  predictions_all = [] with torch.no_grad():     for batch in test_loader:         batch = batch[0].to(device)         predictions, _, _ = model(batch)         predictions = 10**predictions - 1         predictions_all.append(predictions.cpu().numpy())  predictions_all = np.vstack(predictions_all) if predictions_all.shape[-1] == 1:     predictions_all = predictions_all.squeeze(-1)  outputs = np.full((filtered_Rrs.shape[0], filtered_Rrs.shape[1]), np.nan) outputs[mask] = predictions_all  lat_flat = latitude.flatten() lon_flat = longitude.flatten() output_flat = outputs.flatten()  final_output = np.column_stack((lat_flat, lon_flat, output_flat)) if isinstance(final_output, np.ma.MaskedArray):     final_output = final_output.filled(np.nan)  os.makedirs(os.path.dirname(output_path), exist_ok=True) np.save(output_path, final_output) print(f\"\u2705 Processed and saved to: {output_path}\") <p>Part 4. We interpolate the Chl-a results saved in the .npy file generated in Part 3 and overlay them onto the RGB image from the ACOLITE-generated Level-2 Reflectance (L2R) file for visualization of the Chl-a distribution.</p> In\u00a0[\u00a0]: Copied! <pre>nc_file = \"/home/data/Chl-MSI/Harvey/Second/processed data/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2R.nc\"\nchla_data_file = (\n    \"/home/data/20250727/vae/MSI/0-100/run_7/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.npy\"\n)\n\ndataset = nc.Dataset(nc_file)\nlat = dataset.variables[\"lat\"][:]\nlon = dataset.variables[\"lon\"][:]\nheight, width = lat.shape\n\n\ndef find_closest_band(target, var_keys):\n    available = [v for v in var_keys if v.startswith(\"rhos_\")]\n    wavelengths = [int(v.split(\"_\")[1]) for v in available]\n    closest_wave = min(wavelengths, key=lambda w: abs(w - target))\n    return f\"rhos_{closest_wave}\"\n\n\nall_vars = dataset.variables.keys()\n\nR_band = \"rhos_665\" if \"rhos_665\" in all_vars else find_closest_band(665, all_vars)\nG_band = \"rhos_559\" if \"rhos_559\" in all_vars else find_closest_band(559, all_vars)\nB_band = \"rhos_492\" if \"rhos_492\" in all_vars else find_closest_band(492, all_vars)\nprint(f\"Using bands: R={R_band}, G={G_band}, B={B_band}\")\n\nR = dataset.variables[R_band][:]\nG = dataset.variables[G_band][:]\nB = dataset.variables[B_band][:]\n\nlat_flat = lat.flatten()\nlon_flat = lon.flatten()\nR_flat = R.flatten()\nG_flat = G.flatten()\nB_flat = B.flatten()\n\ngrid_lat = np.linspace(lat.min(), lat.max(), height)\ngrid_lon = np.linspace(lon.min(), lon.max(), width)\ngrid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)\n\nR_interp = griddata((lat_flat, lon_flat), R_flat, (grid_lat, grid_lon), method=\"linear\")\nG_interp = griddata((lat_flat, lon_flat), G_flat, (grid_lat, grid_lon), method=\"linear\")\nB_interp = griddata((lat_flat, lon_flat), B_flat, (grid_lat, grid_lon), method=\"linear\")\n\nrgb_image = np.stack((R_interp, G_interp, B_interp), axis=-1)\nrgb_image = rgb_image / np.nanmax(rgb_image)\nrgb_image = np.clip(rgb_image * 5.0, 0, 1)\n\nchla_data = np.load(chla_data_file)\nchla_lat = chla_data[:, 0]\nchla_lon = chla_data[:, 1]\nchla_values = chla_data[:, 2]\n\nchla_interp = griddata(\n    (chla_lat, chla_lon), chla_values, (grid_lat, grid_lon), method=\"nearest\"\n)\nchla_interp = np.ma.masked_invalid(chla_interp)\n\nrgb_valid = np.sum(rgb_image, axis=-1) &gt; 0\nmask = np.zeros_like(chla_interp, dtype=bool)\nmask[rgb_valid] = True\nchla_interp_masked = np.ma.masked_where(~mask, chla_interp)\n\nextent = [lon.min(), lon.max(), lat.min(), lat.max()]\nvmin, vmax = 0, 30\n\nplt.figure(figsize=(12, 8))\nplt.imshow(rgb_image, extent=extent, origin=\"lower\")\nim = plt.imshow(\n    chla_interp_masked,\n    extent=extent,\n    cmap=\"jet\",\n    alpha=1,\n    origin=\"lower\",\n    vmin=vmin,\n    vmax=vmax,\n)\ncbar = plt.colorbar(im)\ncbar.set_label(\"Chl-a (mg m$^{-3}$)\", fontsize=16)\nplt.title(\"Chl-a over RGB (Sentinel-2)\", fontsize=18)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.tight_layout()\nplt.show()\n</pre> nc_file = \"/home/data/Chl-MSI/Harvey/Second/processed data/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2R.nc\" chla_data_file = (     \"/home/data/20250727/vae/MSI/0-100/run_7/S2A_MSI_2017_09_01_17_05_34_T15RUN_L2W.npy\" )  dataset = nc.Dataset(nc_file) lat = dataset.variables[\"lat\"][:] lon = dataset.variables[\"lon\"][:] height, width = lat.shape   def find_closest_band(target, var_keys):     available = [v for v in var_keys if v.startswith(\"rhos_\")]     wavelengths = [int(v.split(\"_\")[1]) for v in available]     closest_wave = min(wavelengths, key=lambda w: abs(w - target))     return f\"rhos_{closest_wave}\"   all_vars = dataset.variables.keys()  R_band = \"rhos_665\" if \"rhos_665\" in all_vars else find_closest_band(665, all_vars) G_band = \"rhos_559\" if \"rhos_559\" in all_vars else find_closest_band(559, all_vars) B_band = \"rhos_492\" if \"rhos_492\" in all_vars else find_closest_band(492, all_vars) print(f\"Using bands: R={R_band}, G={G_band}, B={B_band}\")  R = dataset.variables[R_band][:] G = dataset.variables[G_band][:] B = dataset.variables[B_band][:]  lat_flat = lat.flatten() lon_flat = lon.flatten() R_flat = R.flatten() G_flat = G.flatten() B_flat = B.flatten()  grid_lat = np.linspace(lat.min(), lat.max(), height) grid_lon = np.linspace(lon.min(), lon.max(), width) grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)  R_interp = griddata((lat_flat, lon_flat), R_flat, (grid_lat, grid_lon), method=\"linear\") G_interp = griddata((lat_flat, lon_flat), G_flat, (grid_lat, grid_lon), method=\"linear\") B_interp = griddata((lat_flat, lon_flat), B_flat, (grid_lat, grid_lon), method=\"linear\")  rgb_image = np.stack((R_interp, G_interp, B_interp), axis=-1) rgb_image = rgb_image / np.nanmax(rgb_image) rgb_image = np.clip(rgb_image * 5.0, 0, 1)  chla_data = np.load(chla_data_file) chla_lat = chla_data[:, 0] chla_lon = chla_data[:, 1] chla_values = chla_data[:, 2]  chla_interp = griddata(     (chla_lat, chla_lon), chla_values, (grid_lat, grid_lon), method=\"nearest\" ) chla_interp = np.ma.masked_invalid(chla_interp)  rgb_valid = np.sum(rgb_image, axis=-1) &gt; 0 mask = np.zeros_like(chla_interp, dtype=bool) mask[rgb_valid] = True chla_interp_masked = np.ma.masked_where(~mask, chla_interp)  extent = [lon.min(), lon.max(), lat.min(), lat.max()] vmin, vmax = 0, 30  plt.figure(figsize=(12, 8)) plt.imshow(rgb_image, extent=extent, origin=\"lower\") im = plt.imshow(     chla_interp_masked,     extent=extent,     cmap=\"jet\",     alpha=1,     origin=\"lower\",     vmin=vmin,     vmax=vmax, ) cbar = plt.colorbar(im) cbar.set_label(\"Chl-a (mg m$^{-3}$)\", fontsize=16) plt.title(\"Chl-a over RGB (Sentinel-2)\", fontsize=18) plt.xlabel(\"Longitude\") plt.ylabel(\"Latitude\") plt.tight_layout() plt.show()"},{"location":"workshops/emit/","title":"Emit","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" <p>Import library.</p> In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, run the following cell and enter your NASA Earthdata login credentials.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_emit(\n    bbox=(-83, 25, -81, 28),\n    temporal=(\"2024-04-01\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_emit(     bbox=(-83, 25, -81, 28),     temporal=(\"2024-04-01\", \"2024-05-16\"),     count=10,  # use -1 to return all datasets     return_gdf=True, ) <p>Plot the footprints of the returned datasets on a map.</p> In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p>Uncomment the following cell to download the first dataset from the search results. Note that the download may take some time.</p> In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_emit(results[:1], out_dir=\"data\")\n</pre> # hypercoast.download_emit(results[:1], out_dir=\"data\") <p>Search for EMIT data interactively. Specify pan and zoom to the area of interest. Specify the time range of interest from the search dialog, then click on the Search button.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[30.0262, -90.1345], zoom=8)\nm.search_emit()\nm\n</pre> m = hypercoast.Map(center=[30.0262, -90.1345], zoom=8) m.search_emit() m <p>Uncomment the following cell to display the GeoDataFrame of the search results.</p> In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() <p>Similarly, you can download the first dataset from the search results by uncommenting the following cell.</p> In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_emit(results[:1], out_dir=\"data\")\n</pre> # hypercoast.download_emit(results[:1], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nfilepath = \"../examples/data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nhypercoast.download_file(url, filepath, quiet=True)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" filepath = \"../examples/data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" hypercoast.download_file(url, filepath, quiet=True) In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_emit(filepath)\n# dataset\n</pre> dataset = hypercoast.read_emit(filepath) # dataset <p></p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"SATELLITE\") m.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\") m.add(\"spectral\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>ds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451))\n</pre> ds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451)) <p>Visualize the EMIT data in 3D with an RGB image overlaid on top of the 3D plot.</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.4),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n)\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.4),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\", ) <p>Uncomment the following cell to create an image cube. Note that this function does not work in the Google Colab environment.</p> In\u00a0[\u00a0]: Copied! <pre># p.show()\n</pre> # p.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>ds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93))\n</pre> ds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93)) <p>Drag the plane up and down to slice the data in 3D.</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\",     widget=\"plane\", ) p.add_text(\"Band slicing\", position=\"upper_right\", font_size=14) <p>Uncomment the following cell to display the interactive slicing widget. Note that this function does not work in the Google Colab environment.</p> In\u00a0[\u00a0]: Copied! <pre># p.show()\n</pre> # p.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\",     widget=\"threshold\", ) p.add_text(\"Thresholding\", position=\"upper_right\", font_size=14) <p>Uncomment the following cell to display the thresholded data. Note that this function does not work in the Google Colab environment.</p> In\u00a0[\u00a0]: Copied! <pre># p.show()\n</pre> # p.show() <p></p>"},{"location":"workshops/emit/#working-with-nasa-emit-data-in-hypercoast","title":"Working with NASA EMIT data in HyperCoast\u00b6","text":"<p>This notebook demonstrates how to work with NASA Earth Surface Mineral Dust Source Investigation (EMIT) data in HyperCoast.</p>"},{"location":"workshops/emit/#environment-setup","title":"Environment setup\u00b6","text":"<p>Uncomment and run the following cell to install the required packages.</p>"},{"location":"workshops/emit/#search-for-emit-data","title":"Search for EMIT data\u00b6","text":"<p>Search for EMIT data programmatically. Specify the bounding box and time range of interest. Set <code>count=-1</code> to return all results or set <code>count=10</code> to return the first 10 results.</p>"},{"location":"workshops/emit/#download-a-sample-emit-dataset","title":"Download a sample EMIT dataset\u00b6","text":"<p>Let's download a sample EMIT dataset for the demonstration.</p>"},{"location":"workshops/emit/#read-emit-data","title":"Read EMIT data\u00b6","text":"<p>Read the downloaded EMIT data and process it as an <code>xarray.Dataset</code>. Note that the dataset has 285 bands.</p>"},{"location":"workshops/emit/#visualize-emit-data","title":"Visualize EMIT data\u00b6","text":"<p>Visualize the EMIT data on an interactive map. You can change the band combination and extract spectral profiles interactively. You can also export the spectral profiles as a CSV file.</p>"},{"location":"workshops/emit/#create-an-image-cube","title":"Create an image cube\u00b6","text":"<p>First, select a subset of the data to avoid nodata areas.</p>"},{"location":"workshops/emit/#interactive-slicing","title":"Interactive slicing\u00b6","text":"<p>First, select a subset of the data for demonstration purposes.</p>"},{"location":"workshops/emit/#interactive-thresholding","title":"Interactive thresholding\u00b6","text":"<p>Drag the threshold slider to threshold the data in 3D.</p>"},{"location":"workshops/pace/","title":"Pace","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" <p>Import libraries.</p> In\u00a0[\u00a0]: Copied! <pre>import earthaccess\nimport hypercoast\nimport pandas as pd\n</pre> import earthaccess import hypercoast import pandas as pd In\u00a0[\u00a0]: Copied! <pre>earthaccess.login(persist=True)\n</pre> earthaccess.login(persist=True) In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_pace(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-07-30\", \"2024-08-15\"),\n    short_name=\"PACE_OCI_L2_AOP_NRT\",\n    count=10,\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_pace(     bounding_box=(-83, 25, -81, 28),     temporal=(\"2024-07-30\", \"2024-08-15\"),     short_name=\"PACE_OCI_L2_AOP_NRT\",     count=10,     return_gdf=True, ) <p>Plot the footprints of the returned datasets on a map.</p> In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p>Download the first dataset from the search results. Note that the download may take some time.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_pace(results[:1], out_dir=\"data\")\n</pre> hypercoast.download_pace(results[:1], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[30.0262, -90.1345], zoom=8)\nm.search_pace(default_dataset=\"PACE_OCI_L2_AOP_NRT\")\nm\n</pre> m = hypercoast.Map(center=[30.0262, -90.1345], zoom=8) m.search_pace(default_dataset=\"PACE_OCI_L2_AOP_NRT\") m <p>By default, the <code>search_pace</code> method searches for the <code>PACE_OCI_L2_AOP_NRT</code> dataset, but you can specify the dataset name by setting the <code>default_dataset</code> parameter, such as <code>PACE_OCI_L2_BGC_NRT</code>. For more information about the available datasets, see the PACE Data Products page.</p> <p></p> <p>Uncomment the following cell to display the GeoDataFrame of the search results.</p> In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() <p>Similarly, you can download the first dataset from the search results by uncommenting the following cell.</p> In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_pace(results[:1], out_dir=\"data\")\n</pre> # hypercoast.download_pace(results[:1], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>results = hypercoast.search_pace(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-07-30\", \"2024-08-15\"),\n    short_name=\"PACE_OCI_L2_AOP_NRT\",\n    count=1,\n)\n</pre> results = hypercoast.search_pace(     bounding_box=(-83, 25, -81, 28),     temporal=(\"2024-07-30\", \"2024-08-15\"),     short_name=\"PACE_OCI_L2_AOP_NRT\",     count=1, ) In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_pace(results[:1], out_dir=\"data\")\n</pre> hypercoast.download_pace(results[:1], out_dir=\"data\") <p>Let's make a scatter plot of the pixel locations so we can see the irregular spacing.</p> In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nplot = hypercoast.view_pace_pixel_locations(filepath, step=20)\n</pre> filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" plot = hypercoast.view_pace_pixel_locations(filepath, step=20) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_pace(filepath)\n# dataset\n</pre> dataset = hypercoast.read_pace(filepath) # dataset <p></p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2)\n</pre> hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2) <p>Add custom projection and administrative boundaries to the map. The default projection is <code>PlateCarree</code>. You can specify a custom projection by setting the <code>crs</code> parameter. For more information about the available projections, see the cartopy projection page.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2, crs=\"default\")\n</pre> hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2, crs=\"default\") In\u00a0[\u00a0]: Copied! <pre>latitude = 29.9307\nlongitude = -87.9106\nhypercoast.extract_pace(dataset, latitude, longitude, return_plot=True)\n</pre> latitude = 29.9307 longitude = -87.9106 hypercoast.extract_pace(dataset, latitude, longitude, return_plot=True) <p>To return the extracted values as an xarray <code>DataArray</code>, set <code>return_plot=False</code>.</p> In\u00a0[\u00a0]: Copied! <pre>array = hypercoast.extract_pace(dataset, latitude, longitude, return_plot=False)\n# array\n</pre> array = hypercoast.extract_pace(dataset, latitude, longitude, return_plot=False) # array <p>To plot the spectral signatures of multiple pixels, you can specify the pixel locations as a list of tuples. All pixels within the specified latitude and longitude range will be extracted.</p> In\u00a0[\u00a0]: Copied! <pre>latitude = (29.49, 29.50)\nlongitude = (-88.10, -88.00)\nhypercoast.filter_pace(dataset, latitude, longitude, return_plot=True)\n</pre> latitude = (29.49, 29.50) longitude = (-88.10, -88.00) hypercoast.filter_pace(dataset, latitude, longitude, return_plot=True) <p>Visualize a selected band of the dataset interactively use the <code>add_pace</code> method and speccify the <code>wavelengths</code> parameter.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450]\nm.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\")\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\")\nm.add(\"spectral\")\nm.set_center(-80.7382, 26.5295, zoom=6)\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") wavelengths = [450] m.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\") m.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\") m.add(\"spectral\") m.set_center(-80.7382, 26.5295, zoom=6) m <p>Click on the map to display the spectral signature of the selected pixel.</p> <p></p> <p>Convert the spectral data of the selected pixels to a DataFrame.</p> In\u00a0[\u00a0]: Copied! <pre>df = m.spectral_to_df()\ndf.head()\n</pre> df = m.spectral_to_df() df.head() <p>Convert the spectral data of the selected pixels to a GeoDataFrame.</p> In\u00a0[\u00a0]: Copied! <pre>gdf = m.spectral_to_gdf()\ngdf.head()\n</pre> gdf = m.spectral_to_gdf() gdf.head() <p>Convert the spectral data of the selected pixels to a CSV file.</p> In\u00a0[\u00a0]: Copied! <pre>m.spectral_to_csv(\"data/spectral.csv\")\n</pre> m.spectral_to_csv(\"data/spectral.csv\") In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450, 550, 650]\nm.add_pace(\n    dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\"\n)\nm.add(\"spectral\")\nm.set_center(-80.7382, 26.5295, zoom=6)\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") wavelengths = [450, 550, 650] m.add_pace(     dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\" ) m.add(\"spectral\") m.set_center(-80.7382, 26.5295, zoom=6) m <p></p> In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    bbox=(-90.5642, 29.9749, -89.7143, 30.42),\n    temporal=(\"2024-07-30\", \"2024-08-15\"),\n    count=1,\n    return_gdf=True,\n)\nhypercoast.download_nasa_data(results, out_dir=\"data\")\n</pre> results, gdf = hypercoast.search_nasa_data(     short_name=\"PACE_OCI_L2_BGC_NRT\",     bbox=(-90.5642, 29.9749, -89.7143, 30.42),     temporal=(\"2024-07-30\", \"2024-08-15\"),     count=1,     return_gdf=True, ) hypercoast.download_nasa_data(results, out_dir=\"data\") <p>Load the downloaded dataset as an <code>xarray.Dataset</code>:</p> In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/PACE_OCI.20240730T181157.L2.OC_BGC.V2_0.NRT.nc\"\ndataset = hypercoast.read_pace_bgc(filepath)\n</pre> filepath = \"data/PACE_OCI.20240730T181157.L2.OC_BGC.V2_0.NRT.nc\" dataset = hypercoast.read_pace_bgc(filepath) <p>Let's inspect the data variables contained in the dataset:</p> In\u00a0[\u00a0]: Copied! <pre>dataset.variables\n</pre> dataset.variables <p>We can see that the dataset contains the following variables:</p> <ul> <li>Chlorophyll Concentration</li> <li>Phytoplankton Carbon</li> <li>Particulate Organic Carbon</li> </ul> In\u00a0[\u00a0]: Copied! <pre>chlor_a = hypercoast.grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"linear\")\n</pre> chlor_a = hypercoast.grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"linear\") <p>Plot the gridded Chlorophyll Concentration data:</p> In\u00a0[\u00a0]: Copied! <pre>chlor_a.plot(vmin=0, vmax=20, cmap=\"jet\", size=6)\n</pre> chlor_a.plot(vmin=0, vmax=20, cmap=\"jet\", size=6) <p>Plot the gridded Phytoplankton Carbon data:</p> In\u00a0[\u00a0]: Copied! <pre>carbon_phyto = hypercoast.grid_pace_bgc(\n    dataset, variable=\"carbon_phyto\", method=\"linear\"\n)\ncarbon_phyto.plot(vmin=0, vmax=120, cmap=\"jet\", size=6)\n</pre> carbon_phyto = hypercoast.grid_pace_bgc(     dataset, variable=\"carbon_phyto\", method=\"linear\" ) carbon_phyto.plot(vmin=0, vmax=120, cmap=\"jet\", size=6) <p>Plot the gridded Particulate Organic Carbon data:</p> In\u00a0[\u00a0]: Copied! <pre>poc = hypercoast.grid_pace_bgc(dataset, variable=\"poc\", method=\"linear\")\npoc.plot(vmin=0, vmax=1000, cmap=\"jet\")\n</pre> poc = hypercoast.grid_pace_bgc(dataset, variable=\"poc\", method=\"linear\") poc.plot(vmin=0, vmax=1000, cmap=\"jet\") <p>Plot the gridded BGC data on an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_raster(chlor_a, layer_name=\"Chlorophyll-a\", colormap=\"jet\", vmin=0, vmax=20)\nm.add_raster(\n    carbon_phyto, layer_name=\"Phytoplankton Carbon\", colormap=\"plasma\", vmin=0, vmax=120\n)\nm.add_raster(\n    poc, layer_name=\"Particulate Organic Carbon\", colormap=\"coolwarm\", vmin=0, vmax=1000\n)\nm.add_layer_manager()\n\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=20, label=\"Chlorophyll-a (mg/m3)\")\nm.add_colormap(cmap=\"plasma\", vmin=0, vmax=120, label=\"Phytoplankton Carbon (mg/m3)\")\nm.add_colormap(\n    cmap=\"coolwarm\", vmin=0, vmax=1000, label=\"Particulate Organic Carbon (mg/m3)\"\n)\nm.set_center(-80.7382, 26.5295, zoom=6)\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_raster(chlor_a, layer_name=\"Chlorophyll-a\", colormap=\"jet\", vmin=0, vmax=20) m.add_raster(     carbon_phyto, layer_name=\"Phytoplankton Carbon\", colormap=\"plasma\", vmin=0, vmax=120 ) m.add_raster(     poc, layer_name=\"Particulate Organic Carbon\", colormap=\"coolwarm\", vmin=0, vmax=1000 ) m.add_layer_manager()  m.add_colormap(cmap=\"jet\", vmin=0, vmax=20, label=\"Chlorophyll-a (mg/m3)\") m.add_colormap(cmap=\"plasma\", vmin=0, vmax=120, label=\"Phytoplankton Carbon (mg/m3)\") m.add_colormap(     cmap=\"coolwarm\", vmin=0, vmax=1000, label=\"Particulate Organic Carbon (mg/m3)\" ) m.set_center(-80.7382, 26.5295, zoom=6) m <p></p> In\u00a0[\u00a0]: Copied! <pre>temporal = (\"2024-07-30\", \"2024-08-15\")\nresults = hypercoast.search_pace_chla(temporal=temporal)\nhypercoast.download_nasa_data(results, \"chla\")\n</pre> temporal = (\"2024-07-30\", \"2024-08-15\") results = hypercoast.search_pace_chla(temporal=temporal) hypercoast.download_nasa_data(results, \"chla\") <p>The downloaded datasets can be found in the <code>chla</code> directory, which contains 17 daily files of CCI data in the netCDF format. The date range of the data is from 2024-07-30 to 2024-08-15.</p> In\u00a0[\u00a0]: Copied! <pre>files = \"chla/*nc\"\n</pre> files = \"chla/*nc\" <p>Load all the data files in the <code>chla</code> directory as an xarray DataArray</p> In\u00a0[\u00a0]: Copied! <pre>array = hypercoast.read_pace_chla(files)\n# array\n</pre> array = hypercoast.read_pace_chla(files) # array <p></p> <p>Select a date and visualize the chlorophyll-a concentration data with Matplotlib.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.viz_pace_chla(array, date=\"2024-07-30\", cmap=\"jet\", size=6)\n</pre> hypercoast.viz_pace_chla(array, date=\"2024-07-30\", cmap=\"jet\", size=6) <p>If the date is not specified, the data are averaged over the entire time range.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.viz_pace_chla(array, cmap=\"jet\", size=6)\n</pre> hypercoast.viz_pace_chla(array, cmap=\"jet\", size=6) <p>To visualize the data interactively, we can select either a single date or aggregate the data over a time range.</p> <p>First, let's select a single date from the data array:</p> In\u00a0[\u00a0]: Copied! <pre>single_array = array.sel(date=\"2024-07-30\")\n# single_array\n</pre> single_array = array.sel(date=\"2024-07-30\") # single_array <p>Convert the data array to an image that can be displayed on an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>single_image = hypercoast.pace_chla_to_image(single_array)\n</pre> single_image = hypercoast.pace_chla_to_image(single_array) <p>Create an interactive map and display the image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[40, -100], zoom=4)\nm.add_basemap(\"Hybrid\")\nm.add_raster(\n    single_image,\n    cmap=\"jet\",\n    vmin=-1,\n    vmax=2,\n    layer_name=\"Chlorophyll a\",\n    zoom_to_layer=False,\n)\nlabel = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\"\nm.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label)\nm\n</pre> m = hypercoast.Map(center=[40, -100], zoom=4) m.add_basemap(\"Hybrid\") m.add_raster(     single_image,     cmap=\"jet\",     vmin=-1,     vmax=2,     layer_name=\"Chlorophyll a\",     zoom_to_layer=False, ) label = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\" m.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label) m <p></p> <p>The daily image does not have a global coverage. To visualize the data globally, we can aggregate the data over a time range.</p> In\u00a0[\u00a0]: Copied! <pre>mean_array = array.mean(dim=\"date\")\n</pre> mean_array = array.mean(dim=\"date\") <p>Convert the aggregated data array to an image that can be displayed on an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>image = hypercoast.pace_chla_to_image(mean_array)\n</pre> image = hypercoast.pace_chla_to_image(mean_array) <p>Create an interactive map and display the image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[40, -100], zoom=4)\nm.add_basemap(\"Hybrid\")\nm.add_raster(\n    image, cmap=\"jet\", vmin=-1, vmax=2, layer_name=\"Chlorophyll a\", zoom_to_layer=False\n)\nlabel = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\"\nm.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label)\nm\n</pre> m = hypercoast.Map(center=[40, -100], zoom=4) m.add_basemap(\"Hybrid\") m.add_raster(     image, cmap=\"jet\", vmin=-1, vmax=2, layer_name=\"Chlorophyll a\", zoom_to_layer=False ) label = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\" m.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/Hypoxia_Data_Sheet.xlsx\"\nxls_path = \"data/Hypoxia_Data_Sheet.xlsx\"\nhypercoast.download_file(url, xls_path, overwrite=True)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/Hypoxia_Data_Sheet.xlsx\" xls_path = \"data/Hypoxia_Data_Sheet.xlsx\" hypercoast.download_file(url, xls_path, overwrite=True) In\u00a0[\u00a0]: Copied! <pre>df = pd.read_excel(xls_path)\ndf.head()\n</pre> df = pd.read_excel(xls_path) df.head() <p>Filter the data to select only the sampling locations with latitude and longitude coordinates.</p> In\u00a0[\u00a0]: Copied! <pre>df_filtered = df.dropna(subset=[\"Lon\", \"Lat\"]).reset_index(drop=True)\ndf_filtered.head()\n</pre> df_filtered = df.dropna(subset=[\"Lon\", \"Lat\"]).reset_index(drop=True) df_filtered.head() <p>Download the KML file containing the cruise path.</p> In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://github.com/opengeos/datasets/releases/download/hypercoast/Hypoxia_Path.kml\"\n)\nkml_path = \"data/Hypoxia_Path.kml\"\nhypercoast.download_file(url, kml_path)\n</pre> url = (     \"https://github.com/opengeos/datasets/releases/download/hypercoast/Hypoxia_Path.kml\" ) kml_path = \"data/Hypoxia_Path.kml\" hypercoast.download_file(url, kml_path) <p>We will use the PACE AOP dataset acquired on July 30, 2024, to visualize the cruise sampling locations. The dataset should have been downloaded in the previous section.</p> In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\n</pre> filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" <p>Read the PACE AOP dataset as an xarray Dataset.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_pace(filepath)\n# dataset\n</pre> dataset = hypercoast.read_pace(filepath) # dataset <p>Visualize the cruise sampling locations and PACE data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450, 550, 650]\nm.add_pace(\n    dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\"\n)\nm.add(\"spectral\")\nstyle = {\"weight\": 2, \"color\": \"red\"}\nm.add_kml(kml_path, style=style, layer_name=\"Hypoxia Path\", info_mode=None)\nm.add_points_from_xy(\n    df_filtered,\n    x=\"Lon\",\n    y=\"Lat\",\n    max_cluster_radius=50,\n    layer_name=\"Hypoxia Data Points\",\n)\nm.set_center(-91.46118, 28.89758, zoom=8)\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") wavelengths = [450, 550, 650] m.add_pace(     dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\" ) m.add(\"spectral\") style = {\"weight\": 2, \"color\": \"red\"} m.add_kml(kml_path, style=style, layer_name=\"Hypoxia Path\", info_mode=None) m.add_points_from_xy(     df_filtered,     x=\"Lon\",     y=\"Lat\",     max_cluster_radius=50,     layer_name=\"Hypoxia Data Points\", ) m.set_center(-91.46118, 28.89758, zoom=8) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_sample_points.csv\"\ndata = pd.read_csv(url)\ndata.head()\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_sample_points.csv\" data = pd.read_csv(url) data.head() <p>Again, we will use the PACE AOP dataset acquired on July 30, 2024, to visualize the in-situ data. The dataset should have been downloaded in the previous section.</p> In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\n</pre> filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" <p>Read the PACE dataset as an xarray Dataset.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_pace(filepath)\n</pre> dataset = hypercoast.read_pace(filepath) <p>Visualize the in-situ data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[27.235094, -87.791748], zoom=6)\n\nm.add_basemap(\"Hybrid\")\nwavelengths = [450]\nm.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\")\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\")\nm.add(\"spectral\")\n\nm.add_field_data(\n    data,\n    x_col=\"wavelength\",\n    y_col_prefix=\"(\",\n    x_label=\"Wavelength (nm)\",\n    y_label=\"Reflectance\",\n    use_marker_cluster=True,\n)\nm.set_center(-87.791748, 27.235094, zoom=6)\nm\n</pre> m = hypercoast.Map(center=[27.235094, -87.791748], zoom=6)  m.add_basemap(\"Hybrid\") wavelengths = [450] m.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\") m.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\") m.add(\"spectral\")  m.add_field_data(     data,     x_col=\"wavelength\",     y_col_prefix=\"(\",     x_label=\"Wavelength (nm)\",     y_label=\"Reflectance\",     use_marker_cluster=True, ) m.set_center(-87.791748, 27.235094, zoom=6) m <p>Click on any marker to display the in-situ data.</p> <p></p>"},{"location":"workshops/pace/#working-with-nasa-pace-data-in-hypercoast","title":"Working with NASA PACE data in HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize and analyze Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) data interactively with HyperCoast.</p>"},{"location":"workshops/pace/#environment-setup","title":"Environment setup\u00b6","text":"<p>Uncomment and run the following cell to install the required packages.</p>"},{"location":"workshops/pace/#search-for-pace-data","title":"Search for PACE data\u00b6","text":"<p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, run the following cell and enter your NASA Earthdata login credentials.</p>"},{"location":"workshops/pace/#search-data-programmatically","title":"Search data programmatically\u00b6","text":"<p>To search for PACE data programmatically, specify the bounding box and time range of interest. Set <code>count=-1</code> to return all results or set <code>count=10</code> to return the first 10 results.</p>"},{"location":"workshops/pace/#search-data-interactively","title":"Search data interactively\u00b6","text":"<p>To search for PACE data interactively, pan and zoom to the area of interest. Specify the time range of interest from the search dialog, then click on the Search button.</p>"},{"location":"workshops/pace/#read-pace-data","title":"Read PACE data\u00b6","text":"<p>Let's download a sample PACE Apparent Optical Properties (AOP) dataset for the demonstration.</p>"},{"location":"workshops/pace/#visualize-pace-aop-data","title":"Visualize PACE AOP data\u00b6","text":"<p>Visualize selected bands of the dataset.</p>"},{"location":"workshops/pace/#plot-spectral-signatures","title":"Plot spectral signatures\u00b6","text":"<p>Plot the spectral signature of a pixel using the <code>extract_pace</code> function. Set <code>return_plot=True</code> to return the plot object.</p>"},{"location":"workshops/pace/#interactive-visualization","title":"Interactive visualization\u00b6","text":""},{"location":"workshops/pace/#single-band-visualization","title":"Single-band visualization\u00b6","text":""},{"location":"workshops/pace/#multi-band-visualization","title":"Multi-band visualization\u00b6","text":"<p>Select three spectral bands to visualize as an RGB image.</p>"},{"location":"workshops/pace/#change-band-combination","title":"Change band combination\u00b6","text":"<p>Click on the gear icon on the toolbar to change the band combination.</p> <p></p>"},{"location":"workshops/pace/#pace-bgc-data","title":"PACE BGC data\u00b6","text":"<p>PACE has a variety of data products, including biogeochemical properties. For more information about the available datasets, see the PACE Data Products page.</p> <p>The PACE Biogeochemical (BGC) data products include chlorophyll-a concentration, particulate organic carbon, and particulate inorganic carbon.</p>"},{"location":"workshops/pace/#download-pace-bgc-data","title":"Download PACE BGC data\u00b6","text":"<p>Let's download a sample PACE BGC dataset for the demonstration.</p>"},{"location":"workshops/pace/#visualize-pace-bgc-data","title":"Visualize PACE BGC data\u00b6","text":"<p>Since the datasets are not gridded, we need to transform them into gridded data to visualize them. We can use the <code>grid_pace_bgc</code> function to transform the dataset into a gridded format.</p> <p>First, transform the <code>chlor_a</code> variable into a gridded format:</p>"},{"location":"workshops/pace/#pace-chlorophyll-level-3-data","title":"PACE Chlorophyll Level 3 data\u00b6","text":"<p>PACE Level 3 data products are gridded data products that are derived from Level 2 data. Once of the most common Level 3 data products is the Chlorophyll-Carotenoid Index (CCI) dataset.</p> <p>Let's download some daily PACE Chlorophyll Level 3 data for the demonstration.</p>"},{"location":"workshops/pace/#hypoxia-cruise-data","title":"Hypoxia Cruise data\u00b6","text":"<p>The Hypoxia Cruise collected water quality data in the Gulf of Mexico from July 21 to August 2, 2024. In this section, we will visualize the cruise sampling locations.</p> <p>First, let's download an Excel file containing the cruise sampling locations.</p>"},{"location":"workshops/pace/#visualize-in-situ-data","title":"Visualize in-situ data\u00b6","text":"<p>This section demonstrates how to visualize in-situ data on the map. First, let's download a hypothetical in-situ dataset.</p>"},{"location":"workshops/pace/#analyze-pace-data","title":"Analyze PACE data\u00b6","text":"<p>To anyalyze the PACE data with algorithms, such as K-means clustering, principal component analysis (PCA), or Spectral Angle Mapper (SAM), follow the notebook at https://hypercoast.org/examples/pace_cyano.</p>"}]}