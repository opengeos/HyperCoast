{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to HyperCoast","text":"<p>A Python Package for Visualizing and Analyzing Hyperspectral Data in Coastal Environments</p> <ul> <li>Free software: MIT License</li> <li>Documentation: https://hypercoast.org</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>HyperCoast is a Python package designed to provide an accessible and comprehensive set of tools for visualizing and analyzing hyperspectral data in coastal environments. Hyperspectral data refers to the information collected by sensors that capture light across a wide range of wavelengths, beyond what the human eye can see. This data allows scientists to detect and analyze various materials and conditions on the Earth's surface with great detail. Unlike multispectral data, which captures light in a limited number of broad wavelength bands (typically 3 to 10), hyperspectral data captures light in many narrow, contiguous wavelength bands, often numbering in the hundreds. This provides much more detailed spectral information. Leveraging the capabilities of popular packages like Leafmap and PyVista, HyperCoast streamlines the exploration and interpretation of complex hyperspectral remote sensing data from existing spaceborne and airborne missions. It is also poised to support future hyperspectral missions, such as NASA's SBG and GLIMR. It enables researchers and environmental managers to gain deeper insights into the dynamic processes occurring in aquatic environments.</p> <p>HyperCoast supports the reading and visualization of hyperspectral data from various missions, including AVIRIS, NEON, PACE, EMIT, and DESIS, along with other datasets like ECOSTRESS. Users can interactively explore hyperspectral data, extract spectral signatures, change band combinations and colormaps, visualize data in 3D, and perform interactive slicing and thresholding operations (see Figure 1). Additionally, by leveraging the earthaccess package, HyperCoast provides tools for interactively searching NASA's hyperspectral data. This makes HyperCoast a versatile and powerful tool for working with hyperspectral data globally, with a particular focus on coastal regions.</p> <p> Figure 1. An example of visualizing NASA EMIT hyperspectral data using HyperCoast.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Searching for NASA hyperspectral data interactively</li> <li>Performing atmospheric correction using Acolite</li> <li>Interactive visualization and analysis of hyperspectral data, such as AVIRIS, DESIS, EMIT, PACE, NEON AOP</li> <li>Interactive visualization of NASA ECOSTRESS data</li> <li>Interactive visualization of PACE chlorophyll-a data</li> <li>Interactive extraction and visualization of spectral signatures</li> <li>Changing band combinations and colormaps interactively</li> <li>Visualizing hyperspectral data in 3D</li> <li>Visualizing ERA5 temperature data in 3D</li> <li>Interactive slicing and thresholding of hyperspectral data in 3D</li> <li>Saving spectral signatures as CSV files</li> </ul>"},{"location":"#demos","title":"Demos","text":"<ul> <li>Visualizing hyperspectral data in 3D (notebook)</li> </ul> <ul> <li>Interactive slicing of hyperspectral data in 3D (notebook)</li> </ul> <ul> <li>Interactive thresholding of hyperspectral data in 3D (notebook)</li> </ul> <ul> <li>Visualizing ERA5 temperature data in 3D (notebook)</li> </ul> <ul> <li>Changing band combinations and colormaps interactively (notebook)</li> </ul> <ul> <li>Visualizing NASA AVIRIS hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Visualizing DESIS hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Visualizing NASA EMIT hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Visualizing NASA PACE hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Visualizing NEON AOP hyperspectral data interactively (notebook)</li> </ul> <ul> <li>Interactive visualization of PACE chlorophyll-a data (notebook)</li> </ul>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>The HyperCoast project draws inspiration from the nasa/EMIT-Data-Resources repository. Credits to the original authors. We also acknowledge the NASA EMIT program support through grant no. 80NSSC24K0865.</p>"},{"location":"#license","title":"License","text":"<p>HyperCoast is released under the MIT License. However, some of the modules in HyperCoast adapt code from other open-source projects, which may have different licenses. Please refer to the license notice in each module for more information. Credits to the original authors.</p> <ul> <li>emit.py: Part of the code is adapted from the nasa/EMIT-Data-Resources repository, which is released under the Apache License 2.0.</li> <li>aviris.py: Part of the code is adapted from the jjmcnelis/aviris-ng-notebooks, which is released under the MIT License.</li> </ul>"},{"location":"aviris/","title":"aviris module","text":"<p>This module contains functions to read and process NASA AVIRIS hyperspectral data. More info about the data can be found at https://aviris.jpl.nasa.gov. A portion of the source code is adapted from the jjmcnelis/aviris-ng-notebooks repository available at https://bit.ly/4bRCgqs. It is licensed under the MIT License. Credit goes to the original author Jack McNelis.</p> <p>SPDX-FileCopyrightText = [     \"2024 Jack McNelis jjmcne@gmail.com\", ] SPDX-License-Identifier = \"MIT\"</p>"},{"location":"aviris/#hypercoast.aviris.aviris_to_image","title":"<code>aviris_to_image(dataset, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts an AVIRIS dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The dataset containing the AVIRIS data or the file path to the dataset.</p> required <code>wavelengths</code> <code>np.ndarray</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[rasterio.Dataset]</code> <p>The image converted from the dataset. If     <code>output</code> is provided, the image will be saved to the specified file     and the function will return None.</p> Source code in <code>hypercoast/aviris.py</code> <pre><code>def aviris_to_image(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Optional[np.ndarray] = None,\n    method: str = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Converts an AVIRIS dataset to an image.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The dataset containing the AVIRIS data\n            or the file path to the dataset.\n        wavelengths (np.ndarray, optional): The specific wavelengths to select. If None, all\n            wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n        output (str, optional): The file path where the image will be saved. If\n            None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs (Any): Additional keyword arguments to be passed to\n            `leafmap.array_to_image`.\n\n    Returns:\n        Optional[rasterio.Dataset]: The image converted from the dataset. If\n            `output` is provided, the image will be saved to the specified file\n            and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_aviris(dataset, method=method)\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n\n    return array_to_image(\n        dataset[\"reflectance\"],\n        output=output,\n        transpose=False,\n        dtype=np.float32,\n        **kwargs,\n    )\n</code></pre>"},{"location":"aviris/#hypercoast.aviris.extract_aviris","title":"<code>extract_aviris(dataset, lat, lon, offset=2.0)</code>","text":"<p>Extracts AVIRIS data from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xarray.Dataset</code> <p>The dataset containing the AVIRIS data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <code>offset</code> <code>float</code> <p>The offset from the point to extract. Defaults to 2.0.</p> <code>2.0</code> <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted data.</p> Source code in <code>hypercoast/aviris.py</code> <pre><code>def extract_aviris(\n    dataset: xr.Dataset, lat: float, lon: float, offset: float = 2.0\n) -&gt; xr.DataArray:\n    \"\"\"\n    Extracts AVIRIS data from a given xarray Dataset.\n\n    Args:\n        dataset (xarray.Dataset): The dataset containing the AVIRIS data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n        offset (float, optional): The offset from the point to extract. Defaults to 2.0.\n\n    Returns:\n        xarray.DataArray: The extracted data.\n    \"\"\"\n\n    crs = dataset.attrs[\"crs\"]\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    da = dataset[\"reflectance\"]\n\n    x_con = (da[\"xc\"] &gt; x - offset) &amp; (da[\"xc\"] &lt; x + offset)\n    y_con = (da[\"yc\"] &gt; y - offset) &amp; (da[\"yc\"] &lt; y + offset)\n\n    try:\n        data = da.where(x_con &amp; y_con, drop=True)\n        data = data.mean(dim=[\"x\", \"y\"])\n    except ValueError:\n        data = np.nan * np.ones(da.sizes[\"wavelength\"])\n\n    da = xr.DataArray(\n        data, dims=[\"wavelength\"], coords={\"wavelength\": dataset.coords[\"wavelength\"]}\n    )\n\n    return da\n</code></pre>"},{"location":"aviris/#hypercoast.aviris.read_aviris","title":"<code>read_aviris(filepath, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Reads NASA AVIRIS hyperspectral data and returns an xarray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the AVIRIS data.</p> required <code>wavelengths</code> <code>List[float]</code> <p>The wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the selection method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The dataset containing the reflectance data.</p> Source code in <code>hypercoast/aviris.py</code> <pre><code>def read_aviris(\n    filepath: str,\n    wavelengths: Optional[List[float]] = None,\n    method: str = \"nearest\",\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads NASA AVIRIS hyperspectral data and returns an xarray dataset.\n\n    Args:\n        filepath (str): The path to the AVIRIS data.\n        wavelengths (List[float], optional): The wavelengths to select. If None,\n            all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for selection. Defaults to\n            \"nearest\".\n        **kwargs (Any): Additional arguments to pass to the selection method.\n\n    Returns:\n        xr.Dataset: The dataset containing the reflectance data.\n    \"\"\"\n\n    if filepath.endswith(\".hdr\"):\n        filepath = filepath.replace(\".hdr\", \"\")\n\n    ds = xr.open_dataset(filepath, engine=\"rasterio\")\n\n    wavelength = ds[\"wavelength\"].values.tolist()\n    wavelength = [round(num, 2) for num in wavelength]\n\n    cols = ds.x.size\n    rows = ds.y.size\n\n    rio_transform = ds.rio.transform()\n    geo_transform = list(rio_transform)[:6]\n\n    # get the raster geotransform as its component parts\n    xres, _, xmin, _, yres, ymax = geo_transform\n\n    # generate coordinate arrays\n    xarr = np.array([xmin + i * xres for i in range(0, cols)])\n    yarr = np.array([ymax + i * yres for i in range(0, rows)])\n\n    ds[\"y\"] = xr.DataArray(\n        data=yarr,\n        dims=(\"y\"),\n        name=\"y\",\n        attrs=dict(\n            units=\"m\",\n            standard_name=\"projection_y_coordinate\",\n            long_name=\"y coordinate of projection\",\n        ),\n    )\n\n    ds[\"x\"] = xr.DataArray(\n        data=xarr,\n        dims=(\"x\"),\n        name=\"x\",\n        attrs=dict(\n            units=\"m\",\n            standard_name=\"projection_x_coordinate\",\n            long_name=\"x coordinate of projection\",\n        ),\n    )\n\n    global_atts = ds.attrs\n    global_atts[\"Conventions\"] = \"CF-1.6\"\n    ds.attrs = dict(\n        units=\"unitless\",\n        _FillValue=-9999,\n        grid_mapping=\"crs\",\n        standard_name=\"reflectance\",\n        long_name=\"atmospherically corrected surface reflectance\",\n    )\n    ds.attrs.update(global_atts)\n\n    ds = ds.transpose(\"y\", \"x\", \"band\")\n    ds = ds.drop_vars([\"wavelength\"])\n    ds = ds.rename({\"band\": \"wavelength\", \"band_data\": \"reflectance\"})\n    ds.coords[\"wavelength\"] = wavelength\n    ds.attrs[\"crs\"] = ds.rio.crs.to_string()\n    ds.rio.write_transform(rio_transform)\n\n    if wavelengths is not None:\n        ds = ds.sel(wavelength=wavelengths, method=method, **kwargs)\n    return ds\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v076-aug-07-2024","title":"v0.7.6 - Aug 07, 2024","text":"<p>What's Changed</p> <ul> <li>[pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in #107</li> <li>Clean up file path in notebook by @giswqs in #108</li> <li>Fix read_pace bug for V2 by @giswqs in #109</li> </ul> <p>Full Changelog: v0.7.5...v0.7.6</p>"},{"location":"changelog/#v075-aug-05-2024","title":"v0.7.5 - Aug 05, 2024","text":"<p>What's Changed</p> <ul> <li>ci(Mergify): configuration update by @slowy07 in #99</li> <li>Add support for PCA by @giswqs in #100</li> <li>Update mergify by @giswqs in #101</li> <li>Add support for displaying field data by @giswqs in #106</li> </ul> <p>Full Changelog: v0.7.4...v0.7.5</p>"},{"location":"changelog/#v074-aug-01-2024","title":"v0.7.4 - Aug 01, 2024","text":"<p>What's Changed</p> <ul> <li>Remove AVIRIS image bounds by @giswqs in #96</li> <li>Fix read_pace bug for V2 by @giswqs in #97</li> </ul> <p>Full Changelog: v0.7.3...v0.7.4</p>"},{"location":"changelog/#v073-jul-30-2024","title":"v0.7.3 - Jul 30, 2024","text":"<p>What's Changed</p> <ul> <li>Add reuse license management framework by @giswqs in #83</li> <li>Add reuse dep and file header by @giswqs in #84</li> <li>Pin reuse version by @giswqs in #85</li> <li>Decapitalize Pringle et al title by @giswqs in #89</li> <li>Remove sentence and citations by @giswqs in #90</li> <li>Explain hyperspectral and other jargon by @giswqs in #91</li> <li>Add xlim parameter for the spectral widget by @giswqs in #93</li> </ul> <p>Full Changelog: v0.7.2...v0.7.3</p>"},{"location":"changelog/#v072-jul-23-2024","title":"v0.7.2 - Jul 23, 2024","text":"<p>What's Changed</p> <ul> <li>Update paper by @bingqing-liu in #74</li> <li>Update pyvista dependencies by @giswqs in #75</li> <li>chore: improving testing and typehinting by @slowy07 in #77</li> <li>Add support for generic dataset by @giswqs in #78</li> <li>Clarify license info by @giswqs in #79</li> <li>Add multispectral notebook example by @giswqs in #80</li> <li>Add support for acolite by @giswqs in #81</li> </ul> <p>New Contributors</p> <ul> <li>@slowy07 made their first contribution in #77</li> </ul> <p>Full Changelog: v0.7.0...v0.7.2</p>"},{"location":"changelog/#v071-jul-20-2024","title":"v0.7.1 - Jul 20, 2024","text":"<p>What's Changed</p> <ul> <li>Update paper by @bingqing-liu in #74</li> <li>Update pyvista dependencies by @giswqs in #75</li> <li>chore: improving testing and typehinting by @slowy07 in #77</li> <li>Add support for generic dataset by @giswqs in #78</li> <li>Clarify license info by @giswqs in #79</li> </ul> <p>New Contributors</p> <ul> <li>@slowy07 made their first contribution in #77</li> </ul> <p>Full Changelog: v0.7.0...v0.7.1</p>"},{"location":"changelog/#v070-jul-08-2024","title":"v0.7.0 - Jul 08, 2024","text":"<p>What's Changed</p> <ul> <li>Improve the pace chla function by @giswqs in #70</li> <li>Add JOSS paper draft by @giswqs in #71</li> <li>Add unittests by @giswqs in #72</li> <li>Add EMIT workshop notebook by @giswqs in #73</li> </ul> <p>Full Changelog: v0.6.3...v0.7.0</p>"},{"location":"changelog/#v063-jul-01-2024","title":"v0.6.3 - Jul 01, 2024","text":"<p>What's Changed</p> <ul> <li>Update pace_chla_to_image_function by @giswqs in #69</li> </ul> <p>Full Changelog: v0.6.2...v0.6.3</p>"},{"location":"changelog/#v062-jun-30-2024","title":"v0.6.2 - Jun 30, 2024","text":"<p>What's Changed</p> <ul> <li>Add search_datasets function and improve notebook example by @giswqs in #66</li> <li>Add PACE OCI Level-1 notebook example by @giswqs in #67</li> <li>Add PACE pixel location function by @giswqs in #68</li> </ul> <p>Full Changelog: v0.6.1...v0.6.2</p>"},{"location":"changelog/#v061-jun-30-2024","title":"v0.6.1 - Jun 30, 2024","text":"<p>What's Changed</p> <ul> <li>Add more demos to docs by @giswqs in #63</li> <li>Fix typos by @giswqs in #64</li> <li>Add support for reading PACE OCI L2 data by @giswqs in #65</li> </ul> <p>Full Changelog: v0.6.0...v0.6.1</p>"},{"location":"changelog/#v060-jun-26-2024","title":"v0.6.0 - Jun 26, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for PACE Chlorophyll data by @giswqs in #62</li> </ul> <p>Full Changelog: v0.5.5...v0.6.0</p>"},{"location":"changelog/#v055-jun-25-2024","title":"v0.5.5 - Jun 25, 2024","text":"<p>What's Changed</p> <ul> <li>Add image slicing demos to docs by @giswqs in #56</li> <li>Add dependabot by @giswqs in #58</li> <li>Bump nwtgck/actions-netlify from 2.0 to 3.0 by @dependabot in #59</li> <li>Bump conda-incubator/setup-miniconda from 2 to 3 by @dependabot in #60</li> <li>Add ERA5 temperature data notebook by @giswqs in #61</li> </ul> <p>New Contributors</p> <ul> <li>@dependabot made their first contribution in #59</li> </ul> <p>Full Changelog: v0.5.4...v0.5.5</p>"},{"location":"changelog/#v054-jun-14-2024","title":"v0.5.4 - Jun 14, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for interactive slicing by @giswqs in #54</li> <li>Add image slicing notebook example by @giswqs in #55</li> </ul> <p>Full Changelog: v0.5.3...v0.5.4</p>"},{"location":"changelog/#v053-jun-12-2024","title":"v0.5.3 - Jun 12, 2024","text":"<p>What's Changed</p> <ul> <li>Add EMIT image cube example by @giswqs in #51</li> <li>Refactor read_neon() function to support generalized NEON data reading by @gponce-ars in #52</li> </ul> <p>New Contributors</p> <ul> <li>@gponce-ars made their first contribution in #52</li> </ul> <p>Full Changelog: v0.5.2...v0.5.3</p>"},{"location":"changelog/#v052-jun-11-2024","title":"v0.5.2 - Jun 11, 2024","text":"<p>What's Changed</p> <ul> <li>Fix RGB image reshape bug for image cube by @giswqs in #49</li> </ul> <p>Full Changelog: v0.5.1...v0.5.2</p>"},{"location":"changelog/#v051-jun-11-2024","title":"v0.5.1 - Jun 11, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for 3D visualization by @giswqs in #47</li> <li>Add image cube demo to docs by @giswqs in #48</li> </ul> <p>Full Changelog: v0.5.0...v0.5.1</p>"},{"location":"changelog/#v050-jun-10-2024","title":"v0.5.0 - Jun 10, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for NEON AOP data by @giswqs in #43</li> <li>Add support for AVIRIS data by @giswqs in #44</li> <li>Add support for changing band combinations and colormaps interactively by @giswqs in #46</li> </ul> <p>Full Changelog: v0.4.0...v0.5.0</p>"},{"location":"changelog/#v040-jun-03-2024","title":"v0.4.0 - Jun 03, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for searching and downloading ECOSTRESS data by @giswqs in #37</li> <li>Add support for visualizing DESIS hyperspectral data by @giswqs in #38</li> </ul> <p>Full Changelog: v0.3.3...v0.4.0</p>"},{"location":"changelog/#v033-may-19-2024","title":"v0.3.3 - May 19, 2024","text":"<p>What's Changed</p> <ul> <li>Add functions for searching PACE and EMIT data by @giswqs in #34</li> <li>Add stacking for spectral signatures by @giswqs in #35</li> </ul> <p>Full Changelog: v0.3.2...v0.3.3</p>"},{"location":"changelog/#v032-may-10-2024","title":"v0.3.2 - May 10, 2024","text":"<p>What's Changed</p> <ul> <li>Fix wavelength bug by @giswqs in #27</li> <li>Pin xarray version by @giswqs in #28</li> <li>Fix xr merge error by @giswqs in #29</li> </ul> <p>Full Changelog: v0.3.1...v0.3.2</p>"},{"location":"changelog/#v031-may-09-2024","title":"v0.3.1 - May 09, 2024","text":"<p>Full Changelog: v0.3.0...v0.3.1</p>"},{"location":"changelog/#v030-may-08-2024","title":"v0.3.0 - May 08, 2024","text":"<p>What's Changed</p> <ul> <li>Add filter_pace function by @giswqs in #21</li> <li>Add grid pace function by @giswqs in #22</li> <li>Add fetch depth for docs by @giswqs in #23</li> <li>Add support for multi-band visualization by @giswqs in #24</li> <li>Add extract_pace function by @giswqs in #25</li> <li>Add spectral signature viz for PACE data by @giswqs in #26</li> </ul> <p>Full Changelog: v0.2.0...v0.3.0</p>"},{"location":"changelog/#v020-may-05-2024","title":"v0.2.0 - May 05, 2024","text":"<p>What's Changed</p> <ul> <li>Add support for reading and visualizing PACE data by @giswqs in #20</li> </ul> <p>Full Changelog: v0.1.3...v0.2.0</p>"},{"location":"changelog/#v013-apr-30-2024","title":"v0.1.3 - Apr 30, 2024","text":"<p>What's Changed</p> <ul> <li>Move hvplot import into functions by @giswqs in #18</li> <li>[pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in #17</li> <li>Add pace module by @giswqs in #19</li> </ul> <p>Full Changelog: v0.1.2...v0.1.3</p>"},{"location":"changelog/#v012-apr-25-2024","title":"v0.1.2 - Apr 25, 2024","text":"<p>What's Changed</p> <ul> <li>Add demo gif by @bingqing-liu in #15</li> <li>Fix typos and add method bug by @giswqs in #16</li> </ul> <p>Full Changelog: v0.1.1...v0.1.2</p>"},{"location":"changelog/#v011-apr-22-2024","title":"v0.1.1 - Apr 22, 2024","text":"<p>What's Changed</p> <ul> <li>Add an EMIT notebook by @giswqs in #13</li> <li>Fix dependency issue by @giswqs in #14</li> </ul> <p>Full Changelog: v0.1.0...v0.1.1</p>"},{"location":"changelog/#v010-apr-21-2024","title":"v0.1.0 - Apr 21, 2024","text":"<p>What's Changed</p> <ul> <li>Improve support for visualizing EMIT data by @giswqs in #10</li> <li>Add ui module by @giswqs in #11</li> <li>Add support for displaying spectral signature interactively by @giswqs in #12</li> </ul> <p>Full Changelog: v0.0.3...v0.1.0</p>"},{"location":"changelog/#v003-apr-20-2024","title":"v0.0.3 - Apr 20, 2024","text":"<p>What's Changed</p> <ul> <li>Fix typos by @giswqs in #7</li> <li>Add conda-forge installation instructions by @giswqs in #8</li> <li>Add support for visualizing EMIT data by @giswqs in #9</li> </ul> <p>Full Changelog: v0.0.2...v0.0.3</p>"},{"location":"changelog/#v002-apr-18-2024","title":"v0.0.2 - Apr 18, 2024","text":"<p>What's Changed</p> <ul> <li>[pre-commit.ci] pre-commit autoupdate by @pre-commit-ci in #4</li> <li>Update readme by @Bingqing9027 in #5</li> <li>Add Map class by @giswqs in #6</li> </ul> <p>New Contributors</p> <ul> <li>@pre-commit-ci made their first contribution in #4</li> <li>@Bingqing9027 made their first contribution in #5</li> <li>@giswqs made their first contribution in #6</li> </ul> <p>Full Changelog: v0.0.1...v0.0.2</p>"},{"location":"changelog/#v001-apr-08-2024","title":"v0.0.1 - Apr 08, 2024","text":"<p>What's Changed Full Changelog: v0.0.1</p>"},{"location":"changelog_update/","title":"Changelog update","text":"In\u00a0[1]: Copied! <pre>import re\n</pre> import re In\u00a0[2]: Copied! <pre># Copy the release notes from the GitHub release page\nmarkdown_text = \"\"\"\n## What's Changed\n* Update pace_chla_to_image_function by @giswqs in https://github.com/opengeos/HyperCoast/pull/69\n\n\n**Full Changelog**: https://github.com/opengeos/HyperCoast/compare/v0.6.2...v0.6.3\n\"\"\"\n</pre> # Copy the release notes from the GitHub release page markdown_text = \"\"\" ## What's Changed * Update pace_chla_to_image_function by @giswqs in https://github.com/opengeos/HyperCoast/pull/69   **Full Changelog**: https://github.com/opengeos/HyperCoast/compare/v0.6.2...v0.6.3 \"\"\" In\u00a0[3]: Copied! <pre># Regular expression pattern to match the Markdown hyperlinks\npattern = r\"https://github\\.com/opengeos/HyperCoast/pull/(\\d+)\"\n</pre> # Regular expression pattern to match the Markdown hyperlinks pattern = r\"https://github\\.com/opengeos/HyperCoast/pull/(\\d+)\" In\u00a0[4]: Copied! <pre># Function to replace matched URLs with the desired format\ndef replace_url(match):\n    pr_number = match.group(1)\n    return f\"[#{pr_number}](https://github.com/opengeos/HyperCoast/pull/{pr_number})\"\n</pre> # Function to replace matched URLs with the desired format def replace_url(match):     pr_number = match.group(1)     return f\"[#{pr_number}](https://github.com/opengeos/HyperCoast/pull/{pr_number})\" In\u00a0[5]: Copied! <pre># Use re.sub to replace URLs with the desired format\nformatted_text = re.sub(pattern, replace_url, markdown_text)\n</pre> # Use re.sub to replace URLs with the desired format formatted_text = re.sub(pattern, replace_url, markdown_text) In\u00a0[6]: Copied! <pre>for line in formatted_text.splitlines():\n    if \"Full Changelog\" in line:\n        prefix = line.split(\": \")[0]\n        link = line.split(\": \")[1]\n        version = line.split(\"/\")[-1]\n        formatted_text = (\n            formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")\n            .replace(\"## What's Changed\", \"**What's Changed**\")\n            .replace(\"## New Contributors\", \"**New Contributors**\")\n        )\n</pre> for line in formatted_text.splitlines():     if \"Full Changelog\" in line:         prefix = line.split(\": \")[0]         link = line.split(\": \")[1]         version = line.split(\"/\")[-1]         formatted_text = (             formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")             .replace(\"## What's Changed\", \"**What's Changed**\")             .replace(\"## New Contributors\", \"**New Contributors**\")         ) In\u00a0[7]: Copied! <pre>with open(\"docs/changelog_update.md\", \"w\") as f:\n    f.write(formatted_text)\n</pre> with open(\"docs/changelog_update.md\", \"w\") as f:     f.write(formatted_text) In\u00a0[8]: Copied! <pre># Print the formatted text\nprint(formatted_text)\n</pre> # Print the formatted text print(formatted_text) <pre>\n**What's Changed**\n* Update pace_chla_to_image_function by @giswqs in [#69](https://github.com/opengeos/HyperCoast/pull/69)\n\n\n**Full Changelog**: [v0.6.2...v0.6.3](https://github.com/opengeos/HyperCoast/compare/v0.6.2...v0.6.3)\n\n</pre> <p>Copy the formatted text and paste it to the CHANGELOG.md file</p>"},{"location":"cla/","title":"Fiduciary License Agreement 2.0","text":"<p>based on the</p>"},{"location":"cla/#individual-contributor-exclusive-license-agreement","title":"Individual Contributor Exclusive License Agreement","text":""},{"location":"cla/#including-the-traditional-patent-license-option","title":"(including the Traditional Patent License OPTION)","text":"<p>Thank you for your interest in contributing to Open Geospatial Solutions's HyperCoast (\"We\" or \"Us\").</p> <p>The purpose of this contributor agreement (\"Agreement\") is to clarify and document the rights granted by contributors to Us. To make this document effective, please follow the instructions at https://github.com/opengeos/HyperCoast.</p>"},{"location":"cla/#0-preamble","title":"0. Preamble","text":"<p>Software is deeply embedded in all aspects of our lives and it is important that it empower, rather than restrict us. Free Software gives everybody the rights to use, understand, adapt and share software. These rights help support other fundamental freedoms like freedom of speech, press and privacy.</p> <p>Development of Free Software can follow many patterns. In some cases whole development is handled by a sole programmer or a small group of people. But usually, the creation and maintenance of software is a complex process that requires the contribution of many individuals. This also affects who owns the rights to the software. In the latter case, rights in software are owned jointly by a great number of individuals.</p> <p>To tackle this issue some projects require a full copyright assignment to be signed by all contributors. The problem with such assignments is that they often lack checks and balances that would protect the contributors from potential abuse of power from the new copyright holder.</p> <p>FSFE\u2019s Fiduciary License Agreement (FLA) was created by the Free Software Foundation Europe e.V. with just that in mind \u2013 to concentrate all deciding power within one entity and prevent fragmentation of rights on one hand, while on the other preventing that single entity from abusing its power. The main aim is to ensure that the software covered under the FLA will forever remain Free Software.</p> <p>This process only serves for the transfer of economic rights. So-called moral rights (e.g. authors right to be identified as author) remain with the original author(s) and are inalienable.</p>"},{"location":"cla/#how-to-use-this-fla","title":"How to use this FLA","text":"<p>If You are an employee and have created the Contribution as part of your employment, You need to have Your employer approve this Agreement or sign the Entity version of this document. If You do not own the Copyright in the entire work of authorship, any other author of the Contribution should also sign this \u2013 in any event, please contact Us at opengeos@outlook.com</p>"},{"location":"cla/#1-definitions","title":"1. Definitions","text":"<p>\"You\" means the individual Copyright owner who Submits a Contribution to Us.</p> <p>\"Legal Entity\" means an entity that is not a natural person.</p> <p>\"Affiliate\" means any other Legal Entity that controls, is controlled by, or under common control with that Legal Entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such Legal Entity, whether by contract or otherwise, (ii) ownership of fifty percent (50%) or more of the outstanding shares or securities that vote to elect the management or other persons who direct such Legal Entity or (iii) beneficial ownership of such entity.</p> <p>\"Contribution\" means any original work of authorship, including any original modifications or additions to an existing work of authorship, Submitted by You to Us, in which You own the Copyright.</p> <p>\"Copyright\" means all rights protecting works of authorship, including copyright, moral and neighboring rights, as appropriate, for the full term of their existence.</p> <p>\"Material\" means the software or documentation made available by Us to third parties. When this Agreement covers more than one software project, the Material means the software or documentation to which the Contribution was Submitted. After You Submit the Contribution, it may be included in the Material.</p> <p>\"Submit\" means any act by which a Contribution is transferred to Us by You by means of tangible or intangible media, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Us, but excluding any transfer that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"</p> <p>\"Documentation\" means any non-software portion of a Contribution.</p>"},{"location":"cla/#2-license-grant","title":"2. License grant","text":""},{"location":"cla/#21-copyright-license-to-us","title":"2.1 Copyright license to Us","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Us a worldwide, royalty-free, exclusive, perpetual and irrevocable (except as stated in Section 8.2) license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul>"},{"location":"cla/#22-moral-rights","title":"2.2 Moral rights","text":"<p>Moral Rights remain unaffected to the extent they are recognized and not waivable by applicable law. Notwithstanding, You may add your name to the attribution mechanism customary used in the Materials you Contribute to, such as the header of the source code files of Your Contribution, and We will respect this attribution when using Your Contribution.</p>"},{"location":"cla/#23-copyright-license-back-to-you","title":"2.3 Copyright license back to You","text":"<p>Upon such grant of rights to Us, We immediately grant to You a worldwide, royalty-free, non-exclusive, perpetual and irrevocable license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul> <p>This license back is limited to the Contribution and does not provide any rights to the Material.</p>"},{"location":"cla/#3-patents","title":"3. Patents","text":""},{"location":"cla/#31-patent-license","title":"3.1 Patent license","text":"<p>Subject to the terms and conditions of this Agreement You hereby grant to Us and to recipients of Materials distributed by Us a worldwide, royalty-free, non-exclusive, perpetual and irrevocable (except as stated in Section 3.2) patent license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, to make, have made, use, sell, offer for sale, import and otherwise transfer the Contribution and the Contribution in combination with any Material (and portions of such combination). This license applies to all patents owned or controlled by You, whether already acquired or hereafter acquired, that would be infringed by making, having made, using, selling, offering for sale, importing or otherwise transferring of Your Contribution(s) alone or by combination of Your Contribution(s) with any Material.</p>"},{"location":"cla/#32-revocation-of-patent-license","title":"3.2 Revocation of patent license","text":"<p>You reserve the right to revoke the patent license stated in section 3.1 if We make any infringement claim that is targeted at your Contribution and not asserted for a Defensive Purpose. An assertion of claims of the Patents shall be considered for a \"Defensive Purpose\" if the claims are asserted against an entity that has filed, maintained, threatened, or voluntarily participated in a patent infringement lawsuit against Us or any of Our licensees.</p>"},{"location":"cla/#4-license-obligations-by-us","title":"4. License obligations by Us","text":"<p>We agree to (sub)license the Contribution or any Materials containing, based on or derived from your Contribution under the terms of any licenses the Free Software Foundation classifies as Free Software License and which are approved by the Open Source Initiative as Open Source licenses.</p> <p>We agree to license patents owned or controlled by You only to the extent necessary to (sub)license Your Contribution(s) and the combination of Your Contribution(s) with the Material under the terms of any licenses the Free Software Foundation classifies as Free Software licenses and which are approved by the Open Source Initiative as Open Source licenses..</p>"},{"location":"cla/#5-disclaimer","title":"5. Disclaimer","text":"<p>THE CONTRIBUTION IS PROVIDED \"AS IS\". MORE PARTICULARLY, ALL EXPRESS OR IMPLIED WARRANTIES INCLUDING, WITHOUT LIMITATION, ANY IMPLIED WARRANTY OF SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT ARE EXPRESSLY DISCLAIMED BY YOU TO US AND BY US TO YOU. TO THE EXTENT THAT ANY SUCH WARRANTIES CANNOT BE DISCLAIMED, SUCH WARRANTY IS LIMITED IN DURATION AND EXTENT TO THE MINIMUM PERIOD AND EXTENT PERMITTED BY LAW.</p>"},{"location":"cla/#6-consequential-damage-waiver","title":"6. Consequential damage waiver","text":"<p>TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL YOU OR WE BE LIABLE FOR ANY LOSS OF PROFITS, LOSS OF ANTICIPATED SAVINGS, LOSS OF DATA, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL AND EXEMPLARY DAMAGES ARISING OUT OF THIS AGREEMENT REGARDLESS OF THE LEGAL OR EQUITABLE THEORY (CONTRACT, TORT OR OTHERWISE) UPON WHICH THE CLAIM IS BASED.</p>"},{"location":"cla/#7-approximation-of-disclaimer-and-damage-waiver","title":"7. Approximation of disclaimer and damage waiver","text":"<p>IF THE DISCLAIMER AND DAMAGE WAIVER MENTIONED IN SECTION 5. AND SECTION 6. CANNOT BE GIVEN LEGAL EFFECT UNDER APPLICABLE LOCAL LAW, REVIEWING COURTS SHALL APPLY LOCAL LAW THAT MOST CLOSELY APPROXIMATES AN ABSOLUTE WAIVER OF ALL CIVIL OR CONTRACTUAL LIABILITY IN CONNECTION WITH THE CONTRIBUTION.</p>"},{"location":"cla/#8-term","title":"8. Term","text":"<p>8.1 This Agreement shall come into effect upon Your acceptance of the terms and conditions.</p> <p>8.2 This Agreement shall apply for the term of the copyright and patents licensed here. However, You shall have the right to terminate the Agreement if We do not fulfill the obligations as set forth in Section 4. Such termination must be made in writing.</p> <p>8.3 In the event of a termination of this Agreement Sections 5., 6., 7., 8., and 9. shall survive such termination and shall remain in full force thereafter. For the avoidance of doubt, Free and Open Source Software (sub)licenses that have already been granted for Contributions at the date of the termination shall remain in full force after the termination of this Agreement.</p>"},{"location":"cla/#9-miscellaneous","title":"9. Miscellaneous","text":"<p>9.1 This Agreement and all disputes, claims, actions, suits or other proceedings arising out of this agreement or relating in any way to it shall be governed by the laws of United States excluding its private international law provisions.</p> <p>9.2 This Agreement sets out the entire agreement between You and Us for Your Contributions to Us and overrides all other agreements or understandings.</p> <p>9.3 In case of Your death, this agreement shall continue with Your heirs. In case of more than one heir, all heirs must exercise their rights through a commonly authorized person.</p> <p>9.4 If any provision of this Agreement is found void and unenforceable, such provision will be replaced to the extent possible with a provision that comes closest to the meaning of the original provision and that is enforceable. The terms and conditions set forth in this Agreement shall apply notwithstanding any failure of essential purpose of this Agreement or any limited remedy to the maximum extent possible under law.</p> <p>9.5 You agree to notify Us of any facts or circumstances of which you become aware that would make this Agreement inaccurate in any respect.</p>"},{"location":"cla/#recreate-this-contributor-license-agreement","title":"Recreate this Contributor License Agreement","text":"<p>https://contributoragreements.org/u2s/2kw48zbt5d</p> <p></p> <p></p>"},{"location":"cla/#fiduciary-license-agreement-20_1","title":"Fiduciary License Agreement 2.0","text":"<p>based on the</p>"},{"location":"cla/#entity-contributor-exclusive-license-agreement","title":"Entity Contributor Exclusive License Agreement","text":""},{"location":"cla/#including-the-traditional-patent-license-option_1","title":"(including the Traditional Patent License OPTION)","text":"<p>Thank you for your interest in contributing to Open Geospatial Solutions's HyperCoast (\"We\" or \"Us\").</p> <p>The purpose of this contributor agreement (\"Agreement\") is to clarify and document the rights granted by contributors to Us. To make this document effective, please follow the instructions at https://github.com/opengeos/HyperCoast.</p>"},{"location":"cla/#0-preamble_1","title":"0. Preamble","text":"<p>Software is deeply embedded in all aspects of our lives and it is important that it empower, rather than restrict us. Free Software gives everybody the rights to use, understand, adapt and share software. These rights help support other fundamental freedoms like freedom of speech, press and privacy.</p> <p>Development of Free Software can follow many patterns. In some cases whole development is handled by a sole programmer or a small group of people. But usually, the creation and maintenance of software is a complex process that requires the contribution of many individuals. This also affects who owns the rights to the software. In the latter case, rights in software are owned jointly by a great number of individuals.</p> <p>To tackle this issue some projects require a full copyright assignment to be signed by all contributors. The problem with such assignments is that they often lack checks and balances that would protect the contributors from potential abuse of power from the new copyright holder.</p> <p>FSFE\u2019s Fiduciary License Agreement (FLA) was created by the Free Software Foundation Europe e.V. with just that in mind \u2013 to concentrate all deciding power within one entity and prevent fragmentation of rights on one hand, while on the other preventing that single entity from abusing its power. The main aim is to ensure that the software covered under the FLA will forever remain Free Software.</p> <p>This process only serves for the transfer of economic rights. So-called moral rights (e.g. authors right to be identified as author) remain with the original author(s) and are inalienable.</p>"},{"location":"cla/#how-to-use-this-fla_1","title":"How to use this FLA","text":"<p>If You are an employee and have created the Contribution as part of your employment, You need to have Your employer approve this Agreement or sign the Entity version of this document. If You do not own the Copyright in the entire work of authorship, any other author of the Contribution should also sign this \u2013 in any event, please contact Us at opengeos@outlook.com</p>"},{"location":"cla/#1-definitions_1","title":"1. Definitions","text":"<p>\"You\" means the individual Copyright owner who Submits a Contribution to Us.</p> <p>\"Legal Entity\" means an entity that is not a natural person.</p> <p>\"Affiliate\" means any other Legal Entity that controls, is controlled by, or under common control with that Legal Entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such Legal Entity, whether by contract or otherwise, (ii) ownership of fifty percent (50%) or more of the outstanding shares or securities that vote to elect the management or other persons who direct such Legal Entity or (iii) beneficial ownership of such entity.</p> <p>\"Contribution\" means any original work of authorship, including any original modifications or additions to an existing work of authorship, Submitted by You to Us, in which You own the Copyright.</p> <p>\"Copyright\" means all rights protecting works of authorship, including copyright, moral and neighboring rights, as appropriate, for the full term of their existence.</p> <p>\"Material\" means the software or documentation made available by Us to third parties. When this Agreement covers more than one software project, the Material means the software or documentation to which the Contribution was Submitted. After You Submit the Contribution, it may be included in the Material.</p> <p>\"Submit\" means any act by which a Contribution is transferred to Us by You by means of tangible or intangible media, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Us, but excluding any transfer that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"</p> <p>\"Documentation\" means any non-software portion of a Contribution.</p>"},{"location":"cla/#2-license-grant_1","title":"2. License grant","text":""},{"location":"cla/#21-copyright-license-to-us_1","title":"2.1 Copyright license to Us","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Us a worldwide, royalty-free, exclusive, perpetual and irrevocable (except as stated in Section 8.2) license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul>"},{"location":"cla/#22-moral-rights_1","title":"2.2 Moral rights","text":"<p>Moral Rights remain unaffected to the extent they are recognized and not waivable by applicable law. Notwithstanding, You may add your name to the attribution mechanism customary used in the Materials you Contribute to, such as the header of the source code files of Your Contribution, and We will respect this attribution when using Your Contribution.</p>"},{"location":"cla/#23-copyright-license-back-to-you_1","title":"2.3 Copyright license back to You","text":"<p>Upon such grant of rights to Us, We immediately grant to You a worldwide, royalty-free, non-exclusive, perpetual and irrevocable license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul> <p>This license back is limited to the Contribution and does not provide any rights to the Material.</p>"},{"location":"cla/#3-patents_1","title":"3. Patents","text":""},{"location":"cla/#31-patent-license_1","title":"3.1 Patent license","text":"<p>Subject to the terms and conditions of this Agreement You hereby grant to Us and to recipients of Materials distributed by Us a worldwide, royalty-free, non-exclusive, perpetual and irrevocable (except as stated in Section 3.2) patent license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, to make, have made, use, sell, offer for sale, import and otherwise transfer the Contribution and the Contribution in combination with any Material (and portions of such combination). This license applies to all patents owned or controlled by You, whether already acquired or hereafter acquired, that would be infringed by making, having made, using, selling, offering for sale, importing or otherwise transferring of Your Contribution(s) alone or by combination of Your Contribution(s) with any Material.</p>"},{"location":"cla/#32-revocation-of-patent-license_1","title":"3.2 Revocation of patent license","text":"<p>You reserve the right to revoke the patent license stated in section 3.1 if We make any infringement claim that is targeted at your Contribution and not asserted for a Defensive Purpose. An assertion of claims of the Patents shall be considered for a \"Defensive Purpose\" if the claims are asserted against an entity that has filed, maintained, threatened, or voluntarily participated in a patent infringement lawsuit against Us or any of Our licensees.</p>"},{"location":"cla/#4-license-obligations-by-us_1","title":"4. License obligations by Us","text":"<p>We agree to (sub)license the Contribution or any Materials containing, based on or derived from your Contribution under the terms of any licenses the Free Software Foundation classifies as Free Software License and which are approved by the Open Source Initiative as Open Source licenses.</p> <p>We agree to license patents owned or controlled by You only to the extent necessary to (sub)license Your Contribution(s) and the combination of Your Contribution(s) with the Material under the terms of any licenses the Free Software Foundation classifies as Free Software licenses and which are approved by the Open Source Initiative as Open Source licenses..</p>"},{"location":"cla/#5-disclaimer_1","title":"5. Disclaimer","text":"<p>THE CONTRIBUTION IS PROVIDED \"AS IS\". MORE PARTICULARLY, ALL EXPRESS OR IMPLIED WARRANTIES INCLUDING, WITHOUT LIMITATION, ANY IMPLIED WARRANTY OF SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT ARE EXPRESSLY DISCLAIMED BY YOU TO US AND BY US TO YOU. TO THE EXTENT THAT ANY SUCH WARRANTIES CANNOT BE DISCLAIMED, SUCH WARRANTY IS LIMITED IN DURATION AND EXTENT TO THE MINIMUM PERIOD AND EXTENT PERMITTED BY LAW.</p>"},{"location":"cla/#6-consequential-damage-waiver_1","title":"6. Consequential damage waiver","text":"<p>TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL YOU OR WE BE LIABLE FOR ANY LOSS OF PROFITS, LOSS OF ANTICIPATED SAVINGS, LOSS OF DATA, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL AND EXEMPLARY DAMAGES ARISING OUT OF THIS AGREEMENT REGARDLESS OF THE LEGAL OR EQUITABLE THEORY (CONTRACT, TORT OR OTHERWISE) UPON WHICH THE CLAIM IS BASED.</p>"},{"location":"cla/#7-approximation-of-disclaimer-and-damage-waiver_1","title":"7. Approximation of disclaimer and damage waiver","text":"<p>IF THE DISCLAIMER AND DAMAGE WAIVER MENTIONED IN SECTION 5. AND SECTION 6. CANNOT BE GIVEN LEGAL EFFECT UNDER APPLICABLE LOCAL LAW, REVIEWING COURTS SHALL APPLY LOCAL LAW THAT MOST CLOSELY APPROXIMATES AN ABSOLUTE WAIVER OF ALL CIVIL OR CONTRACTUAL LIABILITY IN CONNECTION WITH THE CONTRIBUTION.</p>"},{"location":"cla/#8-term_1","title":"8. Term","text":"<p>8.1 This Agreement shall come into effect upon Your acceptance of the terms and conditions.</p> <p>8.2 This Agreement shall apply for the term of the copyright and patents licensed here. However, You shall have the right to terminate the Agreement if We do not fulfill the obligations as set forth in Section 4. Such termination must be made in writing.</p> <p>8.3 In the event of a termination of this Agreement Sections 5., 6., 7., 8., and 9. shall survive such termination and shall remain in full force thereafter. For the avoidance of doubt, Free and Open Source Software (sub)licenses that have already been granted for Contributions at the date of the termination shall remain in full force after the termination of this Agreement.</p>"},{"location":"cla/#9-miscellaneous_1","title":"9. Miscellaneous","text":"<p>9.1 This Agreement and all disputes, claims, actions, suits or other proceedings arising out of this agreement or relating in any way to it shall be governed by the laws of United States excluding its private international law provisions.</p> <p>9.2 This Agreement sets out the entire agreement between You and Us for Your Contributions to Us and overrides all other agreements or understandings.</p> <p>9.3 In case of Your death, this agreement shall continue with Your heirs. In case of more than one heir, all heirs must exercise their rights through a commonly authorized person.</p> <p>9.4 If any provision of this Agreement is found void and unenforceable, such provision will be replaced to the extent possible with a provision that comes closest to the meaning of the original provision and that is enforceable. The terms and conditions set forth in this Agreement shall apply notwithstanding any failure of essential purpose of this Agreement or any limited remedy to the maximum extent possible under law.</p> <p>9.5 You agree to notify Us of any facts or circumstances of which you become aware that would make this Agreement inaccurate in any respect.</p>"},{"location":"cla/#recreate-this-contributor-license-agreement_1","title":"Recreate this Contributor License Agreement","text":"<p>https://contributoragreements.org/u2s/2kw48zbt5d</p> <p></p> <p></p>"},{"location":"common/","title":"common module","text":"<p>The common module contains common functions and classes used by the other modules.</p>"},{"location":"common/#hypercoast.common.convert_coords","title":"<code>convert_coords(coords, from_epsg, to_epsg)</code>","text":"<p>Convert a list of coordinates from one EPSG to another.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>List[Tuple[float, float]]</code> <p>List of tuples containing coordinates in the format (latitude, longitude).</p> required <code>from_epsg</code> <code>str</code> <p>Source EPSG code (default is \"epsg:4326\").</p> required <code>to_epsg</code> <code>str</code> <p>Target EPSG code (default is \"epsg:32615\").</p> required <p>Returns:</p> Type Description <code>List[Tuple[float, float]]</code> <p>List of tuples containing converted coordinates in the format (x, y).</p> Source code in <code>hypercoast/common.py</code> <pre><code>def convert_coords(\n    coords: List[Tuple[float, float]], from_epsg: str, to_epsg: str\n) -&gt; List[Tuple[float, float]]:\n    \"\"\"\n    Convert a list of coordinates from one EPSG to another.\n\n    Args:\n        coords: List of tuples containing coordinates in the format (latitude, longitude).\n        from_epsg: Source EPSG code (default is \"epsg:4326\").\n        to_epsg: Target EPSG code (default is \"epsg:32615\").\n\n    Returns:\n        List of tuples containing converted coordinates in the format (x, y).\n    \"\"\"\n    import pyproj\n\n    # Define the coordinate transformation\n    transformer = pyproj.Transformer.from_crs(from_epsg, to_epsg, always_xy=True)\n\n    # Convert each coordinate\n    converted_coords = [transformer.transform(lon, lat) for lat, lon in coords]\n\n    return converted_coords\n</code></pre>"},{"location":"common/#hypercoast.common.download_acolite","title":"<code>download_acolite(outdir='.', platform=None)</code>","text":"<p>Downloads the Acolite release based on the OS platform and extracts it to the specified directory. For more information, see the Acolite manual https://github.com/acolite/acolite/releases.</p> <p>Parameters:</p> Name Type Description Default <code>outdir</code> <code>str</code> <p>The output directory where the file will be Acolite and extracted.</p> <code>'.'</code> <code>platform</code> <code>Optional[str]</code> <p>The platform for which to download acolite. If None, the current system platform is used.                       Valid values are 'linux', 'darwin', and 'windows'.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the extracted Acolite directory.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If the platform is unsupported or the download fails.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def download_acolite(outdir: str = \".\", platform: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Downloads the Acolite release based on the OS platform and extracts it to the specified directory.\n    For more information, see the Acolite manual https://github.com/acolite/acolite/releases.\n\n    Args:\n        outdir (str): The output directory where the file will be Acolite and extracted.\n        platform (Optional[str]): The platform for which to download acolite. If None, the current system platform is used.\n                                  Valid values are 'linux', 'darwin', and 'windows'.\n\n    Returns:\n        str: The path to the extracted Acolite directory.\n\n    Raises:\n        Exception: If the platform is unsupported or the download fails.\n    \"\"\"\n    import platform as pf\n    import requests\n    import tarfile\n    from tqdm import tqdm\n\n    base_url = \"https://github.com/acolite/acolite/releases/download/20231023.0/\"\n\n    if platform is None:\n        platform = pf.system().lower()\n    else:\n        platform = platform.lower()\n\n    if platform == \"linux\":\n        download_url = base_url + \"acolite_py_linux_20231023.0.tar.gz\"\n        root_dir = \"acolite_py_linux\"\n    elif platform == \"darwin\":\n        download_url = base_url + \"acolite_py_mac_20231023.0.tar.gz\"\n        root_dir = \"acolite_py_mac\"\n    elif platform == \"windows\":\n        download_url = base_url + \"acolite_py_win_20231023.0.tar.gz\"\n        root_dir = \"acolite_py_win\"\n    else:\n        print(f\"Unsupported OS platform: {platform}\")\n        return\n\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n\n    extracted_path = os.path.join(outdir, root_dir)\n    file_name = os.path.join(outdir, download_url.split(\"/\")[-1])\n\n    if os.path.exists(file_name):\n        print(f\"{file_name} already exists. Skip downloading.\")\n        return extracted_path\n\n    response = requests.get(download_url, stream=True, timeout=60)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 8192\n\n    if response.status_code == 200:\n        with open(file_name, \"wb\") as file, tqdm(\n            desc=file_name,\n            total=total_size,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar:\n            for chunk in response.iter_content(chunk_size=block_size):\n                if chunk:\n                    file.write(chunk)\n                    bar.update(len(chunk))\n        print(f\"Downloaded {file_name}\")\n    else:\n        print(f\"Failed to download file from {download_url}\")\n        return\n\n    # Unzip the file\n    with tarfile.open(file_name, \"r:gz\") as tar:\n        tar.extractall(path=outdir)\n\n    print(f\"Extracted to {extracted_path}\")\n    return extracted_path\n</code></pre>"},{"location":"common/#hypercoast.common.download_ecostress","title":"<code>download_ecostress(granules, out_dir=None, threads=8)</code>","text":"<p>Downloads NASA ECOSTRESS granules.</p> <p>Parameters:</p> Name Type Description Default <code>granules</code> <code>List[dict]</code> <p>The granules to download.</p> required <code>out_dir</code> <code>str</code> <p>The output directory where the granules will be downloaded. Defaults to None (current directory).</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to use for downloading. Defaults to 8.</p> <code>8</code> Source code in <code>hypercoast/common.py</code> <pre><code>def download_ecostress(\n    granules: List[dict],\n    out_dir: Optional[str] = None,\n    threads: int = 8,\n) -&gt; None:\n    \"\"\"Downloads NASA ECOSTRESS granules.\n\n    Args:\n        granules (List[dict]): The granules to download.\n        out_dir (str, optional): The output directory where the granules will be\n            downloaded. Defaults to None (current directory).\n        threads (int, optional): The number of threads to use for downloading.\n            Defaults to 8.\n    \"\"\"\n\n    download_nasa_data(granules=granules, out_dir=out_dir, threads=threads)\n</code></pre>"},{"location":"common/#hypercoast.common.download_emit","title":"<code>download_emit(granules, out_dir=None, threads=8)</code>","text":"<p>Downloads NASA EMIT granules.</p> <p>Parameters:</p> Name Type Description Default <code>granules</code> <code>List[dict]</code> <p>The granules to download.</p> required <code>out_dir</code> <code>str</code> <p>The output directory where the granules will be downloaded. Defaults to None (current directory).</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to use for downloading. Defaults to 8.</p> <code>8</code> Source code in <code>hypercoast/common.py</code> <pre><code>def download_emit(\n    granules: List[dict],\n    out_dir: Optional[str] = None,\n    threads: int = 8,\n) -&gt; None:\n    \"\"\"Downloads NASA EMIT granules.\n\n    Args:\n        granules (List[dict]): The granules to download.\n        out_dir (str, optional): The output directory where the granules will be\n            downloaded. Defaults to None (current directory).\n        threads (int, optional): The number of threads to use for downloading.\n            Defaults to 8.\n    \"\"\"\n\n    download_nasa_data(granules=granules, out_dir=out_dir, threads=threads)\n</code></pre>"},{"location":"common/#hypercoast.common.download_file","title":"<code>download_file(url=None, output=None, quiet=True, proxy=None, speed=None, use_cookies=True, verify=True, uid=None, fuzzy=False, resume=False, unzip=True, overwrite=False, subfolder=False)</code>","text":"<p>Download a file from URL, including Google Drive shared URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Google Drive URL is also supported. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>Output filename. Default is basename of URL.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress terminal output. Default is True.</p> <code>True</code> <code>proxy</code> <code>str</code> <p>Proxy. Defaults to None.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.</p> <code>None</code> <code>use_cookies</code> <code>bool</code> <p>Flag to use cookies. Defaults to True.</p> <code>True</code> <code>verify</code> <code>bool | str</code> <p>Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.</p> <code>True</code> <code>uid</code> <code>str</code> <p>Google Drive's file ID. Defaults to None.</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Fuzzy extraction of Google Drive's file Id. Defaults to False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Resume the download from existing tmp file if possible. Defaults to False.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Unzip the file. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <code>subfolder</code> <code>bool</code> <p>Create a subfolder with the same name as the file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The output file path.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def download_file(\n    url: Optional[str] = None,\n    output: Optional[str] = None,\n    quiet: Optional[bool] = True,\n    proxy: Optional[str] = None,\n    speed: Optional[float] = None,\n    use_cookies: Optional[bool] = True,\n    verify: Optional[bool] = True,\n    uid: Optional[str] = None,\n    fuzzy: Optional[bool] = False,\n    resume: Optional[bool] = False,\n    unzip: Optional[bool] = True,\n    overwrite: Optional[bool] = False,\n    subfolder: Optional[bool] = False,\n) -&gt; str:\n    \"\"\"Download a file from URL, including Google Drive shared URL.\n\n    Args:\n        url (str, optional): Google Drive URL is also supported. Defaults to None.\n        output (str, optional): Output filename. Default is basename of URL.\n        quiet (bool, optional): Suppress terminal output. Default is True.\n        proxy (str, optional): Proxy. Defaults to None.\n        speed (float, optional): Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.\n        use_cookies (bool, optional): Flag to use cookies. Defaults to True.\n        verify (bool | str, optional): Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string,\n            in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.\n        uid (str, optional): Google Drive's file ID. Defaults to None.\n        fuzzy (bool, optional): Fuzzy extraction of Google Drive's file Id. Defaults to False.\n        resume (bool, optional): Resume the download from existing tmp file if possible. Defaults to False.\n        unzip (bool, optional): Unzip the file. Defaults to True.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n        subfolder (bool, optional): Create a subfolder with the same name as the file. Defaults to False.\n\n    Returns:\n        str: The output file path.\n    \"\"\"\n    import zipfile\n    import tarfile\n    import gdown\n\n    if output is None:\n        if isinstance(url, str) and url.startswith(\"http\"):\n            output = os.path.basename(url)\n\n    out_dir = os.path.abspath(os.path.dirname(output))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(url, str):\n        if os.path.exists(os.path.abspath(output)) and (not overwrite):\n            print(\n                f\"{output} already exists. Skip downloading. Set overwrite=True to overwrite.\"\n            )\n            return os.path.abspath(output)\n        else:\n            url = github_raw_url(url)\n\n    if \"https://drive.google.com/file/d/\" in url:\n        fuzzy = True\n\n    output = gdown.download(\n        url, output, quiet, proxy, speed, use_cookies, verify, uid, fuzzy, resume\n    )\n\n    if unzip:\n        if output.endswith(\".zip\"):\n            with zipfile.ZipFile(output, \"r\") as zip_ref:\n                if not quiet:\n                    print(\"Extracting files...\")\n                if subfolder:\n                    basename = os.path.splitext(os.path.basename(output))[0]\n\n                    output = os.path.join(out_dir, basename)\n                    if not os.path.exists(output):\n                        os.makedirs(output)\n                    zip_ref.extractall(output)\n                else:\n                    zip_ref.extractall(os.path.dirname(output))\n        elif output.endswith(\".tar.gz\") or output.endswith(\".tar\"):\n            if output.endswith(\".tar.gz\"):\n                mode = \"r:gz\"\n            else:\n                mode = \"r\"\n\n            with tarfile.open(output, mode) as tar_ref:\n                if not quiet:\n                    print(\"Extracting files...\")\n                if subfolder:\n                    basename = os.path.splitext(os.path.basename(output))[0]\n                    output = os.path.join(out_dir, basename)\n                    if not os.path.exists(output):\n                        os.makedirs(output)\n                    tar_ref.extractall(output)\n                else:\n                    tar_ref.extractall(os.path.dirname(output))\n\n    return os.path.abspath(output)\n</code></pre>"},{"location":"common/#hypercoast.common.download_nasa_data","title":"<code>download_nasa_data(granules, out_dir=None, provider=None, threads=8)</code>","text":"<p>Downloads NASA Earthdata granules.</p> <p>Parameters:</p> Name Type Description Default <code>granules</code> <code>List[dict]</code> <p>The granules to download.</p> required <code>out_dir</code> <code>str</code> <p>The output directory where the granules will be downloaded. Defaults to None (current directory).</p> <code>None</code> <code>provider</code> <code>str</code> <p>The provider of the granules.</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to use for downloading. Defaults to 8.</p> <code>8</code> Source code in <code>hypercoast/common.py</code> <pre><code>def download_nasa_data(\n    granules: List[dict],\n    out_dir: Optional[str] = None,\n    provider: Optional[str] = None,\n    threads: int = 8,\n) -&gt; None:\n    \"\"\"Downloads NASA Earthdata granules.\n\n    Args:\n        granules (List[dict]): The granules to download.\n        out_dir (str, optional): The output directory where the granules will be downloaded. Defaults to None (current directory).\n        provider (str, optional): The provider of the granules.\n        threads (int, optional): The number of threads to use for downloading. Defaults to 8.\n    \"\"\"\n\n    leafmap.nasa_data_download(\n        granules=granules, out_dir=out_dir, provider=provider, threads=threads\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.download_pace","title":"<code>download_pace(granules, out_dir=None, threads=8)</code>","text":"<p>Downloads NASA PACE granules.</p> <p>Parameters:</p> Name Type Description Default <code>granules</code> <code>List[dict]</code> <p>The granules to download.</p> required <code>out_dir</code> <code>str</code> <p>The output directory where the granules will be downloaded. Defaults to None (current directory).</p> <code>None</code> <code>threads</code> <code>int</code> <p>The number of threads to use for downloading. Defaults to 8.</p> <code>8</code> Source code in <code>hypercoast/common.py</code> <pre><code>def download_pace(\n    granules: List[dict],\n    out_dir: Optional[str] = None,\n    threads: int = 8,\n) -&gt; None:\n    \"\"\"Downloads NASA PACE granules.\n\n    Args:\n        granules (List[dict]): The granules to download.\n        out_dir (str, optional): The output directory where the granules will be\n            downloaded. Defaults to None (current directory).\n        threads (int, optional): The number of threads to use for downloading.\n            Defaults to 8.\n    \"\"\"\n\n    download_nasa_data(granules=granules, out_dir=out_dir, threads=threads)\n</code></pre>"},{"location":"common/#hypercoast.common.extract_date_from_filename","title":"<code>extract_date_from_filename(filename)</code>","text":"<p>Extracts a date from a filename assuming the date is in 'YYYYMMDD' format.</p> <p>This function searches the filename for a sequence of 8 digits that represent a date in 'YYYYMMDD' format. If such a sequence is found, it converts the sequence into a pandas Timestamp object. If no such sequence is found, the function returns None.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename from which to extract the date.</p> required <p>Returns:</p> Type Description <code>Optional[pd.Timestamp]</code> <p>A pandas Timestamp object representing the date found in the filename, or None if no date in 'YYYYMMDD' format is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extract_date_from_filename(\"example_20230101.txt\")\nTimestamp('2023-01-01 00:00:00')\n</code></pre> <pre><code>&gt;&gt;&gt; extract_date_from_filename(\"no_date_in_this_filename.txt\")\nNone\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def extract_date_from_filename(filename: str):\n    \"\"\"\n    Extracts a date from a filename assuming the date is in 'YYYYMMDD' format.\n\n    This function searches the filename for a sequence of 8 digits that represent a date in\n    'YYYYMMDD' format. If such a sequence is found, it converts the sequence into a pandas\n    Timestamp object. If no such sequence is found, the function returns None.\n\n    Args:\n        filename (str): The filename from which to extract the date.\n\n    Returns:\n        Optional[pd.Timestamp]: A pandas Timestamp object representing the date found in the filename,\n        or None if no date in 'YYYYMMDD' format is found.\n\n    Examples:\n        &gt;&gt;&gt; extract_date_from_filename(\"example_20230101.txt\")\n        Timestamp('2023-01-01 00:00:00')\n\n        &gt;&gt;&gt; extract_date_from_filename(\"no_date_in_this_filename.txt\")\n        None\n    \"\"\"\n    import re\n    import pandas as pd\n\n    # Assuming the date format in filename is 'YYYYMMDD'\n    date_match = re.search(r\"\\d{8}\", filename)\n    if date_match:\n        return pd.to_datetime(date_match.group(), format=\"%Y%m%d\")\n    else:\n        return None\n</code></pre>"},{"location":"common/#hypercoast.common.extract_spectral","title":"<code>extract_spectral(ds, lat, lon, name='data')</code>","text":"<p>Extracts spectral signature from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>The dataset containing the spectral data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted data.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def extract_spectral(\n    ds: xr.Dataset, lat: float, lon: float, name: str = \"data\"\n) -&gt; xr.DataArray:\n    \"\"\"\n    Extracts spectral signature from a given xarray Dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the spectral data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n\n    Returns:\n        xarray.DataArray: The extracted data.\n    \"\"\"\n\n    crs = ds.rio.crs\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    values = ds.sel(x=x, y=y, method=\"nearest\")[name].values\n\n    da = xr.DataArray(values, dims=[\"band\"], coords={\"band\": ds.coords[\"band\"]})\n\n    return da\n</code></pre>"},{"location":"common/#hypercoast.common.github_raw_url","title":"<code>github_raw_url(url)</code>","text":"<p>Get the raw URL for a GitHub file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The GitHub URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The raw URL.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def github_raw_url(url: str) -&gt; str:\n    \"\"\"Get the raw URL for a GitHub file.\n\n    Args:\n        url (str): The GitHub URL.\n    Returns:\n        str: The raw URL.\n    \"\"\"\n    if isinstance(url, str) and url.startswith(\"https://github.com/\") and \"blob\" in url:\n        url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n    return url\n</code></pre>"},{"location":"common/#hypercoast.common.image_cube","title":"<code>image_cube(dataset, variable='reflectance', cmap='jet', clim=(0, 0.5), title='Reflectance', rgb_bands=None, rgb_wavelengths=None, rgb_gamma=1.0, rgb_cmap=None, rgb_clim=None, rgb_args=None, widget=None, plotter_args=None, show_axes=True, grid_origin=(0, 0, 0), grid_spacing=(1, 1, 1), **kwargs)</code>","text":"<p>Creates an image cube from a dataset and plots it using PyVista.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[str, xr.Dataset]</code> <p>The dataset to plot. Can be a path to a NetCDF file or an xarray Dataset.</p> required <code>variable</code> <code>str</code> <p>The variable to plot. Defaults to \"reflectance\".</p> <code>'reflectance'</code> <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"jet\".</p> <code>'jet'</code> <code>clim</code> <code>Tuple[float, float]</code> <p>The color limits. Defaults to (0, 0.5).</p> <code>(0, 0.5)</code> <code>title</code> <code>str</code> <p>The title for the scalar bar. Defaults to \"Reflectance\".</p> <code>'Reflectance'</code> <code>rgb_bands</code> <code>Optional[List[int]]</code> <p>The bands to use for the RGB image. Defaults to None.</p> <code>None</code> <code>rgb_wavelengths</code> <code>Optional[List[float]]</code> <p>The wavelengths to use for the RGB image. Defaults to None.</p> <code>None</code> <code>rgb_gamma</code> <code>float</code> <p>The gamma correction for the RGB image. Defaults to 1.</p> <code>1.0</code> <code>rgb_cmap</code> <code>Optional[str]</code> <p>The colormap to use for the RGB image. Defaults to None.</p> <code>None</code> <code>rgb_clim</code> <code>Optional[Tuple[float, float]]</code> <p>The color limits for the RGB image. Defaults to None.</p> <code>None</code> <code>rgb_args</code> <code>Dict[str, Any]</code> <p>Additional arguments for the <code>add_mesh</code> method for the RGB image. Defaults to {}.</p> <code>None</code> <code>widget</code> <code>Optional[str]</code> <p>The widget to use for the image cube. Can be one of the following: \"box\", \"plane\", \"slice\", \"orthogonal\", and \"threshold\". Defaults to None.</p> <code>None</code> <code>plotter_args</code> <code>Dict[str, Any]</code> <p>Additional arguments for the <code>pv.Plotter</code> constructor. Defaults to {}.</p> <code>None</code> <code>show_axes</code> <code>bool</code> <p>Whether to show the axes. Defaults to True.</p> <code>True</code> <code>grid_origin</code> <code>Tuple[float, float, float]</code> <p>The origin of the grid. Defaults to (0, 0, 0).</p> <code>(0, 0, 0)</code> <code>grid_spacing</code> <code>Tuple[float, float, float]</code> <p>The spacing of the grid.</p> <code>(1, 1, 1)</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments for the <code>add_mesh</code> method. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pv.Plotter</code> <p>The PyVista Plotter with the image cube added.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def image_cube(\n    dataset,\n    variable: str = \"reflectance\",\n    cmap: str = \"jet\",\n    clim: Tuple[float, float] = (0, 0.5),\n    title: str = \"Reflectance\",\n    rgb_bands: Optional[List[int]] = None,\n    rgb_wavelengths: Optional[List[float]] = None,\n    rgb_gamma: float = 1.0,\n    rgb_cmap: Optional[str] = None,\n    rgb_clim: Optional[Tuple[float, float]] = None,\n    rgb_args: Dict[str, Any] = None,\n    widget=None,\n    plotter_args: Dict[str, Any] = None,\n    show_axes: bool = True,\n    grid_origin=(0, 0, 0),\n    grid_spacing=(1, 1, 1),\n    **kwargs: Any,\n):\n    \"\"\"\n    Creates an image cube from a dataset and plots it using PyVista.\n\n    Args:\n        dataset (Union[str, xr.Dataset]): The dataset to plot. Can be a path to\n            a NetCDF file or an xarray Dataset.\n        variable (str, optional): The variable to plot. Defaults to \"reflectance\".\n        cmap (str, optional): The colormap to use. Defaults to \"jet\".\n        clim (Tuple[float, float], optional): The color limits. Defaults to (0, 0.5).\n        title (str, optional): The title for the scalar bar. Defaults to \"Reflectance\".\n        rgb_bands (Optional[List[int]], optional): The bands to use for the RGB\n            image. Defaults to None.\n        rgb_wavelengths (Optional[List[float]], optional): The wavelengths to\n            use for the RGB image. Defaults to None.\n        rgb_gamma (float, optional): The gamma correction for the RGB image.\n            Defaults to 1.\n        rgb_cmap (Optional[str], optional): The colormap to use for the RGB image.\n            Defaults to None.\n        rgb_clim (Optional[Tuple[float, float]], optional): The color limits for\n            the RGB image. Defaults to None.\n        rgb_args (Dict[str, Any], optional): Additional arguments for the\n            `add_mesh` method for the RGB image. Defaults to {}.\n        widget (Optional[str], optional): The widget to use for the image cube.\n            Can be one of the following: \"box\", \"plane\", \"slice\", \"orthogonal\",\n            and \"threshold\". Defaults to None.\n        plotter_args (Dict[str, Any], optional): Additional arguments for the\n            `pv.Plotter` constructor. Defaults to {}.\n        show_axes (bool, optional): Whether to show the axes. Defaults to True.\n        grid_origin (Tuple[float, float, float], optional): The origin of the grid.\n            Defaults to (0, 0, 0).\n        grid_spacing (Tuple[float, float, float], optional): The spacing of the grid.\n        **kwargs (Dict[str, Any], optional): Additional arguments for the\n            `add_mesh` method. Defaults to {}.\n\n    Returns:\n        pv.Plotter: The PyVista Plotter with the image cube added.\n    \"\"\"\n\n    import pyvista as pv\n\n    if rgb_args is None:\n        rgb_args = {}\n\n    if plotter_args is None:\n        plotter_args = {}\n\n    allowed_widgets = [\"box\", \"plane\", \"slice\", \"orthogonal\", \"threshold\"]\n\n    if widget is not None:\n        if widget not in allowed_widgets:\n            raise ValueError(f\"widget must be one of the following: {allowed_widgets}\")\n\n    if isinstance(dataset, str):\n        dataset = xr.open_dataset(dataset)\n\n    da = dataset[variable]  # xarray DataArray\n    values = da.to_numpy()\n\n    # Create the spatial reference for the image cube\n    grid = pv.ImageData()\n\n    # Set the grid dimensions: shape because we want to inject our values on the POINT data\n    grid.dimensions = values.shape\n\n    # Edit the spatial reference\n    grid.origin = grid_origin  # The bottom left corner of the data set\n    grid.spacing = grid_spacing  # These are the cell sizes along each axis\n\n    # Add the data values to the cell data\n    grid.point_data[\"values\"] = values.flatten(order=\"F\")  # Flatten the array\n\n    # Plot the image cube with the RGB image overlay\n    p = pv.Plotter(**plotter_args)\n\n    if \"scalar_bar_args\" not in kwargs:\n        kwargs[\"scalar_bar_args\"] = {\"title\": title}\n    else:\n        kwargs[\"scalar_bar_args\"][\"title\"] = title\n\n    if \"show_edges\" not in kwargs:\n        kwargs[\"show_edges\"] = False\n\n    if widget == \"box\":\n        p.add_mesh_clip_box(grid, cmap=cmap, clim=clim, **kwargs)\n    elif widget == \"plane\":\n        if \"normal\" not in kwargs:\n            kwargs[\"normal\"] = (0, 0, 1)\n        if \"invert\" not in kwargs:\n            kwargs[\"invert\"] = True\n        if \"normal_rotation\" not in kwargs:\n            kwargs[\"normal_rotation\"] = False\n        p.add_mesh_clip_plane(grid, cmap=cmap, clim=clim, **kwargs)\n    elif widget == \"slice\":\n        if \"normal\" not in kwargs:\n            kwargs[\"normal\"] = (0, 0, 1)\n        if \"normal_rotation\" not in kwargs:\n            kwargs[\"normal_rotation\"] = False\n        p.add_mesh_slice(grid, cmap=cmap, clim=clim, **kwargs)\n    elif widget == \"orthogonal\":\n        p.add_mesh_slice_orthogonal(grid, cmap=cmap, clim=clim, **kwargs)\n    elif widget == \"threshold\":\n        p.add_mesh_threshold(grid, cmap=cmap, clim=clim, **kwargs)\n    else:\n        p.add_mesh(grid, cmap=cmap, clim=clim, **kwargs)\n\n    if rgb_bands is not None or rgb_wavelengths is not None:\n\n        if rgb_bands is not None:\n            rgb_image = dataset.isel(wavelength=rgb_bands, method=\"nearest\")[\n                variable\n            ].to_numpy()\n        elif rgb_wavelengths is not None:\n            rgb_image = dataset.sel(wavelength=rgb_wavelengths, method=\"nearest\")[\n                variable\n            ].to_numpy()\n\n        x_dim, y_dim = rgb_image.shape[0], rgb_image.shape[1]\n        z_dim = 1\n        im = pv.ImageData(dimensions=(x_dim, y_dim, z_dim))\n\n        # Add scalar data, you may also need to flatten this\n        im.point_data[\"rgb_image\"] = (\n            rgb_image.reshape(-1, rgb_image.shape[2], order=\"F\") * rgb_gamma\n        )\n\n        grid_z_max = grid.bounds[5]\n        im.origin = (0, 0, grid_z_max)\n\n        if rgb_image.shape[2] &lt; 3:\n            if rgb_cmap is None:\n                rgb_cmap = cmap\n            if rgb_clim is None:\n                rgb_clim = clim\n\n            if \"cmap\" not in rgb_args:\n                rgb_args[\"cmap\"] = rgb_cmap\n            if \"clim\" not in rgb_args:\n                rgb_args[\"clim\"] = rgb_clim\n        else:\n            if \"rgb\" not in rgb_args:\n                rgb_args[\"rgb\"] = True\n\n        if \"show_scalar_bar\" not in rgb_args:\n            rgb_args[\"show_scalar_bar\"] = False\n        if \"show_edges\" not in rgb_args:\n            rgb_args[\"show_edges\"] = False\n\n        p.add_mesh(im, **rgb_args)\n\n    if show_axes:\n        p.show_axes()\n\n    return p\n</code></pre>"},{"location":"common/#hypercoast.common.nasa_earth_login","title":"<code>nasa_earth_login(strategy='all', persist=True, **kwargs)</code>","text":"<p>Logs in to NASA Earthdata.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>The login strategy. Defaults to \"all\".</p> <code>'all'</code> <code>persist</code> <code>bool</code> <p>Whether to persist the login. Defaults to True.</p> <code>True</code> Source code in <code>hypercoast/common.py</code> <pre><code>def nasa_earth_login(strategy: str = \"all\", persist: bool = True, **kwargs) -&gt; None:\n    \"\"\"Logs in to NASA Earthdata.\n\n    Args:\n        strategy (str, optional): The login strategy. Defaults to \"all\".\n        persist (bool, optional): Whether to persist the login. Defaults to True.\n    \"\"\"\n\n    leafmap.nasa_data_login(strategy=strategy, persist=persist, **kwargs)\n</code></pre>"},{"location":"common/#hypercoast.common.netcdf_groups","title":"<code>netcdf_groups(filepath)</code>","text":"<p>Get the list of groups in a NetCDF file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the NetCDF file.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of group names in the NetCDF file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; netcdf_groups('path/to/netcdf/file')\n['group1', 'group2', 'group3']\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def netcdf_groups(filepath: str) -&gt; List[str]:\n    \"\"\"\n    Get the list of groups in a NetCDF file.\n\n    Args:\n        filepath (str): The path to the NetCDF file.\n\n    Returns:\n        list: A list of group names in the NetCDF file.\n\n    Example:\n        &gt;&gt;&gt; netcdf_groups('path/to/netcdf/file')\n        ['group1', 'group2', 'group3']\n    \"\"\"\n    import h5netcdf\n\n    with h5netcdf.File(filepath) as file:\n        groups = list(file)\n    return groups\n</code></pre>"},{"location":"common/#hypercoast.common.open_dataset","title":"<code>open_dataset(filename, engine=None, chunks=None, **kwargs)</code>","text":"<p>Opens and returns an xarray Dataset from a file.</p> <p>This function is a wrapper around <code>xarray.open_dataset</code> that allows for additional customization through keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file to open.</p> required <code>engine</code> <code>Optional[str]</code> <p>Name of the engine to use for reading the file. If None, xarray's default engine is used. Examples include 'netcdf4', 'h5netcdf', 'zarr', etc.</p> <code>None</code> <code>chunks</code> <code>Optional[Dict[str, int]]</code> <p>Dictionary specifying how to chunk the dataset along each dimension. For example, <code>{'time': 1}</code> would load the dataset in single-time-step chunks. If None, the dataset is not chunked.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>xarray.open_dataset</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The opened dataset.</p> <p>Examples:</p> <p>Open a NetCDF file without chunking:</p> <pre><code>&gt;&gt;&gt; dataset = open_dataset('path/to/file.nc')\n</code></pre> <p>Open a Zarr dataset, chunking along the 'time' dimension:</p> <pre><code>&gt;&gt;&gt; dataset = open_dataset('path/to/dataset.zarr', engine='zarr', chunks={'time': 10})\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def open_dataset(\n    filename: str,\n    engine: Optional[str] = None,\n    chunks: Optional[Dict[str, int]] = None,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Opens and returns an xarray Dataset from a file.\n\n    This function is a wrapper around `xarray.open_dataset` that allows for additional\n    customization through keyword arguments.\n\n    Args:\n        filename (str): Path to the file to open.\n        engine (Optional[str]): Name of the engine to use for reading the file. If None, xarray's\n            default engine is used. Examples include 'netcdf4', 'h5netcdf', 'zarr', etc.\n        chunks (Optional[Dict[str, int]]): Dictionary specifying how to chunk the dataset along each dimension.\n            For example, `{'time': 1}` would load the dataset in single-time-step chunks. If None,\n            the dataset is not chunked.\n        **kwargs: Additional keyword arguments passed to `xarray.open_dataset`.\n\n    Returns:\n        xr.Dataset: The opened dataset.\n\n    Examples:\n        Open a NetCDF file without chunking:\n        &gt;&gt;&gt; dataset = open_dataset('path/to/file.nc')\n\n        Open a Zarr dataset, chunking along the 'time' dimension:\n        &gt;&gt;&gt; dataset = open_dataset('path/to/dataset.zarr', engine='zarr', chunks={'time': 10})\n    \"\"\"\n\n    try:\n        dataset = xr.open_dataset(filename, engine=engine, chunks=chunks, **kwargs)\n    except OSError:\n        dataset = xr.open_dataset(filename, engine=\"h5netcdf\", chunks=chunks, **kwargs)\n\n    return dataset\n</code></pre>"},{"location":"common/#hypercoast.common.pca","title":"<code>pca(input_file, output_file, n_components=3, **kwargs)</code>","text":"<p>Performs Principal Component Analysis (PCA) on a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The input file containing the data to analyze.</p> required <code>output_file</code> <code>str</code> <p>The output file to save the PCA results.</p> required <code>n_components</code> <code>int</code> <p>The number of principal components to compute. Defaults to 3.</p> <code>3</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the scikit-learn PCA function. For more info, see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>This function does not return a value. It saves the PCA results to the output file.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def pca(input_file, output_file, n_components=3, **kwargs):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on a dataset.\n\n    Args:\n        input_file (str): The input file containing the data to analyze.\n        output_file (str): The output file to save the PCA results.\n        n_components (int, optional): The number of principal components to compute. Defaults to 3.\n        **kwargs: Additional keyword arguments to pass to the scikit-learn PCA function.\n            For more info, see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html.\n\n    Returns:\n        None: This function does not return a value. It saves the PCA results to the output file.\n    \"\"\"\n\n    import rasterio\n    from sklearn.decomposition import PCA\n\n    # Function to load the GeoTIFF image\n    def load_geotiff(file_path):\n        with rasterio.open(file_path) as src:\n            image = src.read()\n            profile = src.profile\n        return image, profile\n\n    # Function to perform PCA\n    def perform_pca(image, n_components=3, **kwargs):\n        # Reshape the image to [n_bands, n_pixels]\n        n_bands, width, height = image.shape\n        image_reshaped = image.reshape(n_bands, width * height).T\n\n        # Perform PCA\n        model = PCA(n_components=n_components, **kwargs)\n        principal_components = model.fit_transform(image_reshaped)\n\n        # Reshape the principal components back to image dimensions\n        pca_image = principal_components.T.reshape(n_components, width, height)\n        return pca_image\n\n    # Function to save the PCA-transformed image\n    def save_geotiff(file_path, image, profile):\n        profile.update(count=image.shape[0])\n        with rasterio.open(file_path, \"w\", **profile) as dst:\n            dst.write(image)\n\n    image, profile = load_geotiff(input_file)\n    pca_image = perform_pca(image, n_components, **kwargs)\n    save_geotiff(output_file, pca_image, profile)\n</code></pre>"},{"location":"common/#hypercoast.common.run_acolite","title":"<code>run_acolite(acolite_dir, settings_file=None, input_file=None, out_dir=None, polygon=None, l2w_parameters=None, rgb_rhot=True, rgb_rhos=True, map_l2w=True, verbose=True, **kwargs)</code>","text":"<p>Runs the Acolite software for atmospheric correction and water quality retrieval. For more information, see the Acolite manual https://github.com/acolite/acolite/releases</p> <p>This function constructs and executes a command to run the Acolite software with the specified parameters. It supports running Acolite with a settings file or with individual parameters specified directly. Additional parameters can be passed as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>acolite_dir</code> <code>str</code> <p>The directory where Acolite is installed.</p> required <code>settings_file</code> <code>Optional[str]</code> <p>The path to the Acolite settings file. If provided, other parameters except <code>verbose</code> are ignored. Defaults to None.</p> <code>None</code> <code>input_file</code> <code>Optional[str]</code> <p>The path to the input file for processing. Defaults to None.</p> <code>None</code> <code>out_dir</code> <code>Optional[str]</code> <p>The directory where output files will be saved. Defaults to None.</p> <code>None</code> <code>polygon</code> <code>Optional[str]</code> <p>The path to a polygon file for spatial subset. Defaults to None.</p> <code>None</code> <code>l2w_parameters</code> <code>Optional[str]</code> <p>Parameters for L2W processing. Defaults to None.</p> <code>None</code> <code>rgb_rhot</code> <code>bool</code> <p>Flag to generate RGB images using rhot. Defaults to True.</p> <code>True</code> <code>rgb_rhos</code> <code>bool</code> <p>Flag to generate RGB images using rhos. Defaults to True.</p> <code>True</code> <code>map_l2w</code> <code>bool</code> <p>Flag to map L2W products. Defaults to True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, prints the command output; otherwise, suppresses it. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional command line arguments to pass to acolite. Such as --l2w_export_geotiff, --merge_tiles, etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>This function does not return a value. It executes the Acolite software.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; run_acolite(\"/path/to/acolite\", input_file=\"/path/to/inputfile\", output=\"/path/to/output\")\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def run_acolite(\n    acolite_dir: str,\n    settings_file: Optional[str] = None,\n    input_file: Optional[str] = None,\n    out_dir: Optional[str] = None,\n    polygon: Optional[str] = None,\n    l2w_parameters: Optional[str] = None,\n    rgb_rhot: bool = True,\n    rgb_rhos: bool = True,\n    map_l2w: bool = True,\n    verbose: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Runs the Acolite software for atmospheric correction and water quality retrieval.\n    For more information, see the Acolite manual https://github.com/acolite/acolite/releases\n\n    This function constructs and executes a command to run the Acolite software with the specified\n    parameters. It supports running Acolite with a settings file or with individual parameters\n    specified directly. Additional parameters can be passed as keyword arguments.\n\n    Args:\n        acolite_dir (str): The directory where Acolite is installed.\n        settings_file (Optional[str], optional): The path to the Acolite settings file. If provided,\n            other parameters except `verbose` are ignored. Defaults to None.\n        input_file (Optional[str], optional): The path to the input file for processing. Defaults to None.\n        out_dir (Optional[str], optional): The directory where output files will be saved. Defaults to None.\n        polygon (Optional[str], optional): The path to a polygon file for spatial subset. Defaults to None.\n        l2w_parameters (Optional[str], optional): Parameters for L2W processing. Defaults to None.\n        rgb_rhot (bool, optional): Flag to generate RGB images using rhot. Defaults to True.\n        rgb_rhos (bool, optional): Flag to generate RGB images using rhos. Defaults to True.\n        map_l2w (bool, optional): Flag to map L2W products. Defaults to True.\n        verbose (bool, optional): If True, prints the command output; otherwise, suppresses it. Defaults to True.\n        **kwargs (Any): Additional command line arguments to pass to acolite. Such as\n            --l2w_export_geotiff, --merge_tiles, etc.\n\n    Returns:\n        None: This function does not return a value. It executes the Acolite software.\n\n    Example:\n        &gt;&gt;&gt; run_acolite(\"/path/to/acolite\", input_file=\"/path/to/inputfile\", output=\"/path/to/output\")\n    \"\"\"\n\n    import subprocess\n    from datetime import datetime\n\n    def get_formatted_current_time(format_str=\"%Y-%m-%d %H:%M:%S\"):\n        current_time = datetime.now()\n        formatted_time = current_time.strftime(format_str)\n        return formatted_time\n\n    acolite_dir_name = os.path.split(os.path.dirname(acolite_dir))[-1]\n    acolite_exe = \"acolite\"\n    if acolite_dir_name.endswith(\"win\"):\n        acolite_exe += \".exe\"\n\n    if isinstance(input_file, list):\n        input_file = \",\".join(input_file)\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    acolite_exe_path = os.path.join(acolite_dir, \"dist\", \"acolite\", acolite_exe)\n\n    acolite_cmd = [acolite_exe_path, \"--cli\"]\n\n    if settings_file is not None:\n        acolite_cmd.extend([\"--settings\", settings_file])\n    else:\n        lines = []\n        lines.append(\"## ACOLITE settings\")\n        lines.append(f\"## Written at {get_formatted_current_time()}\")\n        if input_file is not None:\n            lines.append(f\"inputfile={input_file}\")\n        if out_dir is not None:\n            lines.append(f\"output={out_dir}\")\n        if polygon is not None:\n            lines.append(f\"polygon={polygon}\")\n        else:\n            lines.append(\"polygon=None\")\n        if l2w_parameters is not None:\n            lines.append(f\"l2w_parameters={l2w_parameters}\")\n        if rgb_rhot:\n            lines.append(\"rgb_rhot=True\")\n        else:\n            lines.append(\"rgb_rhot=False\")\n        if rgb_rhos:\n            lines.append(\"rgb_rhos=True\")\n        else:\n            lines.append(\"rgb_rhos=False\")\n        if map_l2w:\n            lines.append(\"map_l2w=True\")\n        else:\n            lines.append(\"map_l2w=False\")\n\n        for key, value in kwargs.items():\n            lines.append(f\"{key}={value}\")\n\n        lines.append(f\"runid={get_formatted_current_time('%Y%m%d_%H%M%S')}\")\n        settings_filename = f\"acolite_run_{get_formatted_current_time('%Y%m%d_%H%M%S')}_settings_user.txt\"\n        settings_file = os.path.join(out_dir, settings_filename)\n        with open(settings_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(lines))\n        acolite_cmd.extend([\"--settings\", settings_file])\n\n    if verbose:\n        subprocess.run(acolite_cmd, check=True)\n    else:\n        subprocess.run(\n            acolite_cmd,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n            check=True,\n        )\n</code></pre>"},{"location":"common/#hypercoast.common.search_datasets","title":"<code>search_datasets(count=-1, **kwargs)</code>","text":"<p>Searches for datasets using the EarthAccess API with optional filters.</p> <p>This function wraps the <code>earthaccess.search_datasets</code> function, allowing for customized search queries based on a count limit and additional keyword arguments which serve as filters for the search.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>The maximum number of datasets to return. A value of -1 indicates no limit. Defaults to -1.</p> <code>-1</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass as search filters to the EarthAccess API. keyword: case-insensitive and supports wildcards ? and * short_name: e.g. ATL08 doi: DOI for a dataset daac: e.g. NSIDC or PODAAC provider: particular to each DAAC, e.g. POCLOUD, LPDAAC etc. temporal: a tuple representing temporal bounds in the form (date_from, date_to) bounding_box: a tuple representing spatial bounds in the form (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat)</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, where each dictionary contains     information about a dataset found in the search.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; results = search_datasets(count=5, keyword='temperature')\n&gt;&gt;&gt; print(results)\n</code></pre> Source code in <code>hypercoast/common.py</code> <pre><code>def search_datasets(count: int = -1, **kwargs: Any) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Searches for datasets using the EarthAccess API with optional filters.\n\n    This function wraps the `earthaccess.search_datasets` function, allowing for\n    customized search queries based on a count limit and additional keyword arguments\n    which serve as filters for the search.\n\n    Args:\n        count (int, optional): The maximum number of datasets to return. A value of -1\n            indicates no limit. Defaults to -1.\n        **kwargs (Any): Additional keyword arguments to pass as search filters to the\n            EarthAccess API.\n            keyword: case-insensitive and supports wildcards ? and *\n            short_name: e.g. ATL08\n            doi: DOI for a dataset\n            daac: e.g. NSIDC or PODAAC\n            provider: particular to each DAAC, e.g. POCLOUD, LPDAAC etc.\n            temporal: a tuple representing temporal bounds in the form (date_from, date_to)\n            bounding_box: a tuple representing spatial bounds in the form\n            (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat)\n\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries, where each dictionary contains\n            information about a dataset found in the search.\n\n    Example:\n        &gt;&gt;&gt; results = search_datasets(count=5, keyword='temperature')\n        &gt;&gt;&gt; print(results)\n    \"\"\"\n\n    import earthaccess\n\n    return earthaccess.search_datasets(count=count, **kwargs)\n</code></pre>"},{"location":"common/#hypercoast.common.search_ecostress","title":"<code>search_ecostress(bbox=None, temporal=None, count=-1, short_name='ECO_L2T_LSTE', output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA ECOSTRESS granules.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset. Defaults to \"ECO_L2T_LSTE\".</p> <code>'ECO_L2T_LSTE'</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_ecostress(\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    count: int = -1,\n    short_name: Optional[str] = \"ECO_L2T_LSTE\",\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA ECOSTRESS granules.\n\n    Args:\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset. Defaults to \"ECO_L2T_LSTE\".\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    return search_nasa_data(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.search_emit","title":"<code>search_emit(bbox=None, temporal=None, count=-1, short_name='EMITL2ARFL', output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA EMIT granules.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset. Defaults to \"EMITL2ARFL\".</p> <code>'EMITL2ARFL'</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_emit(\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    count: int = -1,\n    short_name: Optional[str] = \"EMITL2ARFL\",\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA EMIT granules.\n\n    Args:\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset. Defaults to \"EMITL2ARFL\".\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    return search_nasa_data(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.search_nasa_data","title":"<code>search_nasa_data(count=-1, short_name=None, bbox=None, temporal=None, version=None, doi=None, daac=None, provider=None, output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA Earthdata granules.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset.</p> <code>None</code> <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>version</code> <code>str</code> <p>The version of the dataset.</p> <code>None</code> <code>doi</code> <code>str</code> <p>The Digital Object Identifier (DOI) of the dataset.</p> <code>None</code> <code>daac</code> <code>str</code> <p>The Distributed Active Archive Center (DAAC) of the dataset.</p> <code>None</code> <code>provider</code> <code>str</code> <p>The provider of the dataset.</p> <code>None</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_nasa_data(\n    count: int = -1,\n    short_name: Optional[str] = None,\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    version: Optional[str] = None,\n    doi: Optional[str] = None,\n    daac: Optional[str] = None,\n    provider: Optional[str] = None,\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA Earthdata granules.\n\n    Args:\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset.\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        version (str, optional): The version of the dataset.\n        doi (str, optional): The Digital Object Identifier (DOI) of the dataset.\n        daac (str, optional): The Distributed Active Archive Center (DAAC) of the dataset.\n        provider (str, optional): The provider of the dataset.\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    if isinstance(bbox, list):\n        bbox = tuple(bbox)\n\n    return leafmap.nasa_data_search(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        version=version,\n        doi=doi,\n        daac=daac,\n        provider=provider,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.search_pace","title":"<code>search_pace(bbox=None, temporal=None, count=-1, short_name='PACE_OCI_L2_AOP_NRT', output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA PACE granules.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset. Defaults to \"PACE_OCI_L2_AOP_NRT\".</p> <code>'PACE_OCI_L2_AOP_NRT'</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_pace(\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    count: int = -1,\n    short_name: Optional[str] = \"PACE_OCI_L2_AOP_NRT\",\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA PACE granules.\n\n    Args:\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset. Defaults to \"PACE_OCI_L2_AOP_NRT\".\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    return search_nasa_data(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.search_pace_chla","title":"<code>search_pace_chla(bbox=None, temporal=None, count=-1, short_name='PACE_OCI_L3M_CHL_NRT', granule_name='*.DAY.*.0p1deg.*', output=None, crs='EPSG:4326', return_gdf=False, **kwargs)</code>","text":"<p>Searches for NASA PACE Chlorophyll granules.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>List[float]</code> <p>The bounding box coordinates [xmin, ymin, xmax, ymax].</p> <code>None</code> <code>temporal</code> <code>str</code> <p>The temporal extent of the data.</p> <code>None</code> <code>count</code> <code>int</code> <p>The number of granules to retrieve. Defaults to -1 (retrieve all).</p> <code>-1</code> <code>short_name</code> <code>str</code> <p>The short name of the dataset. Defaults to \"PACE_OCI_L3M_CHL_NRT\".</p> <code>'PACE_OCI_L3M_CHL_NRT'</code> <code>output</code> <code>str</code> <p>The output file path to save the GeoDataFrame as a file.</p> <code>None</code> <code>crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>return_gdf</code> <code>bool</code> <p>Whether to return the GeoDataFrame in addition to the granules. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the earthaccess.search_data() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[dict], tuple]</code> <p>The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def search_pace_chla(\n    bbox: Optional[List[float]] = None,\n    temporal: Optional[str] = None,\n    count: int = -1,\n    short_name: Optional[str] = \"PACE_OCI_L3M_CHL_NRT\",\n    granule_name: Optional[str] = \"*.DAY.*.0p1deg.*\",\n    output: Optional[str] = None,\n    crs: str = \"EPSG:4326\",\n    return_gdf: bool = False,\n    **kwargs,\n) -&gt; Union[List[dict], tuple]:\n    \"\"\"Searches for NASA PACE Chlorophyll granules.\n\n    Args:\n        bbox (List[float], optional): The bounding box coordinates [xmin, ymin, xmax, ymax].\n        temporal (str, optional): The temporal extent of the data.\n        count (int, optional): The number of granules to retrieve. Defaults to -1 (retrieve all).\n        short_name (str, optional): The short name of the dataset. Defaults to \"PACE_OCI_L3M_CHL_NRT\".\n        output (str, optional): The output file path to save the GeoDataFrame as a file.\n        crs (str, optional): The coordinate reference system (CRS) of the GeoDataFrame. Defaults to \"EPSG:4326\".\n        return_gdf (bool, optional): Whether to return the GeoDataFrame in addition to the granules. Defaults to False.\n        **kwargs: Additional keyword arguments for the earthaccess.search_data() function.\n\n    Returns:\n        Union[List[dict], tuple]: The retrieved granules. If return_gdf is True, also returns the resulting GeoDataFrame.\n    \"\"\"\n\n    return search_nasa_data(\n        count=count,\n        short_name=short_name,\n        bbox=bbox,\n        temporal=temporal,\n        granule_name=granule_name,\n        output=output,\n        crs=crs,\n        return_gdf=return_gdf,\n        **kwargs,\n    )\n</code></pre>"},{"location":"common/#hypercoast.common.show_field_data","title":"<code>show_field_data(data, x_col='wavelength', y_col_prefix='(', x_label='Wavelengths (nm)', y_label='Reflectance', use_marker_cluster=True, min_width=400, max_width=600, min_height=200, max_height=250, layer_name='Marker Cluster', m=None, center=(20, 0), zoom=2)</code>","text":"<p>Displays field data on a map with interactive markers and popups showing time series data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, pd.DataFrame]</code> <p>Path to the CSV file or a pandas DataFrame containing the data.</p> required <code>x_col</code> <code>str</code> <p>Column name to use for the x-axis of the charts. Default is \"wavelength\".</p> <code>'wavelength'</code> <code>y_col_prefix</code> <code>str</code> <p>Prefix to identify the columns that contain the location-specific data. Default is \"(\".</p> <code>'('</code> <code>x_label</code> <code>str</code> <p>Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".</p> <code>'Wavelengths (nm)'</code> <code>y_label</code> <code>str</code> <p>Label for the y-axis of the charts. Default is \"Reflectance\".</p> <code>'Reflectance'</code> <code>use_marker_cluster</code> <code>bool</code> <p>Whether to use marker clustering. Default is True.</p> <code>True</code> <code>min_width</code> <code>int</code> <p>Minimum width of the popup. Default is 400.</p> <code>400</code> <code>max_width</code> <code>int</code> <p>Maximum width of the popup. Default is 600.</p> <code>600</code> <code>min_height</code> <code>int</code> <p>Minimum height of the popup. Default is 200.</p> <code>200</code> <code>max_height</code> <code>int</code> <p>Maximum height of the popup. Default is 250.</p> <code>250</code> <code>layer_name</code> <code>str</code> <p>Name of the marker cluster layer. Default is \"Marker Cluster\".</p> <code>'Marker Cluster'</code> <code>m</code> <code>Map</code> <p>An ipyleaflet Map instance to add the markers to. Default is None.</p> <code>None</code> <code>center</code> <code>Tuple[float, float]</code> <p>Center of the map as a tuple of (latitude, longitude). Default is (20, 0).</p> <code>(20, 0)</code> <code>zoom</code> <code>int</code> <p>Zoom level of the map. Default is 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>Map</code> <p>An ipyleaflet Map with the added markers and popups.</p> Source code in <code>hypercoast/common.py</code> <pre><code>def show_field_data(\n    data: Union[str],\n    x_col: str = \"wavelength\",\n    y_col_prefix: str = \"(\",\n    x_label: str = \"Wavelengths (nm)\",\n    y_label: str = \"Reflectance\",\n    use_marker_cluster: bool = True,\n    min_width: int = 400,\n    max_width: int = 600,\n    min_height: int = 200,\n    max_height: int = 250,\n    layer_name: str = \"Marker Cluster\",\n    m: object = None,\n    center: Tuple[float, float] = (20, 0),\n    zoom: int = 2,\n):\n    \"\"\"\n    Displays field data on a map with interactive markers and popups showing time series data.\n\n    Args:\n        data (Union[str, pd.DataFrame]): Path to the CSV file or a pandas DataFrame containing the data.\n        x_col (str): Column name to use for the x-axis of the charts. Default is \"wavelength\".\n        y_col_prefix (str): Prefix to identify the columns that contain the location-specific data. Default is \"(\".\n        x_label (str): Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".\n        y_label (str): Label for the y-axis of the charts. Default is \"Reflectance\".\n        use_marker_cluster (bool): Whether to use marker clustering. Default is True.\n        min_width (int): Minimum width of the popup. Default is 400.\n        max_width (int): Maximum width of the popup. Default is 600.\n        min_height (int): Minimum height of the popup. Default is 200.\n        max_height (int): Maximum height of the popup. Default is 250.\n        layer_name (str): Name of the marker cluster layer. Default is \"Marker Cluster\".\n        m (Map, optional): An ipyleaflet Map instance to add the markers to. Default is None.\n        center (Tuple[float, float]): Center of the map as a tuple of (latitude, longitude). Default is (20, 0).\n        zoom (int): Zoom level of the map. Default is 2.\n\n    Returns:\n        Map: An ipyleaflet Map with the added markers and popups.\n    \"\"\"\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from ipyleaflet import Map, Marker, Popup, MarkerCluster\n    from ipywidgets import Output, VBox\n\n    # Read the CSV file\n    if isinstance(data, str):\n        data = pd.read_csv(data)\n    elif isinstance(data, pd.DataFrame):\n        pass\n    else:\n        raise ValueError(\"data must be a path to a CSV file or a pandas DataFrame\")\n\n    # Extract locations from columns\n    locations = [col for col in data.columns if col.startswith(y_col_prefix)]\n    coordinates = [tuple(map(float, loc.strip(\"()\").split())) for loc in locations]\n\n    # Create the map\n    if m is None:\n        m = Map(center=center, zoom=zoom)\n\n    # Function to create the chart\n    def create_chart(data, title):\n        _, ax = plt.subplots(figsize=(10, 6))  # Adjust the figure size here\n        ax.plot(data[x_col], data[\"values\"])\n        ax.set_title(title)\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n        output = Output()  # Adjust the output widget size here\n        with output:\n            plt.show()\n        return output\n\n    # Define a callback function to create and show the popup\n    def callback_with_popup_creation(location, values):\n        def f(**kwargs):\n            marker_center = kwargs[\"coordinates\"]\n            output = create_chart(values, f\"Location: {location}\")\n            popup = Popup(\n                location=marker_center,\n                child=VBox([output]),\n                min_width=min_width,\n                max_width=max_width,\n                min_height=min_height,\n                max_height=max_height,\n            )\n            m.add_layer(popup)\n\n        return f\n\n    markers = []\n\n    # Add points to the map\n    for i, coord in enumerate(coordinates):\n        location = f\"{coord}\"\n        values = pd.DataFrame({x_col: data[x_col], \"values\": data[locations[i]]})\n        marker = Marker(location=coord, title=location, name=f\"Marker {i + 1}\")\n        marker.on_click(callback_with_popup_creation(location, values))\n        markers.append(marker)\n\n    if use_marker_cluster:\n        marker_cluster = MarkerCluster(markers=markers, name=layer_name)\n        m.add_layer(marker_cluster)\n    else:\n        for marker in markers:\n            m.add_layer(marker)\n\n    return m\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/opengeos/HyperCoast/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>HyperCoast could always use more documentation, whether as part of the official HyperCoast docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/opengeos/HyperCoast/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up HyperCoast for local development.</p> <ol> <li> <p>Fork the HyperCoast repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone https://github.com/&lt;YOUR-GITHUB-USERNAME&gt;/HyperCoast.git\n</code></pre> </li> <li> <p>Create a new conda environment to install HyperCoast and its dependencies. Assuming you have     Anaconda or     Miniconda installed,     this is how you set up your fork for local development:</p> <pre><code>$ conda install -n base mamba -c conda-forge\n$ conda create -n hyper python=3.11\n$ conda activate hyper\n$ mamba install -c conda-forge hypercoast cartopy earthaccess mapclassify pyvista\n$ cd HyperCoast/\n$ pip install -e .\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass     pre-commit and the tests:</p> <pre><code>$ pip install pre-commit\n$ pre-commit install\n$ pre-commit run --all-files\n$ python -m unittest discover tests/\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> <li>Check the status of your pull request on GitHub and make sure     that the tests for the pull request pass for all supported Python versions.</li> <li>Commit more changes to your branch to fix the text errors if necessary.</li> <li>Wait for the pull request to be reviewed by the maintainers.</li> <li>Congratulations! You've made your contribution to HyperCoast!</li> </ol>"},{"location":"contributing/#contributor-agreements","title":"Contributor Agreements","text":"<p>Before your contribution can be accepted, you will need to sign the appropriate contributor agreement. The Contributor License Agreement (CLA) assistant will walk you through the process of signing the CLA. Please follow the instructions provided by the assistant on the pull request.</p> <ul> <li>Individual Contributor Exclusive License Agreement</li> <li>Entity Contributor Exclusive License Agreement</li> </ul>"},{"location":"desis/","title":"desis module","text":"<p>This Module has the functions related to working with a DESIS dataset.</p>"},{"location":"desis/#hypercoast.desis.desis_to_image","title":"<code>desis_to_image(dataset, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts an DESIS dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xarray.Dataset or str</code> <p>The dataset containing the DESIS data or the file path to the dataset.</p> required <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the dataset. If     <code>output</code> is provided, the image will be saved to the specified file     and the function will return None.</p> Source code in <code>hypercoast/desis.py</code> <pre><code>def desis_to_image(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Union[list, tuple] = None,\n    method: Optional[str] = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Converts an DESIS dataset to an image.\n\n    Args:\n        dataset (xarray.Dataset or str): The dataset containing the DESIS data\n            or the file path to the dataset.\n        wavelengths (array-like, optional): The specific wavelengths to select.\n            If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n        output (str, optional): The file path where the image will be saved. If\n            None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to\n            `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the dataset. If\n            `output` is provided, the image will be saved to the specified file\n            and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_desis(dataset, method=method)\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n\n    return array_to_image(\n        dataset[\"reflectance\"], output=output, transpose=False, **kwargs\n    )\n</code></pre>"},{"location":"desis/#hypercoast.desis.extract_desis","title":"<code>extract_desis(ds, lat, lon)</code>","text":"<p>Extracts DESIS data from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>The dataset containing the DESIS data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted data.</p> Source code in <code>hypercoast/desis.py</code> <pre><code>def extract_desis(ds: xr.Dataset, lat: float, lon: float) -&gt; xr.DataArray:\n    \"\"\"\n    Extracts DESIS data from a given xarray Dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the DESIS data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n\n    Returns:\n        xarray.DataArray: The extracted data.\n    \"\"\"\n\n    crs = ds.attrs[\"crs\"]\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    values = ds.sel(x=x, y=y, method=\"nearest\")[\"reflectance\"].values / 10000\n\n    da = xr.DataArray(\n        values, dims=[\"wavelength\"], coords={\"wavelength\": ds.coords[\"wavelength\"]}\n    )\n\n    return da\n</code></pre>"},{"location":"desis/#hypercoast.desis.filter_desis","title":"<code>filter_desis(dataset, lat, lon, return_plot=False, **kwargs)</code>","text":"<p>Filters a DESIS dataset based on latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The DESIS dataset to filter.</p> required <code>lat</code> <code>float or tuple</code> <p>The latitude to filter by. If a tuple or list, it represents a range.</p> required <code>lon</code> <code>float or tuple</code> <p>The longitude to filter by. If a tuple or list, it represents a range.</p> required <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The filtered DESIS data.</p> Source code in <code>hypercoast/desis.py</code> <pre><code>def filter_desis(\n    dataset: xr.Dataset,\n    lat: Union[float, tuple],\n    lon: Union[float, tuple],\n    return_plot: Optional[bool] = False,\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Filters a DESIS dataset based on latitude and longitude.\n\n    Args:\n        dataset (xr.Dataset): The DESIS dataset to filter.\n        lat (float or tuple): The latitude to filter by. If a tuple or list,\n            it represents a range.\n        lon (float or tuple): The longitude to filter by. If a tuple or\n            list, it represents a range.\n\n    Returns:\n        xr.DataArray: The filtered DESIS data.\n    \"\"\"\n\n    if isinstance(lat, list) or isinstance(lat, tuple):\n        min_lat = min(lat)\n        max_lat = max(lat)\n    else:\n        min_lat = lat\n        max_lat = lat\n\n    if isinstance(lon, list) or isinstance(lon, tuple):\n        min_lon = min(lon)\n        max_lon = max(lon)\n    else:\n        min_lon = lon\n        max_lon = lon\n\n    if min_lat == max_lat and min_lon == max_lon:\n        coords = [[min_lat, min_lon]]\n    else:\n        coords = [[min_lat, min_lon], [max_lat, max_lon]]\n    coords = convert_coords(coords, \"epsg:4326\", dataset.rio.crs.to_string())\n\n    if len(coords) == 1:\n        x, y = coords[0]\n        da = dataset.sel(x=x, y=y, method=\"nearest\")[\"reflectance\"]\n    else:\n        x_min, y_min = coords[0]\n        x_max, y_max = coords[1]\n        print(x_min, y_min, x_max, y_max)\n        da = dataset.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))[\"reflectance\"]\n\n    if return_plot:\n        rrs_stack = da.stack(\n            {\"pixel\": [\"latitude\", \"longitude\"]},\n            create_index=False,\n        )\n        rrs_stack.plot.line(hue=\"pixel\", **kwargs)\n    else:\n        return da\n</code></pre>"},{"location":"desis/#hypercoast.desis.read_desis","title":"<code>read_desis(filepath, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Reads DESIS data from a given file and returns an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the file to read.</p> required <code>wavelengths</code> <code>array-like</code> <p>Specific wavelengths to select. If None, all wavelengths are selected.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for selection when wavelengths is not None. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>sel</code> method when bands is not None.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>An xarray Dataset containing the DESIS data.</p> Source code in <code>hypercoast/desis.py</code> <pre><code>def read_desis(\n    filepath: str,\n    wavelengths: Optional[Union[list, tuple]] = None,\n    method: Optional[str] = \"nearest\",\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads DESIS data from a given file and returns an xarray Dataset.\n\n    Args:\n        filepath (str): Path to the file to read.\n        wavelengths (array-like, optional): Specific wavelengths to select. If\n            None, all wavelengths are selected.\n        method (str, optional): Method to use for selection when wavelengths is not\n            None. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to pass to the `sel` method when\n            bands is not None.\n\n    Returns:\n        xr.Dataset: An xarray Dataset containing the DESIS data.\n    \"\"\"\n\n    url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/desis_wavelengths.csv\"\n    df = pd.read_csv(url)\n    dataset = xr.open_dataset(filepath)\n    dataset = dataset.rename(\n        {\"band\": \"wavelength\", \"band_data\": \"reflectance\"}\n    ).transpose(\"y\", \"x\", \"wavelength\")\n    dataset[\"wavelength\"] = df[\"wavelength\"].tolist()\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method, **kwargs)\n\n    dataset.attrs[\"crs\"] = dataset.rio.crs.to_string()\n\n    return dataset\n</code></pre>"},{"location":"emit/","title":"emit module","text":"<p>This Module has the functions related to working with an EMIT dataset. This includes doing things like opening and flattening the data to work in xarray, orthorectification, and visualization.</p> <p>Some source code is adapted from https://github.com/nasa/EMIT-Data-Resources, which is licensed under the Apache License 2.0. Credits to the original authors, including Erik Bolch, Alex Leigh, and others.</p>"},{"location":"emit/#hypercoast.emit.apply_glt","title":"<code>apply_glt(ds_array, glt_array, fill_value=-9999, GLT_NODATA_VALUE=0)</code>","text":"<p>Applies the GLT array to a numpy array of either 2 or 3 dimensions to orthorectify the data.</p> <p>Parameters:</p> Name Type Description Default <code>ds_array</code> <code>numpy.ndarray</code> <p>A numpy array of the desired variable.</p> required <code>glt_array</code> <code>numpy.ndarray</code> <p>A GLT array constructed from EMIT GLT data.</p> required <code>fill_value</code> <code>int</code> <p>The value to fill in the output array where the GLT array has no data. Defaults to -9999.</p> <code>-9999</code> <code>GLT_NODATA_VALUE</code> <code>int</code> <p>The value in the GLT array that indicates no data. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>A numpy array of orthorectified data.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def apply_glt(\n    ds_array: np.ndarray,\n    glt_array: np.ndarray,\n    fill_value: int = -9999,\n    GLT_NODATA_VALUE: Optional[int] = 0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Applies the GLT array to a numpy array of either 2 or 3 dimensions to orthorectify the data.\n\n    Args:\n        ds_array (numpy.ndarray): A numpy array of the desired variable.\n        glt_array (numpy.ndarray): A GLT array constructed from EMIT GLT data.\n        fill_value (int, optional): The value to fill in the output array where the GLT array has no data. Defaults to -9999.\n        GLT_NODATA_VALUE (int, optional): The value in the GLT array that indicates no data. Defaults to 0.\n\n    Returns:\n        numpy.ndarray: A numpy array of orthorectified data.\n    \"\"\"\n\n    # Build Output Dataset\n    if ds_array.ndim == 2:\n        ds_array = ds_array[:, :, np.newaxis]\n    out_ds = np.full(\n        (glt_array.shape[0], glt_array.shape[1], ds_array.shape[-1]),\n        fill_value,\n        dtype=np.float32,\n    )\n    valid_glt = np.all(glt_array != GLT_NODATA_VALUE, axis=-1)\n\n    # Adjust for One based Index - make a copy to prevent decrementing multiple times inside ortho_xr when applying the glt to elev\n    glt_array_copy = glt_array.copy()\n    glt_array_copy[valid_glt] -= 1\n    out_ds[valid_glt, :] = ds_array[\n        glt_array_copy[valid_glt, 1], glt_array_copy[valid_glt, 0], :\n    ]\n    return out_ds\n</code></pre>"},{"location":"emit/#hypercoast.emit.band_mask","title":"<code>band_mask(filepath)</code>","text":"<p>Unpacks the packed band mask to apply to the dataset. Can be used manually or as an input in the emit_xarray() function.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>An EMIT L2A Mask netCDF file.</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>A numpy array that can be used with the emit_xarray function to apply a band mask.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def band_mask(filepath: str) -&gt; np.ndarray:\n    \"\"\"\n    Unpacks the packed band mask to apply to the dataset. Can be used manually or as an input in the emit_xarray() function.\n\n    Args:\n        filepath (str): An EMIT L2A Mask netCDF file.\n\n    Returns:\n        numpy.ndarray: A numpy array that can be used with the emit_xarray function to apply a band mask.\n    \"\"\"\n    # Open Dataset\n    mask_ds = xr.open_dataset(filepath, engine=\"h5netcdf\")\n    # Open band_mask and convert to uint8\n    bmask = mask_ds.band_mask.data.astype(\"uint8\")\n    # Print Flags used\n    unpacked_bmask = np.unpackbits(bmask, axis=-1)\n    # Remove bands &gt; 285\n    unpacked_bmask = unpacked_bmask[:, :, 0:285]\n    # Check for data bands and build mask\n    return unpacked_bmask\n</code></pre>"},{"location":"emit/#hypercoast.emit.coord_vects","title":"<code>coord_vects(ds)</code>","text":"<p>This function calculates the Lat and Lon Coordinate Vectors using the GLT and Metadata from an EMIT dataset read into xarray.</p> <p>lon, lat (numpy.array): longitude and latitude array grid for the dataset</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def coord_vects(ds: xr.Dataset) -&gt; np.ndarray:\n    \"\"\"\n    This function calculates the Lat and Lon Coordinate Vectors using the GLT and Metadata from an EMIT dataset read into xarray.\n\n    Parameters:\n    ds: an xarray.Dataset containing the root variable and metadata of an EMIT dataset\n    loc: an xarray.Dataset containing the 'location' group of an EMIT dataset\n\n    Returns:\n    lon, lat (numpy.array): longitude and latitude array grid for the dataset\n\n    \"\"\"\n    # Retrieve Geotransform from Metadata\n    GT = ds.geotransform\n    # Create Array for Lat and Lon and fill\n    dim_x = ds.glt_x.shape[1]\n    dim_y = ds.glt_x.shape[0]\n    lon = np.zeros(dim_x)\n    lat = np.zeros(dim_y)\n    # Note: no rotation for EMIT Data\n    for x in np.arange(dim_x):\n        x_geo = (GT[0] + 0.5 * GT[1]) + x * GT[1]  # Adjust coordinates to pixel-center\n        lon[x] = x_geo\n    for y in np.arange(dim_y):\n        y_geo = (GT[3] + 0.5 * GT[5]) + y * GT[5]\n        lat[y] = y_geo\n    return lon, lat\n</code></pre>"},{"location":"emit/#hypercoast.emit.emit_to_image","title":"<code>emit_to_image(data, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts an EMIT dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>xarray.Dataset or str</code> <p>The dataset containing the EMIT data or the file path to the dataset.</p> required <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the dataset. If <code>output</code> is provided, the image will be saved to the specified file and the function will return None.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def emit_to_image(\n    data: Union[xr.Dataset, str],\n    wavelengths: Optional[Union[list, tuple]] = None,\n    method: Optional[str] = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Converts an EMIT dataset to an image.\n\n    Args:\n        data (xarray.Dataset or str): The dataset containing the EMIT data or the file path to the dataset.\n        wavelengths (array-like, optional): The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n        output (str, optional): The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the dataset. If `output` is provided, the image will be saved to the specified file and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(data, str):\n        data = read_emit(data, ortho=True)\n\n    ds = data[\"reflectance\"]\n\n    if wavelengths is not None:\n        ds = ds.sel(wavelength=wavelengths, method=method)\n    return array_to_image(ds, transpose=False, output=output, **kwargs)\n</code></pre>"},{"location":"emit/#hypercoast.emit.emit_to_netcdf","title":"<code>emit_to_netcdf(data, output, **kwargs)</code>","text":"<p>Transposes an EMIT dataset and saves it as a NetCDF file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>xarray.Dataset or str</code> <p>The dataset containing the EMIT data or the file path to the dataset.</p> required <code>output</code> <code>str</code> <p>The file path where the NetCDF file will be saved.</p> required <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>xarray.Dataset.to_netcdf</code>.</p> <code>{}</code> Source code in <code>hypercoast/emit.py</code> <pre><code>def emit_to_netcdf(data: Union[xr.Dataset, str], output: str, **kwargs) -&gt; None:\n    \"\"\"\n    Transposes an EMIT dataset and saves it as a NetCDF file.\n\n    Args:\n        data (xarray.Dataset or str): The dataset containing the EMIT data or the file path to the dataset.\n        output (str): The file path where the NetCDF file will be saved.\n        **kwargs: Additional keyword arguments to be passed to `xarray.Dataset.to_netcdf`.\n\n    \"\"\"\n    if isinstance(data, str):\n        data = read_emit(data, ortho=True)\n\n    ds_geo = data.transpose(\"wavelengths\", \"latitude\", \"longitude\")\n    ds_geo.to_netcdf(output, **kwargs)\n</code></pre>"},{"location":"emit/#hypercoast.emit.emit_xarray","title":"<code>emit_xarray(filepath, ortho=False, qmask=None, unpacked_bmask=None, wavelengths=None, method='nearest')</code>","text":"<p>Streamlines opening an EMIT dataset as an xarray.Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>A filepath to an EMIT netCDF file.</p> required <code>ortho</code> <code>bool</code> <p>Whether to orthorectify the dataset or leave in crosstrack/downtrack coordinates. Defaults to False.</p> <code>False</code> <code>qmask</code> <code>numpy.ndarray</code> <p>A numpy array output from the quality_mask function used to mask pixels based on quality flags selected in that function. Any non-orthorectified array with the proper crosstrack and downtrack dimensions can also be used. Defaults to None.</p> <code>None</code> <code>unpacked_bmask</code> <code>numpy.ndarray</code> <p>A numpy array from the band_mask function that can be used to mask band-specific pixels that have been interpolated. Defaults to None.</p> <code>None</code> <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>An xarray.Dataset constructed based on the parameters provided.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def emit_xarray(\n    filepath: str,\n    ortho: Optional[bool] = False,\n    qmask: Optional[np.ndarray] = None,\n    unpacked_bmask: Optional[np.ndarray] = None,\n    wavelengths: Optional[Union[tuple, list]] = None,\n    method: Optional[str] = \"nearest\",\n) -&gt; xr.Dataset:\n    \"\"\"\n    Streamlines opening an EMIT dataset as an xarray.Dataset.\n\n    Args:\n        filepath (str): A filepath to an EMIT netCDF file.\n        ortho (bool, optional): Whether to orthorectify the dataset or leave in crosstrack/downtrack coordinates. Defaults to False.\n        qmask (numpy.ndarray, optional): A numpy array output from the quality_mask function used to mask pixels based on quality flags selected in that function. Any non-orthorectified array with the proper crosstrack and downtrack dimensions can also be used. Defaults to None.\n        unpacked_bmask (numpy.ndarray, optional): A numpy array from the band_mask function that can be used to mask band-specific pixels that have been interpolated. Defaults to None.\n        wavelengths (array-like, optional): The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n\n    Returns:\n        xarray.Dataset: An xarray.Dataset constructed based on the parameters provided.\n    \"\"\"\n    # Grab granule filename to check product\n    from s3fs.core import S3File\n    from fsspec.implementations.http import HTTPFile\n\n    if isinstance(filepath, S3File):\n        granule_id = filepath.info()[\"name\"].split(\"/\", -1)[-1].split(\".\", -1)[0]\n    elif isinstance(filepath, HTTPFile):\n        granule_id = filepath.path.split(\"/\", -1)[-1].split(\".\", -1)[0]\n    else:\n        granule_id = os.path.splitext(os.path.basename(filepath))[0]\n\n    # Read in Data as Xarray Datasets\n    engine, wvl_group = \"h5netcdf\", None\n\n    ds = xr.open_dataset(filepath, engine=engine)\n    loc = xr.open_dataset(filepath, engine=engine, group=\"location\")\n\n    # Check if mineral dataset and read in groups (only ds/loc for minunc)\n\n    if \"L2B_MIN_\" in granule_id:\n        wvl_group = \"mineral_metadata\"\n    elif \"L2B_MINUNC\" not in granule_id:\n        wvl_group = \"sensor_band_parameters\"\n\n    wvl = None\n\n    if wvl_group:\n        wvl = xr.open_dataset(filepath, engine=engine, group=wvl_group)\n\n    # Building Flat Dataset from Components\n    data_vars = {**ds.variables}\n\n    # Format xarray coordinates based upon emit product (no wvl for mineral uncertainty)\n    coords = {\n        \"downtrack\": ([\"downtrack\"], ds.downtrack.data),\n        \"crosstrack\": ([\"crosstrack\"], ds.crosstrack.data),\n        **loc.variables,\n    }\n\n    product_band_map = {\n        \"L2B_MIN_\": \"name\",\n        \"L2A_MASK_\": \"mask_bands\",\n        \"L1B_OBS_\": \"observation_bands\",\n        \"L2A_RFL_\": \"wavelengths\",\n        \"L1B_RAD_\": \"wavelengths\",\n        \"L2A_RFLUNCERT_\": \"wavelengths\",\n    }\n\n    # if band := product_band_map.get(next((k for k in product_band_map.keys() if k in granule_id), 'unknown'), None):\n    # coords['bands'] = wvl[band].data\n\n    if wvl:\n        coords = {**coords, **wvl.variables}\n\n    out_xr = xr.Dataset(data_vars=data_vars, coords=coords, attrs=ds.attrs)\n    out_xr.attrs[\"granule_id\"] = granule_id\n\n    if band := product_band_map.get(\n        next((k for k in product_band_map.keys() if k in granule_id), \"unknown\"), None\n    ):\n        if \"minerals\" in list(out_xr.dims):\n            out_xr = out_xr.swap_dims({\"minerals\": band})\n            out_xr = out_xr.rename({band: \"mineral_name\"})\n        else:\n            out_xr = out_xr.swap_dims({\"bands\": band})\n\n    # Apply Quality and Band Masks, set fill values to NaN\n    for var in list(ds.data_vars):\n        if qmask is not None:\n            out_xr[var].data[qmask == 1] = np.nan\n        if unpacked_bmask is not None:\n            out_xr[var].data[unpacked_bmask == 1] = np.nan\n        out_xr[var].data[out_xr[var].data == -9999] = np.nan\n\n    if ortho is True:\n        out_xr = ortho_xr(out_xr)\n        out_xr.attrs[\"Orthorectified\"] = \"True\"\n\n    if wavelengths is not None:\n        out_xr = out_xr.sel(wavelengths=wavelengths, method=method)\n\n    out_xr = out_xr.rename({\"wavelengths\": \"wavelength\"})\n    return out_xr\n</code></pre>"},{"location":"emit/#hypercoast.emit.envi_header","title":"<code>envi_header(inputpath)</code>","text":"<p>Convert a ENVI binary/header path to a header, handling extensions.</p> <p>Parameters:</p> Name Type Description Default <code>inputpath</code> <code>str</code> <p>Path to ENVI binary file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The header file associated with the input reference. If the header file does not exist, it returns the expected header file path.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def envi_header(inputpath: str) -&gt; str:\n    \"\"\"\n    Convert a ENVI binary/header path to a header, handling extensions.\n\n    Args:\n        inputpath (str): Path to ENVI binary file.\n\n    Returns:\n        str: The header file associated with the input reference. If the header file does not exist, it returns the expected header file path.\n    \"\"\"\n    if (\n        os.path.splitext(inputpath)[-1] == \".img\"\n        or os.path.splitext(inputpath)[-1] == \".dat\"\n        or os.path.splitext(inputpath)[-1] == \".raw\"\n    ):\n        # headers could be at either filename.img.hdr or filename.hdr.  Check both, return the one that exists if it\n        # does, if not return the latter (new file creation presumed).\n        hdrfile = os.path.splitext(inputpath)[0] + \".hdr\"\n        if os.path.isfile(hdrfile):\n            return hdrfile\n        elif os.path.isfile(inputpath + \".hdr\"):\n            return inputpath + \".hdr\"\n        return hdrfile\n    elif os.path.splitext(inputpath)[-1] == \".hdr\":\n        return inputpath\n    else:\n        return inputpath + \".hdr\"\n</code></pre>"},{"location":"emit/#hypercoast.emit.is_adjacent","title":"<code>is_adjacent(scene, same_orbit)</code>","text":"<p>Checks if the scene numbers from the same orbit are adjacent/sequential.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>str</code> <p>The scene number to check.</p> required <code>same_orbit</code> <code>list</code> <p>A list of scene numbers from the same orbit.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the scene numbers are adjacent/sequential, False otherwise.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def is_adjacent(scene: str, same_orbit: list) -&gt; bool:\n    \"\"\"\n    Checks if the scene numbers from the same orbit are adjacent/sequential.\n\n    Args:\n        scene (str): The scene number to check.\n        same_orbit (list): A list of scene numbers from the same orbit.\n\n    Returns:\n        bool: True if the scene numbers are adjacent/sequential, False otherwise.\n    \"\"\"\n    scene_nums = [int(scene.split(\".\")[-2].split(\"_\")[-1]) for scene in same_orbit]\n    return all(b - a == 1 for a, b in zip(scene_nums[:-1], scene_nums[1:]))\n</code></pre>"},{"location":"emit/#hypercoast.emit.merge_emit","title":"<code>merge_emit(datasets, gdf)</code>","text":"<p>Merges xarray datasets formatted using emit_xarray. Note: GDF may only work with a single geometry.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>dict</code> <p>A dictionary of xarray datasets formatted using emit_xarray.</p> required <code>gdf</code> <code>gpd.GeoDataFrame</code> <p>A GeoDataFrame containing the geometry to be used for merging.</p> required <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>A merged xarray dataset.</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>If there are inconsistencies in the 1D variables across datasets.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def merge_emit(datasets: dict, gdf: gpd.GeoDataFrame) -&gt; xr.Dataset:\n    \"\"\"\n    Merges xarray datasets formatted using emit_xarray. Note: GDF may only work with a single geometry.\n\n    Args:\n        datasets (dict): A dictionary of xarray datasets formatted using emit_xarray.\n        gdf (gpd.GeoDataFrame): A GeoDataFrame containing the geometry to be used for merging.\n\n    Returns:\n        xarray.Dataset: A merged xarray dataset.\n\n    Raises:\n        Exception: If there are inconsistencies in the 1D variables across datasets.\n    \"\"\"\n    from rioxarray.merge import merge_arrays\n\n    nested_data_arrays = {}\n    # loop over datasets\n    for dataset in datasets:\n        # create dictionary of arrays for each dataset\n\n        # create dictionary of 1D variables, which should be consistent across datasets\n        one_d_arrays = {}\n\n        # Dictionary of variables to merge\n        data_arrays = {}\n        # Loop over variables in dataset including elevation\n        for var in list(datasets[dataset].data_vars) + [\"elev\"]:\n            # Get 1D for this variable and add to dictionary\n            if not one_d_arrays:\n                # These should be an array describing the others (wavelengths, mask_bands, etc.)\n                one_dim = [\n                    item\n                    for item in list(datasets[dataset].coords)\n                    if item not in [\"latitude\", \"longitude\", \"spatial_ref\"]\n                    and len(datasets[dataset][item].dims) == 1\n                ]\n                # print(one_dim)\n                for od in one_dim:\n                    one_d_arrays[od] = datasets[dataset].coords[od].data\n\n                # Update format for merging - This could probably be improved\n            da = datasets[dataset][var].reset_coords(\"elev\", drop=False)\n            da = da.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n            if len(da.dims) == 3:\n                if any(item in list(da.coords) for item in one_dim):\n                    da = da.drop_vars(one_dim)\n                da = da.drop_vars(\"elev\")\n                da = da.to_array(name=var).squeeze(\"variable\", drop=True)\n                da = da.transpose(da.dims[-1], da.dims[0], da.dims[1])\n                # print(da.dims)\n            if var == \"elev\":\n                da = da.to_array(name=var).squeeze(\"variable\", drop=True)\n            data_arrays[var] = da\n            nested_data_arrays[dataset] = data_arrays\n\n            # Transpose the nested arrays dict. This is horrible to read, but works to pair up variables (ie mask) from the different granules\n    transposed_dict = {\n        inner_key: {\n            outer_key: inner_dict[inner_key]\n            for outer_key, inner_dict in nested_data_arrays.items()\n        }\n        for inner_key in nested_data_arrays[next(iter(nested_data_arrays))]\n    }\n\n    # remove some unused data\n    del nested_data_arrays, data_arrays, da\n\n    # Merge the arrays using rioxarray.merge_arrays()\n    merged = {}\n    for _var in transposed_dict:\n        merged[_var] = merge_arrays(\n            list(transposed_dict[_var].values()),\n            bounds=gdf.unary_union.bounds,\n            nodata=np.nan,\n        )\n\n    # Create a new xarray dataset from the merged arrays\n    # Create Merged Dataset\n    merged_ds = xr.Dataset(data_vars=merged, coords=one_d_arrays)\n    # Rename x and y to longitude and latitude\n    merged_ds = merged_ds.rename({\"y\": \"latitude\", \"x\": \"longitude\"})\n    del transposed_dict, merged\n    return merged_ds\n</code></pre>"},{"location":"emit/#hypercoast.emit.ortho_browse","title":"<code>ortho_browse(url, glt, spatial_ref, geotransform, white_background=True)</code>","text":"<p>Use an EMIT GLT, geotransform, and spatial ref to orthorectify a browse image. (browse images are in native resolution)</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the browse image.</p> required <code>glt</code> <code>numpy.ndarray</code> <p>A GLT array constructed from EMIT GLT data.</p> required <code>spatial_ref</code> <code>str</code> <p>Spatial reference system.</p> required <code>geotransform</code> <code>list</code> <p>A list of six numbers that define the affine transform between pixel coordinates and map coordinates.</p> required <code>white_background</code> <code>bool</code> <p>If True, the fill value for the orthorectified image is white (255). If False, the fill value is black (0). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>An orthorectified browse image in the form of an xarray DataArray.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def ortho_browse(\n    url: str,\n    glt: np.ndarray,\n    spatial_ref: str,\n    geotransform: list,\n    white_background: Optional[bool] = True,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Use an EMIT GLT, geotransform, and spatial ref to orthorectify a browse image. (browse images are in native resolution)\n\n    Args:\n        url (str): URL of the browse image.\n        glt (numpy.ndarray): A GLT array constructed from EMIT GLT data.\n        spatial_ref (str): Spatial reference system.\n        geotransform (list): A list of six numbers that define the affine transform between pixel coordinates and map coordinates.\n        white_background (bool, optional): If True, the fill value for the orthorectified image is white (255). If False, the fill value is black (0). Defaults to True.\n\n    Returns:\n        xarray.DataArray: An orthorectified browse image in the form of an xarray DataArray.\n    \"\"\"\n    from skimage import io\n\n    # Read Data\n    data = io.imread(url)\n    # Orthorectify using GLT and transpose so band is first dimension\n    if white_background is True:\n        fill = 255\n    else:\n        fill = 0\n    ortho_data = apply_glt(data, glt, fill_value=fill).transpose(2, 0, 1)\n    coords = {\n        \"y\": (\n            [\"y\"],\n            (geotransform[3] + 0.5 * geotransform[5])\n            + np.arange(glt.shape[0]) * geotransform[5],\n        ),\n        \"x\": (\n            [\"x\"],\n            (geotransform[0] + 0.5 * geotransform[1])\n            + np.arange(glt.shape[1]) * geotransform[1],\n        ),\n    }\n    ortho_data = ortho_data.astype(int)\n    ortho_data[ortho_data == -1] = 0\n    # Place in xarray.datarray\n    da = xr.DataArray(ortho_data, dims=[\"band\", \"y\", \"x\"], coords=coords)\n    da.rio.write_crs(spatial_ref, inplace=True)\n    return da\n</code></pre>"},{"location":"emit/#hypercoast.emit.ortho_xr","title":"<code>ortho_xr(ds, GLT_NODATA_VALUE=0, fill_value=-9999)</code>","text":"<p>Uses <code>apply_glt</code> to create an orthorectified xarray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>An xarray dataset produced by emit_xarray.</p> required <code>GLT_NODATA_VALUE</code> <code>int</code> <p>No data value for the GLT tables. Defaults to 0.</p> <code>0</code> <code>fill_value</code> <code>int</code> <p>The fill value for EMIT datasets. Defaults to -9999.</p> <code>-9999</code> <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>An orthocorrected xarray dataset.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def ortho_xr(\n    ds: xr.Dataset,\n    GLT_NODATA_VALUE: Optional[int] = 0,\n    fill_value: Optional[int] = -9999,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Uses `apply_glt` to create an orthorectified xarray dataset.\n\n    Args:\n        ds (xarray.Dataset): An xarray dataset produced by emit_xarray.\n        GLT_NODATA_VALUE (int, optional): No data value for the GLT tables. Defaults to 0.\n        fill_value (int, optional): The fill value for EMIT datasets. Defaults to -9999.\n\n    Returns:\n        xarray.Dataset: An orthocorrected xarray dataset.\n    \"\"\"\n    # Build glt_ds\n\n    glt_ds = np.nan_to_num(\n        np.stack([ds[\"glt_x\"].data, ds[\"glt_y\"].data], axis=-1), nan=GLT_NODATA_VALUE\n    ).astype(int)\n\n    # List Variables\n    var_list = list(ds.data_vars)\n\n    # Remove flat field from data vars - the flat field is only useful with additional information before orthorectification\n    if \"flat_field_update\" in var_list:\n        var_list.remove(\"flat_field_update\")\n\n    # Create empty dictionary for orthocorrected data vars\n    data_vars = {}\n\n    # Extract Rawspace Dataset Variable Values (Typically Reflectance)\n    for var in var_list:\n        raw_ds = ds[var].data\n        var_dims = ds[var].dims\n        # Apply GLT to dataset\n        out_ds = apply_glt(raw_ds, glt_ds, GLT_NODATA_VALUE=GLT_NODATA_VALUE)\n\n        # Mask fill values\n        out_ds[out_ds == fill_value] = np.nan\n\n        # Update variables - Only works for 2 or 3 dimensional arrays\n        if raw_ds.ndim == 2:\n            out_ds = out_ds.squeeze()\n            data_vars[var] = ([\"latitude\", \"longitude\"], out_ds)\n        else:\n            data_vars[var] = ([\"latitude\", \"longitude\", var_dims[-1]], out_ds)\n\n        del raw_ds\n\n    # Calculate Lat and Lon Vectors\n    lon, lat = coord_vects(\n        ds\n    )  # Reorder this function to make sense in case of multiple variables\n\n    # Apply GLT to elevation\n    elev_ds = apply_glt(ds[\"elev\"].data, glt_ds)\n    elev_ds[elev_ds == fill_value] = np.nan\n\n    # Delete glt_ds - no longer needed\n    del glt_ds\n\n    # Create Coordinate Dictionary\n    coords = {\n        \"latitude\": ([\"latitude\"], lat),\n        \"longitude\": ([\"longitude\"], lon),\n        **ds.coords,\n    }  # unpack to add appropriate coordinates\n\n    # Remove Unnecessary Coords\n    for key in [\"downtrack\", \"crosstrack\", \"lat\", \"lon\", \"glt_x\", \"glt_y\", \"elev\"]:\n        del coords[key]\n\n    # Add Orthocorrected Elevation\n    coords[\"elev\"] = ([\"latitude\", \"longitude\"], np.squeeze(elev_ds))\n\n    # Build Output xarray Dataset and assign data_vars array attributes\n    out_xr = xr.Dataset(data_vars=data_vars, coords=coords, attrs=ds.attrs)\n\n    del out_ds\n    # Assign Attributes from Original Datasets\n    for var in var_list:\n        out_xr[var].attrs = ds[var].attrs\n    out_xr.coords[\"latitude\"].attrs = ds[\"lat\"].attrs\n    out_xr.coords[\"longitude\"].attrs = ds[\"lon\"].attrs\n    out_xr.coords[\"elev\"].attrs = ds[\"elev\"].attrs\n\n    # Add Spatial Reference in recognizable format\n    out_xr.rio.write_crs(ds.spatial_ref, inplace=True)\n\n    return out_xr\n</code></pre>"},{"location":"emit/#hypercoast.emit.plot_emit","title":"<code>plot_emit(ds, longitude=None, latitude=None, downtrack=None, crosstrack=None, remove_nans=True, x='wavelengths', y='reflectance', color='black', frame_height=400, frame_width=600, title=None, method='nearest', ortho=True, options={}, **kwargs)</code>","text":"<p>Plots a line graph of the reflectance data from a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset or str</code> <p>The dataset containing the reflectance data or the file path to the dataset.</p> required <code>longitude</code> <code>float</code> <p>The longitude coordinate to select for orthorectified data. Defaults to None.</p> <code>None</code> <code>latitude</code> <code>float</code> <p>The latitude coordinate to select for orthorectified data. Defaults to None.</p> <code>None</code> <code>downtrack</code> <code>int</code> <p>The downtrack coordinate to select for non-orthorectified data. Defaults to None.</p> <code>None</code> <code>crosstrack</code> <code>int</code> <p>The crosstrack coordinate to select for non-orthorectified data. Defaults to None.</p> <code>None</code> <code>remove_nans</code> <code>bool</code> <p>If True, replace non-good wavelengths with NaN. Defaults to True.</p> <code>True</code> <code>x</code> <code>str</code> <p>The x-axis label. Defaults to \"wavelengths\".</p> <code>'wavelengths'</code> <code>y</code> <code>str</code> <p>The y-axis label. Defaults to \"reflectance\".</p> <code>'reflectance'</code> <code>color</code> <code>str</code> <p>The color of the line. Defaults to \"black\".</p> <code>'black'</code> <code>frame_height</code> <code>int</code> <p>The height of the frame. Defaults to 400.</p> <code>400</code> <code>frame_width</code> <code>int</code> <p>The width of the frame. Defaults to 600.</p> <code>600</code> <code>title</code> <code>str</code> <p>The title of the plot. If None, a default title will be generated. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>ortho</code> <code>bool</code> <p>If True, the function will use longitude and latitude for data selection. Defaults to True.</p> <code>True</code> <code>options</code> <code>dict</code> <p>Additional options to be passed to <code>hvplot.line</code>. Defaults to {}.</p> <code>{}</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>hvplot.line</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>hvplot.Plot</code> <p>The line plot of the reflectance data.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def plot_emit(\n    ds: Union[xr.Dataset, str],\n    longitude: Optional[float] = None,\n    latitude: Optional[float] = None,\n    downtrack: Optional[int] = None,\n    crosstrack: Optional[int] = None,\n    remove_nans: Optional[bool] = True,\n    x: str = \"wavelengths\",\n    y: str = \"reflectance\",\n    color: str = \"black\",\n    frame_height: Optional[int] = 400,\n    frame_width: Optional[int] = 600,\n    title: str = None,\n    method: Optional[str] = \"nearest\",\n    ortho: Optional[bool] = True,\n    options: Optional[dict] = {},\n    **kwargs,\n):\n    \"\"\"\n    Plots a line graph of the reflectance data from a given dataset.\n\n    Args:\n        ds (xarray.Dataset or str): The dataset containing the reflectance data or the file path to the dataset.\n        longitude (float, optional): The longitude coordinate to select for orthorectified data. Defaults to None.\n        latitude (float, optional): The latitude coordinate to select for orthorectified data. Defaults to None.\n        downtrack (int, optional): The downtrack coordinate to select for non-orthorectified data. Defaults to None.\n        crosstrack (int, optional): The crosstrack coordinate to select for non-orthorectified data. Defaults to None.\n        remove_nans (bool, optional): If True, replace non-good wavelengths with NaN. Defaults to True.\n        x (str, optional): The x-axis label. Defaults to \"wavelengths\".\n        y (str, optional): The y-axis label. Defaults to \"reflectance\".\n        color (str, optional): The color of the line. Defaults to \"black\".\n        frame_height (int, optional): The height of the frame. Defaults to 400.\n        frame_width (int, optional): The width of the frame. Defaults to 600.\n        title (str, optional): The title of the plot. If None, a default title will be generated. Defaults to None.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n        ortho (bool, optional): If True, the function will use longitude and latitude for data selection. Defaults to True.\n        options (dict, optional): Additional options to be passed to `hvplot.line`. Defaults to {}.\n        **kwargs: Additional keyword arguments to be passed to `hvplot.line`.\n\n    Returns:\n        hvplot.Plot: The line plot of the reflectance data.\n    \"\"\"\n\n    import hvplot.xarray  # noqa F401\n\n    if ortho is True:\n        if longitude is None or latitude is None:\n            raise ValueError(\n                \"Longitude and Latitude must be provided for orthorectified data.\"\n            )\n    else:\n        if downtrack is None or crosstrack is None:\n            raise ValueError(\n                \"Downtrack and Crosstrack must be provided for non-orthorectified data.\"\n            )\n\n    if longitude is not None and latitude is not None:\n        ortho = True\n\n    if downtrack is not None and crosstrack is not None:\n        ortho = False\n\n    if isinstance(ds, str):\n        ds = read_emit(ds, ortho=ortho)\n\n    if remove_nans:\n        ds[\"reflectance\"].data[:, :, ds[\"good_wavelengths\"].data == 0] = np.nan\n\n    if ortho:\n        example = ds[\"reflectance\"].sel(\n            longitude=longitude, latitude=latitude, method=method\n        )\n        if title is None:\n            title = f\"Reflectance at longitude={longitude:.3f}, latitude={latitude:.3f}\"\n\n    else:\n        example = ds[\"reflectance\"].sel(\n            downtrack=downtrack, crosstrack=crosstrack, method=method\n        )\n        if title is None:\n            title = f\"Reflectance at downtrack={downtrack}, crosstrack={crosstrack}\"\n\n    line = example.hvplot.line(\n        y=y,\n        x=x,\n        color=color,\n        frame_height=frame_height,\n        frame_width=frame_width,\n        **kwargs,\n    ).opts(title=title, **options)\n    return line\n</code></pre>"},{"location":"emit/#hypercoast.emit.quality_mask","title":"<code>quality_mask(filepath, quality_bands)</code>","text":"<p>Builds a single layer mask to apply based on the bands selected from an EMIT L2A Mask file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>An EMIT L2A Mask netCDF file.</p> required <code>quality_bands</code> <code>list</code> <p>A list of bands (quality flags only) from the mask file that should be used in creation of mask.</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>A numpy array that can be used with the emit_xarray function to apply a quality mask.</p> <p>Exceptions:</p> Type Description <code>AttributeError</code> <p>If the selected flags include a data band (5 or 6) not just flag bands.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def quality_mask(filepath: str, quality_bands: list) -&gt; np.ndarray:\n    \"\"\"\n    Builds a single layer mask to apply based on the bands selected from an EMIT L2A Mask file.\n\n    Args:\n        filepath (str): An EMIT L2A Mask netCDF file.\n        quality_bands (list): A list of bands (quality flags only) from the mask file that should be used in creation of mask.\n\n    Returns:\n        numpy.ndarray: A numpy array that can be used with the emit_xarray function to apply a quality mask.\n\n    Raises:\n        AttributeError: If the selected flags include a data band (5 or 6) not just flag bands.\n    \"\"\"\n    # Open Dataset\n    mask_ds = xr.open_dataset(filepath, engine=\"h5netcdf\")\n    # Open Sensor band Group\n    mask_parameters_ds = xr.open_dataset(\n        filepath, engine=\"h5netcdf\", group=\"sensor_band_parameters\"\n    )\n    # Print Flags used\n    flags_used = mask_parameters_ds[\"mask_bands\"].data[quality_bands]\n    print(f\"Flags used: {flags_used}\")\n    # Check for data bands and build mask\n    if any(x in quality_bands for x in [5, 6]):\n        err_str = \"Selected flags include a data band (5 or 6) not just flag bands\"\n        raise AttributeError(err_str)\n    else:\n        qmask = np.sum(mask_ds[\"mask\"][:, :, quality_bands].values, axis=-1)\n        qmask[qmask &gt; 1] = 1\n    return qmask\n</code></pre>"},{"location":"emit/#hypercoast.emit.raw_spatial_crop","title":"<code>raw_spatial_crop(ds, shape)</code>","text":"<p>Use a polygon to clip the file GLT, then a bounding box to crop the spatially raw data. Regions clipped in the GLT are set to 0 so a mask will be applied when used to orthorectify the data at a later point in a workflow.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>Raw spatial EMIT data (non-orthorectified) opened with the <code>emit_xarray</code> function.</p> required <code>shape</code> <code>geopandas.GeoDataFrame</code> <p>A polygon opened with geopandas.</p> required <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>A clipped GLT and raw spatial data clipped to a bounding box.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def raw_spatial_crop(ds: xr.Dataset, shape: gpd) -&gt; xr.Dataset:\n    \"\"\"\n    Use a polygon to clip the file GLT, then a bounding box to crop the spatially raw data. Regions clipped in the GLT are set to 0 so a mask will be applied when\n    used to orthorectify the data at a later point in a workflow.\n\n    Args:\n        ds (xarray.Dataset): Raw spatial EMIT data (non-orthorectified) opened with the `emit_xarray` function.\n        shape (geopandas.GeoDataFrame): A polygon opened with geopandas.\n\n    Returns:\n        xarray.Dataset: A clipped GLT and raw spatial data clipped to a bounding box.\n    \"\"\"\n    # Reformat the GLT\n    lon, lat = coord_vects(ds)\n    data_vars = {\n        \"glt_x\": ([\"latitude\", \"longitude\"], ds.glt_x.data),\n        \"glt_y\": ([\"latitude\", \"longitude\"], ds.glt_y.data),\n    }\n    coords = {\n        \"latitude\": ([\"latitude\"], lat),\n        \"longitude\": ([\"longitude\"], lon),\n        \"ortho_y\": ([\"latitude\"], ds.ortho_y.data),\n        \"ortho_x\": ([\"longitude\"], ds.ortho_x.data),\n    }\n    glt_ds = xr.Dataset(data_vars=data_vars, coords=coords, attrs=ds.attrs)\n    glt_ds.rio.write_crs(glt_ds.spatial_ref, inplace=True)\n\n    # Clip the emit glt\n    clipped = glt_ds.rio.clip(shape.geometry.values, shape.crs, all_touched=True)\n\n    # Pull new geotransform from clipped glt\n    clipped_gt = np.array(\n        [float(i) for i in clipped[\"spatial_ref\"].GeoTransform.split(\" \")]\n    )  # THIS GEOTRANSFORM IS OFF BY HALF A PIXEL\n\n    # Create Crosstrack and Downtrack masks for spatially raw dataset -1 is to account for 1 based index. May be a more robust way to do this exists\n    crosstrack_mask = (ds.crosstrack &gt;= np.nanmin(clipped.glt_x.data) - 1) &amp; (\n        ds.crosstrack &lt;= np.nanmax(clipped.glt_x.data) - 1\n    )\n    downtrack_mask = (ds.downtrack &gt;= np.nanmin(clipped.glt_y.data) - 1) &amp; (\n        ds.downtrack &lt;= np.nanmax(clipped.glt_y.data) - 1\n    )\n\n    # Mask Areas outside of crosstrack and downtrack covered by the shape\n    clipped_ds = ds.where((crosstrack_mask &amp; downtrack_mask), drop=True)\n    # Replace Full dataset geotransform with clipped geotransform\n    clipped_ds.attrs[\"geotransform\"] = clipped_gt\n\n    # Drop unnecessary vars from dataset\n    clipped_ds = clipped_ds.drop_vars([\"glt_x\", \"glt_y\", \"downtrack\", \"crosstrack\"])\n\n    # Re-index the GLT to the new array\n    glt_x_data = clipped.glt_x.data - np.nanmin(clipped.glt_x)\n    glt_y_data = clipped.glt_y.data - np.nanmin(clipped.glt_y)\n    clipped_ds = clipped_ds.assign_coords(\n        {\n            \"glt_x\": ([\"ortho_y\", \"ortho_x\"], np.nan_to_num(glt_x_data)),\n            \"glt_y\": ([\"ortho_y\", \"ortho_x\"], np.nan_to_num(glt_y_data)),\n        }\n    )\n    clipped_ds = clipped_ds.assign_coords(\n        {\n            \"downtrack\": (\n                [\"downtrack\"],\n                np.arange(0, clipped_ds[list(ds.data_vars.keys())[0]].shape[0]),\n            ),\n            \"crosstrack\": (\n                [\"crosstrack\"],\n                np.arange(0, clipped_ds[list(ds.data_vars.keys())[0]].shape[1]),\n            ),\n        }\n    )\n\n    return clipped_ds\n</code></pre>"},{"location":"emit/#hypercoast.emit.read_emit","title":"<code>read_emit(filepath, ortho=True, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Opens an EMIT dataset from a file path and assigns new coordinates to it.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The file path to the EMIT dataset.</p> required <code>ortho</code> <code>bool</code> <p>If True, the function will return an orthorectified dataset. Defaults to True.</p> <code>True</code> <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>xr.open_dataset</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xarray.Dataset</code> <p>The dataset with new coordinates assigned.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def read_emit(\n    filepath: str,\n    ortho: Optional[bool] = True,\n    wavelengths: Union[tuple, list] = None,\n    method: Optional[str] = \"nearest\",\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Opens an EMIT dataset from a file path and assigns new coordinates to it.\n\n    Args:\n        filepath (str): The file path to the EMIT dataset.\n        ortho (bool, optional): If True, the function will return an orthorectified dataset. Defaults to True.\n        wavelengths (array-like, optional): The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to be passed to `xr.open_dataset`.\n\n    Returns:\n        xarray.Dataset: The dataset with new coordinates assigned.\n\n    \"\"\"\n\n    if ortho is True:\n        return emit_xarray(\n            filepath, ortho=True, wavelengths=wavelengths, method=method, **kwargs\n        )\n    else:\n        ds = xr.open_dataset(filepath, **kwargs)\n        wvl = xr.open_dataset(filepath, group=\"sensor_band_parameters\")\n        loc = xr.open_dataset(filepath, group=\"location\")\n        ds = ds.assign_coords(\n            {\n                \"downtrack\": ([\"downtrack\"], ds.downtrack.data),\n                \"crosstrack\": ([\"crosstrack\"], ds.crosstrack.data),\n                **wvl.variables,\n                **loc.variables,\n            }\n        )\n        ds = ds.swap_dims({\"bands\": \"wavelengths\"})\n        del wvl\n        del loc\n\n        if wavelengths is not None:\n            ds = ds.sel(wavelengths=wavelengths, method=method)\n\n        ds = ds.rename({\"wavelengths\": \"wavelength\"})\n        return ds\n</code></pre>"},{"location":"emit/#hypercoast.emit.viz_emit","title":"<code>viz_emit(ds, wavelengths, cmap='viridis', frame_width=720, method='nearest', ortho=True, aspect='equal', tiles='ESRI', alpha=0.8, title=None, options={}, **kwargs)</code>","text":"<p>Visualizes the reflectance data from a given dataset at specific wavelengths.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset or str</code> <p>The dataset containing the reflectance data or the file path to the dataset.</p> required <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to visualize.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>frame_width</code> <code>int</code> <p>The width of the frame. Defaults to 720.</p> <code>720</code> <code>method</code> <code>str</code> <p>The method to use for data selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>ortho</code> <code>bool</code> <p>If True, the function will return an orthorectified image. Defaults to True.</p> <code>True</code> <code>aspect</code> <code>str</code> <p>The aspect ratio of the plot. Defaults to \"equal\".</p> <code>'equal'</code> <code>tiles</code> <code>str</code> <p>The tile source to use for the background map. Defaults to \"ESRI\".</p> <code>'ESRI'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the image. Defaults to 0.8.</p> <code>0.8</code> <code>title</code> <code>str</code> <p>The title of the plot. If None, a default title will be generated. Defaults to None.</p> <code>None</code> <code>options</code> <code>dict</code> <p>Additional options to be passed to <code>hvplot.image</code>. Defaults to {}.</p> <code>{}</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>hvplot.image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>hvplot.Plot</code> <p>The image plot of the reflectance data at the specified wavelengths.</p> Source code in <code>hypercoast/emit.py</code> <pre><code>def viz_emit(\n    ds: Union[xr.Dataset, str],\n    wavelengths: Union[tuple, list],\n    cmap: Optional[str] = \"viridis\",\n    frame_width: int = 720,\n    method: Optional[str] = \"nearest\",\n    ortho: Optional[bool] = True,\n    aspect: Optional[str] = \"equal\",\n    tiles: str = \"ESRI\",\n    alpha: Optional[float] = 0.8,\n    title: Optional[str] = None,\n    options: Optional[dict] = {},\n    **kwargs,\n):\n    \"\"\"\n    Visualizes the reflectance data from a given dataset at specific wavelengths.\n\n    Args:\n        ds (xarray.Dataset or str): The dataset containing the reflectance data or the file path to the dataset.\n        wavelengths (array-like): The specific wavelengths to visualize.\n        cmap (str, optional): The colormap to use. Defaults to \"viridis\".\n        frame_width (int, optional): The width of the frame. Defaults to 720.\n        method (str, optional): The method to use for data selection. Defaults to \"nearest\".\n        ortho (bool, optional): If True, the function will return an orthorectified image. Defaults to True.\n        aspect (str, optional): The aspect ratio of the plot. Defaults to \"equal\".\n        tiles (str, optional): The tile source to use for the background map. Defaults to \"ESRI\".\n        alpha (float, optional): The alpha value for the image. Defaults to 0.8.\n        title (str, optional): The title of the plot. If None, a default title will be generated. Defaults to None.\n        options (dict, optional): Additional options to be passed to `hvplot.image`. Defaults to {}.\n        **kwargs: Additional keyword arguments to be passed to `hvplot.image`.\n\n    Returns:\n        hvplot.Plot: The image plot of the reflectance data at the specified wavelengths.\n    \"\"\"\n    import hvplot.xarray  # noqa F401\n\n    if isinstance(ds, str):\n        ds = read_emit(ds, ortho=ortho)\n\n    if not isinstance(wavelengths, list):\n        wavelengths = [wavelengths]\n    example = ds.sel(wavelength=wavelengths, method=method)\n\n    wavelengths = \", \".join([f\"{w:.3f}\" for w in example[\"wavelength\"]])\n\n    if title is None:\n        title = f\"Reflectance at {wavelengths} {example.wavelength.units}\"\n\n    if ortho:\n        image = example.hvplot.image(\n            cmap=cmap,\n            geo=ortho,\n            tiles=tiles,\n            alpha=alpha,\n            frame_width=frame_width,\n            **kwargs,\n        ).opts(title=title, **options)\n    else:\n        image = example.hvplot.image(\n            cmap=cmap, aspect=aspect, alpha=alpha, frame_width=frame_width, **kwargs\n        ).opts(title=title, **options)\n\n    return image\n</code></pre>"},{"location":"hypercoast/","title":"hypercoast module","text":"<p>Main module.</p>"},{"location":"hypercoast/#hypercoast.hypercoast.Map","title":"<code> Map            (Map)         </code>","text":"<p>A class that extends leafmap.Map to provide additional functionality for     hypercoast.</p> <p>Methods</p> <p>Any methods inherited from leafmap.Map.</p> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>class Map(leafmap.Map):\n    \"\"\"\n    A class that extends leafmap.Map to provide additional functionality for\n        hypercoast.\n\n    Attributes:\n        Any attributes inherited from leafmap.Map.\n\n    Methods:\n        Any methods inherited from leafmap.Map.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initializes a new instance of the Map class.\n\n        Args:\n            **kwargs: Arbitrary keyword arguments that are passed to the parent\n                class's constructor.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._spectral_data = {}\n        self._plot_options = None\n        self._plot_marker_cluster = None\n\n    def add(self, obj, position=\"topright\", xlim=None, ylim=None, **kwargs):\n        \"\"\"Add a layer to the map.\n\n        Args:\n            obj (str or object): The name of the layer or a layer object.\n            position (str, optional): The position of the layer widget. Can be\n                'topright', 'topleft', 'bottomright', or 'bottomleft'. Defaults\n                to 'topright'.\n            xlim (tuple, optional): The x-axis limits of the plot. Defaults to None.\n            ylim (tuple, optional): The y-axis limits of the plot. Defaults to None.\n            **kwargs: Arbitrary keyword arguments that are passed to the parent\n                class's add_layer method.\n        \"\"\"\n\n        if isinstance(obj, str):\n            if obj == \"spectral\":\n\n                SpectralWidget(self, position=position, xlim=xlim, ylim=ylim, **kwargs)\n                self.set_plot_options(add_marker_cluster=True)\n            else:\n                super().add(obj, **kwargs)\n\n        else:\n            super().add(obj, **kwargs)\n\n    def search_emit(self, default_dataset=\"EMITL2ARFL\"):\n        \"\"\"\n        Adds a NASA Earth Data search tool to the map with a default dataset for\n            EMIT.\n\n        Args:\n            default_dataset (str, optional): The default dataset to search for.\n                Defaults to \"EMITL2ARFL\".\n        \"\"\"\n        self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n\n    def search_pace(self, default_dataset=\"PACE_OCI_L2_AOP_NRT\"):\n        \"\"\"\n        Adds a NASA Earth Data search tool to the map with a default dataset for\n            PACE.\n\n        Args:\n            default_dataset (str, optional): The default dataset to search for.\n                Defaults to \"PACE_OCI_L2_AOP_NRT\".\n        \"\"\"\n        self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n\n    def search_ecostress(self, default_dataset=\"ECO_L2T_LSTE\"):\n        \"\"\"\n        Adds a NASA Earth Data search tool to the map with a default dataset for\n            ECOSTRESS.\n\n        Args:\n            default_dataset (str, optional): The default dataset to search for.\n                Defaults to \"ECO_L2T_LSTE\".\n        \"\"\"\n        self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n\n    def add_raster(\n        self,\n        source,\n        indexes=None,\n        colormap=None,\n        vmin=None,\n        vmax=None,\n        nodata=None,\n        attribution=None,\n        layer_name=\"Raster\",\n        layer_index=None,\n        zoom_to_layer=True,\n        visible=True,\n        opacity=1.0,\n        array_args=None,\n        client_args={\"cors_all\": False},\n        open_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add a local raster dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'Raster'.\n            layer_index (int, optional): The index of the layer. Defaults to None.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to\n                True.\n            opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            client_args (dict, optional): Additional arguments to pass to\n                localtileserver.TileClient. Defaults to { \"cors_all\": False }.\n            open_args (dict, optional): Additional arguments to pass to\n                rioxarray.open_rasterio.\n\n        \"\"\"\n\n        import rioxarray as rxr\n\n        if array_args is None:\n            array_args = {}\n        if open_args is None:\n            open_args = {}\n\n        if nodata is None:\n            nodata = np.nan\n        super().add_raster(\n            source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            layer_index=layer_index,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            opacity=opacity,\n            array_args=array_args,\n            client_args=client_args,\n            **kwargs,\n        )\n\n        if isinstance(source, str):\n            da = rxr.open_rasterio(source, **open_args)\n            dims = da.dims\n            da = da.transpose(dims[1], dims[2], dims[0])\n\n            xds = da.to_dataset(name=\"data\")\n            self.cog_layer_dict[layer_name][\"xds\"] = xds\n            self.cog_layer_dict[layer_name][\"hyper\"] = \"COG\"\n\n    def add_dataset(\n        self,\n        source,\n        indexes=None,\n        colormap=None,\n        vmin=None,\n        vmax=None,\n        nodata=None,\n        attribution=None,\n        layer_name=\"Raster\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        open_args=None,\n        **kwargs,\n    ):\n        import rioxarray as rxr\n        from leafmap import array_to_image\n\n        if array_args is None:\n            array_args = {}\n\n        if open_args is None:\n            open_args = {}\n\n        if isinstance(source, str):\n            da = rxr.open_rasterio(source, **open_args)\n            dims = da.dims\n            da = da.transpose(dims[1], dims[2], dims[0])\n            xds = da.to_dataset(name=\"data\")\n\n        elif not isinstance(source, xr.Dataset):\n            raise ValueError(\n                \"source must be a path to a raster file or an xarray.Dataset object.\"\n            )\n        else:\n            xds = source\n\n        if indexes is None:\n            if xds.sizes[dims[2]] &lt; 3:\n                indexes = [1]\n            elif xds.sizes[dims[2]] &lt; 4:\n                indexes = [1, 2, 3]\n            else:\n                indexes = [3, 2, 1]\n\n        bands = [i - 1 for i in indexes]\n        da = xds.isel(band=bands)[\"data\"]\n        image = array_to_image(da, transpose=False)\n\n        self.add_raster(\n            image,\n            indexes=None,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"type\"] = \"XARRAY\"\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"XARRAY\"\n        self.cog_layer_dict[layer_name][\"band_names\"] = [\n            \"b\" + str(i) for i in xds.coords[\"band\"].values.tolist()\n        ]\n        self.cog_layer_dict[layer_name][\"indexes\"] = indexes\n        self.cog_layer_dict[layer_name][\"vis_bands\"] = [\"b\" + str(i) for i in indexes]\n\n    def add_emit(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=None,\n        vmax=None,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"EMIT\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add an EMIT dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band.\n                    See https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to\n                interpret as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to\n                True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n        \"\"\"\n\n        if array_args is None:\n            array_args = {}\n\n        xds = None\n        if isinstance(source, str):\n\n            xds = read_emit(source)\n            source = emit_to_image(xds, wavelengths=wavelengths)\n        elif isinstance(source, xr.Dataset):\n            xds = source\n            source = emit_to_image(xds, wavelengths=wavelengths)\n\n        self.add_raster(\n            source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"EMIT\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_pace(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=\"jet\",\n        vmin=None,\n        vmax=None,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"PACE\",\n        zoom_to_layer=True,\n        visible=True,\n        method=\"nearest\",\n        gridded=False,\n        array_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add a PACE dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                    https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n        \"\"\"\n\n        if array_args is None:\n            array_args = {}\n\n        if isinstance(source, str):\n\n            source = read_pace(source)\n\n        image = pace_to_image(\n            source, wavelengths=wavelengths, method=method, gridded=gridded\n        )\n\n        if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n            colormap = None\n\n        self.add_raster(\n            image,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = source\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"PACE\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_desis(\n        self,\n        source,\n        wavelengths=[900, 650, 525],\n        indexes=None,\n        colormap=\"jet\",\n        vmin=None,\n        vmax=None,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"DESIS\",\n        zoom_to_layer=True,\n        visible=True,\n        method=\"nearest\",\n        array_args=None,\n        **kwargs,\n    ):\n        \"\"\"Add a DESIS dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is 'jet'.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n        \"\"\"\n        if array_args is None:\n            array_args = {}\n\n        if isinstance(source, str):\n\n            source = read_desis(source)\n\n        image = desis_to_image(source, wavelengths=wavelengths, method=method)\n\n        if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n            colormap = None\n\n        if isinstance(wavelengths, int):\n            wavelengths = [wavelengths]\n\n        if indexes is None:\n            if isinstance(wavelengths, list) and len(wavelengths) == 1:\n                indexes = [1]\n            else:\n                indexes = [1, 2, 3]\n\n        self.add_raster(\n            image,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = source\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"DESIS\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_neon(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=0,\n        vmax=0.5,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"NEON\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        method=\"nearest\",\n        **kwargs,\n    ):\n        \"\"\"Add an NEON AOP dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the NEON AOP HDF5 file.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                    https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to 0.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to 0.5.\n            nodata (float, optional): The value from the band to use to\n                interpret as not valid data. Defaults to np.nan.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'NEON'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults\n                to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            method (str, optional): The method to use for data interpolation.\n                Defaults to \"nearest\".\n        \"\"\"\n\n        if array_args is None:\n            array_args = {}\n        xds = None\n        if isinstance(source, str):\n\n            xds = read_neon(source)\n            source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n        elif isinstance(source, xr.Dataset):\n            xds = source\n            source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n\n        self.add_raster(\n            source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"NEON\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_aviris(\n        self,\n        source,\n        wavelengths=None,\n        indexes=None,\n        colormap=None,\n        vmin=0,\n        vmax=0.5,\n        nodata=np.nan,\n        attribution=None,\n        layer_name=\"AVIRIS\",\n        zoom_to_layer=True,\n        visible=True,\n        array_args=None,\n        method=\"nearest\",\n        **kwargs,\n    ):\n        \"\"\"Add an AVIRIS dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n                (e.g., Binder, Microsoft Planetary Computer) and\n            if the raster does not render properly, try installing\n                jupyter-server-proxy using `pip install jupyter-server-proxy`,\n            then running the following code before calling this function. For\n                more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the AVIRIS file.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band. See\n                    https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                    Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to 0.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to 0.5.\n            nodata (float, optional): The value from the band to use to\n                interpret as not valid data. Defaults to np.nan.\n            attribution (str, optional): Attribution for the source raster. This\n                defaults to a message about it being a local file.. Defaults to None.\n            layer_name (str, optional): The layer name to use. Defaults to 'NEON'.\n            zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n                layer. Defaults to True.\n            visible (bool, optional): Whether the layer is visible. Defaults\n                to True.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            method (str, optional): The method to use for data interpolation.\n                Defaults to \"nearest\".\n        \"\"\"\n        if array_args is None:\n            array_args = {}\n\n        xds = None\n        if isinstance(source, str):\n\n            xds = read_aviris(source)\n            source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n        elif isinstance(source, xr.Dataset):\n            xds = source\n            source = aviris_to_image(xds, wavelengths=wavelengths, method=method)\n\n        self.add_raster(\n            source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            array_args=array_args,\n            **kwargs,\n        )\n\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"AVIRIS\"\n        self._update_band_names(layer_name, wavelengths)\n\n    def add_hyper(self, xds, dtype, wvl_indexes=None, **kwargs):\n        \"\"\"Add a hyperspectral dataset to the map.\n\n        Args:\n            xds (str): The Xarray dataset containing the hyperspectral data.\n            dtype (str): The type of the hyperspectral dataset. Can be one of\n                \"EMIT\", \"PACE\", \"DESIS\", \"NEON\", \"AVIRIS\".\n            **kwargs: Additional keyword arguments to pass to the corresponding\n                add function.\n        \"\"\"\n\n        if wvl_indexes is not None:\n            if dtype == \"XARRAY\":\n                kwargs[\"indexes\"] = [i + 1 for i in wvl_indexes]\n            else:\n\n                kwargs[\"wavelengths\"] = (\n                    xds.isel(wavelength=wvl_indexes)\n                    .coords[\"wavelength\"]\n                    .values.tolist()\n                )\n\n        if dtype == \"EMIT\":\n            self.add_emit(xds, **kwargs)\n        elif dtype == \"PACE\":\n            self.add_pace(xds, **kwargs)\n        elif dtype == \"DESIS\":\n            self.add_desis(xds, **kwargs)\n        elif dtype == \"NEON\":\n            self.add_neon(xds, **kwargs)\n        elif dtype == \"AVIRIS\":\n            self.add_aviris(xds, **kwargs)\n        elif dtype == \"XARRAY\":\n            kwargs.pop(\"wavelengths\", None)\n            self.add_dataset(xds, **kwargs)\n\n    def set_plot_options(\n        self,\n        add_marker_cluster=False,\n        plot_type=None,\n        overlay=False,\n        position=\"bottomright\",\n        min_width=None,\n        max_width=None,\n        min_height=None,\n        max_height=None,\n        **kwargs,\n    ):\n        \"\"\"Sets plotting options.\n\n        Args:\n            add_marker_cluster (bool, optional): Whether to add a marker cluster.\n                Defaults to False.\n            sample_scale (float, optional):  A nominal scale in meters of the\n                projection to sample in . Defaults to None.\n            plot_type (str, optional): The plot type can be one of \"None\", \"bar\",\n                \"scatter\" or \"hist\". Defaults to None.\n            overlay (bool, optional): Whether to overlay plotted lines on the\n                figure. Defaults to False.\n            position (str, optional): Position of the control, can be\n                \u2018bottomleft\u2019, \u2018bottomright\u2019, \u2018topleft\u2019, or \u2018topright\u2019. Defaults\n                to 'bottomright'.\n            min_width (int, optional): Min width of the widget (in pixels), if\n                None it will respect the content size. Defaults to None.\n            max_width (int, optional): Max width of the widget (in pixels), if\n                None it will respect the content size. Defaults to None.\n            min_height (int, optional): Min height of the widget (in pixels), if\n                None it will respect the content size. Defaults to None.\n            max_height (int, optional): Max height of the widget (in pixels), if\n                None it will respect the content size. Defaults to None.\n\n        \"\"\"\n        plot_options_dict = {}\n        plot_options_dict[\"add_marker_cluster\"] = add_marker_cluster\n        plot_options_dict[\"plot_type\"] = plot_type\n        plot_options_dict[\"overlay\"] = overlay\n        plot_options_dict[\"position\"] = position\n        plot_options_dict[\"min_width\"] = min_width\n        plot_options_dict[\"max_width\"] = max_width\n        plot_options_dict[\"min_height\"] = min_height\n        plot_options_dict[\"max_height\"] = max_height\n\n        for key in kwargs:\n            plot_options_dict[key] = kwargs[key]\n\n        self._plot_options = plot_options_dict\n\n        if not hasattr(self, \"_plot_marker_cluster\"):\n            self._plot_marker_cluster = ipyleaflet.MarkerCluster(name=\"Marker Cluster\")\n\n        if add_marker_cluster and (self._plot_marker_cluster not in self.layers):\n            self.add(self._plot_marker_cluster)\n\n    def spectral_to_df(self, **kwargs):\n        \"\"\"Converts the spectral data to a pandas DataFrame.\n\n        Returns:\n            pd.DataFrame: The spectral data as a pandas DataFrame.\n        \"\"\"\n        import pandas as pd\n\n        df = pd.DataFrame(self._spectral_data, **kwargs)\n        return df\n\n    def spectral_to_gdf(self, **kwargs):\n        \"\"\"Converts the spectral data to a GeoPandas GeoDataFrame.\n\n        Returns:\n            gpd.DataFrame: The spectral data as a pandas DataFrame.\n        \"\"\"\n        import geopandas as gpd\n        from shapely.geometry import Point\n\n        df = self.spectral_to_df()\n\n        if len(df) == 0:\n            return None\n\n        # Step 1: Extract the coordinates from the columns\n        df = df.rename(columns={\"wavelength\": \"latlon\"})\n        coordinates = [col.strip(\"()\").split() for col in df.columns[1:]]\n        coords = [(float(lat), float(lon)) for lat, lon in coordinates]\n\n        # Step 2: Create Point geometries for each coordinate\n        points = [Point(lon, lat) for lat, lon in coords]\n\n        # Step 3: Create a GeoDataFrame\n        df_transposed = df.set_index(\"latlon\").T\n\n        # Convert the column names to strings to ensure compatibility with GeoJSON\n        df_transposed.columns = df_transposed.columns.astype(str)\n\n        # Create the GeoDataFrame\n        gdf = gpd.GeoDataFrame(df_transposed, geometry=points, **kwargs)\n\n        # Set the coordinate reference system (CRS)\n        gdf = gdf.set_geometry(\"geometry\").set_crs(\"EPSG:4326\")\n\n        return gdf\n\n    def spectral_to_csv(self, filename, index=True, **kwargs):\n        \"\"\"Saves the spectral data to a CSV file.\n\n        Args:\n            filename (str): The output CSV file.\n            index (bool, optional): Whether to write the index. Defaults to True.\n        \"\"\"\n        df = self.spectral_to_df()\n        df = df.rename_axis(\"band\")\n        df.to_csv(filename, index=index, **kwargs)\n\n    def _update_band_names(self, layer_name, wavelengths):\n\n        # Function to find the nearest indices\n        def find_nearest_indices(\n            dataarray, selected_wavelengths, dim_name=\"wavelength\"\n        ):\n            indices = []\n            for wavelength in selected_wavelengths:\n                if dim_name == \"band\":\n                    nearest_wavelength = dataarray.sel(\n                        band=wavelength, method=\"nearest\"\n                    )\n                else:\n                    nearest_wavelength = dataarray.sel(\n                        wavelength=wavelength, method=\"nearest\"\n                    )\n                nearest_wavelength_index = nearest_wavelength[dim_name].item()\n                nearest_index = (\n                    dataarray[dim_name].values.tolist().index(nearest_wavelength_index)\n                )\n                indices.append(nearest_index + 1)\n            return indices\n\n        if \"xds\" in self.cog_layer_dict[layer_name]:\n            xds = self.cog_layer_dict[layer_name][\"xds\"]\n            dim_name = \"wavelength\"\n\n            if \"band\" in xds:\n                dim_name = \"band\"\n\n            band_count = xds.sizes[dim_name]\n            band_names = [\"b\" + str(band) for band in range(1, band_count + 1)]\n            self.cog_layer_dict[layer_name][\"band_names\"] = band_names\n\n            try:\n                indexes = find_nearest_indices(xds, wavelengths, dim_name=dim_name)\n                vis_bands = [\"b\" + str(index) for index in indexes]\n                self.cog_layer_dict[layer_name][\"indexes\"] = indexes\n                self.cog_layer_dict[layer_name][\"vis_bands\"] = vis_bands\n            except Exception as e:\n                print(e)\n\n    def add_field_data(\n        self,\n        data: Union[str],\n        x_col: str = \"wavelength\",\n        y_col_prefix: str = \"(\",\n        x_label: str = \"Wavelengths (nm)\",\n        y_label: str = \"Reflectance\",\n        use_marker_cluster: bool = True,\n        min_width: int = 400,\n        max_width: int = 600,\n        min_height: int = 200,\n        max_height: int = 250,\n        layer_name: str = \"Marker Cluster\",\n        **kwargs,\n    ):\n        \"\"\"\n        Displays field data on a map with interactive markers and popups showing time series data.\n\n        Args:\n            data (Union[str, pd.DataFrame]): Path to the CSV file or a pandas DataFrame containing the data.\n            x_col (str): Column name to use for the x-axis of the charts. Default is \"wavelength\".\n            y_col_prefix (str): Prefix to identify the columns that contain the location-specific data. Default is \"(\".\n            x_label (str): Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".\n            y_label (str): Label for the y-axis of the charts. Default is \"Reflectance\".\n            use_marker_cluster (bool): Whether to use marker clustering. Default is True.\n            min_width (int): Minimum width of the popup. Default is 400.\n            max_width (int): Maximum width of the popup. Default is 600.\n            min_height (int): Minimum height of the popup. Default is 200.\n            max_height (int): Maximum height of the popup. Default is 250.\n            layer_name (str): Name of the marker cluster layer. Default is \"Marker Cluster\".\n\n        Returns:\n            Map: An ipyleaflet Map with the added markers and popups.\n        \"\"\"\n        show_field_data(\n            data,\n            x_col,\n            y_col_prefix,\n            x_label=x_label,\n            y_label=y_label,\n            use_marker_cluster=use_marker_cluster,\n            min_width=min_width,\n            max_width=max_width,\n            min_height=min_height,\n            max_height=max_height,\n            layer_name=layer_name,\n            m=self,\n            **kwargs,\n        )\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.__init__","title":"<code>__init__(self, **kwargs)</code>  <code>special</code>","text":"<p>Initializes a new instance of the Map class.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Arbitrary keyword arguments that are passed to the parent class's constructor.</p> <code>{}</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a new instance of the Map class.\n\n    Args:\n        **kwargs: Arbitrary keyword arguments that are passed to the parent\n            class's constructor.\n    \"\"\"\n    super().__init__(**kwargs)\n    self._spectral_data = {}\n    self._plot_options = None\n    self._plot_marker_cluster = None\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add","title":"<code>add(self, obj, position='topright', xlim=None, ylim=None, **kwargs)</code>","text":"<p>Add a layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>str or object</code> <p>The name of the layer or a layer object.</p> required <code>position</code> <code>str</code> <p>The position of the layer widget. Can be 'topright', 'topleft', 'bottomright', or 'bottomleft'. Defaults to 'topright'.</p> <code>'topright'</code> <code>xlim</code> <code>tuple</code> <p>The x-axis limits of the plot. Defaults to None.</p> <code>None</code> <code>ylim</code> <code>tuple</code> <p>The y-axis limits of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Arbitrary keyword arguments that are passed to the parent class's add_layer method.</p> <code>{}</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add(self, obj, position=\"topright\", xlim=None, ylim=None, **kwargs):\n    \"\"\"Add a layer to the map.\n\n    Args:\n        obj (str or object): The name of the layer or a layer object.\n        position (str, optional): The position of the layer widget. Can be\n            'topright', 'topleft', 'bottomright', or 'bottomleft'. Defaults\n            to 'topright'.\n        xlim (tuple, optional): The x-axis limits of the plot. Defaults to None.\n        ylim (tuple, optional): The y-axis limits of the plot. Defaults to None.\n        **kwargs: Arbitrary keyword arguments that are passed to the parent\n            class's add_layer method.\n    \"\"\"\n\n    if isinstance(obj, str):\n        if obj == \"spectral\":\n\n            SpectralWidget(self, position=position, xlim=xlim, ylim=ylim, **kwargs)\n            self.set_plot_options(add_marker_cluster=True)\n        else:\n            super().add(obj, **kwargs)\n\n    else:\n        super().add(obj, **kwargs)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_aviris","title":"<code>add_aviris(self, source, wavelengths=None, indexes=None, colormap=None, vmin=0, vmax=0.5, nodata=nan, attribution=None, layer_name='AVIRIS', zoom_to_layer=True, visible=True, array_args=None, method='nearest', **kwargs)</code>","text":"<p>Add an AVIRIS dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the AVIRIS file.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See     https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to 0.5.</p> <code>0.5</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to np.nan.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'NEON'.</p> <code>'AVIRIS'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_aviris(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=0,\n    vmax=0.5,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"AVIRIS\",\n    zoom_to_layer=True,\n    visible=True,\n    array_args=None,\n    method=\"nearest\",\n    **kwargs,\n):\n    \"\"\"Add an AVIRIS dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the AVIRIS file.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to 0.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to 0.5.\n        nodata (float, optional): The value from the band to use to\n            interpret as not valid data. Defaults to np.nan.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'NEON'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults\n            to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n    \"\"\"\n    if array_args is None:\n        array_args = {}\n\n    xds = None\n    if isinstance(source, str):\n\n        xds = read_aviris(source)\n        source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n    elif isinstance(source, xr.Dataset):\n        xds = source\n        source = aviris_to_image(xds, wavelengths=wavelengths, method=method)\n\n    self.add_raster(\n        source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = xds\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"AVIRIS\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_desis","title":"<code>add_desis(self, source, wavelengths=[900, 650, 525], indexes=None, colormap='jet', vmin=None, vmax=None, nodata=nan, attribution=None, layer_name='DESIS', zoom_to_layer=True, visible=True, method='nearest', array_args=None, **kwargs)</code>","text":"<p>Add a DESIS dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See https://matplotlib.org/stable/gallery/color/colormap_reference.html. Default is 'jet'.</p> <code>'jet'</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'EMIT'.</p> <code>'DESIS'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_desis(\n    self,\n    source,\n    wavelengths=[900, 650, 525],\n    indexes=None,\n    colormap=\"jet\",\n    vmin=None,\n    vmax=None,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"DESIS\",\n    zoom_to_layer=True,\n    visible=True,\n    method=\"nearest\",\n    array_args=None,\n    **kwargs,\n):\n    \"\"\"Add a DESIS dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n            https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n            Default is 'jet'.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n    \"\"\"\n    if array_args is None:\n        array_args = {}\n\n    if isinstance(source, str):\n\n        source = read_desis(source)\n\n    image = desis_to_image(source, wavelengths=wavelengths, method=method)\n\n    if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n        colormap = None\n\n    if isinstance(wavelengths, int):\n        wavelengths = [wavelengths]\n\n    if indexes is None:\n        if isinstance(wavelengths, list) and len(wavelengths) == 1:\n            indexes = [1]\n        else:\n            indexes = [1, 2, 3]\n\n    self.add_raster(\n        image,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = source\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"DESIS\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_emit","title":"<code>add_emit(self, source, wavelengths=None, indexes=None, colormap=None, vmin=None, vmax=None, nodata=nan, attribution=None, layer_name='EMIT', zoom_to_layer=True, visible=True, array_args=None, **kwargs)</code>","text":"<p>Add an EMIT dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band.     See https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'EMIT'.</p> <code>'EMIT'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_emit(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=None,\n    vmax=None,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"EMIT\",\n    zoom_to_layer=True,\n    visible=True,\n    array_args=None,\n    **kwargs,\n):\n    \"\"\"Add an EMIT dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band.\n                See https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to\n            interpret as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to\n            True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n    \"\"\"\n\n    if array_args is None:\n        array_args = {}\n\n    xds = None\n    if isinstance(source, str):\n\n        xds = read_emit(source)\n        source = emit_to_image(xds, wavelengths=wavelengths)\n    elif isinstance(source, xr.Dataset):\n        xds = source\n        source = emit_to_image(xds, wavelengths=wavelengths)\n\n    self.add_raster(\n        source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = xds\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"EMIT\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_field_data","title":"<code>add_field_data(self, data, x_col='wavelength', y_col_prefix='(', x_label='Wavelengths (nm)', y_label='Reflectance', use_marker_cluster=True, min_width=400, max_width=600, min_height=200, max_height=250, layer_name='Marker Cluster', **kwargs)</code>","text":"<p>Displays field data on a map with interactive markers and popups showing time series data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, pd.DataFrame]</code> <p>Path to the CSV file or a pandas DataFrame containing the data.</p> required <code>x_col</code> <code>str</code> <p>Column name to use for the x-axis of the charts. Default is \"wavelength\".</p> <code>'wavelength'</code> <code>y_col_prefix</code> <code>str</code> <p>Prefix to identify the columns that contain the location-specific data. Default is \"(\".</p> <code>'('</code> <code>x_label</code> <code>str</code> <p>Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".</p> <code>'Wavelengths (nm)'</code> <code>y_label</code> <code>str</code> <p>Label for the y-axis of the charts. Default is \"Reflectance\".</p> <code>'Reflectance'</code> <code>use_marker_cluster</code> <code>bool</code> <p>Whether to use marker clustering. Default is True.</p> <code>True</code> <code>min_width</code> <code>int</code> <p>Minimum width of the popup. Default is 400.</p> <code>400</code> <code>max_width</code> <code>int</code> <p>Maximum width of the popup. Default is 600.</p> <code>600</code> <code>min_height</code> <code>int</code> <p>Minimum height of the popup. Default is 200.</p> <code>200</code> <code>max_height</code> <code>int</code> <p>Maximum height of the popup. Default is 250.</p> <code>250</code> <code>layer_name</code> <code>str</code> <p>Name of the marker cluster layer. Default is \"Marker Cluster\".</p> <code>'Marker Cluster'</code> <p>Returns:</p> Type Description <code>Map</code> <p>An ipyleaflet Map with the added markers and popups.</p> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_field_data(\n    self,\n    data: Union[str],\n    x_col: str = \"wavelength\",\n    y_col_prefix: str = \"(\",\n    x_label: str = \"Wavelengths (nm)\",\n    y_label: str = \"Reflectance\",\n    use_marker_cluster: bool = True,\n    min_width: int = 400,\n    max_width: int = 600,\n    min_height: int = 200,\n    max_height: int = 250,\n    layer_name: str = \"Marker Cluster\",\n    **kwargs,\n):\n    \"\"\"\n    Displays field data on a map with interactive markers and popups showing time series data.\n\n    Args:\n        data (Union[str, pd.DataFrame]): Path to the CSV file or a pandas DataFrame containing the data.\n        x_col (str): Column name to use for the x-axis of the charts. Default is \"wavelength\".\n        y_col_prefix (str): Prefix to identify the columns that contain the location-specific data. Default is \"(\".\n        x_label (str): Label for the x-axis of the charts. Default is \"Wavelengths (nm)\".\n        y_label (str): Label for the y-axis of the charts. Default is \"Reflectance\".\n        use_marker_cluster (bool): Whether to use marker clustering. Default is True.\n        min_width (int): Minimum width of the popup. Default is 400.\n        max_width (int): Maximum width of the popup. Default is 600.\n        min_height (int): Minimum height of the popup. Default is 200.\n        max_height (int): Maximum height of the popup. Default is 250.\n        layer_name (str): Name of the marker cluster layer. Default is \"Marker Cluster\".\n\n    Returns:\n        Map: An ipyleaflet Map with the added markers and popups.\n    \"\"\"\n    show_field_data(\n        data,\n        x_col,\n        y_col_prefix,\n        x_label=x_label,\n        y_label=y_label,\n        use_marker_cluster=use_marker_cluster,\n        min_width=min_width,\n        max_width=max_width,\n        min_height=min_height,\n        max_height=max_height,\n        layer_name=layer_name,\n        m=self,\n        **kwargs,\n    )\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_hyper","title":"<code>add_hyper(self, xds, dtype, wvl_indexes=None, **kwargs)</code>","text":"<p>Add a hyperspectral dataset to the map.</p> <p>Parameters:</p> Name Type Description Default <code>xds</code> <code>str</code> <p>The Xarray dataset containing the hyperspectral data.</p> required <code>dtype</code> <code>str</code> <p>The type of the hyperspectral dataset. Can be one of \"EMIT\", \"PACE\", \"DESIS\", \"NEON\", \"AVIRIS\".</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the corresponding add function.</p> <code>{}</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_hyper(self, xds, dtype, wvl_indexes=None, **kwargs):\n    \"\"\"Add a hyperspectral dataset to the map.\n\n    Args:\n        xds (str): The Xarray dataset containing the hyperspectral data.\n        dtype (str): The type of the hyperspectral dataset. Can be one of\n            \"EMIT\", \"PACE\", \"DESIS\", \"NEON\", \"AVIRIS\".\n        **kwargs: Additional keyword arguments to pass to the corresponding\n            add function.\n    \"\"\"\n\n    if wvl_indexes is not None:\n        if dtype == \"XARRAY\":\n            kwargs[\"indexes\"] = [i + 1 for i in wvl_indexes]\n        else:\n\n            kwargs[\"wavelengths\"] = (\n                xds.isel(wavelength=wvl_indexes)\n                .coords[\"wavelength\"]\n                .values.tolist()\n            )\n\n    if dtype == \"EMIT\":\n        self.add_emit(xds, **kwargs)\n    elif dtype == \"PACE\":\n        self.add_pace(xds, **kwargs)\n    elif dtype == \"DESIS\":\n        self.add_desis(xds, **kwargs)\n    elif dtype == \"NEON\":\n        self.add_neon(xds, **kwargs)\n    elif dtype == \"AVIRIS\":\n        self.add_aviris(xds, **kwargs)\n    elif dtype == \"XARRAY\":\n        kwargs.pop(\"wavelengths\", None)\n        self.add_dataset(xds, **kwargs)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_neon","title":"<code>add_neon(self, source, wavelengths=None, indexes=None, colormap=None, vmin=0, vmax=0.5, nodata=nan, attribution=None, layer_name='NEON', zoom_to_layer=True, visible=True, array_args=None, method='nearest', **kwargs)</code>","text":"<p>Add an NEON AOP dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the NEON AOP HDF5 file.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See     https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to 0.5.</p> <code>0.5</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to np.nan.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'NEON'.</p> <code>'NEON'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_neon(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=None,\n    vmin=0,\n    vmax=0.5,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"NEON\",\n    zoom_to_layer=True,\n    visible=True,\n    array_args=None,\n    method=\"nearest\",\n    **kwargs,\n):\n    \"\"\"Add an NEON AOP dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the NEON AOP HDF5 file.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to 0.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to 0.5.\n        nodata (float, optional): The value from the band to use to\n            interpret as not valid data. Defaults to np.nan.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'NEON'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults\n            to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n    \"\"\"\n\n    if array_args is None:\n        array_args = {}\n    xds = None\n    if isinstance(source, str):\n\n        xds = read_neon(source)\n        source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n    elif isinstance(source, xr.Dataset):\n        xds = source\n        source = neon_to_image(xds, wavelengths=wavelengths, method=method)\n\n    self.add_raster(\n        source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = xds\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"NEON\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_pace","title":"<code>add_pace(self, source, wavelengths=None, indexes=None, colormap='jet', vmin=None, vmax=None, nodata=nan, attribution=None, layer_name='PACE', zoom_to_layer=True, visible=True, method='nearest', gridded=False, array_args=None, **kwargs)</code>","text":"<p>Add a PACE dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See     https://matplotlib.org/stable/gallery/color/colormap_reference.html.     Default is greyscale.</p> <code>'jet'</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>nan</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'EMIT'.</p> <code>'PACE'</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_pace(\n    self,\n    source,\n    wavelengths=None,\n    indexes=None,\n    colormap=\"jet\",\n    vmin=None,\n    vmax=None,\n    nodata=np.nan,\n    attribution=None,\n    layer_name=\"PACE\",\n    zoom_to_layer=True,\n    visible=True,\n    method=\"nearest\",\n    gridded=False,\n    array_args=None,\n    **kwargs,\n):\n    \"\"\"Add a PACE dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n                https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'EMIT'.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to True.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n    \"\"\"\n\n    if array_args is None:\n        array_args = {}\n\n    if isinstance(source, str):\n\n        source = read_pace(source)\n\n    image = pace_to_image(\n        source, wavelengths=wavelengths, method=method, gridded=gridded\n    )\n\n    if isinstance(wavelengths, list) and len(wavelengths) &gt; 1:\n        colormap = None\n\n    self.add_raster(\n        image,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        array_args=array_args,\n        **kwargs,\n    )\n\n    self.cog_layer_dict[layer_name][\"xds\"] = source\n    self.cog_layer_dict[layer_name][\"hyper\"] = \"PACE\"\n    self._update_band_names(layer_name, wavelengths)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.add_raster","title":"<code>add_raster(self, source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args=None, client_args={'cors_all': False}, open_args=None, **kwargs)</code>","text":"<p>Add a local raster dataset to the map.     If you are using this function in JupyterHub on a remote server         (e.g., Binder, Microsoft Planetary Computer) and     if the raster does not render properly, try installing         jupyter-server-proxy using <code>pip install jupyter-server-proxy</code>,     then running the following code before calling this function. For         more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See https://matplotlib.org/stable/gallery/color/colormap_reference.html. Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>str</code> <p>Attribution for the source raster. This defaults to a message about it being a local file.. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>The layer name to use. Defaults to 'Raster'.</p> <code>'Raster'</code> <code>layer_index</code> <code>int</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the extent of the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>float</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>client_args</code> <code>dict</code> <p>Additional arguments to pass to localtileserver.TileClient. Defaults to { \"cors_all\": False }.</p> <code>{'cors_all': False}</code> <code>open_args</code> <code>dict</code> <p>Additional arguments to pass to rioxarray.open_rasterio.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def add_raster(\n    self,\n    source,\n    indexes=None,\n    colormap=None,\n    vmin=None,\n    vmax=None,\n    nodata=None,\n    attribution=None,\n    layer_name=\"Raster\",\n    layer_index=None,\n    zoom_to_layer=True,\n    visible=True,\n    opacity=1.0,\n    array_args=None,\n    client_args={\"cors_all\": False},\n    open_args=None,\n    **kwargs,\n):\n    \"\"\"Add a local raster dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and\n        if the raster does not render properly, try installing\n            jupyter-server-proxy using `pip install jupyter-server-proxy`,\n        then running the following code before calling this function. For\n            more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band. See\n            https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n            Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        attribution (str, optional): Attribution for the source raster. This\n            defaults to a message about it being a local file.. Defaults to None.\n        layer_name (str, optional): The layer name to use. Defaults to 'Raster'.\n        layer_index (int, optional): The index of the layer. Defaults to None.\n        zoom_to_layer (bool, optional): Whether to zoom to the extent of the\n            layer. Defaults to True.\n        visible (bool, optional): Whether the layer is visible. Defaults to\n            True.\n        opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        client_args (dict, optional): Additional arguments to pass to\n            localtileserver.TileClient. Defaults to { \"cors_all\": False }.\n        open_args (dict, optional): Additional arguments to pass to\n            rioxarray.open_rasterio.\n\n    \"\"\"\n\n    import rioxarray as rxr\n\n    if array_args is None:\n        array_args = {}\n    if open_args is None:\n        open_args = {}\n\n    if nodata is None:\n        nodata = np.nan\n    super().add_raster(\n        source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        attribution=attribution,\n        layer_name=layer_name,\n        layer_index=layer_index,\n        zoom_to_layer=zoom_to_layer,\n        visible=visible,\n        opacity=opacity,\n        array_args=array_args,\n        client_args=client_args,\n        **kwargs,\n    )\n\n    if isinstance(source, str):\n        da = rxr.open_rasterio(source, **open_args)\n        dims = da.dims\n        da = da.transpose(dims[1], dims[2], dims[0])\n\n        xds = da.to_dataset(name=\"data\")\n        self.cog_layer_dict[layer_name][\"xds\"] = xds\n        self.cog_layer_dict[layer_name][\"hyper\"] = \"COG\"\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.search_ecostress","title":"<code>search_ecostress(self, default_dataset='ECO_L2T_LSTE')</code>","text":"<p>Adds a NASA Earth Data search tool to the map with a default dataset for     ECOSTRESS.</p> <p>Parameters:</p> Name Type Description Default <code>default_dataset</code> <code>str</code> <p>The default dataset to search for. Defaults to \"ECO_L2T_LSTE\".</p> <code>'ECO_L2T_LSTE'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def search_ecostress(self, default_dataset=\"ECO_L2T_LSTE\"):\n    \"\"\"\n    Adds a NASA Earth Data search tool to the map with a default dataset for\n        ECOSTRESS.\n\n    Args:\n        default_dataset (str, optional): The default dataset to search for.\n            Defaults to \"ECO_L2T_LSTE\".\n    \"\"\"\n    self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.search_emit","title":"<code>search_emit(self, default_dataset='EMITL2ARFL')</code>","text":"<p>Adds a NASA Earth Data search tool to the map with a default dataset for     EMIT.</p> <p>Parameters:</p> Name Type Description Default <code>default_dataset</code> <code>str</code> <p>The default dataset to search for. Defaults to \"EMITL2ARFL\".</p> <code>'EMITL2ARFL'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def search_emit(self, default_dataset=\"EMITL2ARFL\"):\n    \"\"\"\n    Adds a NASA Earth Data search tool to the map with a default dataset for\n        EMIT.\n\n    Args:\n        default_dataset (str, optional): The default dataset to search for.\n            Defaults to \"EMITL2ARFL\".\n    \"\"\"\n    self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.search_pace","title":"<code>search_pace(self, default_dataset='PACE_OCI_L2_AOP_NRT')</code>","text":"<p>Adds a NASA Earth Data search tool to the map with a default dataset for     PACE.</p> <p>Parameters:</p> Name Type Description Default <code>default_dataset</code> <code>str</code> <p>The default dataset to search for. Defaults to \"PACE_OCI_L2_AOP_NRT\".</p> <code>'PACE_OCI_L2_AOP_NRT'</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def search_pace(self, default_dataset=\"PACE_OCI_L2_AOP_NRT\"):\n    \"\"\"\n    Adds a NASA Earth Data search tool to the map with a default dataset for\n        PACE.\n\n    Args:\n        default_dataset (str, optional): The default dataset to search for.\n            Defaults to \"PACE_OCI_L2_AOP_NRT\".\n    \"\"\"\n    self.add(\"nasa_earth_data\", default_dataset=default_dataset)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.set_plot_options","title":"<code>set_plot_options(self, add_marker_cluster=False, plot_type=None, overlay=False, position='bottomright', min_width=None, max_width=None, min_height=None, max_height=None, **kwargs)</code>","text":"<p>Sets plotting options.</p> <p>Parameters:</p> Name Type Description Default <code>add_marker_cluster</code> <code>bool</code> <p>Whether to add a marker cluster. Defaults to False.</p> <code>False</code> <code>sample_scale</code> <code>float</code> <p>A nominal scale in meters of the projection to sample in . Defaults to None.</p> required <code>plot_type</code> <code>str</code> <p>The plot type can be one of \"None\", \"bar\", \"scatter\" or \"hist\". Defaults to None.</p> <code>None</code> <code>overlay</code> <code>bool</code> <p>Whether to overlay plotted lines on the figure. Defaults to False.</p> <code>False</code> <code>position</code> <code>str</code> <p>Position of the control, can be \u2018bottomleft\u2019, \u2018bottomright\u2019, \u2018topleft\u2019, or \u2018topright\u2019. Defaults to 'bottomright'.</p> <code>'bottomright'</code> <code>min_width</code> <code>int</code> <p>Min width of the widget (in pixels), if None it will respect the content size. Defaults to None.</p> <code>None</code> <code>max_width</code> <code>int</code> <p>Max width of the widget (in pixels), if None it will respect the content size. Defaults to None.</p> <code>None</code> <code>min_height</code> <code>int</code> <p>Min height of the widget (in pixels), if None it will respect the content size. Defaults to None.</p> <code>None</code> <code>max_height</code> <code>int</code> <p>Max height of the widget (in pixels), if None it will respect the content size. Defaults to None.</p> <code>None</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def set_plot_options(\n    self,\n    add_marker_cluster=False,\n    plot_type=None,\n    overlay=False,\n    position=\"bottomright\",\n    min_width=None,\n    max_width=None,\n    min_height=None,\n    max_height=None,\n    **kwargs,\n):\n    \"\"\"Sets plotting options.\n\n    Args:\n        add_marker_cluster (bool, optional): Whether to add a marker cluster.\n            Defaults to False.\n        sample_scale (float, optional):  A nominal scale in meters of the\n            projection to sample in . Defaults to None.\n        plot_type (str, optional): The plot type can be one of \"None\", \"bar\",\n            \"scatter\" or \"hist\". Defaults to None.\n        overlay (bool, optional): Whether to overlay plotted lines on the\n            figure. Defaults to False.\n        position (str, optional): Position of the control, can be\n            \u2018bottomleft\u2019, \u2018bottomright\u2019, \u2018topleft\u2019, or \u2018topright\u2019. Defaults\n            to 'bottomright'.\n        min_width (int, optional): Min width of the widget (in pixels), if\n            None it will respect the content size. Defaults to None.\n        max_width (int, optional): Max width of the widget (in pixels), if\n            None it will respect the content size. Defaults to None.\n        min_height (int, optional): Min height of the widget (in pixels), if\n            None it will respect the content size. Defaults to None.\n        max_height (int, optional): Max height of the widget (in pixels), if\n            None it will respect the content size. Defaults to None.\n\n    \"\"\"\n    plot_options_dict = {}\n    plot_options_dict[\"add_marker_cluster\"] = add_marker_cluster\n    plot_options_dict[\"plot_type\"] = plot_type\n    plot_options_dict[\"overlay\"] = overlay\n    plot_options_dict[\"position\"] = position\n    plot_options_dict[\"min_width\"] = min_width\n    plot_options_dict[\"max_width\"] = max_width\n    plot_options_dict[\"min_height\"] = min_height\n    plot_options_dict[\"max_height\"] = max_height\n\n    for key in kwargs:\n        plot_options_dict[key] = kwargs[key]\n\n    self._plot_options = plot_options_dict\n\n    if not hasattr(self, \"_plot_marker_cluster\"):\n        self._plot_marker_cluster = ipyleaflet.MarkerCluster(name=\"Marker Cluster\")\n\n    if add_marker_cluster and (self._plot_marker_cluster not in self.layers):\n        self.add(self._plot_marker_cluster)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.spectral_to_csv","title":"<code>spectral_to_csv(self, filename, index=True, **kwargs)</code>","text":"<p>Saves the spectral data to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The output CSV file.</p> required <code>index</code> <code>bool</code> <p>Whether to write the index. Defaults to True.</p> <code>True</code> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def spectral_to_csv(self, filename, index=True, **kwargs):\n    \"\"\"Saves the spectral data to a CSV file.\n\n    Args:\n        filename (str): The output CSV file.\n        index (bool, optional): Whether to write the index. Defaults to True.\n    \"\"\"\n    df = self.spectral_to_df()\n    df = df.rename_axis(\"band\")\n    df.to_csv(filename, index=index, **kwargs)\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.spectral_to_df","title":"<code>spectral_to_df(self, **kwargs)</code>","text":"<p>Converts the spectral data to a pandas DataFrame.</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>The spectral data as a pandas DataFrame.</p> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def spectral_to_df(self, **kwargs):\n    \"\"\"Converts the spectral data to a pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: The spectral data as a pandas DataFrame.\n    \"\"\"\n    import pandas as pd\n\n    df = pd.DataFrame(self._spectral_data, **kwargs)\n    return df\n</code></pre>"},{"location":"hypercoast/#hypercoast.hypercoast.Map.spectral_to_gdf","title":"<code>spectral_to_gdf(self, **kwargs)</code>","text":"<p>Converts the spectral data to a GeoPandas GeoDataFrame.</p> <p>Returns:</p> Type Description <code>gpd.DataFrame</code> <p>The spectral data as a pandas DataFrame.</p> Source code in <code>hypercoast/hypercoast.py</code> <pre><code>def spectral_to_gdf(self, **kwargs):\n    \"\"\"Converts the spectral data to a GeoPandas GeoDataFrame.\n\n    Returns:\n        gpd.DataFrame: The spectral data as a pandas DataFrame.\n    \"\"\"\n    import geopandas as gpd\n    from shapely.geometry import Point\n\n    df = self.spectral_to_df()\n\n    if len(df) == 0:\n        return None\n\n    # Step 1: Extract the coordinates from the columns\n    df = df.rename(columns={\"wavelength\": \"latlon\"})\n    coordinates = [col.strip(\"()\").split() for col in df.columns[1:]]\n    coords = [(float(lat), float(lon)) for lat, lon in coordinates]\n\n    # Step 2: Create Point geometries for each coordinate\n    points = [Point(lon, lat) for lat, lon in coords]\n\n    # Step 3: Create a GeoDataFrame\n    df_transposed = df.set_index(\"latlon\").T\n\n    # Convert the column names to strings to ensure compatibility with GeoJSON\n    df_transposed.columns = df_transposed.columns.astype(str)\n\n    # Create the GeoDataFrame\n    gdf = gpd.GeoDataFrame(df_transposed, geometry=points, **kwargs)\n\n    # Set the coordinate reference system (CRS)\n    gdf = gdf.set_geometry(\"geometry\").set_crs(\"EPSG:4326\")\n\n    return gdf\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>hypercoast is available on PyPI. To install hypercoast, run this command in your terminal:</p> <pre><code>pip install hypercoast\n</code></pre> <p>HyperCoast has some optional dependencies that are not installed by default, such as cartopy, earthaccess, mapclassify, and pyvista. To install all optional dependencies all at once, run the following command:</p> <pre><code>pip install \"hypercoast[extra]\"\n</code></pre>"},{"location":"installation/#install-from-conda-forge","title":"Install from conda-forge","text":"<p>hypercoast is also available on conda-forge. If you have Anaconda or Miniconda installed on your computer, you can install hypercoast using the following command:</p> <pre><code>conda install -c conda-forge hypercoast\n</code></pre> <p>Alternatively, you can create a new conda environment and install hypercoast in the new environment. This is a good practice because it avoids potential conflicts with other packages installed in your base environment.</p> <pre><code>conda install -n base mamba -c conda-forge\nconda create -n hyper python=3.11\nconda activate hyper\nmamba install -c conda-forge hypercoast\n</code></pre> <p>To install the optional dependencies, run the following command:</p> <pre><code>mamba install -c conda-forge cartopy earthaccess mapclassify pyvista trame-vtk trame-vuetify\n</code></pre>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>To install the development version from GitHub using Git, run the following command in your terminal:</p> <pre><code>pip install git+https://github.com/opengeos/hypercoast\n</code></pre>"},{"location":"neon/","title":"neon module","text":"<p>This module contains functions to read and process NEON AOP hyperspectral data. More info about the data can be found at https://bit.ly/3Rfszdc. The source code is adapted from https://bit.ly/3KwyZkn. Credit goes to the original authors.</p>"},{"location":"neon/#hypercoast.neon.extract_neon","title":"<code>extract_neon(ds, lat, lon)</code>","text":"<p>Extracts NEON AOP data from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xarray.Dataset</code> <p>The dataset containing the NEON AOP data.</p> required <code>lat</code> <code>float</code> <p>The latitude of the point to extract.</p> required <code>lon</code> <code>float</code> <p>The longitude of the point to extract.</p> required <p>Returns:</p> Type Description <code>xarray.DataArray</code> <p>The extracted data.</p> Source code in <code>hypercoast/neon.py</code> <pre><code>def extract_neon(ds, lat, lon):\n    \"\"\"\n    Extracts NEON AOP data from a given xarray Dataset.\n\n    Args:\n        ds (xarray.Dataset): The dataset containing the NEON AOP data.\n        lat (float): The latitude of the point to extract.\n        lon (float): The longitude of the point to extract.\n\n    Returns:\n        xarray.DataArray: The extracted data.\n    \"\"\"\n\n    crs = ds.attrs[\"crs\"]\n\n    x, y = convert_coords([[lat, lon]], \"epsg:4326\", crs)[0]\n\n    values = ds.sel(x=x, y=y, method=\"nearest\")[\"reflectance\"].values\n\n    da = xr.DataArray(\n        values, dims=[\"wavelength\"], coords={\"wavelength\": ds.coords[\"wavelength\"]}\n    )\n\n    return da\n</code></pre>"},{"location":"neon/#hypercoast.neon.list_neon_datasets","title":"<code>list_neon_datasets(filepath, print_node=False)</code>","text":"<p>Lists all the datasets in an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the HDF5 file.</p> required <code>print_node</code> <code>bool</code> <p>If True, prints the node object of each dataset. If False, prints the name of each dataset. Defaults to False.</p> <code>False</code> Source code in <code>hypercoast/neon.py</code> <pre><code>def list_neon_datasets(filepath: str, print_node: bool = False) -&gt; None:\n    \"\"\"\n    Lists all the datasets in an HDF5 file.\n\n    Args:\n        filepath (str): The path to the HDF5 file.\n        print_node (bool, optional): If True, prints the node object of each dataset.\n            If False, prints the name of each dataset. Defaults to False.\n    \"\"\"\n\n    f = h5py.File(filepath, \"r\")\n\n    if print_node:\n\n        def list_dataset(_, node):\n            if isinstance(node, h5py.Dataset):\n                print(node)\n\n    else:\n\n        def list_dataset(name, node):\n            if isinstance(node, h5py.Dataset):\n                print(name)\n\n    f.visititems(list_dataset)\n</code></pre>"},{"location":"neon/#hypercoast.neon.neon_to_image","title":"<code>neon_to_image(dataset, wavelengths=None, method='nearest', output=None, **kwargs)</code>","text":"<p>Converts an NEON dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The dataset containing the NEON data or the file path to the dataset.</p> required <code>wavelengths</code> <code>np.ndarray</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[rasterio.Dataset]</code> <p>The image converted from the dataset. If     <code>output</code> is provided, the image will be saved to the specified file     and the function will return None.</p> Source code in <code>hypercoast/neon.py</code> <pre><code>def neon_to_image(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Optional[np.ndarray] = None,\n    method: str = \"nearest\",\n    output: Optional[str] = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Converts an NEON dataset to an image.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The dataset containing the NEON data\n            or the file path to the dataset.\n        wavelengths (np.ndarray, optional): The specific wavelengths to select. If None, all\n            wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data interpolation.\n            Defaults to \"nearest\".\n        output (str, optional): The file path where the image will be saved. If\n            None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs (Any): Additional keyword arguments to be passed to\n            `leafmap.array_to_image`.\n\n    Returns:\n        Optional[rasterio.Dataset]: The image converted from the dataset. If\n            `output` is provided, the image will be saved to the specified file\n            and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_neon(dataset, method=method)\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n\n    return array_to_image(\n        dataset[\"reflectance\"],\n        output=output,\n        transpose=False,\n        dtype=np.float32,\n        **kwargs,\n    )\n</code></pre>"},{"location":"neon/#hypercoast.neon.read_neon","title":"<code>read_neon(filepath, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Reads NEON AOP hyperspectral hdf5 files and returns an xarray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the hdf5 file.</p> required <code>wavelengths</code> <code>List[float]</code> <p>The wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for selection. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the selection method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The dataset containing the reflectance data.</p> Source code in <code>hypercoast/neon.py</code> <pre><code>def read_neon(\n    filepath: str,\n    wavelengths: Optional[List[float]] = None,\n    method: str = \"nearest\",\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads NEON AOP hyperspectral hdf5 files and returns an xarray dataset.\n\n    Args:\n        filepath (str): The path to the hdf5 file.\n        wavelengths (List[float], optional): The wavelengths to select. If None,\n            all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for selection. Defaults to\n            \"nearest\".\n        **kwargs (Any): Additional arguments to pass to the selection method.\n\n    Returns:\n        xr.Dataset: The dataset containing the reflectance data.\n    \"\"\"\n    with h5py.File(filepath, \"r\") as f:\n        # Extract site code dynamically from NEON HDF file metadata\n        # At the root of `keys` NEON stores the site code, which is the `root` folder at the [0] index of object `keys`\n        site_code = list(f.keys())[0]\n\n        # Access the reflectance data using the site code\n        site_refl = f[site_code][\"Reflectance\"]\n\n        # Extract wavelengths\n        wavelengths_list = site_refl[\"Metadata\"][\"Spectral_Data\"][\"Wavelength\"][\n            ()\n        ].tolist()\n        wavelengths_list = [round(num, 2) for num in wavelengths_list]\n\n        # Extract EPSG code\n        epsg_code = site_refl[\"Metadata\"][\"Coordinate_System\"][\"EPSG Code\"][()]\n        epsg_code_number = int(epsg_code.decode(\"utf-8\"))\n\n        # Extract map info\n        mapInfo_string = site_refl[\"Metadata\"][\"Coordinate_System\"][\"Map_Info\"][\n            ()\n        ].decode(\"utf-8\")\n        mapInfo_split = mapInfo_string.split(\",\")\n\n        res = float(mapInfo_split[5]), float(mapInfo_split[6])\n\n        # Extract reflectance array and shape\n        site_reflArray = site_refl[\"Reflectance_Data\"]\n        refl_shape = site_reflArray.shape\n\n        # Calculate coordinates\n        xMin = float(mapInfo_split[3])\n        yMax = float(mapInfo_split[4])\n\n        xMax = xMin + (refl_shape[1] * res[0])\n        yMin = yMax - (refl_shape[0] * res[1])\n\n        # Handle scale factor and no-data value\n        scaleFactor = site_reflArray.attrs[\"Scale_Factor\"]\n        noDataValue = site_reflArray.attrs[\"Data_Ignore_Value\"]\n\n        da = site_reflArray[:, :, :].astype(float)\n        da[da == int(noDataValue)] = np.nan\n        da[da &lt; 0] = np.nan\n        da[da &gt; 10000] = np.nan\n        da = da / scaleFactor\n\n        coords = {\n            \"y\": np.linspace(yMax, yMin, da.shape[0]),\n            \"x\": np.linspace(xMin, xMax, da.shape[1]),\n            \"wavelength\": wavelengths_list,\n        }\n\n        xda = xr.DataArray(\n            da,\n            coords=coords,\n            dims=[\"y\", \"x\", \"wavelength\"],\n            attrs={\n                \"scale_factor\": scaleFactor,\n                \"no_data_value\": noDataValue,\n                \"crs\": f\"EPSG:{epsg_code_number}\",\n                \"transform\": (res[0], 0.0, xMin, 0.0, -res[1], yMax),\n            },\n        )\n\n        if wavelengths is not None:\n            xda = xda.sel(wavelength=wavelengths, method=method, **kwargs)\n\n        dataset = xda.to_dataset(name=\"reflectance\")\n        dataset.attrs = dataset[\"reflectance\"].attrs\n\n    return dataset\n</code></pre>"},{"location":"pace/","title":"pace module","text":"<p>This module contains functions to read and process PACE data.</p>"},{"location":"pace/#hypercoast.pace.extract_pace","title":"<code>extract_pace(dataset, latitude, longitude, delta=0.01, return_plot=False, **kwargs)</code>","text":"<p>Extracts data from a PACE dataset for a given latitude and longitude range     and calculates the mean over these dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[xr.Dataset, str]</code> <p>The PACE dataset or path to the dataset file.</p> required <code>latitude</code> <code>Union[float, Tuple[float, float]]</code> <p>The latitude or range of latitudes to extract data for.</p> required <code>longitude</code> <code>Union[float, Tuple[float, float]]</code> <p>The longitude or range of longitudes to extract data for.</p> required <code>delta</code> <code>float</code> <p>The range to add/subtract to the latitude and longitude if they are not ranges. Defaults to 0.01.</p> <code>0.01</code> <code>return_plot</code> <code>bool</code> <p>Whether to return a plot of the data. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the plot function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[xr.DataArray, plt.figure.Figure]</code> <p>The mean data over the latitude     and longitude dimensions, or a plot of this data if return_plot is True.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def extract_pace(\n    dataset: Union[xr.Dataset, str],\n    latitude: Union[float, Tuple[float, float]],\n    longitude: Union[float, Tuple[float, float]],\n    delta: float = 0.01,\n    return_plot: bool = False,\n    **kwargs,\n) -&gt; Union[xr.DataArray, plt.Figure]:\n    \"\"\"\n    Extracts data from a PACE dataset for a given latitude and longitude range\n        and calculates the mean over these dimensions.\n\n    Args:\n        dataset (Union[xr.Dataset, str]): The PACE dataset or path to the dataset file.\n        latitude (Union[float, Tuple[float, float]]): The latitude or range of\n            latitudes to extract data for.\n        longitude (Union[float, Tuple[float, float]]): The longitude or range of\n            longitudes to extract data for.\n        delta (float, optional): The range to add/subtract to the latitude and\n            longitude if they are not ranges. Defaults to 0.01.\n        return_plot (bool, optional): Whether to return a plot of the data. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the plot function.\n\n    Returns:\n        Union[xr.DataArray, plt.figure.Figure]: The mean data over the latitude\n            and longitude dimensions, or a plot of this data if return_plot is True.\n    \"\"\"\n    if isinstance(latitude, list) or isinstance(latitude, tuple):\n        pass\n    else:\n        latitude = (latitude - delta, latitude + delta)\n\n    if isinstance(longitude, list) or isinstance(longitude, tuple):\n        pass\n    else:\n        longitude = (longitude - delta, longitude + delta)\n\n    ds = filter_pace(dataset, latitude, longitude, return_plot=False)\n    data = ds.mean(dim=[\"latitude\", \"longitude\"])\n    if return_plot:\n        return data.plot.line(**kwargs)\n    else:\n        return data\n</code></pre>"},{"location":"pace/#hypercoast.pace.filter_pace","title":"<code>filter_pace(dataset, latitude, longitude, drop=True, return_plot=False, **kwargs)</code>","text":"<p>Filters a PACE dataset based on latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The PACE dataset to filter.</p> required <code>latitude</code> <code>float or tuple</code> <p>The latitude to filter by. If a tuple or list, it represents a range.</p> required <code>longitude</code> <code>float or tuple</code> <p>The longitude to filter by. If a tuple or list, it represents a range.</p> required <code>drop</code> <code>bool</code> <p>Whether to drop the filtered out data. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The filtered PACE data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def filter_pace(dataset, latitude, longitude, drop=True, return_plot=False, **kwargs):\n    \"\"\"\n    Filters a PACE dataset based on latitude and longitude.\n\n    Args:\n        dataset (xr.Dataset): The PACE dataset to filter.\n        latitude (float or tuple): The latitude to filter by. If a tuple or list, it represents a range.\n        longitude (float or tuple): The longitude to filter by. If a tuple or list, it represents a range.\n        drop (bool, optional): Whether to drop the filtered out data. Defaults to True.\n\n    Returns:\n        xr.DataArray: The filtered PACE data.\n    \"\"\"\n    if isinstance(latitude, list) or isinstance(latitude, tuple):\n        lat_con = (dataset[\"latitude\"] &gt; latitude[0]) &amp; (\n            dataset[\"latitude\"] &lt; latitude[1]\n        )\n    else:\n        lat_con = dataset[\"latitude\"] == latitude\n\n    if isinstance(longitude, list) or isinstance(longitude, tuple):\n        lon_con = (dataset[\"longitude\"] &gt; longitude[0]) &amp; (\n            dataset[\"longitude\"] &lt; longitude[1]\n        )\n    else:\n        lon_con = dataset[\"longitude\"] == longitude\n\n    da = dataset[\"Rrs\"].where(lat_con &amp; lon_con, drop=drop, **kwargs)\n    da_filtered = da.dropna(dim=\"latitude\", how=\"all\")\n    da_filtered = da_filtered.dropna(dim=\"longitude\", how=\"all\")\n\n    if return_plot:\n        rrs_stack = da_filtered.stack(\n            {\"pixel\": [\"latitude\", \"longitude\"]},\n            create_index=False,\n        )\n        rrs_stack.plot.line(hue=\"pixel\")\n    else:\n        return da_filtered\n</code></pre>"},{"location":"pace/#hypercoast.pace.grid_pace","title":"<code>grid_pace(dataset, wavelengths=None, method='nearest', **kwargs)</code>","text":"<p>Grids a PACE dataset based on latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The PACE dataset to grid.</p> required <code>wavelengths</code> <code>float or int</code> <p>The wavelength to select.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for griddata interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the xr.Dataset constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The gridded PACE data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def grid_pace(dataset, wavelengths=None, method=\"nearest\", **kwargs):\n    \"\"\"\n    Grids a PACE dataset based on latitude and longitude.\n\n    Args:\n        dataset (xr.Dataset): The PACE dataset to grid.\n        wavelengths (float or int): The wavelength to select.\n        method (str, optional): The method to use for griddata interpolation.\n            Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to pass to the xr.Dataset constructor.\n\n    Returns:\n        xr.DataArray: The gridded PACE data.\n    \"\"\"\n    from scipy.interpolate import griddata\n\n    if wavelengths is None:\n        wavelengths = dataset.coords[\"wavelength\"].values[0]\n\n    # Ensure wavelengths is a list\n    if not isinstance(wavelengths, list):\n        wavelengths = [wavelengths]\n\n    lat = dataset.latitude\n    lon = dataset.longitude\n\n    grid_lat = np.linspace(lat.min(), lat.max(), lat.shape[0])\n    grid_lon = np.linspace(lon.min(), lon.max(), lon.shape[1])\n    grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n\n    gridded_data_dict = {}\n    for wavelength in wavelengths:\n        data = dataset.sel(wavelength=wavelength, method=\"nearest\")[\"Rrs\"]\n        gridded_data = griddata(\n            (lat.data.flatten(), lon.data.flatten()),\n            data.data.flatten(),\n            (grid_lat_2d, grid_lon_2d),\n            method=method,\n        )\n        gridded_data_dict[wavelength] = gridded_data\n\n    # Create a 3D array with dimensions latitude, longitude, and wavelength\n    gridded_data_3d = np.dstack(list(gridded_data_dict.values()))\n\n    dataset2 = xr.Dataset(\n        {\"Rrs\": ((\"latitude\", \"longitude\", \"wavelength\"), gridded_data_3d)},\n        coords={\n            \"latitude\": (\"latitude\", grid_lat),\n            \"longitude\": (\"longitude\", grid_lon),\n            \"wavelength\": (\"wavelength\", list(gridded_data_dict.keys())),\n        },\n        **kwargs,\n    )\n\n    dataset2[\"Rrs\"].rio.write_crs(\"EPSG:4326\", inplace=True)\n\n    return dataset2\n</code></pre>"},{"location":"pace/#hypercoast.pace.grid_pace_bgc","title":"<code>grid_pace_bgc(dataset, variable='chlor_a', method='nearest', **kwargs)</code>","text":"<p>Grids PACE BGC data using specified interpolation method.</p> <p>This function takes an xarray Dataset containing PACE BGC data, interpolates it onto a regular grid using the specified method, and returns the gridded data as an xarray DataArray with the specified variable.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>The input dataset containing PACE BGC data with latitude and longitude coordinates.</p> required <code>variable</code> <code>str</code> <p>The variable within the dataset to grid. Can be one of chlor_a, carbon_phyto, poc, chlor_a_unc, carbon_phyto_unc, and l2_flags. Defaults to \"chlor_a\".</p> <code>'chlor_a'</code> <code>method</code> <code>str</code> <p>The interpolation method to use. Options include \"nearest\", \"linear\", and \"cubic\". Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the xr.Dataset creation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.DataArray</code> <p>The gridded data as an xarray DataArray, with the specified variable and EPSG:4326 CRS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = hypercoast.read_pace_bgc(\"path_to_your_dataset.nc\")\n&gt;&gt;&gt; gridded_data = grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"nearest\")\n&gt;&gt;&gt; print(gridded_data)\n</code></pre> Source code in <code>hypercoast/pace.py</code> <pre><code>def grid_pace_bgc(\n    dataset: xr.Dataset,\n    variable: str = \"chlor_a\",\n    method: str = \"nearest\",\n    **kwargs: Any,\n) -&gt; xr.DataArray:\n    \"\"\"\n    Grids PACE BGC data using specified interpolation method.\n\n    This function takes an xarray Dataset containing PACE BGC data, interpolates it onto a regular grid\n    using the specified method, and returns the gridded data as an xarray DataArray with the specified\n    variable.\n\n    Args:\n        dataset (xr.Dataset): The input dataset containing PACE BGC data with latitude and longitude coordinates.\n        variable (str, optional): The variable within the dataset to grid. Can be\n           one of chlor_a, carbon_phyto, poc, chlor_a_unc, carbon_phyto_unc, and l2_flags.\n           Defaults to \"chlor_a\".\n        method (str, optional): The interpolation method to use. Options include \"nearest\", \"linear\", and \"cubic\".\n            Defaults to \"nearest\".\n        **kwargs (Any): Additional keyword arguments to pass to the xr.Dataset creation.\n\n    Returns:\n        xr.DataArray: The gridded data as an xarray DataArray, with the specified variable and EPSG:4326 CRS.\n\n    Example:\n        &gt;&gt;&gt; dataset = hypercoast.read_pace_bgc(\"path_to_your_dataset.nc\")\n        &gt;&gt;&gt; gridded_data = grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"nearest\")\n        &gt;&gt;&gt; print(gridded_data)\n    \"\"\"\n    import rioxarray\n    from scipy.interpolate import griddata\n\n    lat = dataset.latitude\n    lon = dataset.longitude\n\n    grid_lat = np.linspace(lat.min(), lat.max(), lat.shape[0])\n    grid_lon = np.linspace(lon.min(), lon.max(), lon.shape[1])\n    grid_lon_2d, grid_lat_2d = np.meshgrid(grid_lon, grid_lat)\n\n    data = dataset[variable]\n    gridded_data = griddata(\n        (lat.data.flatten(), lon.data.flatten()),\n        data.data.flatten(),\n        (grid_lat_2d, grid_lon_2d),\n        method=method,\n    )\n\n    dataset2 = xr.Dataset(\n        {variable: ((\"latitude\", \"longitude\"), gridded_data)},\n        coords={\n            \"latitude\": (\"latitude\", grid_lat),\n            \"longitude\": (\"longitude\", grid_lon),\n        },\n        **kwargs,\n    )\n\n    dataset2 = dataset2[variable].rio.write_crs(\"EPSG:4326\")\n\n    return dataset2\n</code></pre>"},{"location":"pace/#hypercoast.pace.pace_chla_to_image","title":"<code>pace_chla_to_image(data, output=None, **kwargs)</code>","text":"<p>Converts PACE chlorophyll-a data to an image.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>xr.DataArray or str</code> <p>The chlorophyll-a data or the file path to the data.</p> required <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the data. If <code>output</code> is provided, the image will be saved to the specified file and the function will return None.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def pace_chla_to_image(data, output=None, **kwargs):\n    \"\"\"\n    Converts PACE chlorophyll-a data to an image.\n\n    Args:\n        data (xr.DataArray or str): The chlorophyll-a data or the file path to the data.\n        output (str, optional): The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the data. If `output` is provided, the image will be saved to the specified file and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image, image_to_geotiff\n\n    if isinstance(data, str):\n        data = read_pace_chla(data)\n    elif not isinstance(data, xr.DataArray):\n        raise ValueError(\"data must be an xarray DataArray\")\n\n    image = array_to_image(data, transpose=False, output=None, **kwargs)\n\n    if output is not None:\n        image_to_geotiff(image, output, dtype=\"float32\")\n\n    return image\n</code></pre>"},{"location":"pace/#hypercoast.pace.pace_to_image","title":"<code>pace_to_image(dataset, wavelengths=None, method='nearest', gridded=False, output=None, **kwargs)</code>","text":"<p>Converts an PACE dataset to an image.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xarray.Dataset or str</code> <p>The dataset containing the EMIT data or the file path to the dataset.</p> required <code>wavelengths</code> <code>array-like</code> <p>The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for data interpolation. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>gridded</code> <code>bool</code> <p>Whether the dataset is a gridded dataset. Defaults to False,</p> <code>False</code> <code>output</code> <code>str</code> <p>The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to <code>leafmap.array_to_image</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>rasterio.Dataset or None</code> <p>The image converted from the dataset. If <code>output</code> is provided, the image will be saved to the specified file and the function will return None.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def pace_to_image(\n    dataset, wavelengths=None, method=\"nearest\", gridded=False, output=None, **kwargs\n):\n    \"\"\"\n    Converts an PACE dataset to an image.\n\n    Args:\n        dataset (xarray.Dataset or str): The dataset containing the EMIT data or the file path to the dataset.\n        wavelengths (array-like, optional): The specific wavelengths to select. If None, all wavelengths are selected. Defaults to None.\n        method (str, optional): The method to use for data interpolation. Defaults to \"nearest\".\n        gridded (bool, optional): Whether the dataset is a gridded dataset. Defaults to False,\n        output (str, optional): The file path where the image will be saved. If None, the image will be returned as a PIL Image object. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to `leafmap.array_to_image`.\n\n    Returns:\n        rasterio.Dataset or None: The image converted from the dataset. If `output` is provided, the image will be saved to the specified file and the function will return None.\n    \"\"\"\n    from leafmap import array_to_image\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset, wavelengths=wavelengths, method=\"nearest\")\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=\"nearest\")\n\n    if not gridded:\n        grid = grid_pace(dataset, wavelengths=wavelengths, method=method)\n    else:\n        grid = dataset\n    data = grid[\"Rrs\"]\n    data.rio.write_crs(\"EPSG:4326\", inplace=True)\n\n    return array_to_image(data, transpose=False, output=output, **kwargs)\n</code></pre>"},{"location":"pace/#hypercoast.pace.read_pace","title":"<code>read_pace(filepath, wavelengths=None, method='nearest', engine='h5netcdf', **kwargs)</code>","text":"<p>Reads PACE data from a given file and returns an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the file to read.</p> required <code>wavelengths</code> <code>array-like</code> <p>Specific wavelengths to select. If None, all wavelengths are selected.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for selection when wavelengths is not None. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>sel</code> method when wavelengths is not None.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>An xarray Dataset containing the PACE data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def read_pace(\n    filepath, wavelengths=None, method=\"nearest\", engine=\"h5netcdf\", **kwargs\n):\n    \"\"\"\n    Reads PACE data from a given file and returns an xarray Dataset.\n\n    Args:\n        filepath (str): Path to the file to read.\n        wavelengths (array-like, optional): Specific wavelengths to select. If None, all wavelengths are selected.\n        method (str, optional): Method to use for selection when wavelengths is not None. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to pass to the `sel` method when wavelengths is not None.\n\n    Returns:\n        xr.Dataset: An xarray Dataset containing the PACE data.\n    \"\"\"\n\n    rrs = xr.open_dataset(filepath, engine=engine, group=\"geophysical_data\")[\"Rrs\"]\n    wvl = xr.open_dataset(filepath, engine=engine, group=\"sensor_band_parameters\")\n    dataset = xr.open_dataset(filepath, engine=engine, group=\"navigation_data\")\n    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n    if \"pixel_control_points\" in dataset.dims:\n        dataset = dataset.rename({\"pixel_control_points\": \"pixels_per_line\"})\n    dataset = xr.merge([rrs, dataset.coords.to_dataset()])\n    dataset.coords[\"wavelength_3d\"] = wvl.coords[\"wavelength_3d\"]\n    dataset = dataset.rename(\n        {\n            \"number_of_lines\": \"latitude\",\n            \"pixels_per_line\": \"longitude\",\n            \"wavelength_3d\": \"wavelength\",\n        }\n    )\n\n    if wavelengths is not None:\n        dataset = dataset.sel(wavelength=wavelengths, method=method, **kwargs)\n\n    return dataset\n</code></pre>"},{"location":"pace/#hypercoast.pace.read_pace_aop","title":"<code>read_pace_aop(filepath, engine='h5netcdf', **kwargs)</code>","text":"<p>Reads PACE data from a given file and returns an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the file to read.</p> required <code>wavelengths</code> <code>array-like</code> <p>Specific wavelengths to select. If None, all wavelengths are selected.</p> required <code>method</code> <code>str</code> <p>Method to use for selection when wavelengths is not None. Defaults to \"nearest\".</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>sel</code> method when wavelengths is not None.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>An xarray Dataset containing the PACE data.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def read_pace_aop(filepath, engine=\"h5netcdf\", **kwargs):\n    \"\"\"\n    Reads PACE data from a given file and returns an xarray Dataset.\n\n    Args:\n        filepath (str): Path to the file to read.\n        wavelengths (array-like, optional): Specific wavelengths to select. If None, all wavelengths are selected.\n        method (str, optional): Method to use for selection when wavelengths is not None. Defaults to \"nearest\".\n        **kwargs: Additional keyword arguments to pass to the `sel` method when wavelengths is not None.\n\n    Returns:\n        xr.Dataset: An xarray Dataset containing the PACE data.\n    \"\"\"\n\n    rrs = xr.open_dataset(filepath, engine=engine, group=\"geophysical_data\", **kwargs)[\n        \"Rrs\"\n    ]\n    wvl = xr.open_dataset(\n        filepath, engine=engine, group=\"sensor_band_parameters\", **kwargs\n    )\n    dataset = xr.open_dataset(\n        filepath, engine=engine, group=\"navigation_data\", **kwargs\n    )\n    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n    if \"pixel_control_points\" in dataset.dims:\n        dataset = dataset.rename({\"pixel_control_points\": \"pixels_per_line\"})\n    dataset = xr.merge([rrs, dataset.coords.to_dataset()])\n    dataset.coords[\"wavelength_3d\"] = wvl.coords[\"wavelength_3d\"]\n\n    return dataset\n</code></pre>"},{"location":"pace/#hypercoast.pace.read_pace_bgc","title":"<code>read_pace_bgc(filepath, variable=None, engine='h5netcdf', **kwargs)</code>","text":"<p>Reads PACE BGC data from a specified file and returns an xarray Dataset.</p> <p>This function opens a dataset from a file using the specified engine, optionally selects a single variable, merges geophysical and navigation data, sets appropriate coordinates, and renames dimensions for easier use.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file containing the PACE BGC data.</p> required <code>variable</code> <code>Optional[str]</code> <p>The specific variable to extract from the geophysical_data group. If None, all variables are read. Defaults to None.</p> <code>None</code> <code>engine</code> <code>str</code> <p>The engine to use for reading the file. Defaults to \"h5netcdf\".</p> <code>'h5netcdf'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to <code>xr.open_dataset</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>An xarray Dataset containing the requested PACE BGC data, with merged geophysical and navigation data, set coordinates, and renamed dimensions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataset = read_pace_bgc(\"path/to/your/datafile.h5\", variable=\"chlor_a\")\n&gt;&gt;&gt; print(dataset)\n</code></pre> Source code in <code>hypercoast/pace.py</code> <pre><code>def read_pace_bgc(\n    filepath: str,\n    variable: Optional[str] = None,\n    engine: str = \"h5netcdf\",\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads PACE BGC data from a specified file and returns an xarray Dataset.\n\n    This function opens a dataset from a file using the specified engine,\n    optionally selects a single variable, merges geophysical and navigation data,\n    sets appropriate coordinates, and renames dimensions for easier use.\n\n    Args:\n        filepath (str): The path to the file containing the PACE BGC data.\n        variable (Optional[str], optional): The specific variable to extract\n            from the geophysical_data group. If None, all variables are read. Defaults to None.\n        engine (str, optional): The engine to use for reading the file. Defaults to \"h5netcdf\".\n        **kwargs (Any): Additional keyword arguments to pass to `xr.open_dataset`.\n\n    Returns:\n        xr.Dataset: An xarray Dataset containing the requested PACE BGC data,\n        with merged geophysical and navigation data, set coordinates, and renamed dimensions.\n\n    Example:\n        &gt;&gt;&gt; dataset = read_pace_bgc(\"path/to/your/datafile.h5\", variable=\"chlor_a\")\n        &gt;&gt;&gt; print(dataset)\n    \"\"\"\n\n    ds = xr.open_dataset(filepath, engine=engine, group=\"geophysical_data\", **kwargs)\n    if variable is not None:\n        ds = ds[variable]\n    dataset = xr.open_dataset(\n        filepath, engine=engine, group=\"navigation_data\", **kwargs\n    )\n    dataset = dataset.set_coords((\"longitude\", \"latitude\"))\n    if \"pixel_control_points\" in dataset.dims:\n        dataset = dataset.rename({\"pixel_control_points\": \"pixels_per_line\"})\n    dataset = xr.merge([ds, dataset.coords.to_dataset()])\n    dataset = dataset.rename(\n        {\n            \"number_of_lines\": \"latitude\",\n            \"pixels_per_line\": \"longitude\",\n        }\n    )\n    attrs = xr.open_dataset(filepath, engine=engine, **kwargs).attrs\n    dataset.attrs.update(attrs)\n\n    return dataset\n</code></pre>"},{"location":"pace/#hypercoast.pace.read_pace_chla","title":"<code>read_pace_chla(filepaths, engine='h5netcdf', **kwargs)</code>","text":"<p>Reads chlorophyll-a data from PACE files and applies a logarithmic transformation.</p> <p>This function supports reading from a single file or multiple files. For multiple files, it combines them into a single dataset. It then extracts the chlorophyll-a variable, applies a logarithmic transformation, and sets the coordinate reference system to EPSG:4326.</p> <p>Parameters:</p> Name Type Description Default <code>filepaths</code> <code>Union[str, List[str]]</code> <p>A string or a list of strings containing the file path(s) to the PACE chlorophyll-a data files.</p> required <code>engine</code> <code>str</code> <p>The backend engine to use for reading files. Defaults to \"h5netcdf\".</p> <code>'h5netcdf'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>xr.open_dataset</code> or <code>xr.open_mfdataset</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>An xarray DataArray containing the logarithmically transformed chlorophyll-a data with updated attributes.</p> <p>Examples:</p> <p>Read chlorophyll-a data from a single file:</p> <pre><code>&gt;&gt;&gt; chla_data = read_pace_chla('path/to/single/file.nc')\n</code></pre> <p>Read and combine chlorophyll-a data from multiple files:</p> <pre><code>&gt;&gt;&gt; chla_data = read_pace_chla(['path/to/file1.nc', 'path/to/file2.nc'], combine='by_coords')\n</code></pre> Source code in <code>hypercoast/pace.py</code> <pre><code>def read_pace_chla(\n    filepaths: Union[str, List[str]], engine: str = \"h5netcdf\", **kwargs\n) -&gt; xr.DataArray:\n    \"\"\"\n    Reads chlorophyll-a data from PACE files and applies a logarithmic transformation.\n\n    This function supports reading from a single file or multiple files. For multiple files,\n    it combines them into a single dataset. It then extracts the chlorophyll-a variable,\n    applies a logarithmic transformation, and sets the coordinate reference system to EPSG:4326.\n\n    Args:\n        filepaths: A string or a list of strings containing the file path(s) to the PACE chlorophyll-a data files.\n        engine: The backend engine to use for reading files. Defaults to \"h5netcdf\".\n        **kwargs: Additional keyword arguments to pass to `xr.open_dataset` or `xr.open_mfdataset`.\n\n    Returns:\n        An xarray DataArray containing the logarithmically transformed chlorophyll-a data with updated attributes.\n\n    Examples:\n        Read chlorophyll-a data from a single file:\n        &gt;&gt;&gt; chla_data = read_pace_chla('path/to/single/file.nc')\n\n        Read and combine chlorophyll-a data from multiple files:\n        &gt;&gt;&gt; chla_data = read_pace_chla(['path/to/file1.nc', 'path/to/file2.nc'], combine='by_coords')\n    \"\"\"\n\n    import os\n    import glob\n    import rioxarray\n\n    date = None\n    if isinstance(filepaths, str) and os.path.isfile(filepaths):\n        filepaths = [filepaths]\n    if \"combine\" not in kwargs:\n        kwargs[\"combine\"] = \"nested\"\n    if \"concat_dim\" not in kwargs:\n        kwargs[\"concat_dim\"] = \"date\"\n    dataset = xr.open_mfdataset(filepaths, engine=engine, **kwargs)\n    if not isinstance(filepaths, list):\n        filepaths = glob.glob(filepaths)\n        filepaths.sort()\n\n    dates = [extract_date_from_filename(f) for f in filepaths]\n    date = [timestamp.strftime(\"%Y-%m-%d\") for timestamp in dates]\n    dataset = dataset.assign_coords(date=(\"date\", date))\n\n    chla = np.log10(dataset[\"chlor_a\"])\n    chla.attrs.update(\n        {\n            \"units\": f'lg({dataset[\"chlor_a\"].attrs[\"units\"]})',\n        }\n    )\n\n    if date is not None:\n        chla.attrs[\"date\"] = date\n\n    chla = chla.transpose(\"lat\", \"lon\", \"date\")\n\n    chla.rio.write_crs(\"EPSG:4326\", inplace=True)\n\n    return chla\n</code></pre>"},{"location":"pace/#hypercoast.pace.view_pace_pixel_locations","title":"<code>view_pace_pixel_locations(filepath, step=20, figsize=(8, 6), **kwargs)</code>","text":"<p>Visualizes a subset of PACE pixel locations on a scatter plot.</p> <p>This function reads PACE AOP data from a specified file, subsamples the data according to a step size, and plots the longitude and latitude of the selected pixels using a scatter plot.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file containing the PACE AOP data.</p> required <code>step</code> <code>int</code> <p>The step size for subsampling the data. A smaller step size results in more data points being plotted. Defaults to 20.</p> <code>20</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>plot.scatter</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>A matplotlib figure object containing the scatter plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot = view_pace_pixel_locations(\"path/to/your/datafile.h5\", step=10)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>hypercoast/pace.py</code> <pre><code>def view_pace_pixel_locations(\n    filepath: str, step: int = 20, figsize: Tuple[float, float] = (8, 6), **kwargs: Any\n) -&gt; plt.Figure:\n    \"\"\"\n    Visualizes a subset of PACE pixel locations on a scatter plot.\n\n    This function reads PACE AOP data from a specified file, subsamples the data according to a step size,\n    and plots the longitude and latitude of the selected pixels using a scatter plot.\n\n    Args:\n        filepath (str): The path to the file containing the PACE AOP data.\n        step (int, optional): The step size for subsampling the data. A smaller step size results in more\n            data points being plotted. Defaults to 20.\n        **kwargs (Any): Additional keyword arguments to pass to the `plot.scatter` method.\n\n    Returns:\n        plt.Figure: A matplotlib figure object containing the scatter plot.\n\n    Example:\n        &gt;&gt;&gt; plot = view_pace_pixel_locations(\"path/to/your/datafile.h5\", step=10)\n        &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    # Create a new figure\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Create the plot\n    dataset = read_pace_aop(filepath)\n    number_of_lines = dataset.sizes[\"number_of_lines\"]\n    pixels_per_line = dataset.sizes[\"pixels_per_line\"]\n\n    ax.scatter(\n        dataset.sel(\n            {\n                \"number_of_lines\": slice(None, None, number_of_lines // step),\n                \"pixels_per_line\": slice(None, None, pixels_per_line // step),\n            }\n        ).longitude,\n        dataset.sel(\n            {\n                \"number_of_lines\": slice(None, None, number_of_lines // step),\n                \"pixels_per_line\": slice(None, None, pixels_per_line // step),\n            }\n        ).latitude,\n        **kwargs,\n    )\n\n    # Set labels and title\n    ax.set_xlabel(\"Longitude\")\n    ax.set_ylabel(\"Latitude\")\n    ax.set_title(\"PACE Pixel Locations\")\n\n    return fig\n</code></pre>"},{"location":"pace/#hypercoast.pace.viz_pace","title":"<code>viz_pace(dataset, wavelengths=None, method='nearest', figsize=(6.4, 4.8), cmap='jet', vmin=0, vmax=0.02, ncols=1, crs=None, xlim=None, ylim=None, **kwargs)</code>","text":"<p>Plots PACE data from a given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>xr.Dataset</code> <p>An xarray Dataset containing the PACE data.</p> required <code>wavelengths</code> <code>array-like</code> <p>Specific wavelengths to select. If None, all wavelengths are selected.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method to use for selection when wavelengths is not None. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>figsize</code> <code>tuple</code> <p>Figure size. Defaults to (6.4, 4.8).</p> <code>(6.4, 4.8)</code> <code>cmap</code> <code>str</code> <p>Colormap to use. Defaults to \"jet\".</p> <code>'jet'</code> <code>vmin</code> <code>float</code> <p>Minimum value for the colormap. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>float</code> <p>Maximum value for the colormap. Defaults to 0.02.</p> <code>0.02</code> <code>ncols</code> <code>int</code> <p>Number of columns in the plot. Defaults to 1.</p> <code>1</code> <code>crs</code> <code>str or cartopy.crs.CRS</code> <p>Coordinate reference system to use. If None, a simple plot is created. Defaults to None. See https://scitools.org.uk/cartopy/docs/latest/reference/projections.html</p> <code>None</code> <code>xlim</code> <code>array-like</code> <p>Limits for the x-axis. Defaults to None.</p> <code>None</code> <code>ylim</code> <code>array-like</code> <p>Limits for the y-axis. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>plt.subplots</code> function.</p> <code>{}</code> Source code in <code>hypercoast/pace.py</code> <pre><code>def viz_pace(\n    dataset: Union[xr.Dataset, str],\n    wavelengths: Optional[Union[List[float], float]] = None,\n    method: str = \"nearest\",\n    figsize: Tuple[float, float] = (6.4, 4.8),\n    cmap: str = \"jet\",\n    vmin: float = 0,\n    vmax: float = 0.02,\n    ncols: int = 1,\n    crs: Optional[str] = None,\n    xlim: Optional[List[float]] = None,\n    ylim: Optional[List[float]] = None,\n    **kwargs,\n):\n    \"\"\"\n    Plots PACE data from a given xarray Dataset.\n\n    Args:\n        dataset (xr.Dataset): An xarray Dataset containing the PACE data.\n        wavelengths (array-like, optional): Specific wavelengths to select. If None, all wavelengths are selected.\n        method (str, optional): Method to use for selection when wavelengths is not None. Defaults to \"nearest\".\n        figsize (tuple, optional): Figure size. Defaults to (6.4, 4.8).\n        cmap (str, optional): Colormap to use. Defaults to \"jet\".\n        vmin (float, optional): Minimum value for the colormap. Defaults to 0.\n        vmax (float, optional): Maximum value for the colormap. Defaults to 0.02.\n        ncols (int, optional): Number of columns in the plot. Defaults to 1.\n        crs (str or cartopy.crs.CRS, optional): Coordinate reference system to use. If None, a simple plot is created. Defaults to None.\n            See https://scitools.org.uk/cartopy/docs/latest/reference/projections.html\n        xlim (array-like, optional): Limits for the x-axis. Defaults to None.\n        ylim (array-like, optional): Limits for the y-axis. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the `plt.subplots` function.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import math\n\n    if isinstance(dataset, str):\n        dataset = read_pace(dataset, wavelengths, method)\n\n    if wavelengths is not None:\n        if not isinstance(wavelengths, list):\n            wavelengths = [wavelengths]\n        dataset = dataset.sel(wavelength=wavelengths, method=method)\n    else:\n        wavelengths = dataset.coords[\"wavelength\"][0].values.tolist()\n\n    lat = dataset.coords[\"latitude\"]\n    lon = dataset.coords[\"longitude\"]\n\n    nrows = math.ceil(len(wavelengths) / ncols)\n\n    if crs is None:\n\n        fig, axes = plt.subplots(\n            nrows=nrows,\n            ncols=ncols,\n            figsize=(figsize[0] * ncols, figsize[1] * nrows),\n            **kwargs,\n        )\n\n        for i in range(nrows):\n            for j in range(ncols):\n                index = i * ncols + j\n                if index &lt; len(wavelengths):\n                    wavelength = wavelengths[index]\n                    data = dataset.sel(wavelength=wavelength, method=method)[\"Rrs\"]\n\n                    if min(nrows, ncols) == 1:\n                        ax = axes[index]\n                    else:\n                        ax = axes[i, j]\n                    im = ax.pcolormesh(\n                        lon, lat, np.squeeze(data), cmap=cmap, vmin=vmin, vmax=vmax\n                    )\n                    ax.set_xlabel(\"Longitude\")\n                    ax.set_ylabel(\"Latitude\")\n                    ax.set_title(\n                        f\"wavelength = {dataset.coords['wavelength'].values[index]} [nm]\"\n                    )\n                    fig.colorbar(im, ax=ax, label=\"Reflectance\")\n\n        plt.tight_layout()\n        plt.show()\n\n    else:\n\n        import cartopy\n        from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n\n        if crs == \"default\":\n            crs = cartopy.crs.PlateCarree()\n\n        if xlim is None:\n            xlim = [math.floor(lon.min()), math.ceil(lon.max())]\n\n        if ylim is None:\n            ylim = [math.floor(lat.min()), math.ceil(lat.max())]\n\n        fig, axes = plt.subplots(\n            nrows=nrows,\n            ncols=ncols,\n            figsize=(figsize[0] * ncols, figsize[1] * nrows),\n            subplot_kw={\"projection\": cartopy.crs.PlateCarree()},\n            **kwargs,\n        )\n\n        for i in range(nrows):\n            for j in range(ncols):\n                index = i * ncols + j\n                if index &lt; len(wavelengths):\n                    wavelength = wavelengths[index]\n                    data = dataset.sel(wavelength=wavelength, method=method)[\"Rrs\"]\n\n                    if min(nrows, ncols) == 1:\n                        ax = axes[index]\n                    else:\n                        ax = axes[i, j]\n                    im = ax.pcolormesh(lon, lat, data, cmap=\"jet\", vmin=0, vmax=0.02)\n                    ax.coastlines()\n                    ax.add_feature(cartopy.feature.STATES, linewidth=0.5)\n                    ax.set_xticks(np.linspace(xlim[0], xlim[1], 5), crs=crs)\n                    ax.set_yticks(np.linspace(ylim[0], ylim[1], 5), crs=crs)\n                    lon_formatter = LongitudeFormatter(zero_direction_label=True)\n                    lat_formatter = LatitudeFormatter()\n                    ax.xaxis.set_major_formatter(lon_formatter)\n                    ax.yaxis.set_major_formatter(lat_formatter)\n                    ax.set_xlabel(\"Longitude\")\n                    ax.set_ylabel(\"Latitude\")\n                    ax.set_title(\n                        f\"wavelength = {dataset.coords['wavelength'].values[index]} [nm]\"\n                    )\n                    plt.colorbar(im, label=\"Reflectance\")\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"pace/#hypercoast.pace.viz_pace_chla","title":"<code>viz_pace_chla(data, date=None, aspect=2, cmap='jet', size=6, **kwargs)</code>","text":"<p>Visualizes PACE chlorophyll-a data using an xarray DataArray.</p> <p>This function supports loading data from a file path (str) or directly using an xarray DataArray. It allows for selection of a specific date for visualization or averages over all dates if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, xr.DataArray]</code> <p>The chlorophyll-a data to visualize. Can be a file path or an xarray DataArray.</p> required <code>date</code> <code>Optional[str]</code> <p>Specific date to visualize. If None, averages over all dates. Defaults to None.</p> <code>None</code> <code>aspect</code> <code>float</code> <p>Aspect ratio of the plot. Defaults to 2.</p> <code>2</code> <code>cmap</code> <code>str</code> <p>Colormap for the plot. Defaults to \"jet\".</p> <code>'jet'</code> <code>size</code> <code>int</code> <p>Size of the plot. Defaults to 6.</p> <code>6</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to <code>xarray.plot</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>xr.plot.facetgrid.FacetGrid</code> <p>The plot generated from the chlorophyll-a data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>data</code> is not a file path (str) or an xarray DataArray.</p> Source code in <code>hypercoast/pace.py</code> <pre><code>def viz_pace_chla(\n    data: Union[str, xr.DataArray],\n    date: Optional[str] = None,\n    aspect: float = 2,\n    cmap: str = \"jet\",\n    size: int = 6,\n    **kwargs: Any,\n) -&gt; xr.plot.facetgrid.FacetGrid:\n    \"\"\"\n    Visualizes PACE chlorophyll-a data using an xarray DataArray.\n\n    This function supports loading data from a file path (str) or directly using an xarray DataArray.\n    It allows for selection of a specific date for visualization or averages over all dates if none is specified.\n\n    Args:\n        data (Union[str, xr.DataArray]): The chlorophyll-a data to visualize. Can be a file path or an xarray DataArray.\n        date (Optional[str], optional): Specific date to visualize. If None, averages over all dates. Defaults to None.\n        aspect (float, optional): Aspect ratio of the plot. Defaults to 2.\n        cmap (str, optional): Colormap for the plot. Defaults to \"jet\".\n        size (int, optional): Size of the plot. Defaults to 6.\n        **kwargs (Any): Additional keyword arguments to pass to `xarray.plot`.\n\n    Returns:\n        xr.plot.facetgrid.FacetGrid: The plot generated from the chlorophyll-a data.\n\n    Raises:\n        ValueError: If `data` is not a file path (str) or an xarray DataArray.\n    \"\"\"\n    if isinstance(data, str):\n        data = read_pace_chla(data)\n    elif not isinstance(data, xr.DataArray):\n        raise ValueError(\"data must be an xarray DataArray\")\n\n    if date is not None:\n        data = data.sel(date=date)\n    else:\n        if \"date\" in data.coords:\n            data = data.mean(dim=\"date\")\n\n    return data.plot(aspect=aspect, cmap=cmap, size=size, **kwargs)\n</code></pre>"},{"location":"ui/","title":"ui module","text":"<p>This module contains the user interface for the hypercoast package.</p>"},{"location":"ui/#hypercoast.ui.SpectralWidget","title":"<code> SpectralWidget            (HBox)         </code>","text":"<p>A widget for spectral data visualization on a map.</p> <p>Attributes:</p> Name Type Description <code>_host_map</code> <code>Map</code> <p>The map to host the widget.</p> <code>on_close</code> <code>function</code> <p>Function to be called when the widget is closed.</p> <code>_output_widget</code> <code>widgets.Output</code> <p>The output widget to display results.</p> <code>_output_control</code> <code>ipyleaflet.WidgetControl</code> <p>The control for the output widget.</p> <code>_on_map_interaction</code> <code>function</code> <p>Function to handle map interactions.</p> <code>_spectral_widget</code> <code>SpectralWidget</code> <p>The spectral widget itself.</p> <code>_spectral_control</code> <code>ipyleaflet.WidgetControl</code> <p>The control for the spectral widget.</p> Source code in <code>hypercoast/ui.py</code> <pre><code>class SpectralWidget(widgets.HBox):\n    \"\"\"\n    A widget for spectral data visualization on a map.\n\n    Attributes:\n        _host_map (Map): The map to host the widget.\n        on_close (function): Function to be called when the widget is closed.\n        _output_widget (widgets.Output): The output widget to display results.\n        _output_control (ipyleaflet.WidgetControl): The control for the output widget.\n        _on_map_interaction (function): Function to handle map interactions.\n        _spectral_widget (SpectralWidget): The spectral widget itself.\n        _spectral_control (ipyleaflet.WidgetControl): The control for the spectral widget.\n    \"\"\"\n\n    def __init__(\n        self, host_map, stack=True, position=\"topright\", xlim=None, ylim=None, **kwargs\n    ):\n        \"\"\"\n        Initializes a new instance of the SpectralWidget class.\n\n        Args:\n            host_map (Map): The map to host the widget.\n            stack (bool, optional): Whether to stack the plots. Defaults to True.\n            position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n            xlim (tuple, optional): The x-axis limits. Defaults to None.\n            ylim (tuple, optional): The y-axis limits. Defaults to None.\n        \"\"\"\n        self._host_map = host_map\n        self.on_close = None\n        self._stack = stack\n        self._show_plot = False\n\n        fig_margin = {\"top\": 20, \"bottom\": 35, \"left\": 50, \"right\": 20}\n        fig = plt.figure(\n            # title=None,\n            fig_margin=fig_margin,\n            layout={\"width\": \"500px\", \"height\": \"300px\"},\n        )\n\n        self._fig = fig\n        self._host_map._fig = fig\n\n        layer_names = list(host_map.cog_layer_dict.keys())\n        layers_widget = widgets.Dropdown(options=layer_names)\n        layers_widget.layout.width = \"18ex\"\n\n        close_btn = widgets.Button(\n            icon=\"times\",\n            tooltip=\"Close the widget\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        reset_btn = widgets.Button(\n            icon=\"trash\",\n            tooltip=\"Remove all markers\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        settings_btn = widgets.Button(\n            icon=\"gear\",\n            tooltip=\"Change layer settings\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        stack_btn = widgets.ToggleButton(\n            value=stack,\n            icon=\"area-chart\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        def settings_btn_click(_):\n            self._host_map._add_layer_editor(\n                position=\"topright\",\n                layer_dict=self._host_map.cog_layer_dict[layers_widget.value],\n            )\n\n        settings_btn.on_click(settings_btn_click)\n\n        def reset_btn_click(_):\n            if hasattr(self._host_map, \"_plot_marker_cluster\"):\n                self._host_map._plot_marker_cluster.markers = []\n                self._host_map._plot_markers = []\n\n            if hasattr(self._host_map, \"_spectral_data\"):\n                self._host_map._spectral_data = {}\n\n            self._output_widget.clear_output()\n            self._show_plot = False\n            plt.clear()\n\n        reset_btn.on_click(reset_btn_click)\n\n        save_btn = widgets.Button(\n            icon=\"floppy-o\",\n            tooltip=\"Save the data to a CSV\",\n            button_style=\"primary\",\n            layout=widgets.Layout(width=\"32px\"),\n        )\n\n        def chooser_callback(chooser):\n            if chooser.selected:\n                file_path = chooser.selected\n                self._host_map.spectral_to_csv(file_path)\n                if (\n                    hasattr(self._host_map, \"_file_chooser_control\")\n                    and self._host_map._file_chooser_control in self._host_map.controls\n                ):\n                    self._host_map.remove_control(self._host_map._file_chooser_control)\n                    self._host_map._file_chooser.close()\n\n        def save_btn_click(_):\n            if not hasattr(self._host_map, \"_spectral_data\"):\n                return\n\n            self._output_widget.clear_output()\n            file_chooser = FileChooser(\n                os.getcwd(), layout=widgets.Layout(width=\"454px\")\n            )\n            file_chooser.filter_pattern = \"*.csv\"\n            file_chooser.use_dir_icons = True\n            file_chooser.title = \"Save spectral data to a CSV file\"\n            file_chooser.default_filename = \"spectral_data.csv\"\n            file_chooser.show_hidden = False\n            file_chooser.register_callback(chooser_callback)\n            file_chooser_control = ipyleaflet.WidgetControl(\n                widget=file_chooser, position=\"topright\"\n            )\n            self._host_map.add(file_chooser_control)\n            setattr(self._host_map, \"_file_chooser\", file_chooser)\n            setattr(self._host_map, \"_file_chooser_control\", file_chooser_control)\n\n        save_btn.on_click(save_btn_click)\n\n        def close_widget(_):\n            self.cleanup()\n\n        close_btn.on_click(close_widget)\n\n        super().__init__(\n            [layers_widget, settings_btn, stack_btn, reset_btn, save_btn, close_btn]\n        )\n\n        output = widgets.Output()\n        output_control = ipyleaflet.WidgetControl(widget=output, position=\"bottomright\")\n        self._output_widget = output\n        self._output_control = output_control\n        self._host_map.add(output_control)\n\n        if not hasattr(self._host_map, \"_spectral_data\"):\n            self._host_map._spectral_data = {}\n\n        def handle_interaction(**kwargs):\n\n            latlon = kwargs.get(\"coordinates\")\n            lat = latlon[0]\n            lon = latlon[1]\n            if kwargs.get(\"type\") == \"click\" and self._host_map._layer_editor is None:\n                layer_name = layers_widget.value\n\n                if not hasattr(self._host_map, \"_plot_markers\"):\n                    self._host_map._plot_markers = []\n                markers = self._host_map._plot_markers\n                marker_cluster = self._host_map._plot_marker_cluster\n                markers.append(ipyleaflet.Marker(location=latlon, draggable=False))\n                marker_cluster.markers = markers\n                self._host_map._plot_marker_cluster = marker_cluster\n\n                xlabel = \"Wavelength (nm)\"\n                ylabel = \"Reflectance\"\n\n                ds = self._host_map.cog_layer_dict[layer_name][\"xds\"]\n\n                if self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"XARRAY\":\n                    da = extract_spectral(ds, lat, lon)\n                    xlabel = \"Band\"\n                    ylabel = \"Value\"\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"EMIT\":\n                    da = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")[\n                        \"reflectance\"\n                    ]\n\n                    if \"wavelength\" not in self._host_map._spectral_data:\n                        self._host_map._spectral_data[\"wavelength\"] = ds[\n                            \"wavelength\"\n                        ].values\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"PACE\":\n                    try:\n                        da = extract_pace(ds, lat, lon)\n                    except:\n                        da = xr.DataArray(\n                            np.full(len(ds[\"wavelength\"]), np.nan),\n                            dims=[\"wavelength\"],\n                            coords={\"wavelength\": ds[\"wavelength\"]},\n                        )\n                    if \"wavelengths\" not in self._host_map._spectral_data:\n                        self._host_map._spectral_data[\"wavelengths\"] = ds[\n                            \"wavelength\"\n                        ].values\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"DESIS\":\n                    da = extract_desis(ds, lat, lon)\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"NEON\":\n                    da = extract_neon(ds, lat, lon)\n\n                elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"AVIRIS\":\n                    da = extract_aviris(ds, lat, lon)\n\n                self._host_map._spectral_data[f\"({lat:.4f} {lon:.4f})\"] = da.values\n\n                if self._host_map.cog_layer_dict[layer_name][\"hyper\"] != \"XARRAY\":\n                    da[da &lt; 0] = np.nan\n                    x_axis_options = {\"label_offset\": \"30px\"}\n                else:\n                    x_axis_options = {\n                        \"label_offset\": \"30px\",\n                        \"tick_format\": \"0d\",\n                        \"num_ticks\": da.sizes[\"band\"],\n                    }\n                axes_options = {\n                    \"x\": x_axis_options,\n                    \"y\": {\"label_offset\": \"35px\"},\n                }\n\n                if not stack_btn.value:\n                    plt.clear()\n                    plt.plot(\n                        da.coords[da.dims[0]].values,\n                        da.values,\n                        axes_options=axes_options,\n                    )\n                else:\n                    color = np.random.rand(\n                        3,\n                    )\n                    plt.plot(\n                        da.coords[da.dims[0]].values,\n                        da.values,\n                        color=color,\n                        axes_options=axes_options,\n                    )\n                    try:\n                        if isinstance(self._fig.axes[0], bqplot.ColorAxis):\n                            self._fig.axes = self._fig.axes[1:]\n                        elif isinstance(self._fig.axes[-1], bqplot.ColorAxis):\n                            self._fig.axes = self._fig.axes[:-1]\n                    except Exception:\n                        pass\n\n                plt.xlabel(xlabel)\n                plt.ylabel(ylabel)\n                if xlim:\n                    plt.xlim(xlim[0], xlim[1])\n                if ylim:\n                    plt.ylim(ylim[0], ylim[1])\n\n                if not self._show_plot:\n                    with self._output_widget:\n                        plt.show()\n                        self._show_plot = True\n\n                self._host_map.default_style = {\"cursor\": \"crosshair\"}\n\n        self._host_map.on_interaction(handle_interaction)\n        self._on_map_interaction = handle_interaction\n\n        self._spectral_widget = self\n        self._spectral_control = ipyleaflet.WidgetControl(\n            widget=self, position=position\n        )\n        self._host_map.add(self._spectral_control)\n\n    def cleanup(self):\n        \"\"\"Removes the widget from the map and performs cleanup.\"\"\"\n        if self._host_map:\n            self._host_map.default_style = {\"cursor\": \"default\"}\n            self._host_map.on_interaction(self._on_map_interaction, remove=True)\n\n            if self._output_control:\n                self._host_map.remove_control(self._output_control)\n\n                if self._output_widget:\n                    self._output_widget.close()\n                    self._output_widget = None\n\n            if self._spectral_control:\n                self._host_map.remove_control(self._spectral_control)\n                self._spectral_control = None\n\n                if self._spectral_widget:\n                    self._spectral_widget.close()\n                    self._spectral_widget = None\n\n            if hasattr(self._host_map, \"_plot_marker_cluster\"):\n                self._host_map._plot_marker_cluster.markers = []\n                self._host_map._plot_markers = []\n\n            if hasattr(self._host_map, \"_spectral_data\"):\n                self._host_map._spectral_data = {}\n\n            if hasattr(self, \"_output_widget\") and self._output_widget is not None:\n                self._output_widget.clear_output()\n\n        if self.on_close is not None:\n            self.on_close()\n</code></pre>"},{"location":"ui/#hypercoast.ui.SpectralWidget.__init__","title":"<code>__init__(self, host_map, stack=True, position='topright', xlim=None, ylim=None, **kwargs)</code>  <code>special</code>","text":"<p>Initializes a new instance of the SpectralWidget class.</p> <p>Parameters:</p> Name Type Description Default <code>host_map</code> <code>Map</code> <p>The map to host the widget.</p> required <code>stack</code> <code>bool</code> <p>Whether to stack the plots. Defaults to True.</p> <code>True</code> <code>position</code> <code>str</code> <p>The position of the widget on the map. Defaults to \"topright\".</p> <code>'topright'</code> <code>xlim</code> <code>tuple</code> <p>The x-axis limits. Defaults to None.</p> <code>None</code> <code>ylim</code> <code>tuple</code> <p>The y-axis limits. Defaults to None.</p> <code>None</code> Source code in <code>hypercoast/ui.py</code> <pre><code>def __init__(\n    self, host_map, stack=True, position=\"topright\", xlim=None, ylim=None, **kwargs\n):\n    \"\"\"\n    Initializes a new instance of the SpectralWidget class.\n\n    Args:\n        host_map (Map): The map to host the widget.\n        stack (bool, optional): Whether to stack the plots. Defaults to True.\n        position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n        xlim (tuple, optional): The x-axis limits. Defaults to None.\n        ylim (tuple, optional): The y-axis limits. Defaults to None.\n    \"\"\"\n    self._host_map = host_map\n    self.on_close = None\n    self._stack = stack\n    self._show_plot = False\n\n    fig_margin = {\"top\": 20, \"bottom\": 35, \"left\": 50, \"right\": 20}\n    fig = plt.figure(\n        # title=None,\n        fig_margin=fig_margin,\n        layout={\"width\": \"500px\", \"height\": \"300px\"},\n    )\n\n    self._fig = fig\n    self._host_map._fig = fig\n\n    layer_names = list(host_map.cog_layer_dict.keys())\n    layers_widget = widgets.Dropdown(options=layer_names)\n    layers_widget.layout.width = \"18ex\"\n\n    close_btn = widgets.Button(\n        icon=\"times\",\n        tooltip=\"Close the widget\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    reset_btn = widgets.Button(\n        icon=\"trash\",\n        tooltip=\"Remove all markers\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    settings_btn = widgets.Button(\n        icon=\"gear\",\n        tooltip=\"Change layer settings\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    stack_btn = widgets.ToggleButton(\n        value=stack,\n        icon=\"area-chart\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    def settings_btn_click(_):\n        self._host_map._add_layer_editor(\n            position=\"topright\",\n            layer_dict=self._host_map.cog_layer_dict[layers_widget.value],\n        )\n\n    settings_btn.on_click(settings_btn_click)\n\n    def reset_btn_click(_):\n        if hasattr(self._host_map, \"_plot_marker_cluster\"):\n            self._host_map._plot_marker_cluster.markers = []\n            self._host_map._plot_markers = []\n\n        if hasattr(self._host_map, \"_spectral_data\"):\n            self._host_map._spectral_data = {}\n\n        self._output_widget.clear_output()\n        self._show_plot = False\n        plt.clear()\n\n    reset_btn.on_click(reset_btn_click)\n\n    save_btn = widgets.Button(\n        icon=\"floppy-o\",\n        tooltip=\"Save the data to a CSV\",\n        button_style=\"primary\",\n        layout=widgets.Layout(width=\"32px\"),\n    )\n\n    def chooser_callback(chooser):\n        if chooser.selected:\n            file_path = chooser.selected\n            self._host_map.spectral_to_csv(file_path)\n            if (\n                hasattr(self._host_map, \"_file_chooser_control\")\n                and self._host_map._file_chooser_control in self._host_map.controls\n            ):\n                self._host_map.remove_control(self._host_map._file_chooser_control)\n                self._host_map._file_chooser.close()\n\n    def save_btn_click(_):\n        if not hasattr(self._host_map, \"_spectral_data\"):\n            return\n\n        self._output_widget.clear_output()\n        file_chooser = FileChooser(\n            os.getcwd(), layout=widgets.Layout(width=\"454px\")\n        )\n        file_chooser.filter_pattern = \"*.csv\"\n        file_chooser.use_dir_icons = True\n        file_chooser.title = \"Save spectral data to a CSV file\"\n        file_chooser.default_filename = \"spectral_data.csv\"\n        file_chooser.show_hidden = False\n        file_chooser.register_callback(chooser_callback)\n        file_chooser_control = ipyleaflet.WidgetControl(\n            widget=file_chooser, position=\"topright\"\n        )\n        self._host_map.add(file_chooser_control)\n        setattr(self._host_map, \"_file_chooser\", file_chooser)\n        setattr(self._host_map, \"_file_chooser_control\", file_chooser_control)\n\n    save_btn.on_click(save_btn_click)\n\n    def close_widget(_):\n        self.cleanup()\n\n    close_btn.on_click(close_widget)\n\n    super().__init__(\n        [layers_widget, settings_btn, stack_btn, reset_btn, save_btn, close_btn]\n    )\n\n    output = widgets.Output()\n    output_control = ipyleaflet.WidgetControl(widget=output, position=\"bottomright\")\n    self._output_widget = output\n    self._output_control = output_control\n    self._host_map.add(output_control)\n\n    if not hasattr(self._host_map, \"_spectral_data\"):\n        self._host_map._spectral_data = {}\n\n    def handle_interaction(**kwargs):\n\n        latlon = kwargs.get(\"coordinates\")\n        lat = latlon[0]\n        lon = latlon[1]\n        if kwargs.get(\"type\") == \"click\" and self._host_map._layer_editor is None:\n            layer_name = layers_widget.value\n\n            if not hasattr(self._host_map, \"_plot_markers\"):\n                self._host_map._plot_markers = []\n            markers = self._host_map._plot_markers\n            marker_cluster = self._host_map._plot_marker_cluster\n            markers.append(ipyleaflet.Marker(location=latlon, draggable=False))\n            marker_cluster.markers = markers\n            self._host_map._plot_marker_cluster = marker_cluster\n\n            xlabel = \"Wavelength (nm)\"\n            ylabel = \"Reflectance\"\n\n            ds = self._host_map.cog_layer_dict[layer_name][\"xds\"]\n\n            if self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"XARRAY\":\n                da = extract_spectral(ds, lat, lon)\n                xlabel = \"Band\"\n                ylabel = \"Value\"\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"EMIT\":\n                da = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")[\n                    \"reflectance\"\n                ]\n\n                if \"wavelength\" not in self._host_map._spectral_data:\n                    self._host_map._spectral_data[\"wavelength\"] = ds[\n                        \"wavelength\"\n                    ].values\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"PACE\":\n                try:\n                    da = extract_pace(ds, lat, lon)\n                except:\n                    da = xr.DataArray(\n                        np.full(len(ds[\"wavelength\"]), np.nan),\n                        dims=[\"wavelength\"],\n                        coords={\"wavelength\": ds[\"wavelength\"]},\n                    )\n                if \"wavelengths\" not in self._host_map._spectral_data:\n                    self._host_map._spectral_data[\"wavelengths\"] = ds[\n                        \"wavelength\"\n                    ].values\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"DESIS\":\n                da = extract_desis(ds, lat, lon)\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"NEON\":\n                da = extract_neon(ds, lat, lon)\n\n            elif self._host_map.cog_layer_dict[layer_name][\"hyper\"] == \"AVIRIS\":\n                da = extract_aviris(ds, lat, lon)\n\n            self._host_map._spectral_data[f\"({lat:.4f} {lon:.4f})\"] = da.values\n\n            if self._host_map.cog_layer_dict[layer_name][\"hyper\"] != \"XARRAY\":\n                da[da &lt; 0] = np.nan\n                x_axis_options = {\"label_offset\": \"30px\"}\n            else:\n                x_axis_options = {\n                    \"label_offset\": \"30px\",\n                    \"tick_format\": \"0d\",\n                    \"num_ticks\": da.sizes[\"band\"],\n                }\n            axes_options = {\n                \"x\": x_axis_options,\n                \"y\": {\"label_offset\": \"35px\"},\n            }\n\n            if not stack_btn.value:\n                plt.clear()\n                plt.plot(\n                    da.coords[da.dims[0]].values,\n                    da.values,\n                    axes_options=axes_options,\n                )\n            else:\n                color = np.random.rand(\n                    3,\n                )\n                plt.plot(\n                    da.coords[da.dims[0]].values,\n                    da.values,\n                    color=color,\n                    axes_options=axes_options,\n                )\n                try:\n                    if isinstance(self._fig.axes[0], bqplot.ColorAxis):\n                        self._fig.axes = self._fig.axes[1:]\n                    elif isinstance(self._fig.axes[-1], bqplot.ColorAxis):\n                        self._fig.axes = self._fig.axes[:-1]\n                except Exception:\n                    pass\n\n            plt.xlabel(xlabel)\n            plt.ylabel(ylabel)\n            if xlim:\n                plt.xlim(xlim[0], xlim[1])\n            if ylim:\n                plt.ylim(ylim[0], ylim[1])\n\n            if not self._show_plot:\n                with self._output_widget:\n                    plt.show()\n                    self._show_plot = True\n\n            self._host_map.default_style = {\"cursor\": \"crosshair\"}\n\n    self._host_map.on_interaction(handle_interaction)\n    self._on_map_interaction = handle_interaction\n\n    self._spectral_widget = self\n    self._spectral_control = ipyleaflet.WidgetControl(\n        widget=self, position=position\n    )\n    self._host_map.add(self._spectral_control)\n</code></pre>"},{"location":"ui/#hypercoast.ui.SpectralWidget.cleanup","title":"<code>cleanup(self)</code>","text":"<p>Removes the widget from the map and performs cleanup.</p> Source code in <code>hypercoast/ui.py</code> <pre><code>def cleanup(self):\n    \"\"\"Removes the widget from the map and performs cleanup.\"\"\"\n    if self._host_map:\n        self._host_map.default_style = {\"cursor\": \"default\"}\n        self._host_map.on_interaction(self._on_map_interaction, remove=True)\n\n        if self._output_control:\n            self._host_map.remove_control(self._output_control)\n\n            if self._output_widget:\n                self._output_widget.close()\n                self._output_widget = None\n\n        if self._spectral_control:\n            self._host_map.remove_control(self._spectral_control)\n            self._spectral_control = None\n\n            if self._spectral_widget:\n                self._spectral_widget.close()\n                self._spectral_widget = None\n\n        if hasattr(self._host_map, \"_plot_marker_cluster\"):\n            self._host_map._plot_marker_cluster.markers = []\n            self._host_map._plot_markers = []\n\n        if hasattr(self._host_map, \"_spectral_data\"):\n            self._host_map._spectral_data = {}\n\n        if hasattr(self, \"_output_widget\") and self._output_widget is not None:\n            self._output_widget.clear_output()\n\n    if self.on_close is not None:\n        self.on_close()\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#import-library","title":"Import library","text":"<p>To use HyperCoast in a project:</p> <pre><code>import hypercoast\n</code></pre>"},{"location":"usage/#search-for-datasets","title":"Search for datasets","text":"<p>To download and access NASA hyperspectral data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, run the following code to log in:</p> <pre><code>hypercoast.nasa_earth_login()\n</code></pre> <p>Collections on NASA Earthdata are discovered with the search_datasets function, which accepts an instrument filter as an easy way to get started. Each of the items in the list of collections returned has a \"short-name\". For example, to search for all datasets with the instrument \"oci\":</p> <pre><code>results = hypercoast.search_datasets(instrument=\"oci\")\ndatasets = set()\nfor item in results:\n    summary = item.summary()\n    short_name = summary[\"short-name\"]\n    if short_name not in datasets:\n        print(short_name)\n    datasets.add(short_name)\nprint(f\"\\nFound {len(datasets)} unique datasets\")\n</code></pre>"},{"location":"usage/#search-for-data-by-short-name","title":"Search for data by short name","text":"<p>Next, we use the <code>search_nasa_data</code> function to find granules within a collection. Let's use the short_name for the PACE/OCI Level-2 data product for bio-optical and biogeochemical properties.</p> <pre><code>results = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    count=1,\n)\n</code></pre> <p>We can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the temporal parameter to request a date range and the bounding_box parameter to request granules that intersect with a bounding box. We can even provide a cloud_cover threshold to limit files that have a lower percentage of cloud cover. We do not provide a count, so we'll get all granules that satisfy the constraints.</p> <pre><code>tspan = (\"2024-04-01\", \"2024-04-16\")\nbbox = (-76.75, 36.97, -75.74, 39.01)\nclouds = (0, 50)\n\nresults, gdf = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    temporal=tspan,\n    bounding_box=bbox,\n    cloud_cover=clouds,\n    return_gdf=True,\n)\n</code></pre> <p>Display the footprints of the granules that match the search criteria.</p> <pre><code>gdf.explore()\n</code></pre> <p>We can also download all the results with one command.</p> <pre><code>hypercoast.download_nasa_data(results, out_dir=\"data\")\n</code></pre>"},{"location":"usage/#search-for-pace-data","title":"Search for PACE data","text":"<p>To search for PACE data, we can use the <code>search_pace</code> function:</p> <pre><code>results, gdf = hypercoast.search_pace(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-05-10\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</code></pre> <p>To download the PACE data, we can use the <code>download_pace</code> function:</p> <pre><code>hypercoast.download_pace(results, out_dir=\"data\")\n</code></pre>"},{"location":"usage/#search-for-emit-data","title":"Search for EMIT data","text":"<p>To search for EMIT data, we can use the <code>search_emit</code> function:</p> <pre><code>results, gdf = hypercoast.search_emit(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-04-01\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</code></pre> <p>To download the EMIT data, we can use the <code>download_emit</code> function:</p> <pre><code>hypercoast.download_emit(results, out_dir=\"data\")\n</code></pre>"},{"location":"usage/#visualize-pace-data","title":"Visualize PACE data","text":"<p>Load the dataset as a <code>xarray.Dataset</code> object:</p> <pre><code>dataset = hypercoast.read_pace(filepath)\n</code></pre> <p>Visualize selected bands of the dataset:</p> <pre><code>hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2, crs=\"default\")\n</code></pre> <p>Visualize the dataset on an interactive map:</p> <pre><code>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450]\nm.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\")\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\")\nm.add(\"spectral\")\nm\n</code></pre>"},{"location":"usage/#visualize-emit-data","title":"Visualize EMIT data","text":"<p>To visualize EMIT data, we can use the <code>read_emit</code> function:</p> <pre><code>dataset = hypercoast.read_emit(filepath)\n</code></pre> <p>Visualize the dataset on an interactive map:</p> <pre><code>m = hypercoast.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\")\nm.add(\"spectral\")\nm\n</code></pre>"},{"location":"usage/#create-an-image-cube","title":"Create an image cube","text":"<p>First , load the dataset as a <code>xarray.Dataset</code> object. Select a region of interest (ROI) using the <code>sel</code> method:</p> <pre><code>dataset = hypercoast.read_emit(filepath)\nds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451))\n</code></pre> <p>Create an image cube using the <code>image_cube</code> function:</p> <pre><code>cube = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.4),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n)\ncube.show()\n</code></pre>"},{"location":"usage/#interactive-slicing-and-thresholding","title":"Interactive slicing and thresholding","text":"<p>First , load the dataset as a <code>xarray.Dataset</code> object. Select a region of interest (ROI) using the <code>sel</code> method:</p> <pre><code>dataset = hypercoast.read_emit(filepath)\nds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93))\n</code></pre> <p>Interactive slicing along the z-axis (band):</p> <pre><code>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\np.show()\n</code></pre> <p>Interactive thresholding:</p> <pre><code>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\np.show()\n</code></pre>"},{"location":"usage/#visualizing-pace-chlorophyll-a-concentration-data","title":"Visualizing PACE chlorophyll-a concentration data","text":"<p>Load all the data files in a directory as an <code>xarray.DataArray</code>:</p> <pre><code>files = \"data/*nc\"\narray = hypercoast.read_pace_chla(files)\n</code></pre> <p>Select a date and visualize the chlorophyll-a concentration data with Matplotlib.</p> <pre><code>hypercoast.viz_pace_chla(array, date=\"2024-06-01\", cmap=\"jet\", size=6)\n</code></pre> <p>If the date is not specified, the data are averaged over the entire time range.</p> <pre><code>hypercoast.viz_pace_chla(array, cmap=\"jet\", size=6)\n</code></pre> <p>Convert the data array to an image that can be displayed on an interactive map.</p> <pre><code>single_image = hypercoast.pace_chla_to_image(single_array)\n</code></pre> <p>Create an interactive map and display the image on the map.</p> <pre><code>m = hypercoast.Map(center=[40, -100], zoom=4)\nm.add_basemap(\"Hybrid\")\nm.add_raster(\n    single_image,\n    cmap=\"jet\",\n    vmin=-1,\n    vmax=2,\n    layer_name=\"Chlorophyll a\",\n    zoom_to_layer=False,\n)\nlabel = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\"\nm.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label)\nm\n</code></pre>"},{"location":"examples/acolite/","title":"Acolite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport hypercoast\n</pre> import os import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/S2A_MSIL1C_20160920T164452_N0204_R083_T15RYN_20160920T164450.SAFE.zip\"\nwork_dir = os.path.expanduser(\"~/Downloads\")\nfilepath = os.path.join(work_dir, os.path.basename(url))\nhypercoast.download_file(url, filepath, quiet=True)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/S2A_MSIL1C_20160920T164452_N0204_R083_T15RYN_20160920T164450.SAFE.zip\" work_dir = os.path.expanduser(\"~/Downloads\") filepath = os.path.join(work_dir, os.path.basename(url)) hypercoast.download_file(url, filepath, quiet=True) In\u00a0[\u00a0]: Copied! <pre>input_dir = filepath.replace(\".zip\", \"\")\nif not os.path.exists(input_dir):\n    raise FileNotFoundError(f\"Directory {input_dir} not found\")\n</pre> input_dir = filepath.replace(\".zip\", \"\") if not os.path.exists(input_dir):     raise FileNotFoundError(f\"Directory {input_dir} not found\") In\u00a0[\u00a0]: Copied! <pre>acolite_dir = hypercoast.download_acolite(work_dir)\nprint(f\"Acolite directory: {acolite_dir}\")\n</pre> acolite_dir = hypercoast.download_acolite(work_dir) print(f\"Acolite directory: {acolite_dir}\") In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(work_dir, \"output\")\n</pre> out_dir = os.path.join(work_dir, \"output\") In\u00a0[\u00a0]: Copied! <pre>hypercoast.run_acolite(\n    acolite_dir=acolite_dir,\n    input_file=input_dir,\n    out_dir=out_dir,\n    l2w_parameters=\"Rrs_*,chl_oc3,chl_re_mishra,spm_nechad2016\",\n    rgb_rhot=True,\n    rgb_rhos=False,\n    map_l2w=True,\n)\n</pre> hypercoast.run_acolite(     acolite_dir=acolite_dir,     input_file=input_dir,     out_dir=out_dir,     l2w_parameters=\"Rrs_*,chl_oc3,chl_re_mishra,spm_nechad2016\",     rgb_rhot=True,     rgb_rhos=False,     map_l2w=True, ) In\u00a0[\u00a0]: Copied! <pre>input_dir = os.path.join(work_dir, \"data\")\ninput_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir)]\ninput_files\n</pre> input_dir = os.path.join(work_dir, \"data\") input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir)] input_files <p>Run the following code to process all images in the <code>data</code> folder.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.run_acolite(\n    acolite_dir=acolite_dir,\n    input_file=input_files,\n    out_dir=out_dir,\n    l2w_parameters=\"Rrs_*,chl_oc3,chl_re_mishra,spm_nechad2016\",\n    rgb_rhot=True,\n    rgb_rhos=False,\n    map_l2w=True,\n)\n</pre> hypercoast.run_acolite(     acolite_dir=acolite_dir,     input_file=input_files,     out_dir=out_dir,     l2w_parameters=\"Rrs_*,chl_oc3,chl_re_mishra,spm_nechad2016\",     rgb_rhot=True,     rgb_rhos=False,     map_l2w=True, )"},{"location":"examples/acolite/#atmospheric-correction-with-acolite","title":"Atmospheric Correction with Acolite\u00b6","text":"<p>Acolite can perform atmospheric correction on a variety of satellite sensors, including Landsat, Sentinel-2, PACE, EMIT, AVIRIS, among others. For more information on how to use Acolite, please refer to the Acolite manual</p> <p>In this example, we will use Acolite to perform atmospheric correction on a Sentinel-2 image.</p>"},{"location":"examples/acolite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/acolite/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/acolite/#download-acolite-software","title":"Download Acolite software\u00b6","text":""},{"location":"examples/acolite/#run-acolite","title":"Run Acolite\u00b6","text":""},{"location":"examples/acolite/#batch-processing","title":"Batch processing\u00b6","text":"<p>To process multiple images, put all the images in a folder. For example, unzip all the images in the <code>data</code> folder. Then, run the following code to make sure that all image folders are listed.</p>"},{"location":"examples/aviris/","title":"Aviris","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Download sample dataset from Access the AVIRIS-NG L2 Surface Reflectance product page via ORNL DAAC https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1988</p> <p></p> <p>Download <code>ang20210401t150456_rfl_v2z1.zip</code> and unzip it.</p> <p></p> <p>The dataset contains 2 files: <code>ang20210401t150456_rfl_v2z1</code> and <code>ang20210401t150456_rfl_v2z1.hdr</code>. We will use the <code>ang20210401t150456_rfl_v2z1</code> file in this notebook.</p> In\u00a0[\u00a0]: Copied! <pre>filepath = \"ang20210401t150456_rfl_v2z1\"\n</pre> filepath = \"ang20210401t150456_rfl_v2z1\" <p>Read the AVIRIS data as an <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>ds = hypercoast.read_aviris(filepath)\nds\n</pre> ds = hypercoast.read_aviris(filepath) ds <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm\n</pre> m = hypercoast.Map() m <p>Add the AVIRIS data to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_aviris(ds, wavelengths=[1000, 700, 400], vmin=0, vmax=0.2)\nm.add(\"spectral\")\n</pre> m.add_aviris(ds, wavelengths=[1000, 700, 400], vmin=0, vmax=0.2) m.add(\"spectral\") <p></p>"},{"location":"examples/aviris/#visualizing-aviris-data-interactively-with-hypercoast","title":"Visualizing AVIRIS data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize AVIRIS hyperspectral data interactively with HyperCoast. For more information about AVIRIS, please visit the links below:</p> <ul> <li>https://aviris.jpl.nasa.gov/</li> <li>https://aviris.jpl.nasa.gov/dataportal/</li> <li>https://popo.jpl.nasa.gov/mmgis-aviris/?s=ujooa</li> <li>https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1988</li> <li>https://github.com/ornldaac/deltax_workshop_2022/tree/main</li> <li>https://github.com/jjmcnelis/aviris-ng-notebooks/tree/master</li> </ul>"},{"location":"examples/chlorophyll_a/","title":"Chlorophyll a","text":"In\u00a0[1]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[2]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>To download PACE data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, you can uncomment the code below to search and download the data.</p> In\u00a0[3]: Copied! <pre># hypercoast.nasa_earth_login()\n# temporal = (\"2024-06-01\", \"2024-07-01\")\n# results= hypercoast.search_pace_chla(temporal=temporal)\n# hypercoast.download_nasa_data(results, \"chla\")\n</pre> # hypercoast.nasa_earth_login() # temporal = (\"2024-06-01\", \"2024-07-01\") # results= hypercoast.search_pace_chla(temporal=temporal) # hypercoast.download_nasa_data(results, \"chla\") <p>Alternatively, you can download some sample data from here.</p> In\u00a0[4]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_chla.zip\"\nhypercoast.download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_chla.zip\" hypercoast.download_file(url) Out[4]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/pace_chla.zip'</pre> <p>The downloaded zip file is automatically extracted and saved in the <code>chla</code> directory, which contains 17 daily files of chlorophyll-a concentration data in the netCDF format. The date range of the data is from 2024-06-01 to 2024-06-17.</p> In\u00a0[5]: Copied! <pre>files = \"chla/*nc\"\n</pre> files = \"chla/*nc\" <p>Load all the data files in the <code>chla</code> directory as an xarray DataArray</p> In\u00a0[6]: Copied! <pre>array = hypercoast.read_pace_chla(files)\narray\n</pre> array = hypercoast.read_pace_chla(files) array Out[6]: <pre>&lt;xarray.DataArray 'chlor_a' (lat: 1800, lon: 3600, date: 17)&gt; Size: 441MB\ndask.array&lt;transpose, shape=(1800, 3600, 17), dtype=float32, chunksize=(512, 1024, 1), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat          (lat) float32 7kB 89.95 89.85 89.75 ... -89.75 -89.85 -89.95\n  * lon          (lon) float32 14kB -179.9 -179.9 -179.8 ... 179.8 179.9 180.0\n  * date         (date) &lt;U10 680B '2024-06-01' '2024-06-02' ... '2024-06-17'\n    spatial_ref  int64 8B 0\nAttributes:\n    long_name:      Chlorophyll Concentration, OCI Algorithm\n    units:          lg(mg m^-3)\n    standard_name:  mass_concentration_of_chlorophyll_in_sea_water\n    valid_min:      0.001\n    valid_max:      100.0\n    reference:      Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a alg...\n    display_scale:  log\n    display_min:    0.01\n    display_max:    20.0\n    date:           ['2024-06-01', '2024-06-02', '2024-06-03', '2024-06-04', ...</pre>xarray.DataArray'chlor_a'<ul><li>lat: 1800</li><li>lon: 3600</li><li>date: 17</li></ul><ul><li>dask.array&lt;chunksize=(512, 1024, 1), meta=np.ndarray&gt;  Array   Chunk   Bytes   420.23 MiB   2.00 MiB   Shape   (1800, 3600, 17)   (512, 1024, 1)   Dask graph   272 chunks in 54 graph layers   Data type   float32 numpy.ndarray  17 3600 1800 </li><li>Coordinates: (4)<ul><li>lat(lat)float3289.95 89.85 89.75 ... -89.85 -89.95long_name :Latitudeunits :degrees_northstandard_name :latitudevalid_min :-90.0valid_max :90.0<pre>array([ 89.95    ,  89.85    ,  89.75    , ..., -89.75    , -89.850006,\n       -89.950005], dtype=float32)</pre></li><li>lon(lon)float32-179.9 -179.9 ... 179.9 180.0long_name :Longitudeunits :degrees_eaststandard_name :longitudevalid_min :-180.0valid_max :180.0<pre>array([-179.95   , -179.85   , -179.75   , ...,  179.75   ,  179.85   ,\n        179.95001], dtype=float32)</pre></li><li>date(date)&lt;U10'2024-06-01' ... '2024-06-17'<pre>array(['2024-06-01', '2024-06-02', '2024-06-03', '2024-06-04', '2024-06-05',\n       '2024-06-06', '2024-06-07', '2024-06-08', '2024-06-09', '2024-06-10',\n       '2024-06-11', '2024-06-12', '2024-06-13', '2024-06-14', '2024-06-15',\n       '2024-06-16', '2024-06-17'], dtype='&lt;U10')</pre></li><li>spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]<pre>array(0)</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.94999694824219,   89.8499984741211,              89.75,\n         89.6500015258789,  89.55000305175781,  89.44999694824219,\n         89.3499984741211,              89.25,   89.1500015258789,\n        89.05000305175781,\n       ...\n       -89.05000305175781,  -89.1500015258789,             -89.25,\n       -89.35000610351562, -89.45000457763672, -89.55000305175781,\n        -89.6500015258789,             -89.75, -89.85000610351562,\n       -89.95000457763672],\n      dtype='float32', name='lat', length=1800))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9499969482422, -179.85000610351562,             -179.75,\n       -179.64999389648438,  -179.5500030517578,  -179.4499969482422,\n       -179.35000610351562,             -179.25, -179.14999389648438,\n        -179.0500030517578,\n       ...\n         179.0500030517578,  179.15000915527344,              179.25,\n        179.35000610351562,  179.45001220703125,   179.5500030517578,\n        179.65000915527344,              179.75,  179.85000610351562,\n        179.95001220703125],\n      dtype='float32', name='lon', length=3600))</pre></li><li>datePandasIndex<pre>PandasIndex(Index(['2024-06-01', '2024-06-02', '2024-06-03', '2024-06-04', '2024-06-05',\n       '2024-06-06', '2024-06-07', '2024-06-08', '2024-06-09', '2024-06-10',\n       '2024-06-11', '2024-06-12', '2024-06-13', '2024-06-14', '2024-06-15',\n       '2024-06-16', '2024-06-17'],\n      dtype='object', name='date'))</pre></li></ul></li><li>Attributes: (10)long_name :Chlorophyll Concentration, OCI Algorithmunits :lg(mg m^-3)standard_name :mass_concentration_of_chlorophyll_in_sea_watervalid_min :0.001valid_max :100.0reference :Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a algorithms for oligotrophic oceans: A novel approach based on three-band reflectance difference, J. Geophys. Res., 117, C01011, doi:10.1029/2011JC007395.display_scale :logdisplay_min :0.01display_max :20.0date :['2024-06-01', '2024-06-02', '2024-06-03', '2024-06-04', '2024-06-05', '2024-06-06', '2024-06-07', '2024-06-08', '2024-06-09', '2024-06-10', '2024-06-11', '2024-06-12', '2024-06-13', '2024-06-14', '2024-06-15', '2024-06-16', '2024-06-17']</li></ul> <p>Select a date and visualize the chlorophyll-a concentration data with Matplotlib.</p> In\u00a0[7]: Copied! <pre>hypercoast.viz_pace_chla(array, date=\"2024-06-01\", cmap=\"jet\", size=6)\n</pre> hypercoast.viz_pace_chla(array, date=\"2024-06-01\", cmap=\"jet\", size=6) Out[7]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f82d4e3a0d0&gt;</pre> <p>If the date is not specified, the data are averaged over the entire time range.</p> In\u00a0[8]: Copied! <pre>hypercoast.viz_pace_chla(array, cmap=\"jet\", size=6)\n</pre> hypercoast.viz_pace_chla(array, cmap=\"jet\", size=6) Out[8]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7f82d0c23e10&gt;</pre> <p>To visualize the data interactively, we can select either a single date or aggregate the data over a time range.</p> <p>First, let's select a single date from the data array:</p> In\u00a0[9]: Copied! <pre>single_array = array.sel(date=\"2024-06-01\")\nsingle_array\n</pre> single_array = array.sel(date=\"2024-06-01\") single_array Out[9]: <pre>&lt;xarray.DataArray 'chlor_a' (lat: 1800, lon: 3600)&gt; Size: 26MB\ndask.array&lt;getitem, shape=(1800, 3600), dtype=float32, chunksize=(512, 1024), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat          (lat) float32 7kB 89.95 89.85 89.75 ... -89.75 -89.85 -89.95\n  * lon          (lon) float32 14kB -179.9 -179.9 -179.8 ... 179.8 179.9 180.0\n    date         &lt;U10 40B '2024-06-01'\n    spatial_ref  int64 8B 0\nAttributes:\n    long_name:      Chlorophyll Concentration, OCI Algorithm\n    units:          lg(mg m^-3)\n    standard_name:  mass_concentration_of_chlorophyll_in_sea_water\n    valid_min:      0.001\n    valid_max:      100.0\n    reference:      Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a alg...\n    display_scale:  log\n    display_min:    0.01\n    display_max:    20.0\n    date:           ['2024-06-01', '2024-06-02', '2024-06-03', '2024-06-04', ...</pre>xarray.DataArray'chlor_a'<ul><li>lat: 1800</li><li>lon: 3600</li></ul><ul><li>dask.array&lt;chunksize=(512, 1024), meta=np.ndarray&gt;  Array   Chunk   Bytes   24.72 MiB   2.00 MiB   Shape   (1800, 3600)   (512, 1024)   Dask graph   16 chunks in 55 graph layers   Data type   float32 numpy.ndarray  3600 1800 </li><li>Coordinates: (4)<ul><li>lat(lat)float3289.95 89.85 89.75 ... -89.85 -89.95long_name :Latitudeunits :degrees_northstandard_name :latitudevalid_min :-90.0valid_max :90.0<pre>array([ 89.95    ,  89.85    ,  89.75    , ..., -89.75    , -89.850006,\n       -89.950005], dtype=float32)</pre></li><li>lon(lon)float32-179.9 -179.9 ... 179.9 180.0long_name :Longitudeunits :degrees_eaststandard_name :longitudevalid_min :-180.0valid_max :180.0<pre>array([-179.95   , -179.85   , -179.75   , ...,  179.75   ,  179.85   ,\n        179.95001], dtype=float32)</pre></li><li>date()&lt;U10'2024-06-01'<pre>array('2024-06-01', dtype='&lt;U10')</pre></li><li>spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]<pre>array(0)</pre></li></ul></li><li>Indexes: (2)<ul><li>latPandasIndex<pre>PandasIndex(Index([ 89.94999694824219,   89.8499984741211,              89.75,\n         89.6500015258789,  89.55000305175781,  89.44999694824219,\n         89.3499984741211,              89.25,   89.1500015258789,\n        89.05000305175781,\n       ...\n       -89.05000305175781,  -89.1500015258789,             -89.25,\n       -89.35000610351562, -89.45000457763672, -89.55000305175781,\n        -89.6500015258789,             -89.75, -89.85000610351562,\n       -89.95000457763672],\n      dtype='float32', name='lat', length=1800))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ -179.9499969482422, -179.85000610351562,             -179.75,\n       -179.64999389648438,  -179.5500030517578,  -179.4499969482422,\n       -179.35000610351562,             -179.25, -179.14999389648438,\n        -179.0500030517578,\n       ...\n         179.0500030517578,  179.15000915527344,              179.25,\n        179.35000610351562,  179.45001220703125,   179.5500030517578,\n        179.65000915527344,              179.75,  179.85000610351562,\n        179.95001220703125],\n      dtype='float32', name='lon', length=3600))</pre></li></ul></li><li>Attributes: (10)long_name :Chlorophyll Concentration, OCI Algorithmunits :lg(mg m^-3)standard_name :mass_concentration_of_chlorophyll_in_sea_watervalid_min :0.001valid_max :100.0reference :Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a algorithms for oligotrophic oceans: A novel approach based on three-band reflectance difference, J. Geophys. Res., 117, C01011, doi:10.1029/2011JC007395.display_scale :logdisplay_min :0.01display_max :20.0date :['2024-06-01', '2024-06-02', '2024-06-03', '2024-06-04', '2024-06-05', '2024-06-06', '2024-06-07', '2024-06-08', '2024-06-09', '2024-06-10', '2024-06-11', '2024-06-12', '2024-06-13', '2024-06-14', '2024-06-15', '2024-06-16', '2024-06-17']</li></ul> <p>Convert the data array to an image that can be displayed on an interactive map.</p> In\u00a0[10]: Copied! <pre>single_image = hypercoast.pace_chla_to_image(single_array)\n</pre> single_image = hypercoast.pace_chla_to_image(single_array) <p>Create an interactive map and display the image on the map.</p> In\u00a0[11]: Copied! <pre>m = hypercoast.Map(center=[40, -100], zoom=4)\nm.add_basemap(\"Hybrid\")\nm.add_raster(\n    single_image,\n    cmap=\"jet\",\n    vmin=-1,\n    vmax=2,\n    layer_name=\"Chlorophyll a\",\n    zoom_to_layer=False,\n)\nlabel = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\"\nm.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label)\nm\n</pre> m = hypercoast.Map(center=[40, -100], zoom=4) m.add_basemap(\"Hybrid\") m.add_raster(     single_image,     cmap=\"jet\",     vmin=-1,     vmax=2,     layer_name=\"Chlorophyll a\",     zoom_to_layer=False, ) label = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\" m.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label) m Out[11]: <p></p> <p>The daily image does not have a global coverage. To visualize the data globally, we can aggregate the data over a time range.</p> In\u00a0[12]: Copied! <pre>mean_array = array.mean(dim=\"date\")\n</pre> mean_array = array.mean(dim=\"date\") <p>Convert the aggregated data array to an image that can be displayed on an interactive map.</p> In\u00a0[13]: Copied! <pre>image = hypercoast.pace_chla_to_image(mean_array)\n</pre> image = hypercoast.pace_chla_to_image(mean_array) <p>Create an interactive map and display the image on the map.</p> In\u00a0[14]: Copied! <pre>m = hypercoast.Map(center=[40, -100], zoom=4)\nm.add_basemap(\"Hybrid\")\nm.add_raster(\n    image, cmap=\"jet\", vmin=-1, vmax=2, layer_name=\"Chlorophyll a\", zoom_to_layer=False\n)\nlabel = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\"\nm.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label)\nm\n</pre> m = hypercoast.Map(center=[40, -100], zoom=4) m.add_basemap(\"Hybrid\") m.add_raster(     image, cmap=\"jet\", vmin=-1, vmax=2, layer_name=\"Chlorophyll a\", zoom_to_layer=False ) label = \"Chlorophyll Concentration [lg(lg(mg m^-3))]\" m.add_colormap(cmap=\"jet\", vmin=-1, vmax=2, label=label) m Out[14]: <p></p>"},{"location":"examples/chlorophyll_a/#visualizing-pace-chlorophyll-a-data-interactively-with-hypercoast","title":"Visualizing PACE chlorophyll-a data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) data interactively with HyperCoast.</p>"},{"location":"examples/desis/","title":"Desis","text":"In\u00a0[1]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[2]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[3]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/desis.tif\"\nfilepath = \"data/desis.tif\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/desis.tif\" filepath = \"data/desis.tif\" hypercoast.download_file(url, filepath) Out[3]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/desis.tif'</pre> <p>Load the dataset as a xarray.Dataset object.</p> In\u00a0[4]: Copied! <pre>dataset = hypercoast.read_desis(filepath)\n</pre> dataset = hypercoast.read_desis(filepath) <p>Plot the spectral signature of a pixel.</p> In\u00a0[5]: Copied! <pre>hypercoast.filter_desis(dataset, lat=29.4315, lon=91.2927, return_plot=True)\n</pre> hypercoast.filter_desis(dataset, lat=29.4315, lon=91.2927, return_plot=True) <p>Visualize a single band of the hyperspectral image.</p> In\u00a0[6]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_desis(filepath, wavelengths=[1000], vmin=0, vmax=5000, nodata=0, colormap=\"jet\")\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=0.5, label=\"Reflectance\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_desis(filepath, wavelengths=[1000], vmin=0, vmax=5000, nodata=0, colormap=\"jet\") m.add_colormap(cmap=\"jet\", vmin=0, vmax=0.5, label=\"Reflectance\") m Out[6]: <p></p> <p>Plot the spectral signature of a pixel interactively.</p> In\u00a0[7]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_desis(filepath, wavelengths=[900, 600, 525], vmin=0, vmax=1000, nodata=0)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_desis(filepath, wavelengths=[900, 600, 525], vmin=0, vmax=1000, nodata=0) m.add(\"spectral\") m Out[7]: <p></p>"},{"location":"examples/desis/#visualizing-desis-data-interactively-with-hypercoast","title":"Visualizing DESIS data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize DESIS hyperspectral data interactively with HyperCoast.</p>"},{"location":"examples/ecostress/","title":"Ecostress","text":"In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_ecostress(\n    bbox=(-120.522, 34.4266, -120.2665, 34.5653),\n    temporal=(\"2023-04-01\", \"2023-04-02\"),\n    count=-1,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_ecostress(     bbox=(-120.522, 34.4266, -120.2665, 34.5653),     temporal=(\"2023-04-01\", \"2023-04-02\"),     count=-1,  # use -1 to return all datasets     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_ecostress(results[:5], out_dir=\"data\")\n</pre> hypercoast.download_ecostress(results[:5], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[34.5014, -120.4032], zoom=11)\nm.search_ecostress()\nm\n</pre> m = hypercoast.Map(center=[34.5014, -120.4032], zoom=11) m.search_ecostress() m In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_ecostress(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")\n</pre> # hypercoast.download_ecostress(m._NASA_DATA_RESULTS[:2], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\" In\u00a0[\u00a0]: Copied! <pre>filepath = \"data/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\"\nhypercoast.download_file(url, filepath)\n</pre> filepath = \"data/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\" hypercoast.download_file(url, filepath) <p>Visualize the data with HyperCoast.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"HYBRID\")\nm.add_raster(filepath, colormap=\"jet\", layer_name=\"LST\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"HYBRID\") m.add_raster(filepath, colormap=\"jet\", layer_name=\"LST\") m.add(\"spectral\") m <p></p>"},{"location":"examples/ecostress/#search-and-download-nasa-ecostress-data-with-hypercoast","title":"Search and download NASA ECOSTRESS data with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to search and visualize NASA ECOSTRESS temperature data with HyperCoast.</p>"},{"location":"examples/ecostress/#search-for-ecostress-data-programmatically","title":"Search for ECOSTRESS data programmatically\u00b6","text":""},{"location":"examples/ecostress/#download-ecostress-data","title":"Download ECOSTRESS data\u00b6","text":""},{"location":"examples/ecostress/#search-for-ecostress-data-interactively","title":"Search for ECOSTRESS data interactively\u00b6","text":""},{"location":"examples/ecostress/#visualize-ecostress-data","title":"Visualize ECOSTRESS data\u00b6","text":"<p>Download a sample ECOSTRESS data file and visualize it with HyperCoast.</p>"},{"location":"examples/emit/","title":"Emit","text":"In\u00a0[1]: Copied! <pre># %pip install hypercoast\n</pre> # %pip install hypercoast In\u00a0[2]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Download a sample EMIT data file from here.</p> In\u00a0[3]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" In\u00a0[4]: Copied! <pre>filepath = \"data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nhypercoast.download_file(url, filepath)\n</pre> filepath = \"data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" hypercoast.download_file(url, filepath) Out[4]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc'</pre> <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[5]: Copied! <pre>dataset = hypercoast.read_emit(filepath)\n</pre> dataset = hypercoast.read_emit(filepath) <p>Visualize the data interactively with HyperCoast. By default, the plot will show all the bands in the dataset.</p> In\u00a0[6]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"SATELLITE\") m.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\") m.add(\"spectral\") m Out[6]: <p>To visualize a certain wavelength range, you can specify the <code>xlim</code> parameter as follows:</p> In\u00a0[7]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\")\nm.add(\"spectral\", xlim=(400, 1200))\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"SATELLITE\") m.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\") m.add(\"spectral\", xlim=(400, 1200)) m Out[7]: <p>To access the selected spectral profiles from the mouse clicked location as a Pandas DataFrame, use the <code>Map.spectral_to_df()</code> method.</p> In\u00a0[8]: Copied! <pre>m.spectral_to_df()\n</pre> m.spectral_to_df() Out[8]: <p>To access the selected spectral profiles from the mouse clicked location as a GeoPandas GeoDataFrame, use the <code>Map.spectral_to_gdf()</code> method.</p> In\u00a0[9]: Copied! <pre>m.spectral_to_gdf()\n</pre> m.spectral_to_gdf() <p></p>"},{"location":"examples/emit/#visualizing-emit-data-interactively-with-hypercoast","title":"Visualizing EMIT data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Earth Surface Mineral Dust Source Investigation (EMIT) data interactively with HyperCoast. This notebook is inspired by the EMIT data visualization tutorial - Exploring_EMIT_L2A_Reflectance.ipynb. We have made it much easier to visualize the data interactively with HyperCoast.</p>"},{"location":"examples/field_data/","title":"Field data","text":"In\u00a0[1]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[2]: Copied! <pre>import hypercoast\nimport pandas as pd\n</pre> import hypercoast import pandas as pd <p>Download a sample filed dataset.</p> In\u00a0[3]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_sample_points.csv\"\ndata = pd.read_csv(url)\ndata.head()\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/pace_sample_points.csv\" data = pd.read_csv(url) data.head() Out[3]: band wavelength (30.1926 -90.1318) (30.1594 -90.2856) (29.3295 -92.3071) (28.8783 -90.4559) (30.5481 -87.9840) (29.5305 -85.0671) (28.8254 -85.6659) (29.4587 -83.8477) (26.8878 -87.7643) (24.6570 -86.4954) (26.8045 -82.4854) 0 0 339.0 NaN NaN 0.000100 NaN NaN NaN NaN NaN 0.001718 0.000710 NaN 1 1 341.0 NaN NaN NaN NaN NaN NaN NaN NaN 0.001554 0.000846 NaN 2 2 344.0 NaN NaN 0.001816 NaN NaN NaN NaN NaN 0.004047 0.003598 NaN 3 3 346.0 0.000346 NaN 0.002393 NaN NaN NaN NaN NaN 0.005045 0.004646 NaN 4 4 348.0 NaN NaN 0.001578 NaN NaN NaN NaN NaN 0.004258 0.003984 NaN <p>Download PACE data.</p> In\u00a0[4]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nfilepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" filepath = \"data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc\" hypercoast.download_file(url, filepath) Out[4]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/PACE_OCI.20240730T181157.L2.OC_AOP.V2_0.NRT.nc'</pre> <p>Read the PACE dataset.</p> In\u00a0[5]: Copied! <pre>dataset = hypercoast.read_pace(filepath)\n</pre> dataset = hypercoast.read_pace(filepath) <p>Run the following cell to show the map. Click on the markers to see the spectral data.</p> In\u00a0[6]: Copied! <pre>m = hypercoast.Map(center=[27.235094, -87.791748], zoom=6)\n\nm.add_basemap(\"Hybrid\")\nwavelengths = [450, 550, 650]\nm.add_pace(\n    dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\"\n)\nm.add(\"spectral\")\n\nm.add_field_data(\n    data,\n    x_col=\"wavelength\",\n    y_col_prefix=\"(\",\n    x_label=\"Wavelength (nm)\",\n    y_label=\"Reflectance\",\n    use_marker_cluster=True,\n)\nm.set_center(-87.791748, 27.235094, zoom=6)\nm\n</pre> m = hypercoast.Map(center=[27.235094, -87.791748], zoom=6)  m.add_basemap(\"Hybrid\") wavelengths = [450, 550, 650] m.add_pace(     dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\" ) m.add(\"spectral\")  m.add_field_data(     data,     x_col=\"wavelength\",     y_col_prefix=\"(\",     x_label=\"Wavelength (nm)\",     y_label=\"Reflectance\",     use_marker_cluster=True, ) m.set_center(-87.791748, 27.235094, zoom=6) m Out[6]: <p></p>"},{"location":"examples/field_data/#visualizing-spectral-data-from-field-measurements","title":"Visualizing Spectral Data from Field Measurements\u00b6","text":"<p>This notebook demonstrates how to visualize spectral data from field measurements.</p>"},{"location":"examples/image_cube/","title":"Image cube","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\"\nfilepath = \"data/neon.h5\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\" filepath = \"data/neon.h5\" hypercoast.download_file(url, filepath) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_neon(filepath)\ndataset\n</pre> dataset = hypercoast.read_neon(filepath) dataset <p>Visualize the NEON AOP hyperspectral data in 3D with a selected band overlaid on top of the 3D plot.</p> In\u00a0[\u00a0]: Copied! <pre>cube = hypercoast.image_cube(\n    dataset,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[700],\n    title=\"Reflectance\",\n)\ncube.show()\n</pre> cube = hypercoast.image_cube(     dataset,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[700],     title=\"Reflectance\", ) cube.show() <p>Visualize the NEON AOP hyperspectral data in 3D with an RGB image overlaid on top of the 3D plot.</p> In\u00a0[\u00a0]: Copied! <pre>cube2 = hypercoast.image_cube(\n    dataset,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"Reflectance\",\n)\ncube2.show()\n</pre> cube2 = hypercoast.image_cube(     dataset,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"Reflectance\", ) cube2.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nfilepath = \"EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nhypercoast.download_file(url)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" filepath = \"EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" hypercoast.download_file(url) In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_emit(filepath)\ndataset\n</pre> dataset = hypercoast.read_emit(filepath) dataset <p>Select a subset of the data to avoid nodata areas.</p> In\u00a0[\u00a0]: Copied! <pre>ds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451))\nds\n</pre> ds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451)) ds <p>Visualize the EMIT data in 3D with an RGB image overlaid on top of the 3D plot.</p> In\u00a0[\u00a0]: Copied! <pre>cube = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.4),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n)\ncube.show()\n</pre> cube = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.4),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\", ) cube.show() <p></p>"},{"location":"examples/image_cube/#visualizing-hyperspectral-data-in-3d","title":"Visualizing Hyperspectral Data in 3D\u00b6","text":"<p>This notebook demonstrates how to visualize hyperspectral data in 3D using the PyVista plotting backend.</p>"},{"location":"examples/image_cube/#visualize-neon-aop-hyperspectral-data","title":"Visualize NEON AOP Hyperspectral Data\u00b6","text":"<p>Download a sample NEON AOP hyperspectral data.</p>"},{"location":"examples/image_cube/#visualize-nasa-emit-hyperspectral-data","title":"Visualize NASA EMIT Hyperspectral Data\u00b6","text":"<p>Download a sample EMIT data file from here.</p>"},{"location":"examples/image_slicing/","title":"Image slicing","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\"\nfilepath = \"data/neon.h5\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\" filepath = \"data/neon.h5\" hypercoast.download_file(url, filepath) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_neon(filepath)\ndataset\n</pre> dataset = hypercoast.read_neon(filepath) dataset <p>Extract a small subset of the dataset for demonstration purposes.</p> In\u00a0[\u00a0]: Copied! <pre>ds = dataset.isel(x=slice(100, 200), y=slice(100, 200))\nds\n</pre> ds = dataset.isel(x=slice(100, 200), y=slice(100, 200)) ds <p>Interactive slicing along the z-axis (band)</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"slice\",\n)\np.add_text(\"Band slicing \", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"slice\", ) p.add_text(\"Band slicing \", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Interactive slicing along the x-axis (longitude).</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"slice\",\n    normal=\"x\",\n)\np.add_text(\"X-axis slicing \", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"slice\",     normal=\"x\", ) p.add_text(\"X-axis slicing \", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Orthogonal slicing.</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"orthogonal\",\n)\np.add_text(\"Orthogonal slicing\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"orthogonal\", ) p.add_text(\"Orthogonal slicing\", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Clip the image cube with a plane (band slicing).</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"plane\", ) p.add_text(\"Band slicing\", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Interactive thresholding.</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     widget=\"threshold\", ) p.add_text(\"Thresholding\", position=\"upper_right\", font_size=14) p.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nfilepath = \"data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/netcdf/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" filepath = \"data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" hypercoast.download_file(url, filepath) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.read_emit(filepath)\ndataset\n</pre> dataset = hypercoast.read_emit(filepath) dataset <p>Select a subset of the data for demonstration purposes.</p> In\u00a0[\u00a0]: Copied! <pre>ds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93))\nds\n</pre> ds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93)) ds <p>Interactive slicing along the z-axis (band).</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\",     widget=\"plane\", ) p.add_text(\"Band slicing\", position=\"upper_right\", font_size=14) p.show() <p></p> <p>Interactive thresholding.</p> In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\np.show()\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\",     widget=\"threshold\", ) p.add_text(\"Thresholding\", position=\"upper_right\", font_size=14) p.show() <p></p>"},{"location":"examples/image_slicing/#interactive-slicing-and-thresholding-of-hyperspectral-data-with-hypercoast","title":"Interactive slicing and thresholding of hyperspectral data with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to perform interactive slicing and thresholding of hyperspectral data with HyperCoast using the PyVista plotting backend.</p>"},{"location":"examples/image_slicing/#neon-aop","title":"NEON AOP\u00b6","text":"<p>Download a sample NEON AOP hyperspectral dataset.</p>"},{"location":"examples/image_slicing/#nasa-emit","title":"NASA EMIT\u00b6","text":"<p>Download a sample NASA EMIT hyperspectral dataset from here.</p>"},{"location":"examples/multispectral/","title":"Multispectral","text":"In\u00a0[1]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[2]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[3]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/cog.tif\"\nfilepath = \"data/cog.tif\"\nhypercoast.download_file(url, filepath, quiet=True)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/cog.tif\" filepath = \"data/cog.tif\" hypercoast.download_file(url, filepath, quiet=True) Out[3]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/cog.tif'</pre> In\u00a0[4]: Copied! <pre>m = hypercoast.Map()\nfilepath = \"data/cog.tif\"  # replace it with your own raster data\nm.add_dataset(\n    filepath, indexes=[4, 1, 2], vmin=0, vmax=2500, layer_name=\"Landsat\", nodata=0\n)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() filepath = \"data/cog.tif\"  # replace it with your own raster data m.add_dataset(     filepath, indexes=[4, 1, 2], vmin=0, vmax=2500, layer_name=\"Landsat\", nodata=0 ) m.add(\"spectral\") m <pre>Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n</pre> Out[4]:"},{"location":"examples/multispectral/#visualizing-multispectral-data-with-hypercoast","title":"Visualizing Multispectral Data with HyperCoast\u00b6","text":""},{"location":"examples/neon/","title":"Neon","text":"In\u00a0[1]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[2]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[3]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\"\nfilepath = \"data/neon.h5\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5\" filepath = \"data/neon.h5\" hypercoast.download_file(url, filepath) Out[3]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/neon.h5'</pre> <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[4]: Copied! <pre>dataset = hypercoast.read_neon(filepath)\ndataset\n</pre> dataset = hypercoast.read_neon(filepath) dataset Out[4]: <pre>&lt;xarray.Dataset&gt; Size: 3GB\nDimensions:      (y: 1000, x: 1000, wavelength: 426)\nCoordinates:\n  * y            (y) float64 8kB 4.307e+06 4.307e+06 ... 4.306e+06 4.306e+06\n  * x            (x) float64 8kB 3.68e+05 3.68e+05 ... 3.69e+05 3.69e+05\n  * wavelength   (wavelength) float64 3kB 383.9 388.9 ... 2.507e+03 2.512e+03\nData variables:\n    reflectance  (y, x, wavelength) float64 3GB 0.1569 0.1206 0.1034 ... nan nan\nAttributes:\n    scale_factor:   10000.0\n    no_data_value:  -9999.0\n    crs:            EPSG:32618\n    transform:      (1.0, 0.0, 368000.0, 0.0, -1.0, 4307000.0)</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>y: 1000</li><li>x: 1000</li><li>wavelength: 426</li></ul></li><li>Coordinates: (3)<ul><li>y(y)float644.307e+06 4.307e+06 ... 4.306e+06<pre>array([4307000.      , 4306998.998999, 4306997.997998, ..., 4306002.002002,\n       4306001.001001, 4306000.      ])</pre></li><li>x(x)float643.68e+05 3.68e+05 ... 3.69e+05<pre>array([368000.      , 368001.001001, 368002.002002, ..., 368997.997998,\n       368998.998999, 369000.      ])</pre></li><li>wavelength(wavelength)float64383.9 388.9 ... 2.507e+03 2.512e+03<pre>array([ 383.88,  388.89,  393.9 , ..., 2502.17, 2507.17, 2512.18])</pre></li></ul></li><li>Data variables: (1)<ul><li>reflectance(y, x, wavelength)float640.1569 0.1206 0.1034 ... nan nanscale_factor :10000.0no_data_value :-9999.0crs :EPSG:32618transform :(1.0, 0.0, 368000.0, 0.0, -1.0, 4307000.0)<pre>array([[[0.1569, 0.1206, 0.1034, ..., 0.1512,    nan,    nan],\n        [0.1593, 0.118 , 0.105 , ...,    nan,    nan,    nan],\n        [0.1415, 0.1147, 0.1061, ..., 0.    , 0.    ,    nan],\n        ...,\n        [0.056 , 0.0319, 0.0292, ...,    nan,    nan,    nan],\n        [0.0604, 0.0412, 0.0257, ...,    nan,    nan,    nan],\n        [0.0723, 0.0444, 0.0366, ..., 0.7853,    nan,    nan]],\n\n       [[0.13  , 0.0988, 0.0842, ...,    nan,    nan,    nan],\n        [0.1515, 0.0961, 0.0826, ...,    nan,    nan,    nan],\n        [0.1431, 0.1253, 0.1069, ...,    nan,    nan,    nan],\n        ...,\n        [0.0586, 0.0411, 0.0224, ...,    nan,    nan,    nan],\n        [0.0467, 0.0424, 0.0289, ..., 0.4019,    nan,    nan],\n        [0.074 , 0.0453, 0.0301, ..., 0.7002,    nan,    nan]],\n\n       [[0.1026, 0.0716, 0.0623, ...,    nan,    nan,    nan],\n        [0.127 , 0.1103, 0.0986, ..., 0.7747,    nan,    nan],\n        [0.168 , 0.1335, 0.1359, ...,    nan,    nan,    nan],\n        ...,\n...\n        ...,\n        [0.0578, 0.0493, 0.0418, ...,    nan,    nan,    nan],\n        [0.1047, 0.0788, 0.057 , ...,    nan,    nan,    nan],\n        [0.0958, 0.0785, 0.0693, ...,    nan,    nan,    nan]],\n\n       [[0.0747, 0.0397, 0.0314, ...,    nan,    nan,    nan],\n        [0.065 , 0.041 , 0.0322, ..., 0.9747,    nan,    nan],\n        [0.0667, 0.0424, 0.0274, ...,    nan,    nan,    nan],\n        ...,\n        [0.1335, 0.1008, 0.0878, ..., 0.    ,    nan,    nan],\n        [0.1369, 0.0864, 0.0771, ...,    nan,    nan,    nan],\n        [0.1144, 0.0958, 0.0857, ...,    nan,    nan,    nan]],\n\n       [[0.0738, 0.0392, 0.031 , ...,    nan,    nan,    nan],\n        [0.0541, 0.0421, 0.0272, ...,    nan,    nan,    nan],\n        [0.0513, 0.0406, 0.0276, ...,    nan,    nan,    nan],\n        ...,\n        [0.082 , 0.0593, 0.0511, ...,    nan,    nan,    nan],\n        [0.0883, 0.0693, 0.0518, ...,    nan,    nan,    nan],\n        [0.1128, 0.1   , 0.0844, ..., 0.5833,    nan,    nan]]])</pre></li></ul></li><li>Indexes: (3)<ul><li>yPandasIndex<pre>PandasIndex(Index([        4307000.0, 4306998.998998999, 4306997.997997998,\n       4306996.996996997, 4306995.995995996, 4306994.994994995,\n       4306993.993993994, 4306992.992992993, 4306991.991991992,\n       4306990.990990991,\n       ...\n       4306009.009009009, 4306008.008008008, 4306007.007007007,\n       4306006.006006006, 4306005.005005005, 4306004.004004004,\n       4306003.003003003, 4306002.002002002, 4306001.001001001,\n               4306000.0],\n      dtype='float64', name='y', length=1000))</pre></li><li>xPandasIndex<pre>PandasIndex(Index([          368000.0,   368001.001001001, 368002.00200200203,\n         368003.003003003,   368004.004004004,   368005.005005005,\n       368006.00600600604,   368007.007007007,   368008.008008008,\n         368009.009009009,\n       ...\n         368990.990990991,   368991.991991992,   368992.992992993,\n       368993.99399399396,   368994.994994995,   368995.995995996,\n         368996.996996997,   368997.997997998,   368998.998998999,\n                 369000.0],\n      dtype='float64', name='x', length=1000))</pre></li><li>wavelengthPandasIndex<pre>PandasIndex(Index([ 383.88,  388.89,   393.9,  398.91,  403.92,  408.92,  413.93,  418.94,\n        423.95,  428.95,\n       ...\n       2467.11, 2472.12, 2477.13, 2482.13, 2487.14, 2492.15, 2497.16, 2502.17,\n       2507.17, 2512.18],\n      dtype='float64', name='wavelength', length=426))</pre></li></ul></li><li>Attributes: (4)scale_factor :10000.0no_data_value :-9999.0crs :EPSG:32618transform :(1.0, 0.0, 368000.0, 0.0, -1.0, 4307000.0)</li></ul> <p>Visualize the data interactively with HyperCoast.</p> In\u00a0[5]: Copied! <pre>m = hypercoast.Map()\nm.add_neon(filepath, wavelengths=[1000, 700, 500], vmin=0, vmax=0.5)\nm\n</pre> m = hypercoast.Map() m.add_neon(filepath, wavelengths=[1000, 700, 500], vmin=0, vmax=0.5) m Out[5]: In\u00a0[6]: Copied! <pre>m.set_center(-76.5134, 38.8973, 16)\n</pre> m.set_center(-76.5134, 38.8973, 16) In\u00a0[7]: Copied! <pre>m.add(\"spectral\")\n</pre> m.add(\"spectral\") <p></p>"},{"location":"examples/neon/#visualizing-neon-aop-hyperspectral-data-interactively-with-hypercoast","title":"Visualizing NEON AOP hyperspectral data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize NEON AOP hyperspectral data interactively with HyperCoast.</p>"},{"location":"examples/overview/","title":"Overview","text":"<ul> <li>Search and download NASA hyperspectral data</li> <li>Search and download NASA ECOSTRESS data</li> <li>Visualize Hyperspectral Data in 3D</li> <li>Interactive slicing and thresholding of hyperspectral data</li> <li>Visualize AVIRIS data interactively</li> <li>Visualize DESIS data interactively</li> <li>Visualize EMIT data interactively</li> <li>Visualize NEON AOP hyperspectral data interactively</li> <li>Visualize PACE data interactively</li> <li>Visualize PACE chlorophyll-a data interactively</li> <li>Visualize ERA5 temperature data interactively</li> <li>Visualize PACE OCI L1 data products</li> <li>Visualize PACE OCI L2 data products</li> <li>Visualize Multispectral data interactively</li> </ul>"},{"location":"examples/pace/","title":"Pace","text":"In\u00a0[1]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[2]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>Download a sample PACE data file from here.</p> In\u00a0[3]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/netcdf/PACE_OCI.20240423T184658.L2.OC_AOP.V1_0_0.NRT.nc\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/netcdf/PACE_OCI.20240423T184658.L2.OC_AOP.V1_0_0.NRT.nc\" In\u00a0[4]: Copied! <pre>filepath = \"data/PACE_OCI.20240423T184658.L2.OC_AOP.V1_0_0.NRT.nc\"\nhypercoast.download_file(url, filepath)\n</pre> filepath = \"data/PACE_OCI.20240423T184658.L2.OC_AOP.V1_0_0.NRT.nc\" hypercoast.download_file(url, filepath) Out[4]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/PACE_OCI.20240423T184658.L2.OC_AOP.V1_0_0.NRT.nc'</pre> <p>Let's make a scatter plot of the pixel locations so we can see the irregular spacing.</p> In\u00a0[5]: Copied! <pre>plot = hypercoast.view_pace_pixel_locations(filepath, step=20)\n</pre> plot = hypercoast.view_pace_pixel_locations(filepath, step=20) <p>Load the dataset as a <code>xarray.Dataset</code> object.</p> In\u00a0[6]: Copied! <pre>dataset = hypercoast.read_pace(filepath)\n</pre> dataset = hypercoast.read_pace(filepath) <p>Visualize selected bands of the dataset.</p> In\u00a0[7]: Copied! <pre>hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2)\n</pre> hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2) <p>Add projection.</p> In\u00a0[8]: Copied! <pre>hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2, crs=\"default\")\n</pre> hypercoast.viz_pace(dataset, wavelengths=[500, 510, 520, 530], ncols=2, crs=\"default\") <p>Plot a spectral signature.</p> In\u00a0[9]: Copied! <pre>latitude = 25.493961\nlongitude = -91.25617\nhypercoast.filter_pace(dataset, latitude, longitude, return_plot=True)\n</pre> latitude = 25.493961 longitude = -91.25617 hypercoast.filter_pace(dataset, latitude, longitude, return_plot=True) <p>Plot multiple spectral signatures.</p> In\u00a0[10]: Copied! <pre>latitude = (25.49, 25.50)\nlongitude = (-92, -91.055)\nhypercoast.filter_pace(dataset, latitude, longitude, return_plot=True)\n</pre> latitude = (25.49, 25.50) longitude = (-92, -91.055) hypercoast.filter_pace(dataset, latitude, longitude, return_plot=True) <p>Single-band visualization.</p> In\u00a0[11]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450]\nm.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\")\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") wavelengths = [450] m.add_pace(dataset, wavelengths, colormap=\"jet\", vmin=0, vmax=0.02, layer_name=\"PACE\") m.add_colormap(cmap=\"jet\", vmin=0, vmax=0.02, label=\"Reflectance\") m.add(\"spectral\") m Out[11]: <p></p> <p>Multiple-band visualization.</p> In\u00a0[12]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nwavelengths = [450, 550, 650]\nm.add_pace(\n    dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\"\n)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") wavelengths = [450, 550, 650] m.add_pace(     dataset, wavelengths, indexes=[3, 2, 1], vmin=0, vmax=0.02, layer_name=\"PACE\" ) m.add(\"spectral\") m Out[12]: <p></p>"},{"location":"examples/pace/#visualizing-pace-data-interactively-with-hypercoast","title":"Visualizing PACE data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) data interactively with HyperCoast.</p>"},{"location":"examples/pace_oci_l1/","title":"Pace oci l1","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\nimport xarray as xr\n</pre> import hypercoast import xarray as xr <p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, you can run the following cell to search and download PACE OCI L1 data products.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n\nshort_name = \"PACE_OCI_L1B_SCI\"\nresults, gdf = hypercoast.search_nasa_data(\n    short_name=short_name,\n    bbox=(-90.5642, 29.9749, -89.7143, 30.42),\n    temporal=(\"2024-06-15\", \"2024-06-16\"),\n    return_gdf=True,\n)\n</pre> hypercoast.nasa_earth_login()  short_name = \"PACE_OCI_L1B_SCI\" results, gdf = hypercoast.search_nasa_data(     short_name=short_name,     bbox=(-90.5642, 29.9749, -89.7143, 30.42),     temporal=(\"2024-06-15\", \"2024-06-16\"),     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p></p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_nasa_data(results[0], out_dir=\"data\")\n</pre> hypercoast.download_nasa_data(results[0], out_dir=\"data\") <p>Alternatively, use the following code block to download a sample dataset from here.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240615T182549.L1B.nc\"\nfilepath = \"data/PACE_OCI.20240615T182549.L1B.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240615T182549.L1B.nc\" filepath = \"data/PACE_OCI.20240615T182549.L1B.nc\" hypercoast.download_file(url, filepath) <p>Let's check the top-level groups in the sample dataset.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.netcdf_groups(filepath)\n</pre> hypercoast.netcdf_groups(filepath) <p>The top-level groups in the sample dataset are:</p> <pre>['sensor_band_parameters',\n 'scan_line_attributes',\n 'geolocation_data',\n 'navigation_data',\n 'observation_data']\n</pre> <p>Let's open the <code>observation_data</code> group, which contains the core science variables.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = xr.open_dataset(filepath, group=\"observation_data\")\nprint(list(dataset.variables))\n</pre> dataset = xr.open_dataset(filepath, group=\"observation_data\") print(list(dataset.variables)) <p>The data variables include:</p> <pre>['rhot_blue', 'qual_blue', 'rhot_red', 'qual_red', 'rhot_SWIR', 'qual_SWIR']\n</pre> <p>The dimensions of the <code>rhot_blue</code> variable are <code>(\"blue_bands\", \"number_of_scans\", \"ccd_pixels\")</code>, and it has shape <code>(119, 1710, 1272)</code>. The sizes attribute of a variable gives us that information as a dictionary.</p> In\u00a0[\u00a0]: Copied! <pre>dataset[\"rhot_blue\"].sizes\n</pre> dataset[\"rhot_blue\"].sizes <p>The dimensions of the <code>rhot_red</code> variable are <code>(\"red_bands\", \"number_of_scans\", \"ccd_pixels\")</code>, and it has shape <code>(163, 1710, 1272)</code></p> In\u00a0[\u00a0]: Copied! <pre>dataset[\"rhot_red\"].sizes\n</pre> dataset[\"rhot_red\"].sizes <p>The dimensions of the <code>rhot_SWIR</code> variable are <code>(\"SWIR_bands\", \"number_of_scans\", \"SWIR_pixels\")</code>, and it has shape <code>(9, 1710, 1272)</code></p> In\u00a0[\u00a0]: Copied! <pre>dataset[\"rhot_SWIR\"].sizes\n</pre> dataset[\"rhot_SWIR\"].sizes <p>Let's plot the reflectance at position <code>100</code> in the <code>blue_bands</code> dimension.</p> In\u00a0[\u00a0]: Copied! <pre>plot = dataset[\"rhot_blue\"].sel({\"blue_bands\": 100}).plot()\n</pre> plot = dataset[\"rhot_blue\"].sel({\"blue_bands\": 100}).plot() <p></p>"},{"location":"examples/pace_oci_l1/#visualizing-pace-oci-l1-data-products-with-hypercoast","title":"Visualizing PACE OCI L1 data products with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) OCI L1 data products. Part of the notebook is adapted from the NASA OB.DAAC tutorial - File Structure at Three Processing Levels for the Ocean Color Instrument (OCI). Credits to the NASA OB.DAAC team for the tutorial.</p>"},{"location":"examples/pace_oci_l2/","title":"Pace oci l2","text":"In\u00a0[1]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[2]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, you can uncomment and run the following cell to search and download PACE OCI L2 data products.</p> In\u00a0[3]: Copied! <pre># hypercoast.nasa_earth_login()\n\n# short_name = \"PACE_OCI_L2_BGC_NRT\"\n# results, gdf = hypercoast.search_nasa_data(\n#     short_name=short_name,\n#     bbox=(-90.5642, 29.9749, -89.7143, 30.42),\n#     temporal=(\"2024-06-15\", \"2024-06-16\"),\n#     return_gdf=True\n#     )\n# hypercoast.download_nasa_data(results, out_dir=\"bgc\")\n</pre> # hypercoast.nasa_earth_login()  # short_name = \"PACE_OCI_L2_BGC_NRT\" # results, gdf = hypercoast.search_nasa_data( #     short_name=short_name, #     bbox=(-90.5642, 29.9749, -89.7143, 30.42), #     temporal=(\"2024-06-15\", \"2024-06-16\"), #     return_gdf=True #     ) # hypercoast.download_nasa_data(results, out_dir=\"bgc\") <p>Alternatively, use the following code block to download a sample dataset from here.</p> In\u00a0[4]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc\"\nfilepath = \"data/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc\" filepath = \"data/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc\" hypercoast.download_file(url, filepath) Out[4]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/PACE_OCI.20240615T182549.L2.OC_BGC.V1_0_0.NRT.nc'</pre> <p>Load the downloaded dataset as an <code>xarray.Dataset</code>:</p> In\u00a0[5]: Copied! <pre>dataset = hypercoast.read_pace_bgc(filepath)\n</pre> dataset = hypercoast.read_pace_bgc(filepath) <p>Let's inspect the data variables contained in the dataset:</p> In\u00a0[6]: Copied! <pre>dataset.variables\n</pre> dataset.variables Out[6]: <pre>Frozen({'chlor_a': &lt;xarray.Variable (latitude: 1710, longitude: 1272)&gt; Size: 9MB\n[2175120 values with dtype=float32]\nAttributes:\n    long_name:      Chlorophyll Concentration, OCI Algorithm\n    units:          mg m^-3\n    standard_name:  mass_concentration_of_chlorophyll_in_sea_water\n    valid_min:      0.001\n    valid_max:      100.0\n    reference:      Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a alg..., 'carbon_phyto': &lt;xarray.Variable (latitude: 1710, longitude: 1272)&gt; Size: 9MB\n[2175120 values with dtype=float32]\nAttributes:\n    long_name:  Phytoplankton Carbon\n    units:      mg m^-3\n    valid_min:  0.0\n    valid_max:  1000.0\n    reference:  Graff, J.R., Westberry, T.K., Milligan, A.J., Brown, M.B., Da..., 'poc': &lt;xarray.Variable (latitude: 1710, longitude: 1272)&gt; Size: 9MB\n[2175120 values with dtype=float32]\nAttributes:\n    long_name:  Particulate Organic Carbon, D. Stramski, 2022 (hybrid version)\n    units:      mg m^-3\n    valid_min:  -32000\n    valid_max:  -22000\n    reference:  Stramski, D., et al. \"Ocean color algorithms to estimate the ..., 'chlor_a_unc': &lt;xarray.Variable (latitude: 1710, longitude: 1272)&gt; Size: 9MB\n[2175120 values with dtype=float32]\nAttributes:\n    long_name:      Uncertainty in chlorophyll a concentration\n    units:          mg m^-3\n    standard_name:  chlorophyll_concentration_in_sea_water standard_error\n    valid_min:      0.001\n    valid_max:      100.0, 'carbon_phyto_unc': &lt;xarray.Variable (latitude: 1710, longitude: 1272)&gt; Size: 9MB\n[2175120 values with dtype=float32]\nAttributes:\n    long_name:  Phytoplankton Carbon standard uncertainty\n    units:      mg m^-3\n    valid_min:  0.0\n    valid_max:  1000.0\n    reference:  Graff, J.R., Westberry, T.K., Milligan, A.J., Brown, M.B., Da..., 'l2_flags': &lt;xarray.Variable (latitude: 1710, longitude: 1272)&gt; Size: 9MB\n[2175120 values with dtype=int32]\nAttributes:\n    long_name:      Level-2 Processing Flags\n    valid_min:      -2147483648\n    valid_max:      2147483647\n    flag_masks:     [          1           2           4           8         ...\n    flag_meanings:  ATMFAIL LAND PRODWARN HIGLINT HILT HISATZEN COASTZ SPARE ..., 'longitude': &lt;xarray.Variable (latitude: 1710, longitude: 1272)&gt; Size: 9MB\n[2175120 values with dtype=float32]\nAttributes:\n    long_name:      Longitude\n    units:          degrees_east\n    standard_name:  longitude\n    valid_min:      -180.0\n    valid_max:      180.0, 'latitude': &lt;xarray.Variable (latitude: 1710, longitude: 1272)&gt; Size: 9MB\n[2175120 values with dtype=float32]\nAttributes:\n    long_name:      Latitude\n    units:          degrees_north\n    standard_name:  latitude\n    valid_min:      -90.0\n    valid_max:      90.0})</pre> <p>We can see that the dataset contains the following variables:</p> <ul> <li>Chlorophyll Concentration</li> <li>Phytoplankton Carbon</li> <li>Particulate Organic Carbon</li> </ul> <p>Transform the xarray dataset into gridded data.</p> <p>Plot the Chlorophyll Concentration.</p> In\u00a0[7]: Copied! <pre>chlor_a = hypercoast.grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"linear\")\nchlor_a.plot(vmin=0, vmax=20, cmap=\"jet\", size=6)\n</pre> chlor_a = hypercoast.grid_pace_bgc(dataset, variable=\"chlor_a\", method=\"linear\") chlor_a.plot(vmin=0, vmax=20, cmap=\"jet\", size=6) Out[7]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7fef77436c10&gt;</pre> <p>Plot the Phytoplankton Carbon.</p> In\u00a0[8]: Copied! <pre>carbon_phyto = hypercoast.grid_pace_bgc(\n    dataset, variable=\"carbon_phyto\", method=\"linear\"\n)\ncarbon_phyto.plot(vmin=0, vmax=120, cmap=\"jet\", size=6)\n</pre> carbon_phyto = hypercoast.grid_pace_bgc(     dataset, variable=\"carbon_phyto\", method=\"linear\" ) carbon_phyto.plot(vmin=0, vmax=120, cmap=\"jet\", size=6) Out[8]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7fef76795ed0&gt;</pre> <p>Particulate Organic Carbon.</p> In\u00a0[9]: Copied! <pre>poc = hypercoast.grid_pace_bgc(dataset, variable=\"poc\", method=\"linear\")\npoc.plot(vmin=0, vmax=1000, cmap=\"jet\")\n</pre> poc = hypercoast.grid_pace_bgc(dataset, variable=\"poc\", method=\"linear\") poc.plot(vmin=0, vmax=1000, cmap=\"jet\") Out[9]: <pre>&lt;matplotlib.collections.QuadMesh at 0x7fef76798ed0&gt;</pre> <p>Plot the data on an interactive map.</p> In\u00a0[10]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_raster(chlor_a, layer_name=\"Chlorophyll-a\", colormap=\"jet\", vmin=0, vmax=20)\nm.add_raster(\n    carbon_phyto, layer_name=\"Phytoplankton Carbon\", colormap=\"plasma\", vmin=0, vmax=120\n)\nm.add_raster(\n    poc, layer_name=\"Particulate Organic Carbon\", colormap=\"coolwarm\", vmin=0, vmax=1000\n)\nm.add_layer_manager()\n\nm.add_colormap(cmap=\"jet\", vmin=0, vmax=20, label=\"Chlorophyll-a (mg/m3)\")\nm.add_colormap(cmap=\"plasma\", vmin=0, vmax=120, label=\"Phytoplankton Carbon (mg/m3)\")\nm.add_colormap(\n    cmap=\"coolwarm\", vmin=0, vmax=1000, label=\"Particulate Organic Carbon (mg/m3)\"\n)\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_raster(chlor_a, layer_name=\"Chlorophyll-a\", colormap=\"jet\", vmin=0, vmax=20) m.add_raster(     carbon_phyto, layer_name=\"Phytoplankton Carbon\", colormap=\"plasma\", vmin=0, vmax=120 ) m.add_raster(     poc, layer_name=\"Particulate Organic Carbon\", colormap=\"coolwarm\", vmin=0, vmax=1000 ) m.add_layer_manager()  m.add_colormap(cmap=\"jet\", vmin=0, vmax=20, label=\"Chlorophyll-a (mg/m3)\") m.add_colormap(cmap=\"plasma\", vmin=0, vmax=120, label=\"Phytoplankton Carbon (mg/m3)\") m.add_colormap(     cmap=\"coolwarm\", vmin=0, vmax=1000, label=\"Particulate Organic Carbon (mg/m3)\" ) m Out[10]: <p></p>"},{"location":"examples/pace_oci_l2/#visualizing-pace-oci-l2-data-products-with-hypercoast","title":"Visualizing PACE OCI L2 data products with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) OCI L2 data products, including the concentration of chlorophyll-a, concentration of phytoplankton carbon, and concentration of particulate organic carbon.</p>"},{"location":"examples/pca/","title":"Pca","text":"In\u00a0[1]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[2]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/Planet_2022-09-23.tif\"\nfilepath = \"data/Planet_2022-09-23.tif\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/Planet_2022-09-23.tif\" filepath = \"data/Planet_2022-09-23.tif\" hypercoast.download_file(url, filepath) Out[2]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/Planet_2022-09-23.tif'</pre> In\u00a0[3]: Copied! <pre>output_file = filepath.replace(\".tif\", \"_pca.tif\")\n</pre> output_file = filepath.replace(\".tif\", \"_pca.tif\") In\u00a0[4]: Copied! <pre>hypercoast.pca(filepath, output_file, n_components=3)\n</pre> hypercoast.pca(filepath, output_file, n_components=3) In\u00a0[5]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"Hybrid\")\nm.add_dataset(\n    filepath, indexes=[6, 4, 2], vmin=0, vmax=2500, layer_name=\"Planet Image\", nodata=0\n)\nm.add_dataset(\n    output_file, indexes=[0, 1, 2], vmin=0, vmax=2500, layer_name=\"PCA Image\", nodata=0\n)\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"Hybrid\") m.add_dataset(     filepath, indexes=[6, 4, 2], vmin=0, vmax=2500, layer_name=\"Planet Image\", nodata=0 ) m.add_dataset(     output_file, indexes=[0, 1, 2], vmin=0, vmax=2500, layer_name=\"PCA Image\", nodata=0 ) m.add(\"spectral\") m Out[5]:"},{"location":"examples/pca/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)\u00b6","text":"<p>PCA is a technique used to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize. In this notebook, we'll perform PCA on a multi-spectral imagery to reduce the dimensionality of the data and visualize the results.</p>"},{"location":"examples/search_data/","title":"Search data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>results = hypercoast.search_datasets(instrument=\"oci\")\n</pre> results = hypercoast.search_datasets(instrument=\"oci\") In\u00a0[\u00a0]: Copied! <pre>datasets = set()\nfor item in results:\n    summary = item.summary()\n    short_name = summary[\"short-name\"]\n    if short_name not in datasets:\n        print(short_name)\n    datasets.add(short_name)\nprint(f\"\\nFound {len(datasets)} unique datasets\")\n</pre> datasets = set() for item in results:     summary = item.summary()     short_name = summary[\"short-name\"]     if short_name not in datasets:         print(short_name)     datasets.add(short_name) print(f\"\\nFound {len(datasets)} unique datasets\") In\u00a0[\u00a0]: Copied! <pre>results = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    count=1,\n)\n</pre> results = hypercoast.search_nasa_data(     short_name=\"PACE_OCI_L2_BGC_NRT\",     count=1, ) <p>We can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the temporal parameter to request a date range and the bounding_box parameter to request granules that intersect with a bounding box. We can even provide a cloud_cover threshold to limit files that have a lower percentage of cloud cover. We do not provide a count, so we'll get all granules that satisfy the constraints.</p> In\u00a0[\u00a0]: Copied! <pre>tspan = (\"2024-04-01\", \"2024-04-16\")\nbbox = (-76.75, 36.97, -75.74, 39.01)\nclouds = (0, 50)\n\nresults, gdf = hypercoast.search_nasa_data(\n    short_name=\"PACE_OCI_L2_BGC_NRT\",\n    temporal=tspan,\n    bounding_box=bbox,\n    cloud_cover=clouds,\n    return_gdf=True,\n)\n</pre> tspan = (\"2024-04-01\", \"2024-04-16\") bbox = (-76.75, 36.97, -75.74, 39.01) clouds = (0, 50)  results, gdf = hypercoast.search_nasa_data(     short_name=\"PACE_OCI_L2_BGC_NRT\",     temporal=tspan,     bounding_box=bbox,     cloud_cover=clouds,     return_gdf=True, ) <p>Display the footprints of the granules that match the search criteria.</p> In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p>Displaying a single result shows a direct download link: try it! The link will download the granule to your local machine, which may or may not be what you want to do. Even if you are running the notebook on a remote host, this download link will open a new browser tab or window and offer to save a file to your local machine. If you are running the notebook locally, this may be of use.</p> In\u00a0[\u00a0]: Copied! <pre>results[0]\n</pre> results[0] <p>We can also download all the results with one command.</p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_nasa_data(results, out_dir=\"data\")\n</pre> hypercoast.download_nasa_data(results, out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_pace(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-05-10\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_pace(     bounding_box=(-83, 25, -81, 28),     temporal=(\"2024-05-10\", \"2024-05-16\"),     count=10,  # use -1 to return all datasets     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p></p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_pace(results[:2], out_dir=\"data\")\n</pre> hypercoast.download_pace(results[:2], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_emit(\n    bounding_box=(-83, 25, -81, 28),\n    temporal=(\"2024-04-01\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_emit(     bounding_box=(-83, 25, -81, 28),     temporal=(\"2024-04-01\", \"2024-05-16\"),     count=10,  # use -1 to return all datasets     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() <p></p> In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_emit(results[:2], out_dir=\"data\")\n</pre> hypercoast.download_emit(results[:2], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>results, gdf = hypercoast.search_ecostress(\n    bbox=(-120.522, 34.4266, -120.2665, 34.5653),\n    temporal=(\"2023-04-01\", \"2023-04-02\"),\n    count=-1,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_ecostress(     bbox=(-120.522, 34.4266, -120.2665, 34.5653),     temporal=(\"2023-04-01\", \"2023-04-02\"),     count=-1,  # use -1 to return all datasets     return_gdf=True, ) In\u00a0[\u00a0]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() In\u00a0[\u00a0]: Copied! <pre>hypercoast.download_ecostress(results[:5], out_dir=\"data\")\n</pre> hypercoast.download_ecostress(results[:5], out_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[27.25, -83.05], zoom=6)\nm.search_pace()\nm\n</pre> m = hypercoast.Map(center=[27.25, -83.05], zoom=6) m.search_pace() m In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_pace(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")\n</pre> # hypercoast.download_pace(m._NASA_DATA_RESULTS[:2], out_dir=\"data\") <p>Search for EMIT data interactively.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[27.25, -83.05], zoom=6)\nm.search_emit()\nm\n</pre> m = hypercoast.Map(center=[27.25, -83.05], zoom=6) m.search_emit() m In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_emit(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")\n</pre> # hypercoast.download_emit(m._NASA_DATA_RESULTS[:2], out_dir=\"data\") <p>Search for ECOSTRESS data interactively.</p> In\u00a0[\u00a0]: Copied! <pre>m = hypercoast.Map(center=[34.5014, -120.4032], zoom=11)\nm.search_ecostress()\nm\n</pre> m = hypercoast.Map(center=[34.5014, -120.4032], zoom=11) m.search_ecostress() m In\u00a0[\u00a0]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() In\u00a0[\u00a0]: Copied! <pre># hypercoast.download_ecostress(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")\n</pre> # hypercoast.download_ecostress(m._NASA_DATA_RESULTS[:2], out_dir=\"data\")"},{"location":"examples/search_data/#search-and-download-nasa-hyperspectral-data-with-hypercoast","title":"Search and download NASA hyperspectral data with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to search and download NASA hyperspectral data (e.g., EMIT, PACE) and ECOSTRESS temperature data with HyperCoast. Part of the source code is adapted from the NASA OB.DAAC tutorial - Access Data from the Ocean Color Instrument (OCI). Credits to the NASA OB.DAAC team.</p>"},{"location":"examples/search_data/#import-library","title":"Import library\u00b6","text":""},{"location":"examples/search_data/#login-to-earthdata","title":"Login to Earthdata\u00b6","text":"<p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov.</p>"},{"location":"examples/search_data/#search-for-datasets","title":"Search for datasets\u00b6","text":"<p>Collections on NASA Earthdata are discovered with the search_datasets function, which accepts an instrument filter as an easy way to get started. Each of the items in the list of collections returned has a \"short-name\".</p>"},{"location":"examples/search_data/#search-for-data-by-short-name","title":"Search for data by short name\u00b6","text":"<p>Next, we use the <code>search_nasa_data</code> function to find granules within a collection. Let's use the <code>short_name</code> for the PACE/OCI Level-2 data product for bio-optical and biogeochemical properties.</p>"},{"location":"examples/search_data/#search-for-pace-data","title":"Search for PACE data\u00b6","text":""},{"location":"examples/search_data/#download-pace-data","title":"Download PACE data\u00b6","text":"<p>Download the first 2 files</p>"},{"location":"examples/search_data/#search-for-emit-data","title":"Search for EMIT data\u00b6","text":""},{"location":"examples/search_data/#download-emit-data","title":"Download EMIT data\u00b6","text":"<p>Download the first 2 files</p>"},{"location":"examples/search_data/#download-ecostress-data","title":"Download ECOSTRESS data\u00b6","text":""},{"location":"examples/search_data/#interactive-search","title":"Interactive search\u00b6","text":"<p>Search for PACE data interactively.</p>"},{"location":"examples/temperature/","title":"Temperature","text":"In\u00a0[\u00a0]: Copied! <pre>import hypercoast\n</pre> import hypercoast In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/ERA5_temperature_2023.nc\"\nfilepath = \"data/ERA5_temperature_2023.nc\"\nhypercoast.download_file(url, filepath)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/ERA5_temperature_2023.nc\" filepath = \"data/ERA5_temperature_2023.nc\" hypercoast.download_file(url, filepath) In\u00a0[\u00a0]: Copied! <pre>dataset = hypercoast.open_dataset(filepath)\ndataset\n</pre> dataset = hypercoast.open_dataset(filepath) dataset In\u00a0[\u00a0]: Copied! <pre>camera_position = [(-479.09, -82.89, -444.45), (89.5, 179.5, 16.5), (0.58, 0.14, -0.80)]\n</pre> camera_position = [(-479.09, -82.89, -444.45), (89.5, 179.5, 16.5), (0.58, 0.14, -0.80)] In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    dataset,\n    variable=\"temperature_2m\",\n    clim=(270, 310),\n    title=\"Temperature\",\n    cmap=\"coolwarm\",\n    widget=\"plane\",\n    invert=False,\n    grid_spacing=(1, 1, 3),\n)\np.camera_position = camera_position\np.show()\n</pre> p = hypercoast.image_cube(     dataset,     variable=\"temperature_2m\",     clim=(270, 310),     title=\"Temperature\",     cmap=\"coolwarm\",     widget=\"plane\",     invert=False,     grid_spacing=(1, 1, 3), ) p.camera_position = camera_position p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    dataset,\n    variable=\"temperature_2m\",\n    clim=(270, 310),\n    title=\"Temperature\",\n    cmap=\"coolwarm\",\n    widget=\"slice\",\n    grid_spacing=(1, 1, 3),\n)\np.camera_position = camera_position\np.show()\n</pre> p = hypercoast.image_cube(     dataset,     variable=\"temperature_2m\",     clim=(270, 310),     title=\"Temperature\",     cmap=\"coolwarm\",     widget=\"slice\",     grid_spacing=(1, 1, 3), ) p.camera_position = camera_position p.show() In\u00a0[\u00a0]: Copied! <pre>p = hypercoast.image_cube(\n    dataset,\n    variable=\"temperature_2m\",\n    clim=(270, 310),\n    title=\"Temperature\",\n    cmap=\"coolwarm\",\n    widget=\"threshold\",\n)\np.camera_position = camera_position\np.show()\n</pre> p = hypercoast.image_cube(     dataset,     variable=\"temperature_2m\",     clim=(270, 310),     title=\"Temperature\",     cmap=\"coolwarm\",     widget=\"threshold\", ) p.camera_position = camera_position p.show()"},{"location":"examples/temperature/#visualizing-era5-temperature-data-interactively-with-hypercoast","title":"Visualizing ERA5 temperature data interactively with HyperCoast\u00b6","text":"<p>This notebook demonstrates how to visualize ERA5 temperature data interactively with HyperCoast.</p>"},{"location":"workshops/emit/","title":"Emit","text":"In\u00a0[1]: Copied! <pre># %pip install \"hypercoast[extra]\"\n</pre> # %pip install \"hypercoast[extra]\" <p>Import library.</p> In\u00a0[2]: Copied! <pre>import hypercoast\n</pre> import hypercoast <p>To download and access the data, you will need to create an Earthdata login. You can register for an account at urs.earthdata.nasa.gov. Once you have an account, run the following cell and enter your NASA Earthdata login credentials.</p> In\u00a0[3]: Copied! <pre>hypercoast.nasa_earth_login()\n</pre> hypercoast.nasa_earth_login() In\u00a0[4]: Copied! <pre>results, gdf = hypercoast.search_emit(\n    bbox=(-83, 25, -81, 28),\n    temporal=(\"2024-04-01\", \"2024-05-16\"),\n    count=10,  # use -1 to return all datasets\n    return_gdf=True,\n)\n</pre> results, gdf = hypercoast.search_emit(     bbox=(-83, 25, -81, 28),     temporal=(\"2024-04-01\", \"2024-05-16\"),     count=10,  # use -1 to return all datasets     return_gdf=True, ) <p>Plot the footprints of the returned datasets on a map.</p> In\u00a0[5]: Copied! <pre>gdf.explore()\n</pre> gdf.explore() Out[5]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook <p>Uncomment the following cell to download the first dataset from the search results. Note that the download may take some time.</p> In\u00a0[6]: Copied! <pre># hypercoast.download_emit(results[:1], out_dir=\"data\")\n</pre> # hypercoast.download_emit(results[:1], out_dir=\"data\") <p>Search for EMIT data interactively. Specify pan and zoom to the area of interest. Specify the time range of interest from the search dialog, then click on the Search button.</p> In\u00a0[7]: Copied! <pre>m = hypercoast.Map(center=[30.0262, -90.1345], zoom=8)\nm.search_emit()\nm\n</pre> m = hypercoast.Map(center=[30.0262, -90.1345], zoom=8) m.search_emit() m Out[7]: <p>Uncomment the following cell to display the GeoDataFrame of the search results.</p> In\u00a0[8]: Copied! <pre># m._NASA_DATA_GDF.head()\n</pre> # m._NASA_DATA_GDF.head() <p>Similarly, you can download the first dataset from the search results by uncommenting the following cell.</p> In\u00a0[9]: Copied! <pre># hypercoast.download_emit(results[:1], out_dir=\"data\")\n</pre> # hypercoast.download_emit(results[:1], out_dir=\"data\") In\u00a0[10]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nfilepath = \"../examples/data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\"\nhypercoast.download_file(url, filepath, quiet=True)\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/hypercoast/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" filepath = \"../examples/data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc\" hypercoast.download_file(url, filepath, quiet=True) <pre>../examples/data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc already exists. Skip downloading. Set overwrite=True to overwrite.\n</pre> Out[10]: <pre>'/home/runner/work/HyperCoast/HyperCoast/docs/examples/data/EMIT_L2A_RFL_001_20240404T161230_2409511_009.nc'</pre> In\u00a0[11]: Copied! <pre>dataset = hypercoast.read_emit(filepath)\n# dataset\n</pre> dataset = hypercoast.read_emit(filepath) # dataset <p></p> In\u00a0[12]: Copied! <pre>m = hypercoast.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\")\nm.add(\"spectral\")\nm\n</pre> m = hypercoast.Map() m.add_basemap(\"SATELLITE\") m.add_emit(dataset, wavelengths=[1000, 600, 500], vmin=0, vmax=0.3, layer_name=\"EMIT\") m.add(\"spectral\") m Out[12]: <p></p> In\u00a0[13]: Copied! <pre>ds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451))\n</pre> ds = dataset.sel(longitude=slice(-90.1482, -89.7321), latitude=slice(30.0225, 29.7451)) <p>Visualize the EMIT data in 3D with an RGB image overlaid on top of the 3D plot.</p> In\u00a0[14]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.4),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n)\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.4),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\", ) <p>Uncomment the following cell to create an image cube. Note that this function does not work in the Google Colab environment.</p> In\u00a0[15]: Copied! <pre># p.show()\n</pre> # p.show() <p></p> In\u00a0[16]: Copied! <pre>ds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93))\n</pre> ds = dataset.sel(longitude=slice(-90.05, -89.99), latitude=slice(30.00, 29.93)) <p>Drag the plane up and down to slice the data in 3D.</p> In\u00a0[17]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"plane\",\n)\np.add_text(\"Band slicing\", position=\"upper_right\", font_size=14)\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\",     widget=\"plane\", ) p.add_text(\"Band slicing\", position=\"upper_right\", font_size=14) Out[17]: <pre>&lt;CornerAnnotation(0x55f442ddce40) at 0x7f2cfcfd9060&gt;</pre> <p>Uncomment the following cell to display the interactive slicing widget. Note that this function does not work in the Google Colab environment.</p> In\u00a0[18]: Copied! <pre># p.show()\n</pre> # p.show() <p></p> In\u00a0[19]: Copied! <pre>p = hypercoast.image_cube(\n    ds,\n    variable=\"reflectance\",\n    cmap=\"jet\",\n    clim=(0, 0.5),\n    rgb_wavelengths=[1000, 700, 500],\n    rgb_gamma=2,\n    title=\"EMIT Reflectance\",\n    widget=\"threshold\",\n)\np.add_text(\"Thresholding\", position=\"upper_right\", font_size=14)\n</pre> p = hypercoast.image_cube(     ds,     variable=\"reflectance\",     cmap=\"jet\",     clim=(0, 0.5),     rgb_wavelengths=[1000, 700, 500],     rgb_gamma=2,     title=\"EMIT Reflectance\",     widget=\"threshold\", ) p.add_text(\"Thresholding\", position=\"upper_right\", font_size=14) Out[19]: <pre>&lt;CornerAnnotation(0x55f442bed290) at 0x7f2cc549d240&gt;</pre> <p>Uncomment the following cell to display the thresholded data. Note that this function does not work in the Google Colab environment.</p> In\u00a0[20]: Copied! <pre># p.show()\n</pre> # p.show() <p></p>"},{"location":"workshops/emit/#working-with-nasa-emit-data-in-hypercoast","title":"Working with NASA EMIT data in HyperCoast\u00b6","text":"<p>This notebook demonstrates how to work with NASA Earth Surface Mineral Dust Source Investigation (EMIT) data in HyperCoast.</p>"},{"location":"workshops/emit/#environment-setup","title":"Environment setup\u00b6","text":"<p>Uncomment and run the following cell to install the required packages.</p>"},{"location":"workshops/emit/#search-for-emit-data","title":"Search for EMIT data\u00b6","text":"<p>Search for EMIT data programmatically. Specify the bounding box and time range of interest. Set <code>count=-1</code> to return all results or set <code>count=10</code> to return the first 10 results.</p>"},{"location":"workshops/emit/#download-a-sample-emit-dataset","title":"Download a sample EMIT dataset\u00b6","text":"<p>Let's download a sample EMIT dataset for the demonstration.</p>"},{"location":"workshops/emit/#read-emit-data","title":"Read EMIT data\u00b6","text":"<p>Read the downloaded EMIT data and process it as an <code>xarray.Dataset</code>. Note that the dataset has 285 bands.</p>"},{"location":"workshops/emit/#visualize-emit-data","title":"Visualize EMIT data\u00b6","text":"<p>Visualize the EMIT data on an interactive map. You can change the band combination and extract spectral profiles interactively. You can also export the spectral profiles as a CSV file.</p>"},{"location":"workshops/emit/#create-an-image-cube","title":"Create an image cube\u00b6","text":"<p>First, select a subset of the data to avoid nodata areas.</p>"},{"location":"workshops/emit/#interactive-slicing","title":"Interactive slicing\u00b6","text":"<p>First, select a subset of the data for demonstration purposes.</p>"},{"location":"workshops/emit/#interactive-thresholding","title":"Interactive thresholding\u00b6","text":"<p>Drag the threshold slider to threshold the data in 3D.</p>"}]}